2023-03-25 20:54:09,050 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 20:54:09,050 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 20:54:10,572 :: INFO :: torch.Size([76085, 384])
2023-03-25 20:54:16,438 :: INFO :: ----- frontend -----
2023-03-25 20:54:16,458 :: INFO :: Environment 0
2023-03-25 21:00:46,702 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 21:00:46,702 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 21:00:47,896 :: INFO :: torch.Size([76085, 384])
2023-03-25 21:00:54,040 :: INFO :: ----- frontend -----
2023-03-25 21:00:54,053 :: INFO :: Environment 0
2023-03-25 21:03:21,232 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 21:03:21,233 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 21:03:22,482 :: INFO :: torch.Size([76085, 384])
2023-03-25 21:03:28,184 :: INFO :: ----- frontend -----
2023-03-25 21:03:28,207 :: INFO :: Environment 0
2023-03-25 21:07:23,231 :: INFO :: Epoch 5: loss tensor(78780.2812), U.norm 10.710773468017578, V.norm 14.95574951171875, MLP.norm 26.580310821533203
2023-03-25 21:11:36,910 :: INFO :: Epoch 10: loss tensor(74338.6562), U.norm 12.743417739868164, V.norm 14.875079154968262, MLP.norm 36.76354217529297
2023-03-25 21:15:47,489 :: INFO :: Epoch 15: loss tensor(70493.6875), U.norm 14.630451202392578, V.norm 15.798015594482422, MLP.norm 43.9197883605957
2023-03-25 21:19:36,280 :: INFO :: Epoch 20: loss tensor(66971.8828), U.norm 16.37555503845215, V.norm 17.27663230895996, MLP.norm 49.04502868652344
2023-03-25 21:19:37,248 :: INFO :: Environment 1
2023-03-25 21:23:28,571 :: INFO :: Epoch 5: loss tensor(78651.0078), U.norm 10.72774887084961, V.norm 14.990690231323242, MLP.norm 26.075809478759766
2023-03-25 21:27:12,210 :: INFO :: Epoch 10: loss tensor(74365.6719), U.norm 12.745518684387207, V.norm 14.886467933654785, MLP.norm 36.00558853149414
2023-03-25 21:30:57,238 :: INFO :: Epoch 15: loss tensor(70649.0625), U.norm 14.618178367614746, V.norm 15.746353149414062, MLP.norm 43.24523162841797
2023-03-25 21:34:48,076 :: INFO :: Epoch 20: loss tensor(67105.2656), U.norm 16.36060333251953, V.norm 17.128013610839844, MLP.norm 48.6616325378418
2023-03-25 21:34:48,751 :: INFO :: Environment 2
2023-03-25 21:38:53,005 :: INFO :: Epoch 5: loss tensor(78543.0859), U.norm 10.695173263549805, V.norm 14.975322723388672, MLP.norm 26.169532775878906
2023-03-25 21:42:37,072 :: INFO :: Epoch 10: loss tensor(74356.5938), U.norm 12.705785751342773, V.norm 14.863210678100586, MLP.norm 35.66987609863281
2023-03-25 21:46:27,194 :: INFO :: Epoch 15: loss tensor(70662.0547), U.norm 14.571556091308594, V.norm 15.71623706817627, MLP.norm 42.91804504394531
2023-03-25 21:50:14,984 :: INFO :: Epoch 20: loss tensor(67158.0469), U.norm 16.31003761291504, V.norm 17.092653274536133, MLP.norm 48.469970703125
2023-03-25 21:50:15,658 :: INFO :: Environment 3
2023-03-25 21:54:02,465 :: INFO :: Epoch 5: loss tensor(79166.6484), U.norm 10.651463508605957, V.norm 14.968517303466797, MLP.norm 25.521623611450195
2023-03-25 21:57:49,789 :: INFO :: Epoch 10: loss tensor(75044.9453), U.norm 12.65029239654541, V.norm 14.902669906616211, MLP.norm 35.48780059814453
2023-03-25 22:01:29,829 :: INFO :: Epoch 15: loss tensor(71449.8750), U.norm 14.526192665100098, V.norm 15.850362777709961, MLP.norm 42.67310333251953
2023-03-25 22:05:16,975 :: INFO :: Epoch 20: loss tensor(67950.5547), U.norm 16.275951385498047, V.norm 17.369108200073242, MLP.norm 48.1007080078125
2023-03-25 22:05:17,629 :: INFO :: Environment 4
2023-03-25 22:09:10,438 :: INFO :: Epoch 5: loss tensor(79249.6797), U.norm 10.66997241973877, V.norm 14.962930679321289, MLP.norm 26.017513275146484
2023-03-25 22:12:44,111 :: INFO :: Epoch 10: loss tensor(74988.2969), U.norm 12.673913955688477, V.norm 14.839290618896484, MLP.norm 35.81971740722656
2023-03-25 22:16:17,937 :: INFO :: Epoch 15: loss tensor(71230.5469), U.norm 14.548011779785156, V.norm 15.670289993286133, MLP.norm 43.1412239074707
2023-03-25 22:19:52,846 :: INFO :: Epoch 20: loss tensor(67631.9609), U.norm 16.28986358642578, V.norm 17.013879776000977, MLP.norm 48.53339767456055
2023-03-25 22:19:53,463 :: INFO :: Environment 5
2023-03-25 22:23:27,793 :: INFO :: Epoch 5: loss tensor(78725.2969), U.norm 10.633424758911133, V.norm 14.982197761535645, MLP.norm 25.779348373413086
2023-03-25 22:26:57,871 :: INFO :: Epoch 10: loss tensor(74560.2734), U.norm 12.61066722869873, V.norm 14.901220321655273, MLP.norm 35.46091079711914
2023-03-25 22:30:29,361 :: INFO :: Epoch 15: loss tensor(70831.9297), U.norm 14.483649253845215, V.norm 15.831314086914062, MLP.norm 42.746822357177734
2023-03-25 22:34:01,547 :: INFO :: Epoch 20: loss tensor(67252.8672), U.norm 16.23482894897461, V.norm 17.328262329101562, MLP.norm 48.07020950317383
2023-03-25 22:34:02,184 :: INFO :: Environment 6
2023-03-25 22:37:34,900 :: INFO :: Epoch 5: loss tensor(78920.0156), U.norm 10.679916381835938, V.norm 14.96037483215332, MLP.norm 25.992536544799805
2023-03-25 22:41:02,768 :: INFO :: Epoch 10: loss tensor(74568.3672), U.norm 12.694758415222168, V.norm 14.82158374786377, MLP.norm 35.8137321472168
2023-03-25 22:44:31,951 :: INFO :: Epoch 15: loss tensor(70746.7969), U.norm 14.566908836364746, V.norm 15.630630493164062, MLP.norm 43.1983757019043
2023-03-25 22:48:01,263 :: INFO :: Epoch 20: loss tensor(67151.4375), U.norm 16.30751609802246, V.norm 16.94659423828125, MLP.norm 48.50672149658203
2023-03-25 22:48:01,872 :: INFO :: Environment 7
2023-03-25 22:51:36,664 :: INFO :: Epoch 5: loss tensor(78673.5312), U.norm 10.650546073913574, V.norm 14.97412395477295, MLP.norm 25.52890396118164
2023-03-25 22:55:09,238 :: INFO :: Epoch 10: loss tensor(74481.5703), U.norm 12.640934944152832, V.norm 14.883415222167969, MLP.norm 35.21791076660156
2023-03-25 23:13:38,858 :: INFO :: Epoch 15: loss tensor(70770.7734), U.norm 14.515196800231934, V.norm 15.787445068359375, MLP.norm 42.6607780456543
2023-03-25 23:17:15,088 :: INFO :: Epoch 20: loss tensor(67187.3438), U.norm 16.26540756225586, V.norm 17.239900588989258, MLP.norm 48.15561294555664
2023-03-25 23:17:15,714 :: INFO :: Environment 8
2023-03-25 23:53:09,045 :: INFO :: Epoch 5: loss tensor(78978.3516), U.norm 10.697601318359375, V.norm 14.948690414428711, MLP.norm 26.770505905151367
2023-03-25 23:58:22,739 :: INFO :: Epoch 10: loss tensor(74511.5234), U.norm 12.708267211914062, V.norm 14.796112060546875, MLP.norm 36.64705276489258
2023-04-11 13:13:51,811 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 13:13:51,811 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 13:13:54,296 :: INFO :: torch.Size([76085, 384])
2023-04-11 13:14:10,733 :: INFO :: ----- frontend -----
2023-04-11 13:14:10,764 :: INFO :: Environment 0
2023-04-11 13:14:34,202 :: INFO :: Epoch 5: loss tensor(63971.6562, device='cuda:0'), U.norm 11.164484024047852, V.norm 14.704377174377441, MLP.norm 44.06386947631836
2023-04-11 13:14:41,780 :: INFO :: Epoch 10: loss tensor(58937.0820, device='cuda:0'), U.norm 12.902695655822754, V.norm 14.22950267791748, MLP.norm 53.26649856567383
2023-04-11 13:14:49,796 :: INFO :: Epoch 15: loss tensor(56739.9883, device='cuda:0'), U.norm 14.388713836669922, V.norm 14.677123069763184, MLP.norm 56.131980895996094
2023-04-11 13:14:56,811 :: INFO :: Epoch 20: loss tensor(55253.0508, device='cuda:0'), U.norm 15.768144607543945, V.norm 15.564013481140137, MLP.norm 57.704986572265625
2023-04-11 13:14:56,811 :: INFO :: Environment 1
2023-04-11 13:15:19,952 :: INFO :: Epoch 5: loss tensor(63968.2500, device='cuda:0'), U.norm 11.175091743469238, V.norm 14.74273681640625, MLP.norm 43.7144889831543
2023-04-11 13:15:27,328 :: INFO :: Epoch 10: loss tensor(58967.0273, device='cuda:0'), U.norm 12.91086196899414, V.norm 14.261296272277832, MLP.norm 52.98832321166992
2023-04-11 13:15:34,236 :: INFO :: Epoch 15: loss tensor(56801.2383, device='cuda:0'), U.norm 14.39350700378418, V.norm 14.683852195739746, MLP.norm 56.008304595947266
2023-04-11 13:15:41,686 :: INFO :: Epoch 20: loss tensor(55296.8398, device='cuda:0'), U.norm 15.767277717590332, V.norm 15.53143310546875, MLP.norm 57.68355941772461
2023-04-11 13:15:41,686 :: INFO :: Environment 2
2023-04-11 13:16:03,499 :: INFO :: Epoch 5: loss tensor(63700.6367, device='cuda:0'), U.norm 11.154010772705078, V.norm 14.728994369506836, MLP.norm 44.57552719116211
2023-04-11 13:16:11,124 :: INFO :: Epoch 10: loss tensor(58724.2148, device='cuda:0'), U.norm 12.894691467285156, V.norm 14.234764099121094, MLP.norm 53.52023696899414
2023-04-11 13:16:18,296 :: INFO :: Epoch 15: loss tensor(56673.9922, device='cuda:0'), U.norm 14.372533798217773, V.norm 14.658438682556152, MLP.norm 55.914241790771484
2023-04-11 13:16:24,608 :: INFO :: Epoch 20: loss tensor(55251.3203, device='cuda:0'), U.norm 15.747417449951172, V.norm 15.518694877624512, MLP.norm 57.440406799316406
2023-04-11 13:16:24,608 :: INFO :: Environment 3
2023-04-11 13:16:45,094 :: INFO :: Epoch 5: loss tensor(64509.9219, device='cuda:0'), U.norm 11.136981010437012, V.norm 14.716168403625488, MLP.norm 43.844810485839844
2023-04-11 13:16:51,640 :: INFO :: Epoch 10: loss tensor(59561.8828, device='cuda:0'), U.norm 12.872424125671387, V.norm 14.246015548706055, MLP.norm 52.81698989868164
2023-04-11 13:16:59,296 :: INFO :: Epoch 15: loss tensor(57428.4492, device='cuda:0'), U.norm 14.357925415039062, V.norm 14.691025733947754, MLP.norm 55.62257385253906
2023-04-11 13:17:06,546 :: INFO :: Epoch 20: loss tensor(55948.4688, device='cuda:0'), U.norm 15.737154960632324, V.norm 15.570406913757324, MLP.norm 57.32096862792969
2023-04-11 13:17:06,546 :: INFO :: Environment 4
2023-04-11 13:17:26,124 :: INFO :: Epoch 5: loss tensor(64294.4062, device='cuda:0'), U.norm 11.140814781188965, V.norm 14.716755867004395, MLP.norm 44.14636993408203
2023-04-11 13:17:32,733 :: INFO :: Epoch 10: loss tensor(59296.6992, device='cuda:0'), U.norm 12.881582260131836, V.norm 14.225922584533691, MLP.norm 53.078330993652344
2023-04-11 13:17:39,827 :: INFO :: Epoch 15: loss tensor(57194.2617, device='cuda:0'), U.norm 14.366961479187012, V.norm 14.644286155700684, MLP.norm 55.77897262573242
2023-04-11 13:17:46,686 :: INFO :: Epoch 20: loss tensor(55712.5625, device='cuda:0'), U.norm 15.74659538269043, V.norm 15.49077033996582, MLP.norm 57.39009094238281
2023-04-11 13:17:46,702 :: INFO :: Environment 5
2023-04-11 13:18:06,202 :: INFO :: Epoch 5: loss tensor(63683.0273, device='cuda:0'), U.norm 11.144392013549805, V.norm 14.72990608215332, MLP.norm 44.3858757019043
2023-04-11 13:18:13,030 :: INFO :: Epoch 10: loss tensor(58734.6367, device='cuda:0'), U.norm 12.883724212646484, V.norm 14.237281799316406, MLP.norm 53.27328109741211
2023-04-11 13:18:20,342 :: INFO :: Epoch 15: loss tensor(56662.8359, device='cuda:0'), U.norm 14.362588882446289, V.norm 14.6655912399292, MLP.norm 55.86144256591797
2023-04-11 13:18:27,797 :: INFO :: Epoch 20: loss tensor(55202.0156, device='cuda:0'), U.norm 15.737030982971191, V.norm 15.535058975219727, MLP.norm 57.47968292236328
2023-04-11 13:18:27,797 :: INFO :: Environment 6
2023-04-11 13:18:48,092 :: INFO :: Epoch 5: loss tensor(64087.6680, device='cuda:0'), U.norm 11.139106750488281, V.norm 14.712862014770508, MLP.norm 43.932254791259766
2023-04-11 13:18:55,202 :: INFO :: Epoch 10: loss tensor(59187.4648, device='cuda:0'), U.norm 12.867537498474121, V.norm 14.211603164672852, MLP.norm 52.865760803222656
2023-04-11 13:19:02,202 :: INFO :: Epoch 15: loss tensor(57041.7422, device='cuda:0'), U.norm 14.345762252807617, V.norm 14.6204195022583, MLP.norm 55.74172592163086
2023-04-11 13:19:09,374 :: INFO :: Epoch 20: loss tensor(55558.1875, device='cuda:0'), U.norm 15.71817398071289, V.norm 15.455787658691406, MLP.norm 57.38603591918945
2023-04-11 13:19:09,374 :: INFO :: Environment 7
2023-04-11 13:19:29,733 :: INFO :: Epoch 5: loss tensor(63755.4062, device='cuda:0'), U.norm 11.141400337219238, V.norm 14.721487998962402, MLP.norm 44.07299041748047
2023-04-11 13:19:36,108 :: INFO :: Epoch 10: loss tensor(58718.5742, device='cuda:0'), U.norm 12.87523365020752, V.norm 14.230205535888672, MLP.norm 53.1664924621582
2023-04-11 13:19:42,829 :: INFO :: Epoch 15: loss tensor(56617.5742, device='cuda:0'), U.norm 14.351407051086426, V.norm 14.657505989074707, MLP.norm 55.886566162109375
2023-04-11 13:19:49,780 :: INFO :: Epoch 20: loss tensor(55150.7695, device='cuda:0'), U.norm 15.722633361816406, V.norm 15.524603843688965, MLP.norm 57.51047134399414
2023-04-11 13:19:49,796 :: INFO :: Environment 8
2023-04-11 13:20:10,686 :: INFO :: Epoch 5: loss tensor(64221.1836, device='cuda:0'), U.norm 11.144296646118164, V.norm 14.70224666595459, MLP.norm 43.927024841308594
2023-04-11 13:20:18,358 :: INFO :: Epoch 10: loss tensor(59192.5703, device='cuda:0'), U.norm 12.877477645874023, V.norm 14.194478034973145, MLP.norm 53.00509262084961
2023-04-11 13:20:25,514 :: INFO :: Epoch 15: loss tensor(57082.4062, device='cuda:0'), U.norm 14.352745056152344, V.norm 14.59787368774414, MLP.norm 55.65082550048828
2023-04-11 13:20:32,655 :: INFO :: Epoch 20: loss tensor(55628.6758, device='cuda:0'), U.norm 15.72201919555664, V.norm 15.432330131530762, MLP.norm 57.15964889526367
2023-04-11 13:20:32,671 :: INFO :: Environment 9
2023-04-11 13:20:51,156 :: INFO :: Epoch 5: loss tensor(64255.2734, device='cuda:0'), U.norm 11.1485595703125, V.norm 14.731783866882324, MLP.norm 44.178932189941406
2023-04-11 13:20:57,374 :: INFO :: Epoch 10: loss tensor(59261.9180, device='cuda:0'), U.norm 12.889558792114258, V.norm 14.261752128601074, MLP.norm 53.191036224365234
2023-04-11 13:21:04,765 :: INFO :: Epoch 15: loss tensor(57123.6445, device='cuda:0'), U.norm 14.378288269042969, V.norm 14.710026741027832, MLP.norm 55.98915100097656
2023-04-11 13:21:11,967 :: INFO :: Epoch 20: loss tensor(55625.3516, device='cuda:0'), U.norm 15.76142406463623, V.norm 15.593192100524902, MLP.norm 57.65121841430664
2023-04-11 14:11:16,749 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:11:16,749 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:11:19,233 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:11:57,686 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:11:57,686 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:12:00,155 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:12:16,311 :: INFO :: ----- frontend -----
2023-04-11 14:12:16,327 :: INFO :: Environment 0
2023-04-11 14:12:39,764 :: INFO :: Epoch 5: loss tensor(63971.6562, device='cuda:0'), U.norm 11.164484024047852, V.norm 14.704377174377441, MLP.norm 44.06386947631836
2023-04-11 14:12:47,405 :: INFO :: Epoch 10: loss tensor(58937.0820, device='cuda:0'), U.norm 12.902695655822754, V.norm 14.22950267791748, MLP.norm 53.26649856567383
2023-04-11 14:12:53,843 :: INFO :: Epoch 15: loss tensor(56739.9883, device='cuda:0'), U.norm 14.388713836669922, V.norm 14.677123069763184, MLP.norm 56.131980895996094
2023-04-11 14:13:01,467 :: INFO :: Epoch 20: loss tensor(55253.0508, device='cuda:0'), U.norm 15.768144607543945, V.norm 15.564013481140137, MLP.norm 57.704986572265625
2023-04-11 14:13:01,467 :: INFO :: Environment 1
2023-04-11 14:13:23,827 :: INFO :: Epoch 5: loss tensor(63968.2500, device='cuda:0'), U.norm 11.175091743469238, V.norm 14.74273681640625, MLP.norm 43.7144889831543
2023-04-11 14:13:31,342 :: INFO :: Epoch 10: loss tensor(58967.0273, device='cuda:0'), U.norm 12.91086196899414, V.norm 14.261296272277832, MLP.norm 52.98832321166992
2023-04-11 14:13:38,405 :: INFO :: Epoch 15: loss tensor(56801.2383, device='cuda:0'), U.norm 14.39350700378418, V.norm 14.683852195739746, MLP.norm 56.008304595947266
2023-04-11 14:13:45,890 :: INFO :: Epoch 20: loss tensor(55296.8398, device='cuda:0'), U.norm 15.767277717590332, V.norm 15.53143310546875, MLP.norm 57.68355941772461
2023-04-11 14:13:45,905 :: INFO :: Environment 2
2023-04-11 14:14:06,546 :: INFO :: Epoch 5: loss tensor(63700.6367, device='cuda:0'), U.norm 11.154010772705078, V.norm 14.728994369506836, MLP.norm 44.57552719116211
2023-04-11 14:14:13,968 :: INFO :: Epoch 10: loss tensor(58724.2148, device='cuda:0'), U.norm 12.894691467285156, V.norm 14.234764099121094, MLP.norm 53.52023696899414
2023-04-11 14:14:21,171 :: INFO :: Epoch 15: loss tensor(56673.9922, device='cuda:0'), U.norm 14.372533798217773, V.norm 14.658438682556152, MLP.norm 55.914241790771484
2023-04-11 14:14:28,577 :: INFO :: Epoch 20: loss tensor(55251.3203, device='cuda:0'), U.norm 15.747417449951172, V.norm 15.518694877624512, MLP.norm 57.440406799316406
2023-04-11 14:14:28,577 :: INFO :: Environment 3
2023-04-11 14:14:50,389 :: INFO :: Epoch 5: loss tensor(64509.9219, device='cuda:0'), U.norm 11.136981010437012, V.norm 14.716168403625488, MLP.norm 43.844810485839844
2023-04-11 14:14:57,374 :: INFO :: Epoch 10: loss tensor(59561.8828, device='cuda:0'), U.norm 12.872424125671387, V.norm 14.246015548706055, MLP.norm 52.81698989868164
2023-04-11 14:15:04,561 :: INFO :: Epoch 15: loss tensor(57428.4492, device='cuda:0'), U.norm 14.357925415039062, V.norm 14.691025733947754, MLP.norm 55.62257385253906
2023-04-11 14:15:11,530 :: INFO :: Epoch 20: loss tensor(55948.4688, device='cuda:0'), U.norm 15.737154960632324, V.norm 15.570406913757324, MLP.norm 57.32096862792969
2023-04-11 14:15:11,546 :: INFO :: Environment 4
2023-04-11 14:15:31,623 :: INFO :: Epoch 5: loss tensor(64294.4062, device='cuda:0'), U.norm 11.140814781188965, V.norm 14.716755867004395, MLP.norm 44.14636993408203
2023-04-11 14:15:38,655 :: INFO :: Epoch 10: loss tensor(59296.6992, device='cuda:0'), U.norm 12.881582260131836, V.norm 14.225922584533691, MLP.norm 53.078330993652344
2023-04-11 14:15:46,233 :: INFO :: Epoch 15: loss tensor(57194.2617, device='cuda:0'), U.norm 14.366961479187012, V.norm 14.644286155700684, MLP.norm 55.77897262573242
2023-04-11 14:15:52,343 :: INFO :: Epoch 20: loss tensor(55712.5625, device='cuda:0'), U.norm 15.74659538269043, V.norm 15.49077033996582, MLP.norm 57.39009094238281
2023-04-11 14:15:52,358 :: INFO :: Environment 5
2023-04-11 14:16:12,467 :: INFO :: Epoch 5: loss tensor(63683.0273, device='cuda:0'), U.norm 11.144392013549805, V.norm 14.72990608215332, MLP.norm 44.3858757019043
2023-04-11 14:16:18,749 :: INFO :: Epoch 10: loss tensor(58734.6367, device='cuda:0'), U.norm 12.883724212646484, V.norm 14.237281799316406, MLP.norm 53.27328109741211
2023-04-11 14:16:25,124 :: INFO :: Epoch 15: loss tensor(56662.8359, device='cuda:0'), U.norm 14.362588882446289, V.norm 14.6655912399292, MLP.norm 55.86144256591797
2023-04-11 14:16:31,874 :: INFO :: Epoch 20: loss tensor(55202.0156, device='cuda:0'), U.norm 15.737030982971191, V.norm 15.535058975219727, MLP.norm 57.47968292236328
2023-04-11 14:16:31,874 :: INFO :: Environment 6
2023-04-11 14:16:52,139 :: INFO :: Epoch 5: loss tensor(64087.6680, device='cuda:0'), U.norm 11.139106750488281, V.norm 14.712862014770508, MLP.norm 43.932254791259766
2023-04-11 14:16:59,624 :: INFO :: Epoch 10: loss tensor(59187.4648, device='cuda:0'), U.norm 12.867537498474121, V.norm 14.211603164672852, MLP.norm 52.865760803222656
2023-04-11 14:17:06,921 :: INFO :: Epoch 15: loss tensor(57041.7422, device='cuda:0'), U.norm 14.345762252807617, V.norm 14.6204195022583, MLP.norm 55.74172592163086
2023-04-11 14:17:13,139 :: INFO :: Epoch 20: loss tensor(55558.1875, device='cuda:0'), U.norm 15.71817398071289, V.norm 15.455787658691406, MLP.norm 57.38603591918945
2023-04-11 14:17:13,155 :: INFO :: Environment 7
2023-04-11 14:17:34,623 :: INFO :: Epoch 5: loss tensor(63755.4062, device='cuda:0'), U.norm 11.141400337219238, V.norm 14.721487998962402, MLP.norm 44.07299041748047
2023-04-11 14:17:41,373 :: INFO :: Epoch 10: loss tensor(58718.5742, device='cuda:0'), U.norm 12.87523365020752, V.norm 14.230205535888672, MLP.norm 53.1664924621582
2023-04-11 14:17:48,405 :: INFO :: Epoch 15: loss tensor(56617.5742, device='cuda:0'), U.norm 14.351407051086426, V.norm 14.657505989074707, MLP.norm 55.886566162109375
2023-04-11 14:17:55,999 :: INFO :: Epoch 20: loss tensor(55150.7695, device='cuda:0'), U.norm 15.722633361816406, V.norm 15.524603843688965, MLP.norm 57.51047134399414
2023-04-11 14:17:55,999 :: INFO :: Environment 8
2023-04-11 14:18:17,171 :: INFO :: Epoch 5: loss tensor(64221.1836, device='cuda:0'), U.norm 11.144296646118164, V.norm 14.70224666595459, MLP.norm 43.927024841308594
2023-04-11 14:18:24,561 :: INFO :: Epoch 10: loss tensor(59192.5703, device='cuda:0'), U.norm 12.877477645874023, V.norm 14.194478034973145, MLP.norm 53.00509262084961
2023-04-11 14:18:31,686 :: INFO :: Epoch 15: loss tensor(57082.4062, device='cuda:0'), U.norm 14.352745056152344, V.norm 14.59787368774414, MLP.norm 55.65082550048828
2023-04-11 14:18:38,342 :: INFO :: Epoch 20: loss tensor(55628.6758, device='cuda:0'), U.norm 15.72201919555664, V.norm 15.432330131530762, MLP.norm 57.15964889526367
2023-04-11 14:18:38,342 :: INFO :: Environment 9
2023-04-11 14:18:58,217 :: INFO :: Epoch 5: loss tensor(64255.2734, device='cuda:0'), U.norm 11.1485595703125, V.norm 14.731783866882324, MLP.norm 44.178932189941406
2023-04-11 14:19:05,046 :: INFO :: Epoch 10: loss tensor(59261.9180, device='cuda:0'), U.norm 12.889558792114258, V.norm 14.261752128601074, MLP.norm 53.191036224365234
2023-04-11 14:19:12,092 :: INFO :: Epoch 15: loss tensor(57123.6445, device='cuda:0'), U.norm 14.378288269042969, V.norm 14.710026741027832, MLP.norm 55.98915100097656
2023-04-11 14:19:19,140 :: INFO :: Epoch 20: loss tensor(55625.3516, device='cuda:0'), U.norm 15.76142406463623, V.norm 15.593192100524902, MLP.norm 57.65121841430664
2023-04-11 14:27:35,561 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:27:35,561 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:27:37,874 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:27:54,280 :: INFO :: ----- frontend -----
2023-04-11 14:27:54,280 :: INFO :: Environment 0
2023-04-11 14:28:10,249 :: INFO :: Epoch 5: loss tensor(823.6648, device='cuda:0'), U.norm 14.646507263183594, V.norm 18.26340675354004, MLP.norm 1.861497402191162
2023-04-11 14:28:10,358 :: INFO :: Epoch 10: loss tensor(811.7224, device='cuda:0'), U.norm 12.039924621582031, V.norm 17.74479866027832, MLP.norm 2.613703727722168
2023-04-11 14:28:10,468 :: INFO :: Epoch 15: loss tensor(794.7003, device='cuda:0'), U.norm 10.375273704528809, V.norm 17.512922286987305, MLP.norm 3.607811212539673
2023-04-11 14:28:10,592 :: INFO :: Epoch 20: loss tensor(773.3453, device='cuda:0'), U.norm 9.180266380310059, V.norm 17.364517211914062, MLP.norm 4.65383768081665
2023-04-11 14:28:10,592 :: INFO :: Environment 1
2023-04-11 14:28:24,092 :: INFO :: Epoch 5: loss tensor(825.8454, device='cuda:0'), U.norm 14.649739265441895, V.norm 18.262313842773438, MLP.norm 1.8743987083435059
2023-04-11 14:28:24,202 :: INFO :: Epoch 10: loss tensor(813.2893, device='cuda:0'), U.norm 12.046407699584961, V.norm 17.739742279052734, MLP.norm 2.6447043418884277
2023-04-11 14:28:24,296 :: INFO :: Epoch 15: loss tensor(795.6776, device='cuda:0'), U.norm 10.385376930236816, V.norm 17.503257751464844, MLP.norm 3.6386396884918213
2023-04-11 14:28:24,389 :: INFO :: Epoch 20: loss tensor(774.0076, device='cuda:0'), U.norm 9.1942720413208, V.norm 17.351211547851562, MLP.norm 4.6785078048706055
2023-04-11 14:28:24,389 :: INFO :: Environment 2
2023-04-11 14:28:37,624 :: INFO :: Epoch 5: loss tensor(824.8957, device='cuda:0'), U.norm 14.647931098937988, V.norm 18.254480361938477, MLP.norm 1.851955533027649
2023-04-11 14:28:37,717 :: INFO :: Epoch 10: loss tensor(812.8655, device='cuda:0'), U.norm 12.042749404907227, V.norm 17.731487274169922, MLP.norm 2.6065449714660645
2023-04-11 14:28:37,811 :: INFO :: Epoch 15: loss tensor(794.7304, device='cuda:0'), U.norm 10.379347801208496, V.norm 17.496137619018555, MLP.norm 3.604365110397339
2023-04-11 14:28:37,889 :: INFO :: Epoch 20: loss tensor(772.5021, device='cuda:0'), U.norm 9.185528755187988, V.norm 17.344900131225586, MLP.norm 4.668241024017334
2023-04-11 14:28:37,905 :: INFO :: Environment 3
2023-04-11 14:28:51,905 :: INFO :: Epoch 5: loss tensor(871.4501, device='cuda:0'), U.norm 14.651307106018066, V.norm 18.35032081604004, MLP.norm 1.8692288398742676
2023-04-11 14:28:51,999 :: INFO :: Epoch 10: loss tensor(857.7139, device='cuda:0'), U.norm 12.049845695495605, V.norm 17.86731719970703, MLP.norm 2.655243158340454
2023-04-11 14:28:52,093 :: INFO :: Epoch 15: loss tensor(837.8387, device='cuda:0'), U.norm 10.391216278076172, V.norm 17.65003776550293, MLP.norm 3.6913483142852783
2023-04-11 14:28:52,186 :: INFO :: Epoch 20: loss tensor(812.9775, device='cuda:0'), U.norm 9.20314884185791, V.norm 17.510906219482422, MLP.norm 4.788106441497803
2023-04-11 14:28:52,202 :: INFO :: Environment 4
2023-04-11 14:29:07,436 :: INFO :: Epoch 5: loss tensor(889.6653, device='cuda:0'), U.norm 14.64863395690918, V.norm 18.379098892211914, MLP.norm 1.886615514755249
2023-04-11 14:29:07,577 :: INFO :: Epoch 10: loss tensor(874.8525, device='cuda:0'), U.norm 12.044795989990234, V.norm 17.900989532470703, MLP.norm 2.7152204513549805
2023-04-11 14:29:07,671 :: INFO :: Epoch 15: loss tensor(853.5810, device='cuda:0'), U.norm 10.38342571258545, V.norm 17.685237884521484, MLP.norm 3.7857394218444824
2023-04-11 14:29:07,796 :: INFO :: Epoch 20: loss tensor(827.2908, device='cuda:0'), U.norm 9.192094802856445, V.norm 17.546886444091797, MLP.norm 4.915922164916992
2023-04-11 14:29:07,811 :: INFO :: Environment 5
2023-04-11 14:29:22,014 :: INFO :: Epoch 5: loss tensor(857.9545, device='cuda:0'), U.norm 14.649359703063965, V.norm 18.32245635986328, MLP.norm 1.8930835723876953
2023-04-11 14:29:22,092 :: INFO :: Epoch 10: loss tensor(844.0444, device='cuda:0'), U.norm 12.046106338500977, V.norm 17.82572364807129, MLP.norm 2.7061805725097656
2023-04-11 14:29:22,186 :: INFO :: Epoch 15: loss tensor(824.9352, device='cuda:0'), U.norm 10.384932518005371, V.norm 17.600799560546875, MLP.norm 3.7379801273345947
2023-04-11 14:29:22,296 :: INFO :: Epoch 20: loss tensor(801.7593, device='cuda:0'), U.norm 9.193687438964844, V.norm 17.45516014099121, MLP.norm 4.81427001953125
2023-04-11 14:29:22,296 :: INFO :: Environment 6
2023-04-11 14:29:35,874 :: INFO :: Epoch 5: loss tensor(843.1569, device='cuda:0'), U.norm 14.648971557617188, V.norm 18.29017448425293, MLP.norm 1.8753410577774048
2023-04-11 14:29:35,952 :: INFO :: Epoch 10: loss tensor(829.5076, device='cuda:0'), U.norm 12.045476913452148, V.norm 17.78328514099121, MLP.norm 2.654620885848999
2023-04-11 14:29:36,046 :: INFO :: Epoch 15: loss tensor(809.8956, device='cuda:0'), U.norm 10.384222984313965, V.norm 17.555116653442383, MLP.norm 3.673460006713867
2023-04-11 14:29:36,124 :: INFO :: Epoch 20: loss tensor(785.8866, device='cuda:0'), U.norm 9.192914962768555, V.norm 17.410888671875, MLP.norm 4.746082782745361
2023-04-11 14:29:36,124 :: INFO :: Environment 7
2023-04-11 14:29:49,967 :: INFO :: Epoch 5: loss tensor(841.5601, device='cuda:0'), U.norm 14.649643898010254, V.norm 18.288536071777344, MLP.norm 1.8546231985092163
2023-04-11 14:29:50,061 :: INFO :: Epoch 10: loss tensor(828.2904, device='cuda:0'), U.norm 12.046640396118164, V.norm 17.777708053588867, MLP.norm 2.6166796684265137
2023-04-11 14:29:50,155 :: INFO :: Epoch 15: loss tensor(809.1725, device='cuda:0'), U.norm 10.38613510131836, V.norm 17.5460262298584, MLP.norm 3.6297316551208496
2023-04-11 14:29:50,233 :: INFO :: Epoch 20: loss tensor(785.8528, device='cuda:0'), U.norm 9.195725440979004, V.norm 17.39707374572754, MLP.norm 4.694681644439697
2023-04-11 14:29:50,233 :: INFO :: Environment 8
2023-04-11 14:30:02,858 :: INFO :: Epoch 5: loss tensor(862.9836, device='cuda:0'), U.norm 14.650801658630371, V.norm 18.33808708190918, MLP.norm 1.8749662637710571
2023-04-11 14:30:02,952 :: INFO :: Epoch 10: loss tensor(849.1979, device='cuda:0'), U.norm 12.049214363098145, V.norm 17.84914779663086, MLP.norm 2.662511110305786
2023-04-11 14:30:03,061 :: INFO :: Epoch 15: loss tensor(829.6909, device='cuda:0'), U.norm 10.390433311462402, V.norm 17.628742218017578, MLP.norm 3.6874279975891113
2023-04-11 14:30:03,186 :: INFO :: Epoch 20: loss tensor(805.5191, device='cuda:0'), U.norm 9.202195167541504, V.norm 17.48768424987793, MLP.norm 4.775859832763672
2023-04-11 14:30:03,186 :: INFO :: Environment 9
2023-04-11 14:30:16,921 :: INFO :: Epoch 5: loss tensor(861.4838, device='cuda:0'), U.norm 14.648391723632812, V.norm 18.33154296875, MLP.norm 1.8717362880706787
2023-04-11 14:30:17,030 :: INFO :: Epoch 10: loss tensor(847.5295, device='cuda:0'), U.norm 12.044227600097656, V.norm 17.83710289001465, MLP.norm 2.66194224357605
2023-04-11 14:30:17,155 :: INFO :: Epoch 15: loss tensor(827.8106, device='cuda:0'), U.norm 10.382287979125977, V.norm 17.61410140991211, MLP.norm 3.695319414138794
2023-04-11 14:30:17,249 :: INFO :: Epoch 20: loss tensor(802.9005, device='cuda:0'), U.norm 9.190075874328613, V.norm 17.470415115356445, MLP.norm 4.7812676429748535
2023-04-11 14:30:17,265 :: INFO :: Ite = 1, Delta = 4088
2023-04-11 14:30:17,265 :: INFO :: ----- backend -----
2023-04-11 14:30:19,749 :: INFO :: Epoch 5: loss tensor(358.5045, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-9.7077e-03, -8.9274e-03, -9.5873e-03, -2.9930e-03, -1.0691e-02,
        -1.3297e-02, -1.0193e-02,  2.4610e-03, -1.2176e-02, -4.1106e-03,
        -1.1943e-02, -1.0227e-02, -9.6882e-03, -8.9737e-03, -5.8937e-03,
         2.2194e-03, -6.0350e-03, -2.3322e-03, -1.1809e-03, -3.5393e-03,
        -7.5479e-03, -3.3291e-03, -6.2313e-03, -1.2107e-02, -9.0905e-03,
        -1.0031e-02, -8.7161e-03, -9.0560e-03, -5.0403e-03, -4.7319e-03,
        -5.9888e-03,  5.1729e-03, -1.0846e-02, -7.2196e-03, -1.2938e-02,
        -1.2731e-02, -7.7023e-03, -9.0036e-03, -1.1768e-02,  5.9445e-04,
        -1.2630e-02, -1.2226e-02, -6.3192e-03, -1.0368e-02, -8.9768e-03,
        -3.9176e-03, -8.8967e-03, -1.3369e-02, -1.0972e-02, -8.8519e-03,
        -1.0814e-02, -5.6119e-03, -1.0373e-02, -5.5862e-03, -9.6758e-03,
         4.5665e-03, -9.2722e-03, -4.1692e-03, -1.2492e-02, -1.3252e-02,
         2.0338e-04, -9.3821e-03, -1.2462e-02, -1.0714e-02, -6.9623e-03,
        -1.1304e-02,  2.4726e-03, -1.1424e-02,  9.1204e-03, -1.2244e-02,
         5.5565e-04, -1.2252e-02,  9.0925e-03, -1.1722e-02, -2.4576e-03,
        -1.2088e-02,  1.0092e-02, -1.2116e-02, -1.8987e-03,  1.0170e-02,
        -8.6450e-03, -1.3507e-02,  8.7030e-04, -3.2832e-03, -1.2976e-02,
        -5.2915e-03,  1.0393e-02, -1.1799e-02, -1.1605e-02, -1.2302e-02,
        -1.0137e-02, -6.1142e-03, -8.0108e-03,  2.2023e-05, -1.2970e-02,
        -9.0809e-03, -2.3395e-03, -8.7797e-03, -1.2889e-02, -9.0783e-03,
        -1.2738e-02, -6.8938e-03, -9.5014e-03, -1.2013e-02, -1.1912e-02,
        -1.3156e-02, -3.9416e-05, -1.1393e-02, -3.9579e-03, -2.2500e-03,
        -1.1543e-02, -2.8294e-03, -1.1231e-02, -9.2374e-03, -1.2250e-02,
        -1.2779e-02, -1.0253e-02, -1.2516e-02, -4.4213e-03, -9.8096e-03,
        -1.0652e-02, -9.5246e-03, -8.3418e-03, -1.2519e-02, -1.0491e-02,
        -1.0834e-02, -1.2540e-02, -8.2321e-03,  6.3135e-03,  5.7454e-03,
         6.1856e-03,  1.4750e-02,  9.9828e-03,  9.5875e-03, -4.5753e-03,
        -9.3302e-03, -1.2222e-02,  1.4023e-02, -2.9943e-03, -1.1339e-02,
         7.7632e-03, -1.0890e-02, -2.1331e-03, -1.2785e-02,  2.0226e-03,
         1.4582e-02,  3.4958e-03, -1.1277e-02, -2.6373e-03, -1.2056e-02,
        -1.1227e-02, -1.9740e-03,  1.0986e-02, -1.2708e-02, -9.4959e-03,
        -1.2830e-02, -1.0597e-02, -1.2863e-02,  1.4340e-02, -1.3756e-02,
         1.4451e-02, -1.3761e-02,  1.3898e-02, -4.6768e-03,  1.0954e-02,
         1.3014e-02, -1.1808e-02, -9.4979e-03, -1.0543e-02, -2.1121e-03,
         7.5102e-03,  9.3992e-03, -1.3228e-02,  6.7666e-03,  1.0492e-02,
        -1.1282e-02,  1.4379e-02,  8.6038e-03, -7.5482e-03, -1.1087e-02,
         2.5626e-03, -1.1292e-02,  1.0570e-02, -1.2933e-02,  8.5019e-03,
         9.7553e-03, -1.2072e-02,  1.2200e-03,  9.2848e-03, -1.2987e-02,
        -1.3523e-02, -7.8680e-03,  1.4679e-02, -1.2364e-02,  3.5341e-03,
         1.0308e-02, -1.0118e-02,  1.3980e-02,  8.1071e-03, -5.4160e-03,
         7.9801e-03, -8.2218e-03,  9.1644e-03,  1.5228e-02,  3.7453e-03,
        -1.0813e-02, -1.1321e-02, -1.3432e-02,  1.1889e-02, -7.5252e-03,
         7.8059e-03,  1.4524e-02,  1.4738e-02, -1.0944e-02,  4.1885e-03,
         1.2495e-03, -7.1217e-03, -1.2368e-02,  1.4426e-02, -1.2733e-02,
        -5.0396e-03, -8.3130e-03,  1.0820e-02,  6.4322e-03,  6.5894e-03,
        -1.4731e-03,  2.3734e-03, -1.6014e-03,  3.7208e-03,  1.0473e-02,
         8.9073e-03,  1.1126e-02,  1.4790e-02,  1.3277e-02,  1.4390e-02,
        -1.1501e-02, -9.7913e-03, -8.7492e-03,  9.7263e-03,  9.1051e-03,
        -3.5435e-03, -4.7225e-03, -9.5119e-03,  1.4781e-02, -9.5534e-03,
         5.7928e-03, -1.2641e-02, -3.3478e-03, -6.4365e-03, -8.8643e-03,
        -1.3052e-02,  1.4460e-02, -7.5370e-03, -7.1744e-03, -1.2910e-02,
        -1.2309e-03, -6.8823e-03, -9.3571e-03, -1.3060e-02,  8.6787e-03,
         4.8648e-03, -9.4839e-04, -1.0003e-02, -7.3854e-03, -7.1208e-04,
        -2.3293e-03, -2.5716e-03, -9.7977e-03, -1.1807e-03, -2.5167e-03,
        -1.0742e-02, -6.9484e-03, -2.6394e-03, -1.1007e-02, -1.9695e-03,
        -7.9503e-03,  5.4073e-04,  1.3331e-04, -5.6899e-03, -3.5629e-03,
        -4.7938e-03, -1.0499e-03, -1.5935e-05, -1.2174e-02, -1.5666e-03,
         3.0631e-03, -8.8007e-03, -7.0870e-03,  8.3157e-03, -7.5275e-03,
        -1.0534e-02, -6.6797e-03, -1.3991e-03,  2.5257e-03, -1.1127e-02,
         4.8054e-03,  3.3348e-03, -7.5451e-03,  5.4774e-03,  4.7290e-03,
        -1.3200e-02,  7.1420e-03, -3.6988e-03, -1.2276e-02,  3.9152e-04,
        -1.2061e-02, -2.9125e-03, -7.0597e-03, -2.8788e-03, -8.6245e-03,
        -8.6544e-03, -6.3556e-04, -3.2481e-03,  4.6289e-03, -4.5156e-03,
         1.2253e-03, -1.7224e-03, -1.0997e-02, -2.1803e-03, -1.3098e-02,
         3.9803e-03, -6.5327e-03,  2.3674e-03, -1.0007e-02, -2.9085e-03,
        -5.9196e-03, -1.3224e-02, -8.6628e-03, -8.5675e-03, -3.6545e-03,
         1.5943e-03, -9.0077e-03, -9.0827e-03, -8.5317e-03,  1.3856e-03,
        -6.6023e-04, -6.8351e-03, -2.0411e-03,  2.5199e-03, -3.7928e-03,
         5.0434e-04, -5.9504e-03, -4.0200e-03,  2.7830e-03, -4.4730e-03,
         4.8331e-04, -2.0027e-04, -8.8600e-03, -1.0660e-02, -5.5064e-03,
        -9.2072e-03, -1.0157e-02, -6.1153e-03, -7.5162e-03, -4.3362e-03,
        -3.3825e-03, -1.2503e-02,  7.5526e-03, -6.5824e-03, -4.7266e-04,
        -9.9564e-03, -1.2555e-02, -4.8138e-03, -5.5322e-03,  9.2895e-03,
         7.2010e-03, -4.4763e-03, -1.1544e-02, -8.5679e-03, -6.5641e-03,
        -7.2200e-03, -1.3274e-02,  3.1105e-03,  1.7319e-03, -8.0847e-04,
        -8.0059e-03, -2.8195e-03, -2.2448e-03, -1.0532e-04, -1.3342e-02,
         3.7387e-03, -7.2024e-03, -9.4215e-03,  3.6195e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(2.5681, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:22,061 :: INFO :: Epoch 10: loss tensor(350.5648, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-3.9143e-03, -3.7580e-03, -7.2848e-03,  7.9325e-03, -9.9659e-03,
        -1.9396e-02, -6.1773e-03,  1.5641e-02, -1.5558e-02,  7.2653e-03,
        -1.4876e-02, -1.0096e-02, -5.2462e-03, -3.0141e-03, -6.8403e-04,
         1.2881e-02,  2.5749e-03,  9.2504e-03,  2.9432e-03,  7.9439e-03,
         2.3549e-04,  5.6517e-03,  3.5597e-03, -1.3434e-02, -6.6885e-03,
        -5.6968e-03, -5.7699e-03, -2.6783e-03,  7.1434e-03,  3.6591e-03,
         3.8911e-03,  1.7635e-02, -1.0344e-02, -9.5141e-04, -2.1276e-02,
        -2.2498e-02, -9.2922e-04, -8.0579e-04, -1.3756e-02,  7.8521e-03,
        -1.7941e-02, -1.5338e-02,  1.7696e-03, -8.6489e-03, -4.8594e-03,
         4.3057e-03, -2.7669e-03, -2.1014e-02, -1.2503e-02, -5.2626e-03,
        -9.6734e-03,  3.3795e-03, -1.0747e-02,  2.4588e-03, -3.9810e-03,
         1.8286e-02, -2.1866e-03,  6.6004e-03, -1.8978e-02, -2.2030e-02,
         1.2350e-02, -7.2640e-03, -1.6625e-02, -9.0640e-03,  4.3232e-04,
        -1.2225e-02,  1.6288e-02, -1.5270e-02,  2.2389e-02, -1.7078e-02,
         1.1209e-02, -1.5709e-02,  1.8042e-02, -1.1214e-02,  1.0011e-02,
        -1.6651e-02,  1.6679e-02, -1.6483e-02,  9.8458e-03,  1.3944e-02,
        -1.3947e-03, -1.8579e-02,  1.4858e-02,  9.0546e-03, -2.1059e-02,
         5.9426e-03,  2.0668e-02, -1.5321e-02, -1.2264e-02, -1.7966e-02,
        -7.5952e-03,  5.3996e-03, -9.6138e-04,  1.2753e-02, -1.7614e-02,
        -1.7637e-03,  1.0425e-02, -6.8125e-03, -2.0120e-02, -5.9362e-03,
        -1.9592e-02,  1.8530e-03, -4.2749e-03, -1.4080e-02, -1.1924e-02,
        -2.2570e-02,  8.5186e-03, -1.4877e-02,  5.0730e-03,  1.0476e-02,
        -1.4528e-02,  7.4408e-03, -1.1056e-02, -7.3195e-03, -2.0092e-02,
        -1.9459e-02, -9.8574e-03, -1.6469e-02,  3.0421e-03, -7.5363e-03,
        -1.0223e-02, -4.7067e-03, -4.2923e-03, -2.0128e-02, -7.9815e-03,
        -1.3065e-02, -1.6475e-02, -3.0174e-04,  2.3010e-02,  2.2041e-02,
         2.2593e-02,  3.1008e-02,  2.7363e-02,  2.6500e-02,  9.6027e-03,
        -1.8129e-03, -1.8206e-02,  2.9312e-02,  1.2575e-02, -1.0228e-02,
         2.4537e-02, -8.9829e-03,  1.3879e-02, -1.7720e-02,  1.8462e-02,
         3.0411e-02,  1.9887e-02, -1.2289e-02,  1.2281e-02, -9.5992e-03,
        -1.2801e-02,  1.4492e-02,  2.7555e-02, -2.2356e-02, -3.2490e-04,
        -1.9120e-02, -7.2257e-03, -2.3055e-02,  2.9644e-02, -2.5867e-02,
         3.0933e-02, -2.6162e-02,  3.0165e-02,  9.0735e-03,  2.6804e-02,
         2.2907e-02, -1.5790e-02, -1.1444e-03, -5.8691e-03,  1.3544e-02,
         2.4790e-02,  2.5438e-02, -2.4215e-02,  2.3090e-02,  2.7446e-02,
        -1.4720e-02,  2.9528e-02,  2.4900e-02,  4.1549e-03, -1.1618e-02,
         1.8587e-02, -1.3454e-02,  2.6606e-02, -2.3103e-02,  2.4559e-02,
         2.6608e-02, -1.9943e-02,  1.7961e-02,  2.6453e-02, -2.3419e-02,
        -2.4626e-02,  2.0323e-03,  3.0192e-02, -2.0447e-02,  2.0187e-02,
         2.6865e-02, -8.9308e-04,  2.5539e-02,  2.4937e-02,  8.7364e-03,
         2.4381e-02,  3.3249e-03,  2.5858e-02,  3.2056e-02,  2.0232e-02,
        -5.5733e-03, -1.0086e-02, -2.5121e-02,  2.7773e-02,  3.6891e-03,
         2.4837e-02,  3.0095e-02,  2.9827e-02, -1.1234e-02,  2.1004e-02,
         1.7949e-02,  4.3488e-03, -1.7056e-02,  3.0001e-02, -2.2689e-02,
         1.0317e-02,  4.4991e-03,  2.8138e-02,  2.3518e-02,  2.3829e-02,
         1.4978e-02,  1.8495e-02,  1.4523e-02,  1.9798e-02,  2.7512e-02,
         2.5342e-02,  2.6581e-02,  3.0815e-02,  2.9480e-02,  3.0153e-02,
        -1.3541e-02, -2.6686e-03, -8.0578e-05,  2.4874e-02,  2.5394e-02,
         1.1489e-02,  9.2654e-03, -2.0625e-03,  3.0997e-02, -4.4818e-03,
         2.1963e-02, -2.1865e-02,  1.1639e-02,  5.4199e-03, -1.9546e-04,
        -2.3388e-02,  3.0476e-02,  4.1587e-03,  5.1213e-03, -2.2984e-02,
         1.4368e-02,  7.3329e-03, -2.8363e-03, -1.3926e-02,  2.5625e-02,
         2.1590e-02,  1.5134e-02,  5.3738e-04,  6.3051e-03,  1.5386e-02,
         1.3849e-02,  1.3219e-02,  7.1302e-04,  1.5077e-02,  1.3048e-02,
        -7.3472e-03,  7.1319e-03,  1.3409e-02, -5.1378e-03,  1.4108e-02,
         5.2486e-03,  1.7127e-02,  1.6573e-02,  8.5862e-03,  1.2294e-02,
         1.0325e-02,  1.5506e-02,  1.6485e-02, -1.1988e-02,  1.4501e-02,
         1.9864e-02,  3.0396e-03,  6.5913e-03,  2.5232e-02,  5.4751e-03,
        -9.1758e-04,  7.2741e-03,  1.5015e-02,  1.9278e-02, -1.0413e-02,
         2.1569e-02,  2.0191e-02,  5.6239e-03,  2.2439e-02,  2.1586e-02,
        -1.9604e-02,  2.4163e-02,  1.2101e-02, -1.4782e-02,  1.6731e-02,
        -1.1648e-02,  1.2459e-02,  6.2698e-03,  1.3005e-02,  4.1027e-03,
         1.8513e-03,  1.5634e-02,  1.2711e-02,  2.1525e-02,  1.1233e-02,
         1.7843e-02,  1.4458e-02, -5.8861e-03,  1.3647e-02, -1.6958e-02,
         2.0693e-02,  7.7758e-03,  1.8999e-02, -4.3111e-04,  1.3021e-02,
         7.3502e-03, -1.6965e-02,  3.9416e-03,  3.0906e-03,  1.2134e-02,
         1.7973e-02,  2.7762e-03,  2.6469e-03,  4.2538e-03,  1.7783e-02,
         1.5699e-02,  6.1186e-03,  1.3970e-02,  1.9431e-02,  1.1454e-02,
         1.6956e-02,  8.7950e-03,  1.1811e-02,  1.9505e-02,  1.0819e-02,
         1.6870e-02,  1.6268e-02,  2.3017e-03, -1.0430e-02,  9.0804e-03,
         1.7362e-03, -4.6011e-03,  8.5289e-03,  4.4936e-03,  1.0876e-02,
         1.2349e-02, -1.7966e-02,  2.4560e-02,  6.5651e-03,  1.5782e-02,
        -2.4616e-04, -1.7185e-02,  1.0479e-02,  9.5276e-03,  2.6429e-02,
         2.4210e-02,  1.0465e-02, -1.0130e-02,  9.1357e-04,  7.7845e-03,
         6.0557e-03, -1.8866e-02,  1.9955e-02,  1.8331e-02,  1.5636e-02,
         3.8788e-03,  1.2975e-02,  1.3595e-02,  1.6285e-02, -2.0564e-02,
         2.0520e-02,  6.6743e-03,  1.7492e-03,  2.0533e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.0746, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:24,108 :: INFO :: Epoch 15: loss tensor(341.6158, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.1584e-02,  1.1207e-02,  7.2007e-03,  2.5076e-02,  7.9842e-04,
        -1.2921e-02,  1.0641e-02,  3.1275e-02, -6.2887e-03,  2.2651e-02,
        -3.6215e-03, -2.5626e-04,  7.1240e-03,  1.3104e-02,  1.4402e-02,
         2.6410e-02,  1.5622e-02,  2.6552e-02,  1.3460e-02,  2.5077e-02,
         1.4468e-02,  1.9821e-02,  2.0217e-02, -2.7308e-04,  6.1014e-03,
         6.6812e-03,  6.2233e-03,  1.3159e-02,  2.6210e-02,  1.7683e-02,
         2.0167e-02,  3.3628e-02,  5.5441e-03,  1.4481e-02, -1.7895e-02,
        -2.6058e-02,  1.3033e-02,  1.3668e-02, -5.7950e-03,  2.4855e-02,
        -1.3373e-02, -7.6059e-03,  1.8747e-02,  1.9469e-03,  8.1862e-03,
         1.8455e-02,  1.0611e-02, -1.7501e-02,  7.8379e-06,  6.9978e-03,
         3.2918e-03,  1.8365e-02, -2.1659e-03,  1.5945e-02,  1.3739e-02,
         3.5334e-02,  1.4464e-02,  2.4405e-02, -9.9212e-03, -1.8204e-02,
         2.7020e-02,  3.7773e-03, -6.2143e-03,  3.0937e-03,  1.7068e-02,
        -2.8748e-03,  3.5252e-02, -4.2924e-03,  3.7221e-02, -9.2523e-03,
         2.5938e-02, -3.4622e-03,  2.8681e-02,  9.8400e-06,  2.6758e-02,
        -8.7094e-03,  3.0530e-02, -7.0637e-03,  2.6617e-02,  2.3926e-02,
         1.4806e-02, -5.4748e-03,  3.0859e-02,  2.6845e-02, -1.8980e-02,
         2.4413e-02,  3.4570e-02, -4.3428e-03, -2.3737e-03, -1.3927e-02,
         4.2584e-03,  2.2154e-02,  1.3222e-02,  2.9246e-02, -4.1755e-03,
         1.5635e-02,  2.5760e-02,  8.0281e-03, -1.8482e-02,  8.1143e-03,
        -9.7699e-03,  1.5602e-02,  1.0737e-02, -5.8523e-03,  2.2652e-03,
        -2.4511e-02,  2.3747e-02, -5.3134e-03,  1.9598e-02,  2.7298e-02,
        -4.5870e-03,  2.0908e-02,  4.1514e-03,  6.7486e-03, -2.0824e-02,
        -1.7365e-02,  3.9037e-03, -9.1576e-03,  1.4118e-02,  5.6144e-03,
         1.1171e-03,  9.2103e-03,  1.0023e-02, -1.6882e-02,  4.2173e-03,
        -7.0865e-03, -7.2326e-03,  1.5551e-02,  4.0461e-02,  3.9230e-02,
         4.0233e-02,  4.7420e-02,  4.5499e-02,  4.4876e-02,  2.7061e-02,
         1.4099e-02, -1.5518e-02,  4.2719e-02,  3.0382e-02,  2.6019e-03,
         4.2808e-02,  3.7270e-03,  3.2584e-02, -1.1640e-02,  3.6858e-02,
         4.6932e-02,  3.7072e-02, -1.0020e-03,  3.0599e-02,  5.9566e-03,
        -3.8814e-03,  3.3296e-02,  4.5046e-02, -2.9501e-02,  1.6563e-02,
        -1.4928e-02,  6.8025e-03, -3.0106e-02,  4.5212e-02, -3.5589e-02,
         4.7757e-02, -3.6565e-02,  4.7801e-02,  2.7009e-02,  4.2885e-02,
         3.5085e-02, -1.0307e-02,  1.5278e-02,  9.5918e-03,  3.1276e-02,
         4.3824e-02,  4.2039e-02, -3.2463e-02,  4.0695e-02,  4.5741e-02,
        -8.4879e-03,  4.3875e-02,  4.1988e-02,  2.1584e-02, -2.3112e-03,
         3.6091e-02, -4.5566e-03,  4.2988e-02, -2.9903e-02,  4.1326e-02,
         4.4988e-02, -2.3435e-02,  3.6770e-02,  4.5809e-02, -3.1205e-02,
        -3.2295e-02,  1.8966e-02,  4.5982e-02, -2.3224e-02,  3.7582e-02,
         4.4735e-02,  1.7122e-02,  3.4530e-02,  4.2727e-02,  2.7253e-02,
         4.2059e-02,  2.1819e-02,  4.3040e-02,  4.9896e-02,  3.8291e-02,
         1.1073e-02,  2.9903e-03, -3.4763e-02,  4.3616e-02,  2.0643e-02,
         4.3573e-02,  4.6051e-02,  4.6310e-02, -1.2572e-03,  3.9048e-02,
         3.6972e-02,  2.1496e-02, -1.2037e-02,  4.6640e-02, -2.9429e-02,
         2.8917e-02,  2.2843e-02,  4.6667e-02,  4.1671e-02,  4.2862e-02,
         3.3964e-02,  3.5724e-02,  3.3461e-02,  3.7046e-02,  4.5885e-02,
         4.2478e-02,  4.2008e-02,  4.6509e-02,  4.7292e-02,  4.6376e-02,
        -3.6247e-03,  1.3631e-02,  1.6122e-02,  3.8316e-02,  4.2991e-02,
         2.9259e-02,  2.7262e-02,  1.4891e-02,  4.7542e-02,  1.0160e-02,
         3.9357e-02, -2.6960e-02,  2.9771e-02,  2.2475e-02,  1.6313e-02,
        -2.9835e-02,  4.7747e-02,  2.2391e-02,  2.3563e-02, -2.9158e-02,
         3.2011e-02,  2.6421e-02,  1.3600e-02, -8.3077e-04,  4.4113e-02,
         4.0165e-02,  3.4095e-02,  1.8776e-02,  2.4876e-02,  3.4211e-02,
         3.2908e-02,  3.2087e-02,  1.8898e-02,  3.3874e-02,  3.1599e-02,
         7.7503e-03,  2.5866e-02,  3.2477e-02,  1.1543e-02,  3.2822e-02,
         2.3781e-02,  3.5948e-02,  3.5320e-02,  2.7266e-02,  3.1213e-02,
         2.9138e-02,  3.4685e-02,  3.5332e-02,  1.2670e-03,  3.3512e-02,
         3.8409e-02,  2.1123e-02,  2.4846e-02,  4.3510e-02,  2.3618e-02,
         1.7092e-02,  2.5882e-02,  3.4196e-02,  3.7848e-02,  2.6381e-03,
         3.9985e-02,  3.9048e-02,  2.4297e-02,  4.1086e-02,  4.0166e-02,
        -1.5495e-02,  4.3028e-02,  3.1347e-02, -4.9739e-03,  3.5368e-02,
         2.3184e-03,  3.0905e-02,  2.4766e-02,  3.1831e-02,  2.3085e-02,
         2.0103e-02,  3.4655e-02,  3.1703e-02,  4.0410e-02,  3.0220e-02,
         3.6605e-02,  3.3198e-02,  1.0547e-02,  3.2224e-02, -8.4505e-03,
         3.9208e-02,  2.6767e-02,  3.7640e-02,  1.7537e-02,  3.1976e-02,
         2.5664e-02, -7.6652e-03,  2.2364e-02,  2.1588e-02,  3.0978e-02,
         3.6478e-02,  2.1364e-02,  2.0963e-02,  2.2895e-02,  3.6656e-02,
         3.4211e-02,  2.4183e-02,  3.2689e-02,  3.8475e-02,  3.0027e-02,
         3.5689e-02,  2.7659e-02,  3.0798e-02,  3.8314e-02,  2.9545e-02,
         3.5121e-02,  3.4875e-02,  2.0553e-02,  1.6343e-03,  2.7518e-02,
         2.0013e-02,  1.2100e-02,  2.7080e-02,  2.2124e-02,  2.9656e-02,
         3.1218e-02, -1.2340e-02,  4.3342e-02,  2.5061e-02,  3.4484e-02,
         1.7683e-02, -1.0411e-02,  2.9347e-02,  2.8718e-02,  4.5233e-02,
         4.2781e-02,  2.8986e-02,  3.7618e-03,  1.8700e-02,  2.6391e-02,
         2.4353e-02, -1.2002e-02,  3.8618e-02,  3.7022e-02,  3.4844e-02,
         2.2491e-02,  3.1660e-02,  3.1935e-02,  3.5041e-02, -1.8602e-02,
         3.9120e-02,  2.5305e-02,  1.9617e-02,  3.9346e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(5.5809, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:26,374 :: INFO :: Epoch 20: loss tensor(334.1760, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0309,  0.0301,  0.0268,  0.0456,  0.0173,  0.0025,  0.0310,  0.0501,
         0.0100,  0.0408,  0.0149,  0.0168,  0.0228,  0.0337,  0.0349,  0.0439,
         0.0315,  0.0464,  0.0313,  0.0450,  0.0328,  0.0378,  0.0399,  0.0196,
         0.0251,  0.0227,  0.0237,  0.0331,  0.0472,  0.0355,  0.0393,  0.0533,
         0.0259,  0.0337, -0.0029, -0.0206,  0.0309,  0.0306,  0.0085,  0.0453,
         0.0003,  0.0070,  0.0384,  0.0182,  0.0258,  0.0365,  0.0274, -0.0032,
         0.0196,  0.0240,  0.0220,  0.0370,  0.0134,  0.0327,  0.0344,  0.0551,
         0.0338,  0.0455,  0.0091, -0.0034,  0.0439,  0.0212,  0.0119,  0.0206,
         0.0374,  0.0135,  0.0555,  0.0153,  0.0547,  0.0072,  0.0441,  0.0155,
         0.0440,  0.0167,  0.0467,  0.0086,  0.0496,  0.0114,  0.0463,  0.0415,
         0.0345,  0.0146,  0.0493,  0.0474, -0.0058,  0.0449,  0.0528,  0.0143,
         0.0141, -0.0014,  0.0209,  0.0416,  0.0308,  0.0477,  0.0161,  0.0356,
         0.0430,  0.0284, -0.0077,  0.0274,  0.0092,  0.0321,  0.0294,  0.0094,
         0.0224, -0.0163,  0.0436,  0.0121,  0.0379,  0.0466,  0.0138,  0.0368,
         0.0244,  0.0263, -0.0122, -0.0040,  0.0235,  0.0054,  0.0299,  0.0246,
         0.0190,  0.0279,  0.0292, -0.0028,  0.0216,  0.0076,  0.0105,  0.0350,
         0.0575,  0.0562,  0.0581,  0.0625,  0.0624,  0.0640,  0.0447,  0.0319,
        -0.0054,  0.0511,  0.0479,  0.0192,  0.0614,  0.0201,  0.0518,  0.0024,
         0.0556,  0.0635,  0.0533,  0.0162,  0.0495,  0.0250,  0.0107,  0.0525,
         0.0626, -0.0351,  0.0342, -0.0027,  0.0239, -0.0349,  0.0611, -0.0435,
         0.0641, -0.0453,  0.0659,  0.0453,  0.0589,  0.0498,  0.0007,  0.0332,
         0.0276,  0.0487,  0.0632,  0.0584, -0.0387,  0.0587,  0.0643,  0.0045,
         0.0570,  0.0589,  0.0396,  0.0112,  0.0538,  0.0105,  0.0591, -0.0341,
         0.0580,  0.0640, -0.0236,  0.0560,  0.0661, -0.0372, -0.0370,  0.0372,
         0.0618, -0.0219,  0.0535,  0.0633,  0.0366,  0.0428,  0.0601,  0.0466,
         0.0601,  0.0416,  0.0597,  0.0674,  0.0564,  0.0306,  0.0195, -0.0430,
         0.0587,  0.0385,  0.0628,  0.0620,  0.0629,  0.0133,  0.0567,  0.0568,
         0.0397, -0.0011,  0.0639, -0.0336,  0.0478,  0.0414,  0.0653,  0.0597,
         0.0617,  0.0536,  0.0525,  0.0532,  0.0539,  0.0646,  0.0592,  0.0565,
         0.0609,  0.0643,  0.0623,  0.0128,  0.0319,  0.0338,  0.0473,  0.0611,
         0.0471,  0.0460,  0.0342,  0.0638,  0.0277,  0.0571, -0.0286,  0.0481,
         0.0403,  0.0343, -0.0324,  0.0658,  0.0421,  0.0432, -0.0312,  0.0497,
         0.0469,  0.0327,  0.0180,  0.0629,  0.0592,  0.0542,  0.0384,  0.0444,
         0.0542,  0.0528,  0.0515,  0.0384,  0.0530,  0.0505,  0.0258,  0.0456,
         0.0523,  0.0304,  0.0520,  0.0433,  0.0555,  0.0547,  0.0464,  0.0506,
         0.0485,  0.0552,  0.0545,  0.0196,  0.0538,  0.0568,  0.0405,  0.0441,
         0.0615,  0.0430,  0.0363,  0.0454,  0.0546,  0.0565,  0.0196,  0.0584,
         0.0584,  0.0435,  0.0597,  0.0590, -0.0032,  0.0628,  0.0517,  0.0120,
         0.0547,  0.0211,  0.0496,  0.0440,  0.0511,  0.0432,  0.0397,  0.0546,
         0.0515,  0.0599,  0.0497,  0.0556,  0.0523,  0.0304,  0.0515,  0.0070,
         0.0580,  0.0468,  0.0567,  0.0370,  0.0517,  0.0446,  0.0092,  0.0420,
         0.0409,  0.0503,  0.0556,  0.0410,  0.0406,  0.0426,  0.0562,  0.0526,
         0.0428,  0.0519,  0.0583,  0.0487,  0.0550,  0.0472,  0.0504,  0.0578,
         0.0488,  0.0526,  0.0532,  0.0401,  0.0186,  0.0469,  0.0394,  0.0311,
         0.0465,  0.0396,  0.0494,  0.0506,  0.0014,  0.0625,  0.0442,  0.0537,
         0.0374,  0.0047,  0.0494,  0.0492,  0.0645,  0.0615,  0.0483,  0.0219,
         0.0376,  0.0458,  0.0438,  0.0031,  0.0576,  0.0559,  0.0552,  0.0427,
         0.0505,  0.0499,  0.0541, -0.0088,  0.0581,  0.0449,  0.0391,  0.0584],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.0518, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:28,702 :: INFO :: Epoch 25: loss tensor(330.1114, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0486,  0.0466,  0.0450,  0.0642,  0.0331,  0.0203,  0.0502,  0.0677,
         0.0264,  0.0575,  0.0338,  0.0331,  0.0377,  0.0525,  0.0529,  0.0586,
         0.0449,  0.0644,  0.0465,  0.0637,  0.0492,  0.0534,  0.0574,  0.0391,
         0.0427,  0.0378,  0.0388,  0.0512,  0.0673,  0.0504,  0.0567,  0.0700,
         0.0453,  0.0510,  0.0133, -0.0115,  0.0453,  0.0465,  0.0228,  0.0646,
         0.0154,  0.0229,  0.0566,  0.0330,  0.0410,  0.0528,  0.0422,  0.0128,
         0.0386,  0.0381,  0.0383,  0.0531,  0.0277,  0.0464,  0.0542,  0.0736,
         0.0531,  0.0653,  0.0280,  0.0134,  0.0582,  0.0378,  0.0308,  0.0360,
         0.0553,  0.0289,  0.0748,  0.0337,  0.0705,  0.0237,  0.0595,  0.0338,
         0.0562,  0.0342,  0.0648,  0.0245,  0.0673,  0.0294,  0.0632,  0.0592,
         0.0531,  0.0355,  0.0662,  0.0667,  0.0094,  0.0642,  0.0686,  0.0318,
         0.0293,  0.0117,  0.0369,  0.0595,  0.0463,  0.0641,  0.0362,  0.0541,
         0.0573,  0.0469,  0.0055,  0.0446,  0.0283,  0.0461,  0.0457,  0.0260,
         0.0415, -0.0032,  0.0609,  0.0275,  0.0548,  0.0639,  0.0323,  0.0511,
         0.0434,  0.0433, -0.0011,  0.0096,  0.0406,  0.0215,  0.0434,  0.0421,
         0.0358,  0.0451,  0.0482,  0.0119,  0.0381,  0.0228,  0.0276,  0.0527,
         0.0741,  0.0724,  0.0756,  0.0759,  0.0780,  0.0827,  0.0614,  0.0497,
         0.0089,  0.0562,  0.0647,  0.0365,  0.0797,  0.0373,  0.0706,  0.0194,
         0.0741,  0.0795,  0.0679,  0.0353,  0.0682,  0.0444,  0.0275,  0.0717,
         0.0796, -0.0394,  0.0513,  0.0133,  0.0414, -0.0377,  0.0750, -0.0496,
         0.0794, -0.0527,  0.0834,  0.0632,  0.0737,  0.0604,  0.0141,  0.0508,
         0.0459,  0.0654,  0.0821,  0.0736, -0.0434,  0.0763,  0.0827,  0.0205,
         0.0678,  0.0749,  0.0568,  0.0252,  0.0708,  0.0275,  0.0731, -0.0361,
         0.0737,  0.0830, -0.0212,  0.0748,  0.0860, -0.0419, -0.0384,  0.0553,
         0.0765, -0.0174,  0.0676,  0.0817,  0.0564,  0.0476,  0.0768,  0.0659,
         0.0779,  0.0617,  0.0752,  0.0831,  0.0741,  0.0507,  0.0365, -0.0498,
         0.0719,  0.0557,  0.0818,  0.0770,  0.0758,  0.0292,  0.0735,  0.0766,
         0.0575,  0.0126,  0.0808, -0.0358,  0.0666,  0.0592,  0.0837,  0.0774,
         0.0789,  0.0733,  0.0681,  0.0732,  0.0700,  0.0831,  0.0750,  0.0687,
         0.0738,  0.0783,  0.0771,  0.0314,  0.0503,  0.0512,  0.0535,  0.0784,
         0.0642,  0.0646,  0.0538,  0.0793,  0.0457,  0.0744, -0.0275,  0.0661,
         0.0577,  0.0524, -0.0314,  0.0840,  0.0620,  0.0629, -0.0296,  0.0666,
         0.0677,  0.0525,  0.0384,  0.0812,  0.0781,  0.0744,  0.0580,  0.0639,
         0.0744,  0.0728,  0.0710,  0.0576,  0.0715,  0.0684,  0.0439,  0.0652,
         0.0722,  0.0495,  0.0707,  0.0628,  0.0752,  0.0739,  0.0648,  0.0695,
         0.0675,  0.0763,  0.0732,  0.0397,  0.0747,  0.0742,  0.0600,  0.0636,
         0.0784,  0.0626,  0.0555,  0.0649,  0.0757,  0.0744,  0.0370,  0.0764,
         0.0775,  0.0627,  0.0774,  0.0774,  0.0128,  0.0831,  0.0721,  0.0318,
         0.0743,  0.0414,  0.0672,  0.0629,  0.0700,  0.0633,  0.0596,  0.0749,
         0.0715,  0.0793,  0.0691,  0.0741,  0.0708,  0.0514,  0.0705,  0.0249,
         0.0765,  0.0671,  0.0753,  0.0564,  0.0716,  0.0627,  0.0287,  0.0620,
         0.0596,  0.0696,  0.0746,  0.0605,  0.0604,  0.0621,  0.0760,  0.0699,
         0.0604,  0.0711,  0.0782,  0.0663,  0.0741,  0.0665,  0.0699,  0.0773,
         0.0678,  0.0686,  0.0707,  0.0597,  0.0370,  0.0662,  0.0584,  0.0501,
         0.0659,  0.0557,  0.0691,  0.0696,  0.0192,  0.0814,  0.0630,  0.0729,
         0.0571,  0.0235,  0.0698,  0.0701,  0.0835,  0.0798,  0.0677,  0.0411,
         0.0558,  0.0652,  0.0633,  0.0220,  0.0761,  0.0744,  0.0760,  0.0635,
         0.0686,  0.0660,  0.0728,  0.0058,  0.0766,  0.0646,  0.0591,  0.0768],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.4390, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:30,624 :: INFO :: Epoch 30: loss tensor(327.6672, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0632,  0.0591,  0.0609,  0.0806,  0.0469,  0.0382,  0.0669,  0.0827,
         0.0416,  0.0707,  0.0514,  0.0464,  0.0512,  0.0680,  0.0671,  0.0692,
         0.0556,  0.0793,  0.0565,  0.0798,  0.0628,  0.0651,  0.0724,  0.0573,
         0.0567,  0.0504,  0.0498,  0.0663,  0.0860,  0.0625,  0.0705,  0.0833,
         0.0625,  0.0654,  0.0279, -0.0025,  0.0559,  0.0614,  0.0358,  0.0818,
         0.0294,  0.0380,  0.0728,  0.0446,  0.0521,  0.0665,  0.0541,  0.0276,
         0.0559,  0.0482,  0.0493,  0.0651,  0.0402,  0.0565,  0.0727,  0.0895,
         0.0716,  0.0831,  0.0444,  0.0295,  0.0694,  0.0516,  0.0485,  0.0480,
         0.0695,  0.0417,  0.0925,  0.0488,  0.0838,  0.0387,  0.0700,  0.0493,
         0.0637,  0.0518,  0.0791,  0.0370,  0.0826,  0.0446,  0.0766,  0.0756,
         0.0699,  0.0559,  0.0804,  0.0839,  0.0225,  0.0815,  0.0799,  0.0465,
         0.0427,  0.0228,  0.0502,  0.0747,  0.0591,  0.0776,  0.0551,  0.0704,
         0.0689,  0.0614,  0.0183,  0.0581,  0.0463,  0.0566,  0.0585,  0.0424,
         0.0586,  0.0107,  0.0742,  0.0393,  0.0688,  0.0787,  0.0496,  0.0635,
         0.0597,  0.0567,  0.0093,  0.0205,  0.0545,  0.0378,  0.0535,  0.0569,
         0.0492,  0.0581,  0.0655,  0.0238,  0.0523,  0.0356,  0.0422,  0.0678,
         0.0898,  0.0876,  0.0927,  0.0873,  0.0913,  0.1002,  0.0771,  0.0670,
         0.0253,  0.0606,  0.0804,  0.0534,  0.0971,  0.0544,  0.0889,  0.0378,
         0.0921,  0.0947,  0.0803,  0.0548,  0.0863,  0.0632,  0.0449,  0.0907,
         0.0959, -0.0427,  0.0674,  0.0306,  0.0589, -0.0389,  0.0868, -0.0540,
         0.0937, -0.0588,  0.1002,  0.0802,  0.0872,  0.0655,  0.0283,  0.0679,
         0.0638,  0.0811,  0.1005,  0.0876, -0.0465,  0.0935,  0.1006,  0.0379,
         0.0758,  0.0899,  0.0733,  0.0393,  0.0872,  0.0451,  0.0852, -0.0363,
         0.0884,  0.1017, -0.0169,  0.0927,  0.1052, -0.0454, -0.0369,  0.0730,
         0.0900, -0.0102,  0.0797,  0.0997,  0.0760,  0.0480,  0.0928,  0.0847,
         0.0953,  0.0814,  0.0897,  0.0969,  0.0913,  0.0708,  0.0529, -0.0554,
         0.0832,  0.0723,  0.1003,  0.0909,  0.0840,  0.0454,  0.0895,  0.0963,
         0.0747,  0.0271,  0.0971, -0.0363,  0.0850,  0.0760,  0.1018,  0.0949,
         0.0937,  0.0927,  0.0823,  0.0929,  0.0847,  0.1011,  0.0897,  0.0783,
         0.0852,  0.0882,  0.0907,  0.0505,  0.0682,  0.0682,  0.0571,  0.0946,
         0.0804,  0.0828,  0.0733,  0.0941,  0.0636,  0.0912, -0.0241,  0.0834,
         0.0745,  0.0700, -0.0271,  0.1019,  0.0817,  0.0822, -0.0253,  0.0828,
         0.0884,  0.0726,  0.0594,  0.0985,  0.0965,  0.0947,  0.0770,  0.0826,
         0.0947,  0.0924,  0.0902,  0.0762,  0.0889,  0.0851,  0.0612,  0.0841,
         0.0917,  0.0684,  0.0885,  0.0816,  0.0948,  0.0927,  0.0823,  0.0877,
         0.0857,  0.0976,  0.0909,  0.0600,  0.0957,  0.0903,  0.0790,  0.0826,
         0.0940,  0.0818,  0.0742,  0.0837,  0.0968,  0.0910,  0.0539,  0.0935,
         0.0958,  0.0813,  0.0940,  0.0949,  0.0309,  0.1036,  0.0925,  0.0528,
         0.0937,  0.0625,  0.0833,  0.0813,  0.0883,  0.0829,  0.0793,  0.0950,
         0.0913,  0.0983,  0.0877,  0.0917,  0.0887,  0.0729,  0.0888,  0.0440,
         0.0941,  0.0872,  0.0932,  0.0752,  0.0911,  0.0797,  0.0490,  0.0816,
         0.0775,  0.0881,  0.0929,  0.0792,  0.0796,  0.0808,  0.0957,  0.0856,
         0.0761,  0.0896,  0.0981,  0.0824,  0.0925,  0.0850,  0.0884,  0.0963,
         0.0859,  0.0826,  0.0870,  0.0790,  0.0555,  0.0850,  0.0764,  0.0693,
         0.0847,  0.0698,  0.0883,  0.0875,  0.0389,  0.1000,  0.0810,  0.0913,
         0.0764,  0.0439,  0.0904,  0.0911,  0.1019,  0.0970,  0.0866,  0.0603,
         0.0733,  0.0839,  0.0824,  0.0422,  0.0935,  0.0920,  0.0968,  0.0843,
         0.0854,  0.0803,  0.0903,  0.0231,  0.0942,  0.0836,  0.0787,  0.0943],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.7337, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:32,702 :: INFO :: Epoch 35: loss tensor(320.3222, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0772,  0.0692,  0.0771,  0.0960,  0.0601,  0.0546,  0.0825,  0.0974,
         0.0549,  0.0826,  0.0680,  0.0584,  0.0632,  0.0828,  0.0799,  0.0796,
         0.0635,  0.0928,  0.0648,  0.0961,  0.0762,  0.0744,  0.0864,  0.0748,
         0.0704,  0.0622,  0.0581,  0.0806,  0.1042,  0.0732,  0.0842,  0.0948,
         0.0775,  0.0786,  0.0411,  0.0066,  0.0643,  0.0759,  0.0479,  0.0971,
         0.0436,  0.0520,  0.0865,  0.0548,  0.0615,  0.0776,  0.0650,  0.0424,
         0.0724,  0.0561,  0.0588,  0.0753,  0.0516,  0.0642,  0.0904,  0.1050,
         0.0881,  0.1014,  0.0599,  0.0446,  0.0789,  0.0651,  0.0641,  0.0591,
         0.0822,  0.0533,  0.1076,  0.0622,  0.0963,  0.0524,  0.0801,  0.0635,
         0.0709,  0.0692,  0.0926,  0.0479,  0.0971,  0.0602,  0.0877,  0.0904,
         0.0865,  0.0753,  0.0940,  0.1013,  0.0346,  0.0976,  0.0898,  0.0603,
         0.0545,  0.0324,  0.0629,  0.0893,  0.0709,  0.0890,  0.0727,  0.0851,
         0.0788,  0.0748,  0.0314,  0.0697,  0.0617,  0.0658,  0.0706,  0.0591,
         0.0754,  0.0249,  0.0865,  0.0497,  0.0813,  0.0929,  0.0671,  0.0743,
         0.0748,  0.0697,  0.0199,  0.0308,  0.0671,  0.0540,  0.0620,  0.0716,
         0.0615,  0.0709,  0.0813,  0.0331,  0.0654,  0.0485,  0.0562,  0.0825,
         0.1042,  0.1013,  0.1087,  0.0953,  0.1016,  0.1165,  0.0912,  0.0831,
         0.0419,  0.0629,  0.0946,  0.0689,  0.1133,  0.0703,  0.1060,  0.0570,
         0.1090,  0.1088,  0.0899,  0.0740,  0.1031,  0.0810,  0.0616,  0.1090,
         0.1109, -0.0455,  0.0816,  0.0480,  0.0750, -0.0391,  0.0967, -0.0569,
         0.1061, -0.0636,  0.1157,  0.0957,  0.0994,  0.0669,  0.0415,  0.0835,
         0.0805,  0.0952,  0.1179,  0.1000, -0.0487,  0.1097,  0.1176,  0.0552,
         0.0817,  0.1032,  0.0878,  0.0524,  0.1021,  0.0619,  0.0941, -0.0355,
         0.1016,  0.1195, -0.0116,  0.1092,  0.1238, -0.0480, -0.0327,  0.0895,
         0.1022, -0.0021,  0.0888,  0.1167,  0.0949,  0.0475,  0.1071,  0.1025,
         0.1117,  0.1003,  0.1025,  0.1078,  0.1071,  0.0903,  0.0675, -0.0600,
         0.0921,  0.0874,  0.1177,  0.1032,  0.0876,  0.0602,  0.1035,  0.1154,
         0.0907,  0.0407,  0.1122, -0.0358,  0.1023,  0.0909,  0.1190,  0.1115,
         0.1061,  0.1115,  0.0946,  0.1120,  0.0965,  0.1182,  0.1025,  0.0843,
         0.0952,  0.0941,  0.1025,  0.0691,  0.0848,  0.0839,  0.0585,  0.1091,
         0.0950,  0.0999,  0.0920,  0.1078,  0.0805,  0.1071, -0.0195,  0.0991,
         0.0899,  0.0866, -0.0202,  0.1192,  0.1005,  0.1008, -0.0191,  0.0974,
         0.1087,  0.0922,  0.0803,  0.1145,  0.1146,  0.1148,  0.0949,  0.1002,
         0.1151,  0.1122,  0.1085,  0.0938,  0.1050,  0.0997,  0.0771,  0.1021,
         0.1109,  0.0864,  0.1048,  0.0994,  0.1142,  0.1107,  0.0986,  0.1044,
         0.1029,  0.1187,  0.1075,  0.0801,  0.1166,  0.1045,  0.0969,  0.1007,
         0.1080,  0.1001,  0.0925,  0.1016,  0.1179,  0.1061,  0.0692,  0.1098,
         0.1132,  0.0992,  0.1087,  0.1110,  0.0497,  0.1238,  0.1129,  0.0739,
         0.1130,  0.0833,  0.0972,  0.0993,  0.1051,  0.1018,  0.0984,  0.1150,
         0.1107,  0.1170,  0.1057,  0.1078,  0.1057,  0.0941,  0.1060,  0.0633,
         0.1104,  0.1072,  0.1098,  0.0932,  0.1104,  0.0952,  0.0694,  0.1009,
         0.0938,  0.1055,  0.1101,  0.0967,  0.0978,  0.0985,  0.1149,  0.0989,
         0.0896,  0.1070,  0.1177,  0.0960,  0.1097,  0.1023,  0.1057,  0.1150,
         0.1027,  0.0944,  0.1014,  0.0977,  0.0734,  0.1029,  0.0930,  0.0876,
         0.1024,  0.0816,  0.1068,  0.1042,  0.0590,  0.1181,  0.0975,  0.1088,
         0.0956,  0.0645,  0.1107,  0.1121,  0.1200,  0.1128,  0.1046,  0.0793,
         0.0892,  0.1017,  0.1005,  0.0626,  0.1095,  0.1083,  0.1176,  0.1048,
         0.1004,  0.0921,  0.1063,  0.0414,  0.1106,  0.1017,  0.0976,  0.1109],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.9451, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:34,686 :: INFO :: Epoch 40: loss tensor(318.8211, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0887,  0.0768,  0.0926,  0.1089,  0.0712,  0.0703,  0.0959,  0.1093,
         0.0674,  0.0920,  0.0837,  0.0690,  0.0729,  0.0954,  0.0900,  0.0877,
         0.0694,  0.1033,  0.0700,  0.1105,  0.0863,  0.0800,  0.0988,  0.0908,
         0.0827,  0.0711,  0.0622,  0.0930,  0.1213,  0.0810,  0.0964,  0.1029,
         0.0909,  0.0884,  0.0529,  0.0148,  0.0702,  0.0887,  0.0579,  0.1103,
         0.0568,  0.0645,  0.0990,  0.0616,  0.0684,  0.0859,  0.0731,  0.0559,
         0.0881,  0.0613,  0.0661,  0.0815,  0.0614,  0.0671,  0.1073,  0.1190,
         0.1037,  0.1191,  0.0729,  0.0584,  0.0844,  0.0760,  0.0786,  0.0679,
         0.0921,  0.0606,  0.1217,  0.0742,  0.1066,  0.0646,  0.0870,  0.0767,
         0.0742,  0.0860,  0.1033,  0.0550,  0.1088,  0.0737,  0.0953,  0.1030,
         0.1018,  0.0944,  0.1052,  0.1174,  0.0447,  0.1119,  0.0969,  0.0723,
         0.0641,  0.0385,  0.0735,  0.1024,  0.0800,  0.0977,  0.0892,  0.0983,
         0.0859,  0.0863,  0.0433,  0.0786,  0.0760,  0.0720,  0.0796,  0.0741,
         0.0906,  0.0387,  0.0945,  0.0576,  0.0911,  0.1044,  0.0840,  0.0829,
         0.0890,  0.0798,  0.0288,  0.0374,  0.0774,  0.0702,  0.0671,  0.0840,
         0.0713,  0.0809,  0.0955,  0.0405,  0.0758,  0.0593,  0.0684,  0.0956,
         0.1176,  0.1139,  0.1234,  0.1005,  0.1088,  0.1316,  0.1037,  0.0979,
         0.0581,  0.0646,  0.1074,  0.0829,  0.1276,  0.0851,  0.1214,  0.0761,
         0.1247,  0.1214,  0.0960,  0.0922,  0.1185,  0.0976,  0.0773,  0.1260,
         0.1241, -0.0476,  0.0942,  0.0647,  0.0901, -0.0383,  0.1043, -0.0582,
         0.1174, -0.0672,  0.1287,  0.1097,  0.1102,  0.0640,  0.0541,  0.0979,
         0.0959,  0.1079,  0.1341,  0.1106, -0.0498,  0.1248,  0.1335,  0.0718,
         0.0855,  0.1147,  0.1007,  0.0651,  0.1154,  0.0777,  0.0986, -0.0336,
         0.1130,  0.1361, -0.0058,  0.1237,  0.1414, -0.0499, -0.0264,  0.1044,
         0.1126,  0.0066,  0.0951,  0.1325,  0.1127,  0.0451,  0.1201,  0.1189,
         0.1268,  0.1177,  0.1138,  0.1145,  0.1210,  0.1086,  0.0808, -0.0635,
         0.0986,  0.1011,  0.1331,  0.1134,  0.0877,  0.0739,  0.1157,  0.1333,
         0.1050,  0.0537,  0.1252, -0.0343,  0.1186,  0.1038,  0.1349,  0.1275,
         0.1154,  0.1289,  0.1052,  0.1300,  0.1048,  0.1340,  0.1130,  0.0875,
         0.1039,  0.0943,  0.1125,  0.0865,  0.1001,  0.0984,  0.0604,  0.1208,
         0.1079,  0.1158,  0.1097,  0.1208,  0.0963,  0.1220, -0.0139,  0.1133,
         0.1040,  0.1020, -0.0116,  0.1355,  0.1180,  0.1182, -0.0117,  0.1102,
         0.1285,  0.1109,  0.1006,  0.1290,  0.1321,  0.1343,  0.1117,  0.1167,
         0.1352,  0.1318,  0.1260,  0.1107,  0.1197,  0.1127,  0.0920,  0.1191,
         0.1297,  0.1036,  0.1196,  0.1162,  0.1334,  0.1278,  0.1137,  0.1198,
         0.1188,  0.1393,  0.1226,  0.0997,  0.1371,  0.1165,  0.1138,  0.1178,
         0.1207,  0.1176,  0.1099,  0.1184,  0.1388,  0.1198,  0.0830,  0.1251,
         0.1294,  0.1167,  0.1214,  0.1258,  0.0687,  0.1436,  0.1330,  0.0945,
         0.1317,  0.1035,  0.1091,  0.1170,  0.1206,  0.1200,  0.1167,  0.1347,
         0.1295,  0.1353,  0.1233,  0.1225,  0.1220,  0.1147,  0.1221,  0.0824,
         0.1253,  0.1265,  0.1251,  0.1103,  0.1292,  0.1094,  0.0895,  0.1197,
         0.1091,  0.1217,  0.1263,  0.1130,  0.1150,  0.1149,  0.1335,  0.1099,
         0.1007,  0.1233,  0.1369,  0.1070,  0.1258,  0.1183,  0.1219,  0.1329,
         0.1184,  0.1041,  0.1142,  0.1157,  0.0907,  0.1199,  0.1081,  0.1052,
         0.1191,  0.0916,  0.1243,  0.1194,  0.0790,  0.1359,  0.1128,  0.1253,
         0.1146,  0.0850,  0.1306,  0.1327,  0.1376,  0.1271,  0.1219,  0.0981,
         0.1042,  0.1183,  0.1177,  0.0825,  0.1237,  0.1231,  0.1381,  0.1246,
         0.1138,  0.1020,  0.1204,  0.0602,  0.1254,  0.1188,  0.1158,  0.1264],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.0627, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:34,686 :: INFO :: ----- frontend -----
2023-04-11 14:30:34,686 :: INFO :: Environment 0
2023-04-11 14:30:49,484 :: INFO :: Epoch 5: loss tensor(837.3739, device='cuda:0'), U.norm 14.64841365814209, V.norm 18.28533935546875, MLP.norm 1.8741796016693115
2023-04-11 14:30:49,577 :: INFO :: Epoch 10: loss tensor(825.1317, device='cuda:0'), U.norm 12.043856620788574, V.norm 17.775676727294922, MLP.norm 2.653385877609253
2023-04-11 14:30:49,655 :: INFO :: Epoch 15: loss tensor(808.0425, device='cuda:0'), U.norm 10.381181716918945, V.norm 17.547607421875, MLP.norm 3.6683878898620605
2023-04-11 14:30:49,765 :: INFO :: Epoch 20: loss tensor(787.5581, device='cuda:0'), U.norm 9.188216209411621, V.norm 17.401762008666992, MLP.norm 4.728372573852539
2023-04-11 14:30:49,765 :: INFO :: Environment 1
2023-04-11 14:31:02,530 :: INFO :: Epoch 5: loss tensor(861.1701, device='cuda:0'), U.norm 14.648165702819824, V.norm 18.32138442993164, MLP.norm 1.8677492141723633
2023-04-11 14:31:02,624 :: INFO :: Epoch 10: loss tensor(846.8743, device='cuda:0'), U.norm 12.044111251831055, V.norm 17.82631492614746, MLP.norm 2.6577627658843994
2023-04-11 14:31:02,766 :: INFO :: Epoch 15: loss tensor(826.2785, device='cuda:0'), U.norm 10.38238525390625, V.norm 17.5998592376709, MLP.norm 3.688204765319824
2023-04-11 14:31:02,874 :: INFO :: Epoch 20: loss tensor(801.5701, device='cuda:0'), U.norm 9.190545082092285, V.norm 17.452880859375, MLP.norm 4.758709907531738
2023-04-11 14:31:02,874 :: INFO :: Environment 2
2023-04-11 14:31:16,671 :: INFO :: Epoch 5: loss tensor(854.0235, device='cuda:0'), U.norm 14.648995399475098, V.norm 18.31740951538086, MLP.norm 1.8660005331039429
2023-04-11 14:31:16,780 :: INFO :: Epoch 10: loss tensor(840.2184, device='cuda:0'), U.norm 12.045333862304688, V.norm 17.817676544189453, MLP.norm 2.6497957706451416
2023-04-11 14:31:16,874 :: INFO :: Epoch 15: loss tensor(820.2673, device='cuda:0'), U.norm 10.384086608886719, V.norm 17.59027862548828, MLP.norm 3.668842315673828
2023-04-11 14:31:16,968 :: INFO :: Epoch 20: loss tensor(796.3965, device='cuda:0'), U.norm 9.192893981933594, V.norm 17.443634033203125, MLP.norm 4.7335357666015625
2023-04-11 14:31:16,968 :: INFO :: Environment 3
2023-04-11 14:31:31,905 :: INFO :: Epoch 5: loss tensor(811.7056, device='cuda:0'), U.norm 14.647021293640137, V.norm 18.2384033203125, MLP.norm 1.864817500114441
2023-04-11 14:31:32,014 :: INFO :: Epoch 10: loss tensor(799.7125, device='cuda:0'), U.norm 12.040950775146484, V.norm 17.708337783813477, MLP.norm 2.6159827709198
2023-04-11 14:31:32,108 :: INFO :: Epoch 15: loss tensor(782.2836, device='cuda:0'), U.norm 10.376625061035156, V.norm 17.46896743774414, MLP.norm 3.5978598594665527
2023-04-11 14:31:32,202 :: INFO :: Epoch 20: loss tensor(761.2525, device='cuda:0'), U.norm 9.181694984436035, V.norm 17.31846046447754, MLP.norm 4.632116317749023
2023-04-11 14:31:32,217 :: INFO :: Environment 4
2023-04-11 14:31:46,327 :: INFO :: Epoch 5: loss tensor(826.6943, device='cuda:0'), U.norm 14.648125648498535, V.norm 18.258237838745117, MLP.norm 1.865452527999878
2023-04-11 14:31:46,436 :: INFO :: Epoch 10: loss tensor(814.0945, device='cuda:0'), U.norm 12.043144226074219, V.norm 17.735687255859375, MLP.norm 2.6282148361206055
2023-04-11 14:31:46,514 :: INFO :: Epoch 15: loss tensor(795.7532, device='cuda:0'), U.norm 10.380337715148926, V.norm 17.500818252563477, MLP.norm 3.630688190460205
2023-04-11 14:31:46,608 :: INFO :: Epoch 20: loss tensor(773.1682, device='cuda:0'), U.norm 9.187175750732422, V.norm 17.3480224609375, MLP.norm 4.692960262298584
2023-04-11 14:31:46,608 :: INFO :: Environment 5
2023-04-11 14:32:01,108 :: INFO :: Epoch 5: loss tensor(879.9066, device='cuda:0'), U.norm 14.648436546325684, V.norm 18.36918067932129, MLP.norm 1.8873140811920166
2023-04-11 14:32:01,186 :: INFO :: Epoch 10: loss tensor(864.7968, device='cuda:0'), U.norm 12.044864654541016, V.norm 17.892934799194336, MLP.norm 2.702763557434082
2023-04-11 14:32:01,280 :: INFO :: Epoch 15: loss tensor(843.5372, device='cuda:0'), U.norm 10.383983612060547, V.norm 17.67953872680664, MLP.norm 3.747802495956421
2023-04-11 14:32:01,358 :: INFO :: Epoch 20: loss tensor(817.6461, device='cuda:0'), U.norm 9.193202018737793, V.norm 17.542457580566406, MLP.norm 4.8395233154296875
2023-04-11 14:32:01,373 :: INFO :: Environment 6
2023-04-11 14:32:15,123 :: INFO :: Epoch 5: loss tensor(921.8500, device='cuda:0'), U.norm 14.650724411010742, V.norm 18.439367294311523, MLP.norm 1.8718888759613037
2023-04-11 14:32:15,202 :: INFO :: Epoch 10: loss tensor(906.0626, device='cuda:0'), U.norm 12.04949951171875, V.norm 17.987287521362305, MLP.norm 2.6966753005981445
2023-04-11 14:32:15,311 :: INFO :: Epoch 15: loss tensor(883.6223, device='cuda:0'), U.norm 10.39145565032959, V.norm 17.7822265625, MLP.norm 3.790344715118408
2023-04-11 14:32:15,405 :: INFO :: Epoch 20: loss tensor(855.9398, device='cuda:0'), U.norm 9.204073905944824, V.norm 17.649797439575195, MLP.norm 4.943329811096191
2023-04-11 14:32:15,405 :: INFO :: Environment 7
2023-04-11 14:32:29,608 :: INFO :: Epoch 5: loss tensor(885.1347, device='cuda:0'), U.norm 14.649225234985352, V.norm 18.369796752929688, MLP.norm 1.8714370727539062
2023-04-11 14:32:29,686 :: INFO :: Epoch 10: loss tensor(871.6733, device='cuda:0'), U.norm 12.045918464660645, V.norm 17.89261817932129, MLP.norm 2.656594753265381
2023-04-11 14:32:29,780 :: INFO :: Epoch 15: loss tensor(852.6095, device='cuda:0'), U.norm 10.385329246520996, V.norm 17.6790714263916, MLP.norm 3.682591438293457
2023-04-11 14:32:29,874 :: INFO :: Epoch 20: loss tensor(828.5251, device='cuda:0'), U.norm 9.194942474365234, V.norm 17.542545318603516, MLP.norm 4.761242389678955
2023-04-11 14:32:29,874 :: INFO :: Environment 8
2023-04-11 14:32:44,202 :: INFO :: Epoch 5: loss tensor(799.2241, device='cuda:0'), U.norm 14.644018173217773, V.norm 18.219623565673828, MLP.norm 1.8431288003921509
2023-04-11 14:32:44,296 :: INFO :: Epoch 10: loss tensor(787.6042, device='cuda:0'), U.norm 12.03520393371582, V.norm 17.67987060546875, MLP.norm 2.5780513286590576
2023-04-11 14:32:44,405 :: INFO :: Epoch 15: loss tensor(769.8510, device='cuda:0'), U.norm 10.367904663085938, V.norm 17.437480926513672, MLP.norm 3.5602660179138184
2023-04-11 14:32:44,515 :: INFO :: Epoch 20: loss tensor(748.2712, device='cuda:0'), U.norm 9.16986083984375, V.norm 17.282888412475586, MLP.norm 4.615426063537598
2023-04-11 14:32:44,515 :: INFO :: Environment 9
2023-04-11 14:32:58,811 :: INFO :: Epoch 5: loss tensor(825.6633, device='cuda:0'), U.norm 14.646759033203125, V.norm 18.258115768432617, MLP.norm 1.8592575788497925
2023-04-11 14:32:58,952 :: INFO :: Epoch 10: loss tensor(813.1041, device='cuda:0'), U.norm 12.040780067443848, V.norm 17.735380172729492, MLP.norm 2.616558790206909
2023-04-11 14:32:59,046 :: INFO :: Epoch 15: loss tensor(794.9452, device='cuda:0'), U.norm 10.376705169677734, V.norm 17.499563217163086, MLP.norm 3.624211311340332
2023-04-11 14:32:59,139 :: INFO :: Epoch 20: loss tensor(773.3002, device='cuda:0'), U.norm 9.181949615478516, V.norm 17.348247528076172, MLP.norm 4.686493396759033
2023-04-11 14:32:59,139 :: INFO :: Ite = 1, Delta = 4041
2023-04-11 14:32:59,139 :: INFO :: ----- backend -----
2023-04-11 14:33:01,843 :: INFO :: Epoch 5: loss tensor(178.3159, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.0109e-02, -5.3789e-03, -1.1551e-02, -3.3010e-03, -5.3627e-03,
        -1.3667e-02, -9.4143e-03,  6.1969e-03, -9.7752e-03, -2.5624e-03,
        -1.5026e-02, -5.0446e-03, -8.2564e-03,  8.4976e-04,  6.3391e-03,
         5.8553e-03,  1.5487e-03,  2.9528e-03,  1.0903e-03, -8.6810e-03,
        -4.3216e-03, -6.1156e-03,  5.6020e-03, -1.3440e-02, -2.3170e-03,
        -2.2048e-03, -3.5426e-03, -7.2161e-03, -1.1287e-02, -7.4318e-03,
         3.1785e-04,  1.0938e-02, -7.5638e-03, -9.6145e-03, -1.1589e-02,
        -1.2844e-02,  4.6178e-03,  2.6058e-03, -6.7010e-03, -5.5813e-03,
        -1.0416e-02, -1.1726e-02, -2.7023e-03,  8.2256e-03, -7.1149e-04,
        -3.0188e-03, -6.0562e-03, -9.3504e-03, -8.8089e-03, -3.9696e-03,
        -1.0415e-02, -2.1741e-03, -2.0301e-03, -1.6262e-03, -1.3219e-02,
         8.6010e-03, -5.6215e-03, -8.3584e-03, -1.1604e-02, -1.5078e-02,
        -6.8350e-04, -4.7010e-03, -1.1199e-02, -5.8217e-03, -4.1301e-03,
        -7.7324e-03, -1.1079e-03,  4.6037e-03,  9.7140e-03, -1.1321e-02,
        -1.8737e-03, -1.1191e-02,  2.3049e-03, -4.0067e-03,  8.2381e-07,
        -1.0216e-02,  1.1110e-02, -1.2242e-02,  4.5298e-06,  9.0573e-03,
        -7.4855e-03, -1.4069e-02,  5.1840e-03, -3.2884e-03, -1.2514e-02,
        -8.6021e-03,  4.1890e-03, -8.5202e-03, -7.1130e-03, -1.3239e-02,
        -8.3758e-03,  1.4770e-03, -6.9069e-03,  4.4089e-03, -1.1697e-02,
        -8.1453e-03, -1.6659e-03, -7.0227e-03, -1.2453e-02, -8.3518e-03,
        -9.1697e-03, -4.4614e-04,  1.4041e-03, -1.2842e-02, -1.3038e-02,
        -1.2777e-02,  4.6913e-03, -6.9427e-03, -1.0620e-03,  1.0550e-03,
        -1.3403e-02,  3.0343e-03, -1.2996e-02, -2.7379e-06, -1.1871e-02,
        -9.7307e-03, -9.3538e-03, -1.1637e-02,  1.4345e-03, -1.1429e-02,
        -6.6025e-03, -5.2601e-03, -7.1819e-03, -3.5711e-03,  7.3269e-04,
        -2.1809e-03, -9.4232e-03, -3.3272e-03,  1.0197e-02,  7.0805e-03,
        -1.3818e-03,  2.0493e-02,  6.1896e-03,  1.6040e-02, -3.4731e-03,
        -1.4785e-02, -1.7640e-02,  1.7341e-02, -4.8891e-03, -1.5493e-02,
         1.3463e-02, -1.6770e-02,  3.8685e-03, -4.2104e-03,  8.1526e-04,
         2.0226e-02,  7.0827e-03, -1.3007e-02,  2.7709e-04, -1.0316e-02,
        -1.7323e-02, -2.8250e-03,  1.0242e-02, -1.8173e-02, -1.4104e-02,
        -1.7476e-02, -1.6435e-02, -1.8358e-02,  2.0172e-02, -1.5049e-02,
         2.0059e-02, -1.5412e-02,  1.6989e-02, -1.2116e-02,  1.9680e-02,
         1.5533e-02, -1.7043e-02, -1.4692e-02, -1.5975e-02, -1.7247e-03,
         1.3760e-02,  1.2885e-02, -1.7898e-02,  4.0827e-03,  1.1792e-02,
        -1.7416e-02,  1.9873e-02,  1.3968e-02, -1.3472e-02, -1.3312e-02,
         1.5879e-03, -1.7051e-02,  1.6872e-02, -1.8138e-02,  1.0332e-02,
         1.4050e-02, -1.8191e-02,  7.7799e-03,  2.0194e-02, -1.8279e-02,
        -1.4647e-02, -1.4667e-02,  1.9930e-02, -1.8294e-02,  6.2737e-03,
         1.3914e-02, -3.2734e-03,  1.9797e-02,  2.9145e-03, -4.0821e-03,
         7.9845e-03,  1.7499e-03,  9.2688e-03,  1.9024e-02,  1.6497e-03,
        -5.4646e-03, -1.5330e-02, -1.7132e-02,  1.7565e-02, -1.1426e-02,
         1.9826e-02,  1.9827e-02,  1.6880e-02, -1.7338e-02,  1.1324e-03,
         1.0137e-02, -1.2120e-02, -1.6780e-02,  1.6566e-02, -1.8449e-02,
        -1.0257e-03, -1.2197e-02,  1.5382e-02,  4.5399e-03,  8.9589e-03,
         7.2708e-03,  7.2188e-03,  2.1808e-03,  6.6102e-03,  1.6666e-02,
         7.2758e-03,  1.6988e-02,  2.0346e-02,  1.5619e-02,  2.0094e-02,
        -1.1171e-02, -1.5211e-02, -1.5033e-02,  1.5026e-02,  1.5344e-02,
        -3.6101e-03, -9.9600e-03, -1.3360e-02,  1.6915e-02, -1.6541e-02,
        -1.5138e-03, -1.8331e-02, -7.9686e-03, -1.3231e-02, -1.5291e-02,
        -1.5109e-02,  2.0246e-02, -5.9538e-03, -4.6301e-04, -1.8358e-02,
        -2.7145e-03, -6.2798e-03, -2.6251e-03, -2.4428e-03, -2.1736e-03,
        -1.3636e-03, -4.9260e-03, -1.6524e-02, -1.5237e-02, -1.4752e-03,
        -3.8777e-03,  2.6151e-03, -1.0711e-02, -4.1931e-03, -4.1576e-03,
        -6.9265e-03, -1.4195e-02, -7.6338e-03, -8.1459e-03, -8.4953e-03,
        -1.0053e-02, -7.4517e-04, -3.3869e-03, -1.1566e-03, -7.2471e-03,
        -1.1389e-02,  4.7958e-03, -5.5606e-03, -7.8290e-03, -1.5805e-03,
         4.6225e-03, -1.6387e-02, -1.2658e-02,  7.2728e-03, -1.6145e-02,
        -6.7151e-03, -1.4419e-02, -3.9815e-03, -7.7467e-03, -7.7183e-03,
         1.9788e-03, -4.5653e-03, -1.3127e-02,  2.8653e-03, -4.9272e-03,
        -1.0932e-02,  1.6260e-02,  2.0321e-03, -8.8855e-03,  2.3146e-03,
        -1.0923e-03, -6.4662e-03, -7.6068e-03, -2.6644e-03, -5.0406e-03,
        -1.1431e-03, -1.0268e-02, -2.9302e-04,  8.3295e-04, -6.8072e-03,
        -8.6749e-03, -6.0675e-03,  4.7876e-03, -1.4883e-02, -1.1096e-02,
        -6.5403e-03, -6.7512e-03, -5.2712e-03, -8.1519e-03, -2.1377e-03,
         2.3389e-03, -7.8683e-03, -1.3050e-02, -8.2588e-03, -1.3198e-02,
        -1.2880e-02, -1.0979e-02, -1.6080e-02, -1.5748e-02, -3.7666e-03,
        -1.2718e-03, -8.6313e-03, -1.4033e-02, -5.0999e-03, -4.3848e-03,
        -1.1332e-02, -1.3527e-02, -1.3732e-02, -9.9297e-04, -7.1592e-03,
         4.4571e-03, -4.7867e-03, -6.2572e-03, -7.9848e-03, -1.6487e-02,
        -1.0538e-02, -1.1104e-02, -1.4774e-02, -6.9583e-03, -6.5200e-03,
        -1.1361e-02, -8.8821e-03,  1.9815e-03, -7.1335e-03, -1.0170e-02,
        -5.2787e-03, -7.6266e-03,  1.1377e-02, -1.1341e-02, -2.0486e-03,
         3.0597e-03, -1.1625e-02, -5.1126e-03, -1.2894e-04, -1.5569e-02,
        -1.6541e-02, -1.4463e-02, -8.4726e-04, -7.0820e-03,  1.1865e-03,
         1.1203e-02,  8.4139e-04,  1.0956e-02, -7.1157e-03, -1.1946e-02,
        -9.0046e-03, -1.6086e-02, -1.5984e-02,  4.8168e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.0482, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:04,577 :: INFO :: Epoch 10: loss tensor(173.0975, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.1296e-02,  1.2261e-04, -2.0442e-02,  1.5812e-03,  4.7906e-03,
        -2.1632e-02, -9.1328e-03,  1.7332e-02, -9.8288e-03,  1.9252e-04,
        -2.4100e-02, -3.2558e-03, -1.7211e-04,  5.9756e-03,  1.2108e-02,
         1.3118e-02,  9.2451e-03,  1.1165e-02, -9.2519e-05, -1.0972e-02,
        -2.4681e-03, -8.0223e-03,  1.6838e-02, -1.9946e-02, -7.2758e-05,
         7.9527e-03, -9.2895e-04, -6.4052e-03, -1.6398e-02, -7.4651e-03,
         1.3059e-02,  2.0355e-02, -9.4520e-03, -1.3473e-02, -1.6700e-02,
        -2.0883e-02,  1.1017e-02,  1.7586e-02, -8.3538e-04, -8.2920e-03,
        -1.2245e-02, -1.3862e-02, -4.8228e-04,  1.9910e-02,  7.8187e-03,
        -1.0425e-03, -4.4480e-03, -1.1920e-02, -1.1508e-02, -2.6507e-03,
        -1.4110e-02,  2.0938e-03, -1.6400e-03,  3.4784e-03, -2.1884e-02,
         1.9990e-02, -2.2289e-03, -1.1722e-02, -2.0631e-02, -2.3821e-02,
         4.9641e-03, -6.6145e-03, -1.6509e-02, -2.5743e-03, -4.3595e-03,
        -8.5539e-03,  7.4696e-04,  6.5330e-03,  2.0339e-02, -1.5625e-02,
        -2.6299e-03, -1.5503e-02,  1.6076e-03,  3.4608e-03,  1.0554e-02,
        -1.6855e-02,  1.4087e-02, -2.1452e-02,  7.9462e-03,  1.0447e-02,
         4.1816e-03, -2.1704e-02,  2.0407e-02,  3.9468e-03, -1.8501e-02,
        -8.1377e-03,  9.1378e-03, -1.3675e-02, -5.0934e-03, -1.9446e-02,
        -3.8880e-03,  1.5361e-02, -6.0497e-03,  1.5427e-02, -2.0540e-02,
        -1.9809e-03,  3.6897e-03, -1.1322e-02, -1.7367e-02, -1.1228e-02,
        -1.4874e-02,  8.1505e-03,  1.5979e-02, -1.4604e-02, -2.2800e-02,
        -2.2232e-02,  7.9212e-03, -7.4967e-03,  5.9037e-03,  1.0079e-02,
        -2.3753e-02,  1.1283e-02, -1.9961e-02,  1.1122e-03, -1.5668e-02,
        -1.6723e-02, -1.4827e-02, -1.0095e-02,  1.6307e-03, -1.8127e-02,
        -8.1751e-03,  5.3123e-04, -8.4390e-03, -7.7085e-03,  8.6482e-03,
        -4.2756e-03, -1.2536e-02,  5.3476e-03,  3.2693e-02,  2.8951e-02,
         2.0652e-02,  4.2697e-02,  2.9465e-02,  3.7902e-02,  1.7092e-02,
        -1.0905e-02, -2.5908e-02,  3.6408e-02,  1.3408e-02, -2.0094e-02,
         3.7028e-02, -2.1753e-02,  2.7524e-02,  1.0088e-02,  2.3865e-02,
         4.2361e-02,  2.9651e-02,  4.7855e-06,  2.3165e-02,  8.3329e-03,
        -2.4218e-02,  1.7614e-02,  3.3568e-02, -3.1222e-02, -9.8568e-03,
        -2.6601e-02, -1.9598e-02, -3.2352e-02,  3.8704e-02, -2.7741e-02,
         4.2229e-02, -2.8355e-02,  3.8633e-02, -3.3754e-03,  4.0218e-02,
         2.3990e-02, -2.6584e-02, -1.1771e-02, -1.9411e-02,  1.9105e-02,
         3.7477e-02,  3.4706e-02, -3.1774e-02,  2.7056e-02,  3.5531e-02,
        -2.5461e-02,  3.6230e-02,  3.6644e-02, -6.7060e-03, -1.0938e-02,
         2.4289e-02, -2.2254e-02,  3.8689e-02, -3.1828e-02,  3.2929e-02,
         3.7456e-02, -3.0489e-02,  3.1632e-02,  4.4087e-02, -3.1777e-02,
        -2.4971e-02, -9.2662e-03,  4.1319e-02, -3.0944e-02,  2.9038e-02,
         3.6911e-02,  1.8188e-02,  3.3338e-02,  2.4166e-02,  1.7952e-02,
         3.1215e-02,  2.5557e-02,  3.0733e-02,  4.0844e-02,  2.4430e-02,
         1.5261e-02, -2.0415e-02, -3.0751e-02,  3.8587e-02,  1.6432e-03,
         4.3375e-02,  4.1500e-02,  2.5322e-02, -2.4297e-02,  2.3550e-02,
         3.4019e-02,  2.6305e-04, -2.6248e-02,  3.8139e-02, -3.2226e-02,
         2.1667e-02, -8.0965e-03,  3.8890e-02,  2.6604e-02,  3.1139e-02,
         3.1280e-02,  2.9935e-02,  2.5159e-02,  2.9137e-02,  4.0164e-02,
         3.0113e-02,  3.8240e-02,  4.0751e-02,  3.4839e-02,  4.1846e-02,
         5.0243e-03, -1.1754e-02, -1.1728e-02,  3.1232e-02,  3.8035e-02,
         1.7983e-02,  7.3857e-03, -1.9287e-03,  3.8165e-02, -1.9697e-02,
         2.0107e-02, -3.1885e-02,  1.0691e-02, -4.1603e-03, -1.3453e-02,
        -2.3672e-02,  4.2512e-02,  1.4249e-02,  2.2723e-02, -3.1680e-02,
         1.9251e-02,  1.5034e-02,  1.9768e-02,  2.0095e-02,  2.0216e-02,
         2.1539e-02,  1.7387e-02, -1.7675e-02, -7.8099e-03,  2.0563e-02,
         1.8212e-02,  2.5829e-02,  4.8292e-03,  1.8021e-02,  1.8212e-02,
         1.3413e-02, -2.5637e-03,  1.2278e-02,  1.2062e-02,  1.1987e-02,
         8.1666e-03,  2.2186e-02,  1.8907e-02,  2.1309e-02,  1.3885e-02,
         5.7793e-03,  2.8427e-02,  1.6223e-02,  1.2016e-02,  2.1643e-02,
         2.7799e-02, -1.5810e-02,  1.5726e-03,  3.0449e-02, -1.2912e-02,
         1.4139e-02, -2.3289e-03,  1.8476e-02,  1.2361e-02,  1.1979e-02,
         2.4883e-02,  1.7912e-02, -2.0754e-03,  2.5773e-02,  1.6927e-02,
         6.0568e-03,  3.9981e-02,  2.5628e-02,  1.0189e-02,  2.5587e-02,
         2.1185e-02,  1.4535e-02,  1.1691e-02,  1.9901e-02,  1.6428e-02,
         2.1908e-02,  8.8239e-03,  2.2439e-02,  2.3783e-02,  1.4141e-02,
         1.2000e-02,  1.5002e-02,  2.8521e-02, -7.6609e-03,  4.5910e-03,
         1.5147e-02,  1.4138e-02,  1.6751e-02,  1.0201e-02,  2.0098e-02,
         2.5376e-02,  1.2477e-02, -2.1444e-04,  1.2505e-02, -5.1960e-04,
         1.5049e-03,  5.5905e-03, -1.3525e-02, -1.2131e-02,  1.8013e-02,
         2.1625e-02,  1.0081e-02, -2.5490e-03,  1.5439e-02,  1.7578e-02,
         6.8755e-03,  4.0873e-04, -5.1335e-03,  2.1705e-02,  1.3671e-02,
         2.7517e-02,  1.6306e-02,  1.4742e-02,  1.1091e-02, -1.9904e-02,
         8.2801e-03,  6.6254e-03, -4.3345e-03,  1.3221e-02,  1.4315e-02,
         4.5412e-03,  1.0564e-02,  2.5284e-02,  1.3926e-02,  9.1020e-03,
         1.6165e-02,  1.2648e-02,  3.5088e-02,  6.5075e-03,  2.0900e-02,
         2.5991e-02,  4.2601e-03,  1.6077e-02,  2.2787e-02, -8.2221e-03,
        -1.7010e-02, -3.1015e-03,  2.2322e-02,  1.4067e-02,  2.4704e-02,
         3.4858e-02,  2.3646e-02,  3.3847e-02,  1.3079e-02,  3.0716e-03,
         1.1139e-02, -1.4800e-02, -8.4436e-03,  2.8109e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.8928, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:06,953 :: INFO :: Epoch 15: loss tensor(168.2888, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0013,  0.0158, -0.0235,  0.0170,  0.0217, -0.0214,  0.0058,  0.0340,
         0.0015,  0.0106, -0.0246,  0.0053,  0.0142,  0.0233,  0.0271,  0.0270,
         0.0204,  0.0285,  0.0043, -0.0052,  0.0067, -0.0007,  0.0365, -0.0142,
         0.0107,  0.0231,  0.0080,  0.0042, -0.0080,  0.0014,  0.0332,  0.0368,
         0.0018, -0.0077, -0.0092, -0.0202,  0.0236,  0.0358,  0.0110, -0.0016,
        -0.0032, -0.0088,  0.0112,  0.0342,  0.0222,  0.0090,  0.0039, -0.0024,
        -0.0050,  0.0061, -0.0056,  0.0163,  0.0033,  0.0135, -0.0215,  0.0393,
         0.0108, -0.0045, -0.0215, -0.0228,  0.0162, -0.0030, -0.0112,  0.0101,
         0.0065, -0.0022,  0.0140,  0.0173,  0.0349, -0.0109,  0.0039, -0.0040,
         0.0031,  0.0166,  0.0303, -0.0155,  0.0247, -0.0229,  0.0245,  0.0179,
         0.0269, -0.0175,  0.0399,  0.0228, -0.0126,  0.0028,  0.0202, -0.0093,
         0.0047, -0.0170,  0.0082,  0.0350,  0.0013,  0.0332, -0.0206,  0.0189,
         0.0128, -0.0061, -0.0136, -0.0061, -0.0090,  0.0207,  0.0376, -0.0063,
        -0.0240, -0.0260,  0.0185,  0.0013,  0.0187,  0.0265, -0.0290,  0.0237,
        -0.0138,  0.0119, -0.0085, -0.0173, -0.0110,  0.0020,  0.0058, -0.0161,
        -0.0023,  0.0153, -0.0020, -0.0044,  0.0229, -0.0020, -0.0059,  0.0235,
         0.0549,  0.0512,  0.0448,  0.0637,  0.0529,  0.0620,  0.0401,  0.0055,
        -0.0239,  0.0465,  0.0359, -0.0136,  0.0615, -0.0150,  0.0526,  0.0339,
         0.0488,  0.0645,  0.0512,  0.0242,  0.0481,  0.0344, -0.0211,  0.0431,
         0.0574, -0.0410,  0.0072, -0.0249, -0.0110, -0.0424,  0.0567, -0.0377,
         0.0638, -0.0386,  0.0615,  0.0164,  0.0601,  0.0373, -0.0289,  0.0034,
        -0.0099,  0.0422,  0.0628,  0.0561, -0.0421,  0.0512,  0.0602, -0.0244,
         0.0484,  0.0593,  0.0120,  0.0006,  0.0483, -0.0146,  0.0593, -0.0414,
         0.0557,  0.0621, -0.0382,  0.0567,  0.0712, -0.0417, -0.0300,  0.0088,
         0.0626, -0.0384,  0.0504,  0.0606,  0.0444,  0.0426,  0.0475,  0.0433,
         0.0554,  0.0521,  0.0525,  0.0628,  0.0486,  0.0415, -0.0150, -0.0415,
         0.0581,  0.0232,  0.0681,  0.0632,  0.0469, -0.0215,  0.0478,  0.0598,
         0.0218, -0.0287,  0.0600, -0.0420,  0.0467,  0.0086,  0.0638,  0.0504,
         0.0560,  0.0574,  0.0528,  0.0505,  0.0514,  0.0645,  0.0533,  0.0574,
         0.0585,  0.0572,  0.0633,  0.0299,  0.0055,  0.0036,  0.0407,  0.0616,
         0.0421,  0.0313,  0.0208,  0.0591, -0.0105,  0.0440, -0.0407,  0.0347,
         0.0152,  0.0012, -0.0246,  0.0656,  0.0392,  0.0483, -0.0392,  0.0435,
         0.0416,  0.0453,  0.0462,  0.0452,  0.0467,  0.0439, -0.0013,  0.0154,
         0.0469,  0.0438,  0.0512,  0.0292,  0.0432,  0.0431,  0.0380,  0.0220,
         0.0376,  0.0371,  0.0375,  0.0331,  0.0477,  0.0446,  0.0461,  0.0395,
         0.0314,  0.0547,  0.0416,  0.0377,  0.0485,  0.0518,  0.0028,  0.0260,
         0.0540,  0.0076,  0.0395,  0.0223,  0.0455,  0.0368,  0.0368,  0.0493,
         0.0439,  0.0207,  0.0494,  0.0418,  0.0312,  0.0656,  0.0520,  0.0363,
         0.0511,  0.0471,  0.0391,  0.0362,  0.0449,  0.0419,  0.0478,  0.0349,
         0.0483,  0.0490,  0.0397,  0.0373,  0.0395,  0.0546,  0.0146,  0.0292,
         0.0406,  0.0402,  0.0424,  0.0352,  0.0455,  0.0499,  0.0384,  0.0239,
         0.0380,  0.0237,  0.0261,  0.0303,  0.0073,  0.0083,  0.0440,  0.0462,
         0.0335,  0.0213,  0.0407,  0.0423,  0.0327,  0.0254,  0.0172,  0.0468,
         0.0387,  0.0509,  0.0404,  0.0403,  0.0357, -0.0061,  0.0337,  0.0318,
         0.0196,  0.0370,  0.0399,  0.0295,  0.0364,  0.0505,  0.0395,  0.0348,
         0.0415,  0.0387,  0.0609,  0.0326,  0.0462,  0.0507,  0.0292,  0.0413,
         0.0477,  0.0152,  0.0009,  0.0214,  0.0474,  0.0391,  0.0513,  0.0604,
         0.0483,  0.0567,  0.0381,  0.0278,  0.0366,  0.0041,  0.0150,  0.0529],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.6706, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:09,296 :: INFO :: Epoch 20: loss tensor(165.7211, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0165,  0.0358, -0.0198,  0.0388,  0.0406, -0.0132,  0.0283,  0.0548,
         0.0191,  0.0266, -0.0168,  0.0191,  0.0304,  0.0473,  0.0488,  0.0458,
         0.0345,  0.0509,  0.0160,  0.0087,  0.0218,  0.0150,  0.0603,  0.0017,
         0.0282,  0.0410,  0.0220,  0.0220,  0.0105,  0.0171,  0.0558,  0.0584,
         0.0222,  0.0059,  0.0079, -0.0115,  0.0405,  0.0548,  0.0261,  0.0138,
         0.0129,  0.0019,  0.0296,  0.0515,  0.0400,  0.0248,  0.0174,  0.0149,
         0.0101,  0.0203,  0.0109,  0.0364,  0.0133,  0.0274, -0.0117,  0.0632,
         0.0300,  0.0122, -0.0129, -0.0130,  0.0315,  0.0070,  0.0026,  0.0287,
         0.0254,  0.0109,  0.0344,  0.0367,  0.0529,  0.0005,  0.0181,  0.0172,
         0.0095,  0.0341,  0.0538, -0.0057,  0.0434, -0.0162,  0.0457,  0.0340,
         0.0520, -0.0028,  0.0610,  0.0472,  0.0030,  0.0206,  0.0371,  0.0048,
         0.0194, -0.0072,  0.0245,  0.0578,  0.0138,  0.0545, -0.0107,  0.0435,
         0.0252,  0.0087, -0.0031,  0.0066,  0.0077,  0.0366,  0.0610,  0.0085,
        -0.0166, -0.0233,  0.0361,  0.0158,  0.0365,  0.0468, -0.0285,  0.0402,
         0.0022,  0.0301,  0.0063, -0.0105,  0.0014,  0.0197,  0.0162, -0.0062,
         0.0111,  0.0350,  0.0126,  0.0080,  0.0417,  0.0072,  0.0094,  0.0459,
         0.0758,  0.0724,  0.0685,  0.0820,  0.0738,  0.0855,  0.0623,  0.0258,
        -0.0135,  0.0517,  0.0577, -0.0008,  0.0858, -0.0020,  0.0772,  0.0610,
         0.0734,  0.0860,  0.0700,  0.0505,  0.0726,  0.0606, -0.0110,  0.0691,
         0.0806, -0.0489,  0.0273, -0.0149,  0.0037, -0.0499,  0.0737, -0.0457,
         0.0838, -0.0471,  0.0842,  0.0377,  0.0792,  0.0542, -0.0261,  0.0226,
         0.0062,  0.0642,  0.0877,  0.0763, -0.0500,  0.0749,  0.0843, -0.0161,
         0.0568,  0.0812,  0.0333,  0.0146,  0.0718,  0.0010,  0.0777, -0.0481,
         0.0774,  0.0868, -0.0430,  0.0809,  0.0983, -0.0496, -0.0292,  0.0303,
         0.0830, -0.0420,  0.0679,  0.0841,  0.0715,  0.0506,  0.0698,  0.0687,
         0.0793,  0.0792,  0.0731,  0.0824,  0.0722,  0.0689, -0.0035, -0.0505,
         0.0752,  0.0460,  0.0929,  0.0841,  0.0655, -0.0122,  0.0707,  0.0861,
         0.0450, -0.0263,  0.0818, -0.0492,  0.0717,  0.0279,  0.0887,  0.0743,
         0.0789,  0.0839,  0.0743,  0.0764,  0.0720,  0.0885,  0.0754,  0.0733,
         0.0734,  0.0762,  0.0836,  0.0563,  0.0270,  0.0227,  0.0480,  0.0847,
         0.0655,  0.0557,  0.0459,  0.0793,  0.0050,  0.0676, -0.0464,  0.0585,
         0.0367,  0.0206, -0.0179,  0.0888,  0.0651,  0.0741, -0.0413,  0.0671,
         0.0687,  0.0711,  0.0733,  0.0693,  0.0716,  0.0713,  0.0211,  0.0409,
         0.0741,  0.0696,  0.0765,  0.0537,  0.0672,  0.0665,  0.0619,  0.0479,
         0.0632,  0.0620,  0.0622,  0.0582,  0.0733,  0.0702,  0.0697,  0.0642,
         0.0572,  0.0820,  0.0663,  0.0641,  0.0763,  0.0739,  0.0266,  0.0515,
         0.0754,  0.0323,  0.0652,  0.0481,  0.0735,  0.0602,  0.0606,  0.0728,
         0.0695,  0.0448,  0.0712,  0.0656,  0.0574,  0.0920,  0.0791,  0.0635,
         0.0768,  0.0736,  0.0620,  0.0603,  0.0692,  0.0677,  0.0738,  0.0621,
         0.0743,  0.0741,  0.0652,  0.0618,  0.0634,  0.0815,  0.0392,  0.0551,
         0.0653,  0.0668,  0.0677,  0.0603,  0.0710,  0.0733,  0.0652,  0.0495,
         0.0631,  0.0491,  0.0511,  0.0553,  0.0324,  0.0327,  0.0706,  0.0690,
         0.0550,  0.0462,  0.0665,  0.0654,  0.0583,  0.0508,  0.0417,  0.0717,
         0.0634,  0.0719,  0.0631,  0.0658,  0.0600,  0.0144,  0.0590,  0.0572,
         0.0451,  0.0585,  0.0659,  0.0544,  0.0631,  0.0755,  0.0645,  0.0603,
         0.0668,  0.0659,  0.0873,  0.0602,  0.0712,  0.0744,  0.0549,  0.0666,
         0.0717,  0.0408,  0.0248,  0.0484,  0.0716,  0.0632,  0.0788,  0.0864,
         0.0719,  0.0766,  0.0623,  0.0540,  0.0613,  0.0282,  0.0412,  0.0767],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3746, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:11,561 :: INFO :: Epoch 25: loss tensor(162.8123, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 3.3868e-02,  5.2766e-02, -1.2496e-02,  5.9085e-02,  5.6568e-02,
        -2.0763e-04,  4.9734e-02,  7.2891e-02,  3.6398e-02,  4.1358e-02,
        -4.0104e-03,  3.2703e-02,  4.5328e-02,  6.8687e-02,  6.7974e-02,
         6.1253e-02,  4.7555e-02,  7.0395e-02,  2.7991e-02,  2.4180e-02,
         3.5147e-02,  3.0674e-02,  8.0731e-02,  1.9174e-02,  4.4680e-02,
         5.5725e-02,  3.4579e-02,  3.9417e-02,  3.1287e-02,  3.2502e-02,
         7.4452e-02,  7.7245e-02,  4.3172e-02,  2.0962e-02,  2.5096e-02,
        -2.3851e-04,  5.5081e-02,  7.1495e-02,  3.9874e-02,  3.0627e-02,
         2.8464e-02,  1.5561e-02,  4.7755e-02,  6.5511e-02,  5.4954e-02,
         4.0206e-02,  3.0400e-02,  3.2332e-02,  2.8170e-02,  3.3558e-02,
         2.6294e-02,  5.3365e-02,  2.4553e-02,  3.9622e-02,  2.3576e-03,
         8.3890e-02,  5.0779e-02,  3.0391e-02, -5.6206e-04,  1.6928e-04,
         4.5056e-02,  1.8988e-02,  1.8596e-02,  4.5520e-02,  4.3316e-02,
         2.3731e-02,  5.4046e-02,  5.4624e-02,  6.8438e-02,  1.3519e-02,
         3.2412e-02,  3.7737e-02,  1.6511e-02,  5.2335e-02,  7.3429e-02,
         5.9553e-03,  6.1928e-02, -5.9637e-03,  6.2905e-02,  5.2330e-02,
         7.3986e-02,  1.6872e-02,  7.8353e-02,  7.0231e-02,  1.9676e-02,
         3.9155e-02,  5.1671e-02,  2.0854e-02,  3.3577e-02,  4.4993e-03,
         3.9140e-02,  7.7974e-02,  2.5726e-02,  7.2121e-02,  3.8967e-03,
         6.6084e-02,  3.5665e-02,  2.4383e-02,  9.2827e-03,  2.1239e-02,
         2.6829e-02,  4.9348e-02,  8.0345e-02,  2.4572e-02, -4.9897e-03,
        -1.6494e-02,  5.2739e-02,  2.9712e-02,  5.4029e-02,  6.3785e-02,
        -2.4027e-02,  5.5288e-02,  2.0197e-02,  4.7960e-02,  2.1582e-02,
        -1.9855e-03,  1.5353e-02,  3.7579e-02,  2.6738e-02,  5.7433e-03,
         2.4668e-02,  5.2178e-02,  3.0035e-02,  2.2423e-02,  5.7948e-02,
         1.8348e-02,  2.5079e-02,  6.5854e-02,  9.5711e-02,  9.2102e-02,
         9.0891e-02,  9.6803e-02,  9.1451e-02,  1.0687e-01,  8.2505e-02,
         4.6307e-02,  1.4848e-03,  5.4537e-02,  7.7997e-02,  1.4584e-02,
         1.0883e-01,  1.3716e-02,  1.0055e-01,  8.9625e-02,  9.6981e-02,
         1.0611e-01,  8.5025e-02,  7.6925e-02,  9.5802e-02,  8.5365e-02,
         2.9613e-03,  9.5174e-02,  1.0230e-01, -5.5355e-02,  4.7323e-02,
        -6.9719e-05,  2.0571e-02, -5.5148e-02,  8.7019e-02, -5.1139e-02,
         1.0181e-01, -5.3295e-02,  1.0555e-01,  5.8489e-02,  9.6284e-02,
         6.3689e-02, -1.9887e-02,  4.2174e-02,  2.4528e-02,  8.4787e-02,
         1.1138e-01,  9.3944e-02, -5.5661e-02,  9.7408e-02,  1.0719e-01,
        -3.3657e-03,  6.2152e-02,  1.0121e-01,  5.3682e-02,  2.8716e-02,
         9.3852e-02,  1.9586e-02,  9.1397e-02, -5.2526e-02,  9.7287e-02,
         1.1078e-01, -4.5335e-02,  1.0346e-01,  1.2396e-01, -5.5840e-02,
        -2.2188e-02,  5.1592e-02,  1.0146e-01, -4.2810e-02,  8.1039e-02,
         1.0647e-01,  9.8733e-02,  5.2947e-02,  9.0501e-02,  9.3290e-02,
         1.0209e-01,  1.0585e-01,  9.1965e-02,  9.8783e-02,  9.4204e-02,
         9.6108e-02,  1.0621e-02, -5.7387e-02,  8.8086e-02,  6.7534e-02,
         1.1677e-01,  1.0305e-01,  7.7477e-02,  1.4571e-04,  9.1605e-02,
         1.1210e-01,  6.7026e-02, -2.0378e-02,  1.0248e-01, -5.4220e-02,
         9.6216e-02,  4.7132e-02,  1.1304e-01,  9.8089e-02,  9.8028e-02,
         1.0997e-01,  9.3784e-02,  1.0203e-01,  8.9003e-02,  1.1151e-01,
         9.5377e-02,  8.4195e-02,  8.5325e-02,  8.9190e-02,  1.0200e-01,
         8.2453e-02,  4.8295e-02,  4.2135e-02,  5.3338e-02,  1.0625e-01,
         8.7067e-02,  7.9011e-02,  7.0768e-02,  9.7986e-02,  2.2637e-02,
         9.0026e-02, -4.9532e-02,  8.0502e-02,  5.7312e-02,  4.0539e-02,
        -4.4473e-03,  1.1140e-01,  9.0500e-02,  9.9027e-02, -3.9165e-02,
         8.9059e-02,  9.5937e-02,  9.6315e-02,  1.0047e-01,  9.1585e-02,
         9.5638e-02,  9.8420e-02,  4.3913e-02,  6.5479e-02,  1.0150e-01,
         9.5387e-02,  1.0132e-01,  7.6599e-02,  8.9029e-02,  8.7243e-02,
         8.3973e-02,  7.2823e-02,  8.8221e-02,  8.5781e-02,  8.4908e-02,
         8.2233e-02,  9.8731e-02,  9.4946e-02,  9.1354e-02,  8.7357e-02,
         8.1496e-02,  1.0940e-01,  8.9290e-02,  8.9940e-02,  1.0431e-01,
         9.2996e-02,  5.0416e-02,  7.6008e-02,  9.4372e-02,  5.6950e-02,
         9.0417e-02,  7.2658e-02,  1.0162e-01,  8.1631e-02,  8.2353e-02,
         9.4770e-02,  9.3623e-02,  6.8404e-02,  8.9757e-02,  8.7468e-02,
         8.3427e-02,  1.1836e-01,  1.0623e-01,  9.0818e-02,  1.0207e-01,
         9.9995e-02,  8.1801e-02,  8.3678e-02,  9.1685e-02,  9.3024e-02,
         9.9445e-02,  8.9030e-02,  1.0008e-01,  9.8663e-02,  9.0144e-02,
         8.4058e-02,  8.6008e-02,  1.0851e-01,  6.2553e-02,  8.0800e-02,
         8.7908e-02,  9.3488e-02,  9.1294e-02,  8.4377e-02,  9.6005e-02,
         9.4588e-02,  9.1600e-02,  7.4759e-02,  8.6743e-02,  7.3281e-02,
         7.4902e-02,  7.9140e-02,  5.7204e-02,  5.6323e-02,  9.6864e-02,
         8.8666e-02,  7.3105e-02,  6.9908e-02,  9.2141e-02,  8.5447e-02,
         8.2303e-02,  7.4772e-02,  6.4924e-02,  9.5848e-02,  8.6830e-02,
         9.0067e-02,  8.3756e-02,  9.0281e-02,  8.2965e-02,  3.5709e-02,
         8.2390e-02,  8.1759e-02,  6.9677e-02,  7.6750e-02,  9.0949e-02,
         7.7732e-02,  8.9730e-02,  1.0001e-01,  8.7731e-02,  8.4352e-02,
         9.1626e-02,  9.3213e-02,  1.1370e-01,  8.7965e-02,  9.5564e-02,
         9.6333e-02,  7.9547e-02,  9.1410e-02,  9.3945e-02,  6.5446e-02,
         4.8842e-02,  7.5975e-02,  9.3532e-02,  8.5417e-02,  1.0612e-01,
         1.1232e-01,  9.3281e-02,  9.2477e-02,  8.4451e-02,  8.0093e-02,
         8.4259e-02,  5.2087e-02,  6.6969e-02,  9.8970e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(9.9601, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:13,952 :: INFO :: Epoch 30: loss tensor(161.3350, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0503,  0.0653, -0.0024,  0.0775,  0.0699,  0.0142,  0.0692,  0.0882,
         0.0518,  0.0551,  0.0110,  0.0451,  0.0585,  0.0875,  0.0854,  0.0741,
         0.0584,  0.0866,  0.0394,  0.0403,  0.0472,  0.0442,  0.0986,  0.0371,
         0.0606,  0.0677,  0.0451,  0.0554,  0.0520,  0.0468,  0.0907,  0.0930,
         0.0627,  0.0356,  0.0411,  0.0118,  0.0669,  0.0858,  0.0515,  0.0475,
         0.0431,  0.0298,  0.0646,  0.0760,  0.0663,  0.0536,  0.0428,  0.0489,
         0.0467,  0.0440,  0.0405,  0.0674,  0.0365,  0.0495,  0.0182,  0.1022,
         0.0700,  0.0501,  0.0132,  0.0143,  0.0563,  0.0315,  0.0342,  0.0597,
         0.0598,  0.0360,  0.0711,  0.0708,  0.0813,  0.0267,  0.0451,  0.0564,
         0.0236,  0.0710,  0.0900,  0.0181,  0.0795,  0.0066,  0.0764,  0.0701,
         0.0942,  0.0383,  0.0919,  0.0924,  0.0352,  0.0566,  0.0643,  0.0371,
         0.0461,  0.0164,  0.0519,  0.0961,  0.0362,  0.0864,  0.0203,  0.0856,
         0.0448,  0.0392,  0.0221,  0.0360,  0.0454,  0.0589,  0.0964,  0.0415,
         0.0085, -0.0067,  0.0672,  0.0424,  0.0705,  0.0777, -0.0164,  0.0679,
         0.0382,  0.0645,  0.0356,  0.0074,  0.0293,  0.0551,  0.0358,  0.0186,
         0.0376,  0.0671,  0.0481,  0.0368,  0.0709,  0.0305,  0.0405,  0.0835,
         0.1139,  0.1096,  0.1113,  0.1075,  0.1052,  0.1255,  0.1001,  0.0654,
         0.0185,  0.0555,  0.0962,  0.0304,  0.1296,  0.0298,  0.1218,  0.1180,
         0.1187,  0.1240,  0.0959,  0.1021,  0.1171,  0.1085,  0.0179,  0.1193,
         0.1217, -0.0607,  0.0651,  0.0173,  0.0368, -0.0590,  0.0963, -0.0546,
         0.1175, -0.0577,  0.1236,  0.0778,  0.1112,  0.0668, -0.0121,  0.0602,
         0.0430,  0.1031,  0.1334,  0.1085, -0.0598,  0.1181,  0.1286,  0.0114,
         0.0657,  0.1186,  0.0722,  0.0411,  0.1135,  0.0385,  0.1001, -0.0552,
         0.1146,  0.1333, -0.0464,  0.1237,  0.1479, -0.0608, -0.0111,  0.0708,
         0.1173, -0.0417,  0.0889,  0.1270,  0.1242,  0.0516,  0.1094,  0.1160,
         0.1231,  0.1307,  0.1080,  0.1097,  0.1135,  0.1218,  0.0254, -0.0629,
         0.0964,  0.0870,  0.1382,  0.1195,  0.0827,  0.0129,  0.1101,  0.1368,
         0.0866, -0.0126,  0.1204, -0.0577,  0.1193,  0.0648,  0.1352,  0.1208,
         0.1133,  0.1344,  0.1106,  0.1261,  0.1010,  0.1328,  0.1125,  0.0889,
         0.0945,  0.0964,  0.1175,  0.1070,  0.0682,  0.0599,  0.0559,  0.1244,
         0.1064,  0.1005,  0.0944,  0.1149,  0.0402,  0.1107, -0.0507,  0.1002,
         0.0758,  0.0593,  0.0120,  0.1328,  0.1142,  0.1225, -0.0340,  0.1085,
         0.1227,  0.1205,  0.1271,  0.1116,  0.1185,  0.1247,  0.0650,  0.0879,
         0.1284,  0.1210,  0.1251,  0.0975,  0.1081,  0.1047,  0.1040,  0.0956,
         0.1123,  0.1082,  0.1050,  0.1046,  0.1236,  0.1185,  0.1107,  0.1082,
         0.1037,  0.1364,  0.1096,  0.1148,  0.1320,  0.1082,  0.0728,  0.0990,
         0.1111,  0.0802,  0.1147,  0.0953,  0.1295,  0.1006,  0.1014,  0.1152,
         0.1156,  0.0908,  0.1046,  0.1066,  0.1087,  0.1443,  0.1329,  0.1176,
         0.1268,  0.1258,  0.0984,  0.1064,  0.1117,  0.1173,  0.1245,  0.1153,
         0.1250,  0.1224,  0.1138,  0.1032,  0.1074,  0.1352,  0.0837,  0.1057,
         0.1078,  0.1195,  0.1128,  0.1066,  0.1200,  0.1135,  0.1173,  0.0989,
         0.1084,  0.0956,  0.0967,  0.1012,  0.0801,  0.0778,  0.1225,  0.1043,
         0.0874,  0.0916,  0.1171,  0.1016,  0.1040,  0.0962,  0.0861,  0.1189,
         0.1084,  0.1054,  0.1015,  0.1134,  0.1047,  0.0555,  0.1036,  0.1052,
         0.0925,  0.0923,  0.1148,  0.0987,  0.1159,  0.1240,  0.1088,  0.1064,
         0.1158,  0.1201,  0.1396,  0.1152,  0.1190,  0.1157,  0.1030,  0.1155,
         0.1142,  0.0880,  0.0714,  0.1029,  0.1128,  0.1052,  0.1329,  0.1378,
         0.1119,  0.1037,  0.1037,  0.1056,  0.1045,  0.0743,  0.0917,  0.1193],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.4227, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:16,171 :: INFO :: Epoch 35: loss tensor(159.3541, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0647,  0.0733,  0.0090,  0.0932,  0.0799,  0.0283,  0.0859,  0.1006,
         0.0649,  0.0673,  0.0265,  0.0552,  0.0686,  0.1030,  0.0989,  0.0830,
         0.0665,  0.0982,  0.0493,  0.0566,  0.0559,  0.0547,  0.1125,  0.0533,
         0.0745,  0.0754,  0.0524,  0.0692,  0.0723,  0.0583,  0.1033,  0.1046,
         0.0796,  0.0486,  0.0539,  0.0230,  0.0749,  0.0973,  0.0606,  0.0621,
         0.0554,  0.0438,  0.0781,  0.0826,  0.0736,  0.0643,  0.0529,  0.0633,
         0.0645,  0.0520,  0.0524,  0.0765,  0.0474,  0.0550,  0.0343,  0.1171,
         0.0864,  0.0699,  0.0273,  0.0272,  0.0639,  0.0433,  0.0474,  0.0714,
         0.0731,  0.0462,  0.0846,  0.0833,  0.0910,  0.0383,  0.0562,  0.0718,
         0.0296,  0.0891,  0.1029,  0.0285,  0.0938,  0.0195,  0.0847,  0.0857,
         0.1122,  0.0593,  0.1009,  0.1126,  0.0480,  0.0721,  0.0736,  0.0524,
         0.0559,  0.0267,  0.0624,  0.1122,  0.0452,  0.0956,  0.0370,  0.1021,
         0.0517,  0.0515,  0.0347,  0.0492,  0.0609,  0.0642,  0.1073,  0.0577,
         0.0230,  0.0041,  0.0789,  0.0518,  0.0837,  0.0880, -0.0068,  0.0767,
         0.0557,  0.0788,  0.0469,  0.0160,  0.0420,  0.0715,  0.0421,  0.0309,
         0.0485,  0.0793,  0.0646,  0.0496,  0.0803,  0.0422,  0.0545,  0.0980,
         0.1304,  0.1246,  0.1294,  0.1137,  0.1153,  0.1409,  0.1147,  0.0825,
         0.0359,  0.0563,  0.1119,  0.0457,  0.1478,  0.0451,  0.1404,  0.1450,
         0.1385,  0.1393,  0.1022,  0.1258,  0.1362,  0.1298,  0.0329,  0.1416,
         0.1384, -0.0651,  0.0812,  0.0353,  0.0518, -0.0615,  0.1005, -0.0566,
         0.1303, -0.0608,  0.1375,  0.0953,  0.1232,  0.0659, -0.0036,  0.0761,
         0.0604,  0.1187,  0.1536,  0.1198, -0.0627,  0.1366,  0.1481,  0.0267,
         0.0660,  0.1331,  0.0882,  0.0516,  0.1302,  0.0566,  0.1027, -0.0567,
         0.1287,  0.1538, -0.0465,  0.1411,  0.1698, -0.0649,  0.0025,  0.0880,
         0.1299, -0.0391,  0.0924,  0.1451,  0.1476,  0.0492,  0.1263,  0.1368,
         0.1419,  0.1536,  0.1212,  0.1161,  0.1298,  0.1454,  0.0398, -0.0673,
         0.0998,  0.1039,  0.1570,  0.1324,  0.0837,  0.0255,  0.1259,  0.1598,
         0.1037, -0.0041,  0.1350, -0.0599,  0.1402,  0.0806,  0.1544,  0.1421,
         0.1247,  0.1567,  0.1246,  0.1482,  0.1070,  0.1520,  0.1264,  0.0886,
         0.1018,  0.0983,  0.1297,  0.1297,  0.0863,  0.0758,  0.0565,  0.1393,
         0.1230,  0.1200,  0.1165,  0.1299,  0.0567,  0.1292, -0.0504,  0.1173,
         0.0919,  0.0768,  0.0296,  0.1523,  0.1358,  0.1442, -0.0266,  0.1251,
         0.1481,  0.1435,  0.1528,  0.1289,  0.1406,  0.1498,  0.0844,  0.1081,
         0.1546,  0.1461,  0.1483,  0.1165,  0.1245,  0.1184,  0.1222,  0.1165,
         0.1358,  0.1292,  0.1221,  0.1251,  0.1478,  0.1405,  0.1280,  0.1264,
         0.1237,  0.1626,  0.1274,  0.1388,  0.1591,  0.1197,  0.0935,  0.1200,
         0.1260,  0.1020,  0.1376,  0.1160,  0.1568,  0.1173,  0.1176,  0.1341,
         0.1354,  0.1124,  0.1157,  0.1227,  0.1331,  0.1696,  0.1590,  0.1436,
         0.1509,  0.1507,  0.1110,  0.1284,  0.1291,  0.1405,  0.1487,  0.1406,
         0.1490,  0.1455,  0.1364,  0.1193,  0.1276,  0.1611,  0.1026,  0.1297,
         0.1251,  0.1450,  0.1319,  0.1271,  0.1430,  0.1300,  0.1422,  0.1216,
         0.1279,  0.1159,  0.1161,  0.1210,  0.1015,  0.0971,  0.1470,  0.1159,
         0.0977,  0.1110,  0.1415,  0.1135,  0.1231,  0.1156,  0.1048,  0.1410,
         0.1281,  0.1189,  0.1168,  0.1353,  0.1250,  0.0745,  0.1222,  0.1271,
         0.1133,  0.1057,  0.1370,  0.1169,  0.1413,  0.1475,  0.1273,  0.1265,
         0.1392,  0.1462,  0.1648,  0.1419,  0.1418,  0.1324,  0.1247,  0.1389,
         0.1324,  0.1088,  0.0926,  0.1287,  0.1290,  0.1225,  0.1590,  0.1623,
         0.1275,  0.1101,  0.1195,  0.1306,  0.1223,  0.0950,  0.1152,  0.1378],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.7612, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:18,436 :: INFO :: Epoch 40: loss tensor(158.5820, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0761,  0.0777,  0.0210,  0.1063,  0.0871,  0.0413,  0.0997,  0.1104,
         0.0753,  0.0774,  0.0415,  0.0634,  0.0761,  0.1153,  0.1096,  0.0894,
         0.0719,  0.1058,  0.0572,  0.0715,  0.0617,  0.0623,  0.1229,  0.0676,
         0.0867,  0.0799,  0.0568,  0.0810,  0.0913,  0.0671,  0.1132,  0.1124,
         0.0933,  0.0589,  0.0640,  0.0328,  0.0796,  0.1066,  0.0677,  0.0746,
         0.0659,  0.0566,  0.0883,  0.0863,  0.0772,  0.0708,  0.0605,  0.0755,
         0.0807,  0.0578,  0.0621,  0.0820,  0.0574,  0.0575,  0.0497,  0.1290,
         0.0996,  0.0899,  0.0398,  0.0386,  0.0681,  0.0536,  0.0575,  0.0805,
         0.0837,  0.0536,  0.0944,  0.0925,  0.0980,  0.0482,  0.0646,  0.0835,
         0.0342,  0.1064,  0.1123,  0.0375,  0.1049,  0.0319,  0.0887,  0.0981,
         0.1281,  0.0796,  0.1071,  0.1307,  0.0578,  0.0853,  0.0803,  0.0650,
         0.0633,  0.0348,  0.0709,  0.1260,  0.0528,  0.1010,  0.0522,  0.1149,
         0.0565,  0.0615,  0.0460,  0.0609,  0.0729,  0.0659,  0.1148,  0.0724,
         0.0376,  0.0155,  0.0873,  0.0581,  0.0945,  0.0955,  0.0041,  0.0826,
         0.0710,  0.0906,  0.0560,  0.0229,  0.0518,  0.0864,  0.0455,  0.0417,
         0.0573,  0.0887,  0.0790,  0.0599,  0.0861,  0.0534,  0.0670,  0.1099,
         0.1448,  0.1369,  0.1449,  0.1157,  0.1214,  0.1527,  0.1262,  0.0971,
         0.0524,  0.0568,  0.1248,  0.0595,  0.1630,  0.0588,  0.1562,  0.1706,
         0.1559,  0.1518,  0.1047,  0.1472,  0.1523,  0.1483,  0.0468,  0.1609,
         0.1521, -0.0688,  0.0939,  0.0521,  0.0649, -0.0631,  0.1009, -0.0575,
         0.1401, -0.0630,  0.1474,  0.1099,  0.1326,  0.0627,  0.0048,  0.0894,
         0.0757,  0.1316,  0.1716,  0.1278, -0.0648,  0.1527,  0.1655,  0.0411,
         0.0647,  0.1443,  0.1013,  0.0601,  0.1439,  0.0727,  0.1000, -0.0572,
         0.1396,  0.1719, -0.0458,  0.1552,  0.1894, -0.0682,  0.0173,  0.1022,
         0.1392, -0.0359,  0.0922,  0.1606,  0.1681,  0.0460,  0.1407,  0.1552,
         0.1584,  0.1739,  0.1313,  0.1168,  0.1424,  0.1664,  0.0528, -0.0709,
         0.0986,  0.1180,  0.1724,  0.1418,  0.0801,  0.0366,  0.1386,  0.1809,
         0.1175,  0.0045,  0.1458, -0.0613,  0.1589,  0.0937,  0.1706,  0.1617,
         0.1319,  0.1763,  0.1356,  0.1681,  0.1077,  0.1687,  0.1365,  0.0846,
         0.1066,  0.0959,  0.1382,  0.1497,  0.1015,  0.0894,  0.0571,  0.1494,
         0.1368,  0.1370,  0.1366,  0.1428,  0.0714,  0.1455, -0.0493,  0.1310,
         0.1053,  0.0922,  0.0467,  0.1702,  0.1548,  0.1636, -0.0187,  0.1385,
         0.1720,  0.1651,  0.1772,  0.1441,  0.1616,  0.1740,  0.1022,  0.1263,
         0.1798,  0.1700,  0.1708,  0.1339,  0.1387,  0.1286,  0.1387,  0.1356,
         0.1580,  0.1488,  0.1366,  0.1440,  0.1707,  0.1611,  0.1437,  0.1424,
         0.1419,  0.1875,  0.1430,  0.1616,  0.1848,  0.1279,  0.1122,  0.1395,
         0.1397,  0.1222,  0.1597,  0.1350,  0.1830,  0.1326,  0.1312,  0.1520,
         0.1533,  0.1334,  0.1235,  0.1367,  0.1564,  0.1937,  0.1845,  0.1681,
         0.1738,  0.1743,  0.1204,  0.1501,  0.1444,  0.1624,  0.1721,  0.1646,
         0.1720,  0.1676,  0.1580,  0.1331,  0.1471,  0.1859,  0.1196,  0.1524,
         0.1399,  0.1694,  0.1490,  0.1460,  0.1649,  0.1444,  0.1657,  0.1433,
         0.1456,  0.1343,  0.1335,  0.1391,  0.1211,  0.1141,  0.1704,  0.1240,
         0.1052,  0.1283,  0.1646,  0.1216,  0.1403,  0.1329,  0.1214,  0.1619,
         0.1463,  0.1317,  0.1304,  0.1560,  0.1437,  0.0922,  0.1386,  0.1477,
         0.1323,  0.1180,  0.1577,  0.1326,  0.1653,  0.1703,  0.1440,  0.1447,
         0.1615,  0.1713,  0.1883,  0.1675,  0.1640,  0.1470,  0.1449,  0.1614,
         0.1489,  0.1276,  0.1123,  0.1532,  0.1422,  0.1381,  0.1839,  0.1856,
         0.1403,  0.1131,  0.1321,  0.1545,  0.1379,  0.1143,  0.1370,  0.1548],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.9839, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:18,436 :: INFO :: ----- frontend -----
2023-04-11 14:33:18,436 :: INFO :: Environment 0
2023-04-11 14:33:32,952 :: INFO :: Epoch 5: loss tensor(894.5388, device='cuda:0'), U.norm 14.65379524230957, V.norm 18.38434410095215, MLP.norm 1.8876240253448486
2023-04-11 14:33:33,061 :: INFO :: Epoch 10: loss tensor(879.9222, device='cuda:0'), U.norm 12.055262565612793, V.norm 17.90987205505371, MLP.norm 2.7109875679016113
2023-04-11 14:33:33,155 :: INFO :: Epoch 15: loss tensor(859.2750, device='cuda:0'), U.norm 10.399970054626465, V.norm 17.695152282714844, MLP.norm 3.7741401195526123
2023-04-11 14:33:33,248 :: INFO :: Epoch 20: loss tensor(834.0741, device='cuda:0'), U.norm 9.215624809265137, V.norm 17.556541442871094, MLP.norm 4.885380268096924
2023-04-11 14:33:33,264 :: INFO :: Environment 1
2023-04-11 14:33:46,655 :: INFO :: Epoch 5: loss tensor(916.7294, device='cuda:0'), U.norm 14.651824951171875, V.norm 18.426198959350586, MLP.norm 1.8875352144241333
2023-04-11 14:33:46,765 :: INFO :: Epoch 10: loss tensor(901.1984, device='cuda:0'), U.norm 12.051736831665039, V.norm 17.970293045043945, MLP.norm 2.718963146209717
2023-04-11 14:33:46,858 :: INFO :: Epoch 15: loss tensor(878.7108, device='cuda:0'), U.norm 10.394953727722168, V.norm 17.76483917236328, MLP.norm 3.801055669784546
2023-04-11 14:33:46,952 :: INFO :: Epoch 20: loss tensor(850.5258, device='cuda:0'), U.norm 9.209013938903809, V.norm 17.63256072998047, MLP.norm 4.945972919464111
2023-04-11 14:33:46,952 :: INFO :: Environment 2
2023-04-11 14:34:01,202 :: INFO :: Epoch 5: loss tensor(777.3432, device='cuda:0'), U.norm 14.644787788391113, V.norm 18.17815399169922, MLP.norm 1.8350152969360352
2023-04-11 14:34:01,296 :: INFO :: Epoch 10: loss tensor(767.1918, device='cuda:0'), U.norm 12.03645133972168, V.norm 17.63092613220215, MLP.norm 2.547416925430298
2023-04-11 14:34:01,390 :: INFO :: Epoch 15: loss tensor(752.0007, device='cuda:0'), U.norm 10.369559288024902, V.norm 17.38736915588379, MLP.norm 3.487640619277954
2023-04-11 14:34:01,467 :: INFO :: Epoch 20: loss tensor(733.6307, device='cuda:0'), U.norm 9.1718168258667, V.norm 17.233762741088867, MLP.norm 4.477759838104248
2023-04-11 14:34:01,467 :: INFO :: Environment 3
2023-04-11 14:34:14,873 :: INFO :: Epoch 5: loss tensor(863.8544, device='cuda:0'), U.norm 14.646376609802246, V.norm 18.32630157470703, MLP.norm 1.8742793798446655
2023-04-11 14:34:14,967 :: INFO :: Epoch 10: loss tensor(850.3865, device='cuda:0'), U.norm 12.039825439453125, V.norm 17.83001708984375, MLP.norm 2.6647045612335205
2023-04-11 14:34:15,061 :: INFO :: Epoch 15: loss tensor(831.0716, device='cuda:0'), U.norm 10.37489128112793, V.norm 17.604646682739258, MLP.norm 3.695297956466675
2023-04-11 14:34:15,155 :: INFO :: Epoch 20: loss tensor(807.3835, device='cuda:0'), U.norm 9.179485321044922, V.norm 17.46078109741211, MLP.norm 4.776248931884766
2023-04-11 14:34:15,155 :: INFO :: Environment 4
2023-04-11 14:34:29,733 :: INFO :: Epoch 5: loss tensor(799.6936, device='cuda:0'), U.norm 14.644957542419434, V.norm 18.218351364135742, MLP.norm 1.8522101640701294
2023-04-11 14:34:29,811 :: INFO :: Epoch 10: loss tensor(789.1824, device='cuda:0'), U.norm 12.036465644836426, V.norm 17.683494567871094, MLP.norm 2.5774953365325928
2023-04-11 14:34:29,905 :: INFO :: Epoch 15: loss tensor(772.9573, device='cuda:0'), U.norm 10.369324684143066, V.norm 17.446033477783203, MLP.norm 3.5387461185455322
2023-04-11 14:34:29,999 :: INFO :: Epoch 20: loss tensor(753.0526, device='cuda:0'), U.norm 9.171339988708496, V.norm 17.29547691345215, MLP.norm 4.57301664352417
2023-04-11 14:34:29,999 :: INFO :: Environment 5
2023-04-11 14:34:45,499 :: INFO :: Epoch 5: loss tensor(880.3274, device='cuda:0'), U.norm 14.646906852722168, V.norm 18.355867385864258, MLP.norm 1.8825514316558838
2023-04-11 14:34:45,592 :: INFO :: Epoch 10: loss tensor(865.9647, device='cuda:0'), U.norm 12.04190731048584, V.norm 17.87177276611328, MLP.norm 2.6855361461639404
2023-04-11 14:34:45,670 :: INFO :: Epoch 15: loss tensor(846.1929, device='cuda:0'), U.norm 10.378859519958496, V.norm 17.652835845947266, MLP.norm 3.7303626537323
2023-04-11 14:34:45,764 :: INFO :: Epoch 20: loss tensor(821.9562, device='cuda:0'), U.norm 9.185568809509277, V.norm 17.513914108276367, MLP.norm 4.82747220993042
2023-04-11 14:34:45,764 :: INFO :: Environment 6
2023-04-11 14:34:59,608 :: INFO :: Epoch 5: loss tensor(825.4675, device='cuda:0'), U.norm 14.6473970413208, V.norm 18.26229476928711, MLP.norm 1.8483976125717163
2023-04-11 14:34:59,717 :: INFO :: Epoch 10: loss tensor(813.6189, device='cuda:0'), U.norm 12.041337013244629, V.norm 17.742870330810547, MLP.norm 2.5956950187683105
2023-04-11 14:34:59,827 :: INFO :: Epoch 15: loss tensor(796.1737, device='cuda:0'), U.norm 10.37690544128418, V.norm 17.50921630859375, MLP.norm 3.5891036987304688
2023-04-11 14:34:59,952 :: INFO :: Epoch 20: loss tensor(774.4465, device='cuda:0'), U.norm 9.182196617126465, V.norm 17.358627319335938, MLP.norm 4.644659996032715
2023-04-11 14:34:59,952 :: INFO :: Environment 7
2023-04-11 14:35:15,030 :: INFO :: Epoch 5: loss tensor(892.9253, device='cuda:0'), U.norm 14.649234771728516, V.norm 18.389667510986328, MLP.norm 1.8880053758621216
2023-04-11 14:35:15,123 :: INFO :: Epoch 10: loss tensor(876.2776, device='cuda:0'), U.norm 12.046866416931152, V.norm 17.917465209960938, MLP.norm 2.722612142562866
2023-04-11 14:35:15,218 :: INFO :: Epoch 15: loss tensor(853.5200, device='cuda:0'), U.norm 10.387066841125488, V.norm 17.70317268371582, MLP.norm 3.796128749847412
2023-04-11 14:35:15,327 :: INFO :: Epoch 20: loss tensor(825.4651, device='cuda:0'), U.norm 9.197308540344238, V.norm 17.564546585083008, MLP.norm 4.926065921783447
2023-04-11 14:35:15,327 :: INFO :: Environment 8
2023-04-11 14:35:28,249 :: INFO :: Epoch 5: loss tensor(801.3393, device='cuda:0'), U.norm 14.647775650024414, V.norm 18.216909408569336, MLP.norm 1.850866675376892
2023-04-11 14:35:28,327 :: INFO :: Epoch 10: loss tensor(789.2790, device='cuda:0'), U.norm 12.042227745056152, V.norm 17.675222396850586, MLP.norm 2.5951123237609863
2023-04-11 14:35:28,405 :: INFO :: Epoch 15: loss tensor(770.8939, device='cuda:0'), U.norm 10.37871265411377, V.norm 17.429746627807617, MLP.norm 3.594111680984497
2023-04-11 14:35:28,483 :: INFO :: Epoch 20: loss tensor(749.2645, device='cuda:0'), U.norm 9.1847505569458, V.norm 17.271150588989258, MLP.norm 4.649590969085693
2023-04-11 14:35:28,483 :: INFO :: Environment 9
2023-04-11 14:35:44,030 :: INFO :: Epoch 5: loss tensor(851.0930, device='cuda:0'), U.norm 14.648991584777832, V.norm 18.313119888305664, MLP.norm 1.8743083477020264
2023-04-11 14:35:44,140 :: INFO :: Epoch 10: loss tensor(836.8433, device='cuda:0'), U.norm 12.045454025268555, V.norm 17.811628341674805, MLP.norm 2.6613845825195312
2023-04-11 14:35:44,233 :: INFO :: Epoch 15: loss tensor(816.3601, device='cuda:0'), U.norm 10.384438514709473, V.norm 17.584238052368164, MLP.norm 3.6935067176818848
2023-04-11 14:35:44,359 :: INFO :: Epoch 20: loss tensor(792.2039, device='cuda:0'), U.norm 9.193598747253418, V.norm 17.43659782409668, MLP.norm 4.78265905380249
2023-04-11 14:35:44,359 :: INFO :: Ite = 1, Delta = 4167
2023-04-11 14:35:44,359 :: INFO :: ----- backend -----
2023-04-11 14:35:46,733 :: INFO :: Epoch 5: loss tensor(186.2415, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0120, -0.0095, -0.0149, -0.0040, -0.0091, -0.0140, -0.0133,  0.0005,
        -0.0124, -0.0074, -0.0145, -0.0104, -0.0074, -0.0009, -0.0018,  0.0051,
        -0.0091,  0.0008,  0.0023, -0.0106, -0.0068, -0.0070, -0.0054, -0.0151,
        -0.0108, -0.0067, -0.0099, -0.0110, -0.0113, -0.0094, -0.0049,  0.0074,
        -0.0058, -0.0076, -0.0139, -0.0152, -0.0011, -0.0061, -0.0109, -0.0057,
        -0.0146, -0.0127, -0.0031, -0.0044, -0.0037, -0.0063, -0.0072, -0.0155,
        -0.0093, -0.0090, -0.0094, -0.0065, -0.0030, -0.0044, -0.0145,  0.0058,
        -0.0048, -0.0036, -0.0132, -0.0158, -0.0043, -0.0009, -0.0144, -0.0082,
        -0.0042, -0.0087, -0.0008, -0.0022,  0.0022, -0.0140, -0.0063, -0.0130,
         0.0091, -0.0069, -0.0021, -0.0142,  0.0137, -0.0125, -0.0038,  0.0154,
        -0.0112, -0.0145, -0.0002, -0.0091, -0.0136, -0.0096,  0.0065, -0.0135,
        -0.0099, -0.0140, -0.0108, -0.0033, -0.0099, -0.0021, -0.0121, -0.0086,
        -0.0060, -0.0070, -0.0145, -0.0047, -0.0047, -0.0035, -0.0077, -0.0133,
        -0.0152, -0.0147, -0.0039, -0.0063,  0.0011,  0.0017, -0.0145,  0.0004,
        -0.0135, -0.0106, -0.0143, -0.0130, -0.0084, -0.0127,  0.0027, -0.0114,
        -0.0096, -0.0087, -0.0021, -0.0042, -0.0055, -0.0087, -0.0148, -0.0045,
         0.0056, -0.0071, -0.0117,  0.0164, -0.0032,  0.0156, -0.0114, -0.0142,
        -0.0148,  0.0122, -0.0145, -0.0162,  0.0090, -0.0154, -0.0042, -0.0077,
        -0.0100,  0.0140,  0.0055, -0.0108, -0.0134, -0.0157, -0.0147, -0.0038,
        -0.0083, -0.0144, -0.0162, -0.0161, -0.0152, -0.0158,  0.0173, -0.0176,
         0.0111, -0.0176,  0.0154, -0.0159,  0.0126,  0.0172, -0.0158, -0.0150,
        -0.0160, -0.0126,  0.0111,  0.0080, -0.0167, -0.0010, -0.0068, -0.0143,
         0.0156,  0.0036, -0.0157, -0.0148, -0.0019, -0.0154,  0.0140, -0.0162,
         0.0059,  0.0121, -0.0150, -0.0045,  0.0177, -0.0158, -0.0174, -0.0145,
         0.0168, -0.0155,  0.0020,  0.0110, -0.0018,  0.0177, -0.0132, -0.0096,
         0.0050,  0.0038, -0.0123,  0.0157, -0.0116, -0.0024, -0.0164, -0.0169,
         0.0116, -0.0135,  0.0184,  0.0146,  0.0172, -0.0153, -0.0146,  0.0108,
        -0.0119, -0.0163,  0.0171, -0.0157, -0.0024, -0.0165,  0.0161, -0.0056,
         0.0053,  0.0073,  0.0004,  0.0034,  0.0030, -0.0087, -0.0081,  0.0128,
         0.0150,  0.0193,  0.0163, -0.0128, -0.0155, -0.0141,  0.0157,  0.0135,
        -0.0106, -0.0132, -0.0119,  0.0088, -0.0145, -0.0084, -0.0157, -0.0152,
        -0.0139, -0.0149, -0.0156,  0.0175, -0.0033, -0.0094, -0.0159, -0.0097,
        -0.0091, -0.0106, -0.0114, -0.0098, -0.0136, -0.0049, -0.0153, -0.0148,
        -0.0131, -0.0123, -0.0124, -0.0140, -0.0114, -0.0124, -0.0141, -0.0135,
        -0.0152, -0.0148, -0.0122, -0.0150, -0.0126, -0.0114, -0.0129, -0.0131,
        -0.0141, -0.0108, -0.0107, -0.0134, -0.0028, -0.0079, -0.0148, -0.0140,
        -0.0090, -0.0145, -0.0142, -0.0136, -0.0154, -0.0122, -0.0147, -0.0118,
        -0.0090, -0.0160,  0.0002, -0.0127, -0.0145, -0.0035, -0.0154, -0.0144,
        -0.0108, -0.0106, -0.0085, -0.0142, -0.0122, -0.0151, -0.0111, -0.0141,
        -0.0124, -0.0140, -0.0155, -0.0134, -0.0111, -0.0084, -0.0149, -0.0145,
        -0.0108, -0.0161, -0.0066, -0.0150, -0.0136, -0.0113, -0.0151, -0.0162,
        -0.0141, -0.0148, -0.0140, -0.0143, -0.0151, -0.0152, -0.0093, -0.0064,
        -0.0136, -0.0135, -0.0153, -0.0088, -0.0130, -0.0150, -0.0145, -0.0086,
        -0.0119, -0.0074, -0.0147, -0.0138, -0.0119, -0.0158, -0.0139, -0.0118,
        -0.0136, -0.0143, -0.0136, -0.0133, -0.0145, -0.0051, -0.0121, -0.0133,
        -0.0131, -0.0123, -0.0136, -0.0170, -0.0061, -0.0090, -0.0128, -0.0138,
        -0.0116, -0.0148, -0.0150, -0.0148, -0.0102, -0.0085, -0.0168,  0.0011,
        -0.0114, -0.0087, -0.0126, -0.0150, -0.0119, -0.0146, -0.0144, -0.0090],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.8935, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:49,280 :: INFO :: Epoch 10: loss tensor(183.7529, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.5890e-02, -1.0987e-02, -2.6234e-02, -1.7009e-03, -5.6542e-03,
        -2.4590e-02, -2.0246e-02,  8.7998e-03, -1.9137e-02, -5.5571e-03,
        -2.5460e-02, -1.3460e-02, -3.4671e-04,  3.7016e-03, -9.5566e-04,
         1.1201e-02, -1.0241e-02,  7.0955e-03,  5.8688e-03, -1.3014e-02,
        -5.6398e-03, -6.9715e-03,  4.1728e-04, -2.6150e-02, -1.5917e-02,
        -1.5544e-04, -1.1698e-02, -1.2797e-02, -1.6760e-02, -1.3829e-02,
         1.5112e-04,  1.5821e-02, -7.8088e-03, -1.0276e-02, -2.4528e-02,
        -2.8372e-02,  2.9984e-03,  4.2146e-04, -1.2670e-02, -1.0580e-02,
        -2.5180e-02, -1.7491e-02, -3.3796e-03, -2.2486e-04,  1.4776e-04,
        -7.2973e-03, -3.0661e-03, -2.6544e-02, -1.4266e-02, -1.0471e-02,
        -1.3195e-02, -6.4221e-03, -2.8535e-03, -1.1912e-03, -2.5948e-02,
         1.7296e-02, -2.2185e-03, -5.4194e-04, -2.3227e-02, -2.8610e-02,
         8.1404e-04,  3.6584e-03, -2.6604e-02, -6.3646e-03, -3.5955e-03,
        -7.0677e-03, -2.0526e-03, -4.6503e-03,  1.0087e-02, -2.4213e-02,
        -5.5508e-03, -2.2045e-02,  1.4702e-02, -1.4092e-03,  5.9031e-03,
        -2.4089e-02,  1.6457e-02, -2.0281e-02,  2.1385e-03,  2.0089e-02,
        -1.1539e-02, -2.7078e-02,  1.1035e-02, -8.6020e-03, -2.2575e-02,
        -1.3826e-02,  1.0453e-02, -2.3465e-02, -1.2202e-02, -2.3563e-02,
        -9.9578e-03,  6.8523e-03, -1.0011e-02,  5.0408e-03, -2.3255e-02,
        -7.7807e-03, -3.6427e-03, -1.2722e-02, -2.4149e-02, -3.7098e-03,
        -1.0803e-02,  6.2992e-03, -3.9842e-03, -2.1051e-02, -2.6998e-02,
        -2.7975e-02, -2.7524e-03, -8.2637e-03,  8.5026e-03,  1.1807e-02,
        -2.5924e-02,  7.1789e-03, -2.3531e-02, -1.4901e-02, -2.4275e-02,
        -2.2529e-02, -1.2085e-02, -1.8928e-02,  7.7962e-03, -1.6545e-02,
        -1.2649e-02, -7.4635e-03,  2.8007e-04, -5.9719e-03,  8.6549e-04,
        -1.2083e-02, -2.5078e-02,  4.2110e-03,  2.5451e-02,  5.9921e-03,
        -6.0708e-03,  3.4188e-02,  1.1212e-02,  3.6705e-02, -3.9567e-03,
        -2.0496e-02, -2.4738e-02,  2.6089e-02, -1.4111e-02, -2.7882e-02,
         2.9076e-02, -2.5718e-02,  1.5246e-02,  7.2502e-03,  1.7560e-03,
         3.2103e-02,  2.4356e-02, -9.4654e-04, -1.4466e-02, -2.4148e-02,
        -2.4352e-02,  1.8171e-02,  5.4127e-03, -2.5066e-02, -2.6924e-02,
        -2.8254e-02, -2.5079e-02, -2.8062e-02,  3.2765e-02, -3.2337e-02,
         2.9347e-02, -3.2176e-02,  3.3532e-02, -2.6136e-02,  2.8353e-02,
         2.9473e-02, -2.7296e-02, -2.3377e-02, -2.7405e-02, -5.4490e-03,
         3.4291e-02,  2.4252e-02, -3.0013e-02,  1.7289e-02,  1.1197e-02,
        -2.2654e-02,  2.9326e-02,  2.1255e-02, -2.5361e-02, -2.2174e-02,
         1.5525e-02, -2.5878e-02,  3.1758e-02, -2.8773e-02,  2.3660e-02,
         3.1857e-02, -2.5905e-02,  1.6495e-02,  3.4636e-02, -2.7677e-02,
        -2.9742e-02, -2.1977e-02,  3.2926e-02, -2.7187e-02,  2.3367e-02,
         2.9455e-02,  2.0094e-02,  3.1645e-02, -5.9593e-03,  3.0476e-03,
         2.4245e-02,  2.6162e-02, -6.1028e-03,  3.5991e-02, -3.3570e-03,
         1.9088e-02, -2.8631e-02, -3.0271e-02,  2.6963e-02, -1.5508e-02,
         3.8830e-02,  3.1199e-02,  2.4306e-02, -2.5718e-02, -1.3013e-02,
         3.1851e-02, -9.7557e-03, -2.8463e-02,  3.4235e-02, -2.7934e-02,
         1.9808e-02, -2.5957e-02,  3.7351e-02,  1.3742e-02,  1.9758e-02,
         3.0133e-02,  1.7509e-02,  2.4085e-02,  2.0780e-02,  6.9199e-03,
         4.5877e-03,  2.8383e-02,  3.0389e-02,  3.4246e-02,  3.2329e-02,
        -9.8902e-03, -2.5135e-02, -2.0779e-02,  2.9986e-02,  3.3068e-02,
        -3.1650e-04, -1.3925e-02, -7.8038e-03,  2.4615e-02, -2.3170e-02,
         2.2839e-03, -2.7844e-02, -2.0107e-02, -1.9540e-02, -2.3596e-02,
        -2.2394e-02,  3.5130e-02,  1.4923e-02,  4.2338e-03, -2.8557e-02,
         1.7324e-03,  9.2360e-03,  3.8517e-03,  2.5261e-03,  5.7511e-03,
        -9.4662e-03,  1.5168e-02, -2.1924e-02, -1.6880e-02, -5.4649e-03,
        -1.8955e-03, -1.3423e-04, -1.0522e-02,  2.5271e-03, -1.8893e-03,
        -1.5995e-02, -6.8758e-03, -1.9119e-02, -1.4378e-02,  3.7676e-04,
        -2.0633e-02,  1.8268e-05, -9.8116e-05, -6.8152e-03, -6.2790e-03,
        -9.7673e-03,  6.8784e-03,  3.7057e-03, -1.8429e-03,  1.8201e-02,
         1.0519e-02, -1.5923e-02, -1.5275e-02,  8.0832e-03, -2.0421e-02,
        -1.3987e-02, -8.6385e-03, -9.1867e-03, -3.3502e-04, -1.7261e-02,
        -8.4419e-04,  8.3046e-03, -1.9162e-02,  2.2376e-02, -2.6363e-03,
        -1.6348e-02,  1.8096e-02, -1.0409e-02, -1.6734e-02,  3.1844e-03,
         4.2704e-03,  9.4536e-03, -1.1507e-02, -2.1808e-04, -2.0502e-02,
         2.6648e-03, -1.1662e-02, -4.3022e-03, -8.0971e-03, -1.8261e-02,
        -6.0209e-03,  1.0408e-03,  9.8194e-03, -1.7157e-02, -1.7914e-02,
         3.4863e-03, -1.9546e-02,  1.3186e-02, -1.7080e-02, -6.7475e-03,
        -2.2928e-03, -1.4774e-02, -2.4109e-02, -1.1720e-02, -1.8607e-02,
        -1.1752e-02, -1.5136e-02, -1.9875e-02, -1.7552e-02,  8.2606e-03,
         1.4181e-02, -1.2557e-02, -1.1771e-02, -1.2749e-02,  9.1978e-03,
        -6.1669e-03, -1.5151e-02, -1.2254e-02,  7.8453e-03, -2.0947e-03,
         1.2489e-02, -1.6374e-02, -1.1253e-02, -3.2435e-03, -2.3956e-02,
        -7.5480e-03,  1.2407e-03, -9.2041e-03, -1.3318e-02, -1.2972e-02,
        -1.0853e-02, -1.5360e-02,  1.5895e-02, -1.6251e-03, -8.2242e-03,
        -3.1325e-03, -8.4781e-04, -3.9283e-03, -2.2820e-02,  1.4466e-02,
         8.5339e-03, -5.9856e-03, -9.7449e-03,  2.2033e-03, -1.7219e-02,
        -1.9065e-02, -2.0571e-02,  5.3490e-03,  9.5759e-03, -1.6092e-02,
         2.3709e-02,  2.1712e-03,  5.8564e-03, -4.0690e-03, -2.0997e-02,
         2.0482e-05, -1.6981e-02, -2.0527e-02,  9.8078e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.5740, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:51,952 :: INFO :: Epoch 15: loss tensor(179.1891, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.8212e-02, -1.5096e-02, -3.5752e-02, -4.9820e-03, -2.5561e-03,
        -3.2927e-02, -2.4348e-02,  9.9295e-03, -2.4833e-02, -8.2185e-03,
        -3.3674e-02, -1.9317e-02,  5.3319e-03,  1.1659e-03, -6.7166e-03,
         7.0306e-03, -1.4946e-02,  5.7403e-03, -5.2583e-04, -1.5891e-02,
        -9.6598e-03, -1.1851e-02,  2.6824e-03, -3.4849e-02, -2.2741e-02,
         3.5448e-03, -1.6303e-02, -1.4693e-02, -1.8960e-02, -2.0879e-02,
         4.9740e-04,  1.4901e-02, -1.2532e-02, -1.6270e-02, -3.4419e-02,
        -4.0558e-02, -1.4852e-03,  1.6931e-03, -1.5009e-02, -1.8826e-02,
        -3.4053e-02, -2.0815e-02, -8.4008e-03, -2.6280e-03, -2.1807e-03,
        -1.2814e-02, -3.4901e-03, -3.5669e-02, -2.0429e-02, -1.5208e-02,
        -1.9393e-02, -1.1935e-02, -1.0212e-02, -4.8773e-03, -3.4645e-02,
         2.2472e-02, -2.7331e-03, -1.3669e-03, -3.2373e-02, -3.8916e-02,
        -6.9957e-04,  7.7977e-04, -3.6965e-02, -7.4247e-03, -9.2345e-03,
        -8.5593e-03, -8.2747e-03, -1.2781e-02,  9.0000e-03, -3.3431e-02,
        -1.0693e-02, -2.9443e-02,  9.8787e-03,  9.6094e-04,  8.2552e-03,
        -3.3483e-02,  9.6536e-03, -2.8049e-02,  1.3821e-03,  1.4619e-02,
        -1.0263e-02, -3.6517e-02,  1.5948e-02, -9.2471e-03, -3.1323e-02,
        -1.8416e-02,  5.2859e-03, -3.2414e-02, -1.6670e-02, -3.2432e-02,
        -9.2078e-03,  1.2997e-02, -1.2424e-02,  5.7982e-03, -3.3856e-02,
        -6.6736e-03, -8.5814e-03, -2.2443e-02, -3.1369e-02, -8.2658e-03,
        -2.1719e-02,  1.1253e-02, -2.5011e-03, -2.6452e-02, -3.6463e-02,
        -3.9960e-02, -7.7571e-03, -1.4942e-02,  8.0384e-03,  1.4819e-02,
        -3.5179e-02,  6.4637e-03, -2.9771e-02, -1.9368e-02, -3.2923e-02,
        -3.2371e-02, -1.9240e-02, -2.2012e-02,  2.7304e-03, -2.2365e-02,
        -1.9059e-02, -9.3373e-03, -5.5017e-03, -1.4611e-02,  3.5367e-03,
        -2.0043e-02, -3.3010e-02,  9.3786e-03,  4.6415e-02,  2.1307e-02,
         9.0278e-03,  5.1483e-02,  3.4341e-02,  5.9653e-02,  1.0152e-02,
        -2.0490e-02, -2.9486e-02,  3.8076e-02, -2.0996e-03, -3.5486e-02,
         4.9975e-02, -3.2078e-02,  3.7726e-02,  2.9289e-02,  2.2125e-02,
         4.9170e-02,  4.2345e-02,  2.1111e-02, -3.7815e-03, -2.1966e-02,
        -2.9895e-02,  4.4236e-02,  2.4151e-02, -3.3013e-02, -3.2285e-02,
        -3.6567e-02, -3.0830e-02, -3.7783e-02,  4.4919e-02, -4.4480e-02,
         4.7528e-02, -4.4223e-02,  5.1359e-02, -3.0898e-02,  4.1366e-02,
         3.4646e-02, -3.5525e-02, -2.6163e-02, -3.4213e-02,  1.0779e-02,
         5.9519e-02,  3.8444e-02, -4.0935e-02,  3.6812e-02,  3.3879e-02,
        -2.5137e-02,  3.9904e-02,  3.8514e-02, -2.9135e-02, -2.4821e-02,
         3.3868e-02, -3.1827e-02,  4.7461e-02, -3.8696e-02,  4.0290e-02,
         5.2595e-02, -3.3934e-02,  4.1011e-02,  5.7896e-02, -3.6919e-02,
        -3.5629e-02, -2.2995e-02,  4.6782e-02, -3.5965e-02,  4.4609e-02,
         4.8248e-02,  4.5446e-02,  3.9427e-02,  1.2263e-02,  2.4198e-02,
         4.4037e-02,  5.0856e-02,  6.5540e-03,  5.6267e-02,  1.3927e-02,
         4.4314e-02, -3.6893e-02, -4.1454e-02,  3.9573e-02, -9.6385e-03,
         6.0346e-02,  4.5928e-02,  3.2164e-02, -3.2308e-02,  2.3994e-03,
         5.4546e-02,  1.1479e-03, -3.7427e-02,  5.0162e-02, -3.7684e-02,
         4.5008e-02, -2.6062e-02,  6.0480e-02,  3.6661e-02,  3.9048e-02,
         5.4863e-02,  3.4841e-02,  4.6448e-02,  3.8018e-02,  2.8566e-02,
         2.1550e-02,  4.0594e-02,  4.5684e-02,  5.4036e-02,  4.6776e-02,
         6.7563e-03, -2.9018e-02, -2.1827e-02,  4.1237e-02,  5.3066e-02,
         1.7606e-02, -4.5647e-03,  8.5536e-03,  3.9350e-02, -2.7031e-02,
         1.8550e-02, -3.7265e-02, -1.4612e-02, -1.8905e-02, -2.7270e-02,
        -1.7156e-02,  5.2134e-02,  3.6486e-02,  2.6724e-02, -3.8231e-02,
         1.9471e-02,  3.4652e-02,  2.8043e-02,  2.6671e-02,  2.9177e-02,
         9.1124e-03,  3.9766e-02, -1.5688e-02, -2.6938e-03,  1.5498e-02,
         2.1160e-02,  2.3351e-02,  9.3773e-03,  2.6175e-02,  2.0528e-02,
        -3.3361e-03,  1.5391e-02, -7.9000e-03,  1.4445e-03,  2.4186e-02,
        -1.3715e-02,  2.3052e-02,  2.3146e-02,  1.3876e-02,  1.5644e-02,
         1.0953e-02,  3.2804e-02,  2.8884e-02,  2.2068e-02,  4.2929e-02,
         3.3726e-02, -2.4463e-04, -1.7378e-03,  3.0517e-02, -1.4661e-02,
         1.0876e-03,  1.2042e-02,  1.3411e-02,  2.2283e-02, -4.2601e-03,
         2.1197e-02,  3.3494e-02, -6.2522e-03,  4.6053e-02,  1.9065e-02,
        -3.1178e-03,  4.3707e-02,  1.0365e-02, -4.1911e-03,  2.6809e-02,
         2.9285e-02,  3.4036e-02,  6.2114e-03,  2.3521e-02, -1.2465e-02,
         2.6277e-02,  6.6621e-03,  1.7507e-02,  1.2993e-02, -5.1264e-03,
         1.5060e-02,  2.2523e-02,  3.4667e-02, -3.2054e-03, -7.7263e-03,
         2.7491e-02, -6.6867e-03,  3.8053e-02, -2.2136e-03,  1.4693e-02,
         1.9958e-02,  1.8300e-03, -2.1664e-02,  6.8992e-03, -8.1854e-03,
         6.2428e-03,  8.5612e-05, -8.6274e-03, -3.4853e-03,  3.2756e-02,
         3.8455e-02,  3.0372e-03,  5.9460e-03,  6.3895e-03,  3.3459e-02,
         1.6167e-02,  1.3565e-03,  6.4435e-03,  3.0779e-02,  2.0202e-02,
         3.6422e-02, -4.0076e-03,  7.8945e-03,  1.8751e-02, -2.0417e-02,
         1.4106e-02,  2.5063e-02,  1.1340e-02,  1.2146e-03,  1.7646e-03,
         7.5352e-03, -8.8165e-04,  4.0586e-02,  2.1435e-02,  1.2414e-02,
         1.9554e-02,  2.2691e-02,  1.9043e-02, -1.3746e-02,  3.9573e-02,
         3.2902e-02,  1.4919e-02,  9.8039e-03,  2.6276e-02, -2.9204e-03,
        -7.4894e-03, -1.4144e-02,  2.9402e-02,  3.3924e-02,  2.3765e-03,
         4.8882e-02,  2.5938e-02,  2.7555e-02,  1.7850e-02, -1.4807e-02,
         2.3617e-02, -3.1700e-03, -1.3808e-02,  3.4894e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(6.1159, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:54,593 :: INFO :: Epoch 20: loss tensor(178.1380, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0121, -0.0101, -0.0368,  0.0009,  0.0051, -0.0335, -0.0179,  0.0158,
        -0.0214, -0.0039, -0.0348, -0.0187,  0.0136,  0.0079, -0.0033,  0.0089,
        -0.0138,  0.0136, -0.0005, -0.0096, -0.0067, -0.0086,  0.0124, -0.0359,
        -0.0218,  0.0108, -0.0142, -0.0081, -0.0085, -0.0199,  0.0088,  0.0213,
        -0.0062, -0.0118, -0.0351, -0.0468,  0.0017,  0.0058, -0.0118, -0.0165,
        -0.0368, -0.0185, -0.0026, -0.0002,  0.0020, -0.0110,  0.0013, -0.0373,
        -0.0174, -0.0131, -0.0172, -0.0085, -0.0126, -0.0020, -0.0336,  0.0334,
         0.0060,  0.0076, -0.0321, -0.0427,  0.0037,  0.0031, -0.0397, -0.0011,
        -0.0046, -0.0042, -0.0021, -0.0112,  0.0137, -0.0356, -0.0074, -0.0257,
         0.0089,  0.0077,  0.0171, -0.0361,  0.0118, -0.0273,  0.0090,  0.0161,
        -0.0009, -0.0368,  0.0240,  0.0008, -0.0324, -0.0134,  0.0078, -0.0320,
        -0.0140, -0.0357, -0.0036,  0.0232, -0.0095,  0.0134, -0.0338,  0.0041,
        -0.0069, -0.0220, -0.0330, -0.0047, -0.0215,  0.0190,  0.0059, -0.0264,
        -0.0381, -0.0466, -0.0038, -0.0130,  0.0132,  0.0231, -0.0387,  0.0108,
        -0.0260, -0.0144, -0.0350, -0.0357, -0.0169, -0.0169,  0.0019, -0.0192,
        -0.0178, -0.0048, -0.0027, -0.0154,  0.0110, -0.0217, -0.0343,  0.0197,
         0.0674,  0.0368,  0.0264,  0.0662,  0.0586,  0.0834,  0.0257, -0.0163,
        -0.0302,  0.0472,  0.0142, -0.0396,  0.0705, -0.0355,  0.0600,  0.0549,
         0.0435,  0.0660,  0.0581,  0.0457,  0.0127, -0.0110, -0.0327,  0.0706,
         0.0428, -0.0398, -0.0332, -0.0419, -0.0336, -0.0460,  0.0575, -0.0544,
         0.0656, -0.0541,  0.0700, -0.0316,  0.0544,  0.0438, -0.0415, -0.0249,
        -0.0376,  0.0288,  0.0843,  0.0522, -0.0501,  0.0563,  0.0561, -0.0233,
         0.0495,  0.0552, -0.0285, -0.0238,  0.0520, -0.0344,  0.0617, -0.0469,
         0.0562,  0.0737, -0.0404,  0.0649,  0.0856, -0.0448, -0.0351, -0.0194,
         0.0604, -0.0429,  0.0629,  0.0672,  0.0713,  0.0462,  0.0321,  0.0465,
         0.0637,  0.0760,  0.0211,  0.0741,  0.0328,  0.0702, -0.0421, -0.0509,
         0.0508,  0.0007,  0.0821,  0.0601,  0.0499, -0.0360,  0.0207,  0.0779,
         0.0158, -0.0440,  0.0665, -0.0459,  0.0698, -0.0190,  0.0841,  0.0599,
         0.0646,  0.0799,  0.0514,  0.0697,  0.0540,  0.0504,  0.0381,  0.0507,
         0.0609,  0.0768,  0.0607,  0.0288, -0.0285, -0.0189,  0.0487,  0.0733,
         0.0363,  0.0100,  0.0290,  0.0547, -0.0276,  0.0361, -0.0451, -0.0029,
        -0.0137, -0.0273, -0.0028,  0.0695,  0.0590,  0.0503, -0.0456,  0.0379,
         0.0610,  0.0533,  0.0529,  0.0516,  0.0312,  0.0657, -0.0008,  0.0178,
         0.0405,  0.0467,  0.0478,  0.0325,  0.0496,  0.0428,  0.0152,  0.0399,
         0.0112,  0.0220,  0.0482,  0.0006,  0.0477,  0.0475,  0.0365,  0.0393,
         0.0348,  0.0602,  0.0541,  0.0478,  0.0691,  0.0552,  0.0220,  0.0178,
         0.0509, -0.0012,  0.0220,  0.0358,  0.0403,  0.0449,  0.0143,  0.0437,
         0.0581,  0.0135,  0.0677,  0.0410,  0.0171,  0.0703,  0.0359,  0.0160,
         0.0521,  0.0556,  0.0579,  0.0281,  0.0476,  0.0037,  0.0512,  0.0312,
         0.0418,  0.0366,  0.0151,  0.0375,  0.0447,  0.0610,  0.0167,  0.0097,
         0.0517,  0.0145,  0.0625,  0.0188,  0.0391,  0.0435,  0.0251, -0.0112,
         0.0297,  0.0091,  0.0283,  0.0209,  0.0104,  0.0173,  0.0581,  0.0609,
         0.0208,  0.0281,  0.0301,  0.0562,  0.0403,  0.0231,  0.0288,  0.0545,
         0.0433,  0.0585,  0.0122,  0.0313,  0.0426, -0.0086,  0.0380,  0.0497,
         0.0352,  0.0184,  0.0220,  0.0295,  0.0205,  0.0652,  0.0460,  0.0359,
         0.0440,  0.0488,  0.0448,  0.0052,  0.0642,  0.0563,  0.0383,  0.0334,
         0.0503,  0.0180,  0.0123,  0.0008,  0.0524,  0.0572,  0.0270,  0.0747,
         0.0496,  0.0471,  0.0406, -0.0007,  0.0472,  0.0180,  0.0015,  0.0593],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.6240, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:57,342 :: INFO :: Epoch 25: loss tensor(175.0796, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.6087e-03, -9.5967e-04, -3.1484e-02,  1.1223e-02,  1.4311e-02,
        -2.8387e-02, -6.2100e-03,  2.4514e-02, -1.3022e-02,  4.0292e-03,
        -3.0107e-02, -1.3249e-02,  2.2131e-02,  1.9177e-02,  4.9250e-03,
         1.4327e-02, -9.7528e-03,  2.5451e-02,  3.7941e-03,  1.4440e-03,
         4.4474e-05, -1.2833e-03,  2.5165e-02, -3.1273e-02, -1.5577e-02,
         1.9037e-02, -8.7064e-03,  2.7445e-03,  7.3772e-03, -1.4297e-02,
         1.9851e-02,  3.1550e-02,  5.8443e-03, -2.0739e-03, -2.9915e-02,
        -4.8118e-02,  8.8157e-03,  1.2119e-02, -5.7746e-03, -8.3309e-03,
        -3.5022e-02, -1.1931e-02,  8.9468e-03,  5.5550e-03,  9.2770e-03,
        -5.1423e-03,  8.9426e-03, -3.3181e-02, -8.5821e-03, -7.3626e-03,
        -1.0740e-02, -5.5873e-04, -1.0306e-02,  3.9261e-03, -2.5893e-02,
         4.6696e-02,  1.9885e-02,  2.1956e-02, -2.5653e-02, -4.1265e-02,
         1.0323e-02,  8.9345e-03, -3.6719e-02,  8.2084e-03,  4.3476e-03,
         3.1969e-03,  1.0097e-02, -3.8586e-03,  2.1727e-02, -3.3268e-02,
         3.4891e-04, -1.5796e-02,  1.1374e-02,  1.7365e-02,  2.8685e-02,
        -3.3889e-02,  1.9645e-02, -2.0550e-02,  2.0044e-02,  2.3215e-02,
         1.1212e-02, -3.0071e-02,  3.2775e-02,  1.5270e-02, -2.8472e-02,
        -3.3174e-03,  1.4176e-02, -2.5216e-02, -6.8657e-03, -3.4938e-02,
         4.5568e-03,  3.4730e-02, -3.4619e-03,  2.4676e-02, -2.6367e-02,
         1.9138e-02, -1.6980e-03, -1.6065e-02, -3.0141e-02,  3.4685e-03,
        -1.4001e-02,  2.7806e-02,  1.6482e-02, -2.2222e-02, -3.4148e-02,
        -4.7880e-02,  5.2127e-03, -6.6298e-03,  2.1790e-02,  3.4276e-02,
        -3.5848e-02,  1.8211e-02, -1.5730e-02, -4.3829e-03, -3.2302e-02,
        -3.4288e-02, -9.6411e-03, -8.0516e-03,  4.2772e-03, -1.0733e-02,
        -1.2189e-02,  2.1183e-03,  4.8089e-03, -1.1409e-02,  2.0523e-02,
        -1.9066e-02, -3.0252e-02,  3.2253e-02,  8.7899e-02,  5.1455e-02,
         4.3730e-02,  7.8088e-02,  7.7983e-02,  1.0556e-01,  4.0729e-02,
        -9.4995e-03, -2.7623e-02,  5.3499e-02,  3.1037e-02, -4.1053e-02,
         9.0231e-02, -3.6640e-02,  8.1384e-02,  8.1427e-02,  6.3907e-02,
         8.2155e-02,  7.1307e-02,  7.0591e-02,  3.0380e-02,  4.4621e-03,
        -3.3069e-02,  9.6251e-02,  5.9890e-02, -4.5687e-02, -3.1091e-02,
        -4.4502e-02, -3.4268e-02, -5.3052e-02,  6.9825e-02, -6.2320e-02,
         8.2981e-02, -6.2521e-02,  8.8209e-02, -2.9198e-02,  6.7174e-02,
         5.3481e-02, -4.5861e-02, -2.0912e-02, -3.8005e-02,  4.6461e-02,
         1.0770e-01,  6.4966e-02, -5.7982e-02,  7.4962e-02,  7.7399e-02,
        -1.8196e-02,  5.7998e-02,  7.0794e-02, -2.4874e-02, -2.0247e-02,
         6.8999e-02, -3.4197e-02,  7.3822e-02, -5.3917e-02,  7.0963e-02,
         9.4765e-02, -4.5717e-02,  8.7453e-02,  1.1275e-01, -5.1968e-02,
        -2.9031e-02, -1.3026e-02,  7.3228e-02, -4.8595e-02,  7.7380e-02,
         8.5932e-02,  9.6727e-02,  5.1756e-02,  5.0854e-02,  6.8581e-02,
         8.2807e-02,  1.0083e-01,  3.5237e-02,  8.8405e-02,  5.0747e-02,
         9.5755e-02, -4.4826e-02, -5.9099e-02,  6.0787e-02,  1.2712e-02,
         1.0342e-01,  7.3437e-02,  6.5413e-02, -3.7918e-02,  3.8172e-02,
         1.0140e-01,  3.1208e-02, -4.8527e-02,  8.2778e-02, -5.2791e-02,
         9.3851e-02, -7.7077e-03,  1.0689e-01,  8.2696e-02,  8.6983e-02,
         1.0454e-01,  6.6841e-02,  9.2890e-02,  6.8096e-02,  7.1279e-02,
         5.3035e-02,  5.8471e-02,  7.5388e-02,  9.3551e-02,  7.3776e-02,
         5.2090e-02, -2.5043e-02, -1.3406e-02,  5.3172e-02,  9.2438e-02,
         5.3894e-02,  2.6265e-02,  5.0362e-02,  7.0046e-02, -2.5742e-02,
         5.3534e-02, -5.1580e-02,  1.0938e-02, -6.2851e-03, -2.4271e-02,
         1.6527e-02,  8.6809e-02,  8.1510e-02,  7.3490e-02, -5.1191e-02,
         5.5127e-02,  8.7224e-02,  7.8291e-02,  7.9454e-02,  7.0947e-02,
         5.3453e-02,  9.1632e-02,  1.6940e-02,  3.8821e-02,  6.6740e-02,
         7.2986e-02,  7.1962e-02,  5.5044e-02,  7.0528e-02,  6.2132e-02,
         3.4485e-02,  6.3501e-02,  3.3312e-02,  4.2907e-02,  7.0001e-02,
         1.7284e-02,  7.3089e-02,  7.0842e-02,  5.7385e-02,  6.1293e-02,
         5.7336e-02,  8.8376e-02,  7.7111e-02,  7.3439e-02,  9.5876e-02,
         7.2992e-02,  4.4656e-02,  3.8296e-02,  6.8046e-02,  1.5131e-02,
         4.4358e-02,  5.8965e-02,  6.8635e-02,  6.5281e-02,  3.3249e-02,
         6.4527e-02,  8.0612e-02,  3.5200e-02,  8.5770e-02,  6.0344e-02,
         3.9667e-02,  9.7165e-02,  6.2778e-02,  4.0029e-02,  7.7485e-02,
         8.1970e-02,  7.8953e-02,  5.0962e-02,  6.9665e-02,  2.3131e-02,
         7.5800e-02,  5.7001e-02,  6.5692e-02,  6.0135e-02,  3.7201e-02,
         5.7905e-02,  6.6130e-02,  8.7687e-02,  3.6846e-02,  2.9926e-02,
         7.3712e-02,  3.8893e-02,  8.5118e-02,  4.0454e-02,  6.3657e-02,
         6.5501e-02,  4.9759e-02,  3.8502e-03,  5.1451e-02,  2.6832e-02,
         4.9651e-02,  4.2022e-02,  3.1347e-02,  3.8743e-02,  8.2942e-02,
         7.9660e-02,  3.6603e-02,  4.9723e-02,  5.4825e-02,  7.5791e-02,
         6.2843e-02,  4.4296e-02,  5.0070e-02,  7.7746e-02,  6.5102e-02,
         7.7570e-02,  2.6808e-02,  5.4910e-02,  6.6077e-02,  6.8559e-03,
         6.0317e-02,  7.3344e-02,  5.8455e-02,  3.4319e-02,  4.3395e-02,
         5.0153e-02,  4.4280e-02,  8.9391e-02,  6.8790e-02,  5.8227e-02,
         6.8120e-02,  7.5657e-02,  7.1614e-02,  2.8847e-02,  8.8221e-02,
         7.6727e-02,  6.1068e-02,  5.7452e-02,  7.2232e-02,  3.9627e-02,
         3.3751e-02,  2.0309e-02,  7.2291e-02,  7.8200e-02,  5.3573e-02,
         1.0066e-01,  7.0283e-02,  6.0343e-02,  6.0854e-02,  1.8101e-02,
         6.8543e-02,  4.0140e-02,  1.9920e-02,  8.1562e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(9.0633, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:59,748 :: INFO :: Epoch 30: loss tensor(175.7254, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.3548e-03,  5.7602e-03, -2.5160e-02,  1.8364e-02,  2.0382e-02,
        -2.0329e-02,  5.3447e-03,  3.0131e-02, -5.3963e-03,  9.9065e-03,
        -2.2860e-02, -8.4877e-03,  2.8275e-02,  2.7956e-02,  1.1880e-02,
         1.7808e-02, -6.7377e-03,  3.4731e-02,  6.9765e-03,  1.0797e-02,
         3.8319e-03,  4.2822e-03,  3.5328e-02, -2.5461e-02, -1.0071e-02,
         2.3396e-02, -4.9955e-03,  1.1272e-02,  2.3234e-02, -1.0614e-02,
         2.7328e-02,  3.8543e-02,  1.6719e-02,  5.3563e-03, -2.4224e-02,
        -4.7610e-02,  1.3287e-02,  1.4840e-02, -1.9497e-03, -5.4822e-04,
        -3.2228e-02, -4.5210e-03,  2.0369e-02,  8.2121e-03,  1.3922e-02,
        -1.5474e-04,  1.4519e-02, -2.7730e-02,  1.3798e-04, -3.0326e-03,
        -5.4869e-03,  4.6307e-03, -7.7542e-03,  7.5249e-03, -1.6775e-02,
         5.6314e-02,  3.3414e-02,  3.3956e-02, -1.7702e-02, -3.7355e-02,
         1.4695e-02,  1.3628e-02, -3.1571e-02,  1.5153e-02,  1.1511e-02,
         7.8482e-03,  2.0885e-02,  2.2811e-03,  2.6814e-02, -3.0199e-02,
         6.2028e-03, -5.8410e-03,  1.2873e-02,  2.5293e-02,  3.6442e-02,
        -3.0879e-02,  2.6690e-02, -1.3979e-02,  2.7477e-02,  2.9988e-02,
         2.0525e-02, -1.9934e-02,  3.7954e-02,  2.6568e-02, -2.3761e-02,
         5.3594e-03,  1.9004e-02, -1.8160e-02, -9.1392e-04, -3.3623e-02,
         1.0270e-02,  4.2397e-02,  2.4924e-04,  3.2262e-02, -1.7444e-02,
         3.1601e-02,  7.7014e-04, -1.0425e-02, -2.6502e-02,  9.3928e-03,
        -6.4838e-03,  3.3170e-02,  2.3804e-02, -1.6657e-02, -2.8011e-02,
        -4.6323e-02,  1.2639e-02, -1.3682e-03,  2.8635e-02,  4.2048e-02,
        -3.0521e-02,  2.3238e-02, -4.0595e-03,  4.4166e-03, -2.8869e-02,
        -3.3236e-02, -3.8002e-03,  1.3977e-03,  5.9534e-03, -3.3441e-03,
        -7.8721e-03,  6.4256e-03,  1.1302e-02, -8.0720e-03,  2.6803e-02,
        -1.6726e-02, -2.5637e-02,  4.0895e-02,  1.0706e-01,  6.4600e-02,
         6.0210e-02,  8.6823e-02,  9.1320e-02,  1.2488e-01,  5.4329e-02,
        -1.4509e-03, -2.2426e-02,  5.7790e-02,  4.7002e-02, -4.0341e-02,
         1.0816e-01, -3.5994e-02,  1.0116e-01,  1.0749e-01,  8.3052e-02,
         9.6886e-02,  8.1227e-02,  9.4832e-02,  4.7434e-02,  2.1342e-02,
        -3.1688e-02,  1.2014e-01,  7.4839e-02, -5.0881e-02, -2.6844e-02,
        -4.4615e-02, -3.3152e-02, -5.8960e-02,  7.9678e-02, -6.8643e-02,
         9.8974e-02, -6.9454e-02,  1.0424e-01, -2.4606e-02,  7.8598e-02,
         5.8401e-02, -4.8715e-02, -1.5235e-02, -3.5823e-02,  6.2634e-02,
         1.2947e-01,  7.5951e-02, -6.4597e-02,  9.2338e-02,  9.7228e-02,
        -1.0957e-02,  6.3965e-02,  8.4462e-02, -1.9407e-02, -1.5293e-02,
         8.4014e-02, -3.1692e-02,  8.1600e-02, -5.9693e-02,  8.3672e-02,
         1.1486e-01, -5.0192e-02,  1.0754e-01,  1.3801e-01, -5.8088e-02,
        -1.9749e-02, -4.9092e-03,  8.4238e-02, -5.2876e-02,  8.7672e-02,
         1.0362e-01,  1.2035e-01,  5.3840e-02,  6.7905e-02,  8.9441e-02,
         1.0041e-01,  1.2430e-01,  4.7686e-02,  9.8657e-02,  6.6748e-02,
         1.1984e-01, -4.5450e-02, -6.6175e-02,  6.8375e-02,  2.4710e-02,
         1.2314e-01,  8.4704e-02,  7.4693e-02, -3.8287e-02,  5.3851e-02,
         1.2398e-01,  4.5568e-02, -5.1388e-02,  9.7278e-02, -5.8529e-02,
         1.1603e-01,  4.8153e-03,  1.2740e-01,  1.0426e-01,  1.0331e-01,
         1.2786e-01,  8.0404e-02,  1.1498e-01,  7.9218e-02,  9.0835e-02,
         6.5486e-02,  6.3172e-02,  8.7988e-02,  1.0247e-01,  8.5075e-02,
         7.4663e-02, -1.9381e-02, -6.4106e-03,  5.5849e-02,  1.0896e-01,
         6.9656e-02,  4.2488e-02,  7.1577e-02,  8.4767e-02, -2.1994e-02,
         6.9949e-02, -5.6767e-02,  2.4726e-02,  2.3530e-03, -1.9136e-02,
         3.6012e-02,  1.0332e-01,  1.0247e-01,  9.5708e-02, -5.5099e-02,
         7.0206e-02,  1.1342e-01,  1.0304e-01,  1.0606e-01,  8.7506e-02,
         7.5856e-02,  1.1708e-01,  3.5197e-02,  5.8966e-02,  9.3152e-02,
         9.9274e-02,  9.5418e-02,  7.6134e-02,  8.8997e-02,  7.7820e-02,
         5.3146e-02,  8.5857e-02,  5.6074e-02,  6.2684e-02,  8.9385e-02,
         3.5314e-02,  9.8394e-02,  9.3250e-02,  7.6346e-02,  8.1104e-02,
         7.8064e-02,  1.1638e-01,  9.7958e-02,  9.8528e-02,  1.2251e-01,
         8.7391e-02,  6.6519e-02,  5.8363e-02,  8.3148e-02,  3.3161e-02,
         6.6837e-02,  8.0803e-02,  9.6978e-02,  8.3404e-02,  5.0830e-02,
         8.4642e-02,  1.0085e-01,  5.6950e-02,  1.0012e-01,  7.6816e-02,
         6.3251e-02,  1.2373e-01,  8.9915e-02,  6.5217e-02,  1.0287e-01,
         1.0808e-01,  9.7722e-02,  7.4238e-02,  8.9572e-02,  4.3767e-02,
         1.0005e-01,  8.3023e-02,  8.9450e-02,  8.3196e-02,  6.0281e-02,
         7.5525e-02,  8.6909e-02,  1.1434e-01,  5.5934e-02,  5.1529e-02,
         9.3506e-02,  6.4309e-02,  1.0599e-01,  6.1170e-02,  8.8045e-02,
         8.5213e-02,  7.5016e-02,  2.1815e-02,  7.1547e-02,  4.3477e-02,
         6.9450e-02,  6.2105e-02,  5.2295e-02,  5.9359e-02,  1.0743e-01,
         9.4428e-02,  5.0137e-02,  7.0086e-02,  7.9762e-02,  9.2091e-02,
         8.3736e-02,  6.4011e-02,  6.9907e-02,  1.0064e-01,  8.5562e-02,
         9.4884e-02,  3.9541e-02,  7.8022e-02,  8.8467e-02,  2.3643e-02,
         8.0463e-02,  9.5777e-02,  8.0804e-02,  4.8850e-02,  6.4838e-02,
         6.9067e-02,  6.8918e-02,  1.1303e-01,  8.9653e-02,  7.9068e-02,
         9.1899e-02,  1.0240e-01,  9.8045e-02,  5.4320e-02,  1.1144e-01,
         9.4311e-02,  8.3131e-02,  8.1135e-02,  9.1795e-02,  6.0444e-02,
         5.5150e-02,  4.2583e-02,  8.9461e-02,  9.6805e-02,  8.0538e-02,
         1.2630e-01,  8.8287e-02,  6.8999e-02,  7.8364e-02,  3.9107e-02,
         8.7531e-02,  6.1934e-02,  3.9981e-02,  1.0227e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.4068, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:02,405 :: INFO :: Epoch 35: loss tensor(171.2412, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0158,  0.0113, -0.0175,  0.0244,  0.0258, -0.0108,  0.0164,  0.0358,
         0.0021,  0.0162, -0.0138, -0.0031,  0.0339,  0.0360,  0.0180,  0.0209,
        -0.0035,  0.0423,  0.0106,  0.0199,  0.0076,  0.0096,  0.0446, -0.0189,
        -0.0048,  0.0276, -0.0010,  0.0196,  0.0383, -0.0062,  0.0338,  0.0446,
         0.0267,  0.0125, -0.0182, -0.0455,  0.0176,  0.0178,  0.0013,  0.0073,
        -0.0281,  0.0034,  0.0316,  0.0108,  0.0178,  0.0048,  0.0200, -0.0205,
         0.0084,  0.0017, -0.0010,  0.0081, -0.0039,  0.0105, -0.0065,  0.0642,
         0.0467,  0.0460, -0.0089, -0.0316,  0.0182,  0.0193, -0.0250,  0.0218,
         0.0179,  0.0121,  0.0307,  0.0076,  0.0314, -0.0262,  0.0119,  0.0033,
         0.0151,  0.0344,  0.0428, -0.0272,  0.0341, -0.0070,  0.0337,  0.0367,
         0.0290, -0.0084,  0.0421,  0.0365, -0.0186,  0.0124,  0.0236, -0.0106,
         0.0043, -0.0313,  0.0152,  0.0489,  0.0047,  0.0383, -0.0076,  0.0418,
         0.0031, -0.0054, -0.0216,  0.0161,  0.0013,  0.0379,  0.0300, -0.0096,
        -0.0206, -0.0426,  0.0195,  0.0033,  0.0359,  0.0483, -0.0226,  0.0280,
         0.0077,  0.0129, -0.0248, -0.0311,  0.0017,  0.0113,  0.0083,  0.0039,
        -0.0039,  0.0101,  0.0179, -0.0039,  0.0324, -0.0129, -0.0197,  0.0484,
         0.1247,  0.0766,  0.0749,  0.0926,  0.0991,  0.1408,  0.0664,  0.0071,
        -0.0160,  0.0607,  0.0611, -0.0383,  0.1242, -0.0341,  0.1190,  0.1324,
         0.1003,  0.1105,  0.0882,  0.1176,  0.0626,  0.0372, -0.0294,  0.1423,
         0.0874, -0.0556, -0.0216, -0.0431, -0.0308, -0.0641,  0.0874, -0.0736,
         0.1132, -0.0752,  0.1185, -0.0190,  0.0890,  0.0603, -0.0506, -0.0085,
        -0.0320,  0.0773,  0.1492,  0.0855, -0.0703,  0.1085,  0.1153, -0.0028,
         0.0680,  0.0963, -0.0132, -0.0095,  0.0973, -0.0279,  0.0856, -0.0647,
         0.0948,  0.1339, -0.0539,  0.1248,  0.1609, -0.0635, -0.0083,  0.0035,
         0.0940, -0.0564,  0.0944,  0.1202,  0.1422,  0.0544,  0.0831,  0.1086,
         0.1167,  0.1462,  0.0585,  0.1052,  0.0800,  0.1425, -0.0446, -0.0724,
         0.0740,  0.0363,  0.1411,  0.0946,  0.0788, -0.0378,  0.0666,  0.1456,
         0.0587, -0.0530,  0.1106, -0.0635,  0.1363,  0.0169,  0.1458,  0.1242,
         0.1142,  0.1496,  0.0923,  0.1360,  0.0874,  0.1082,  0.0752,  0.0655,
         0.0988,  0.1051,  0.0950,  0.0958, -0.0127,  0.0012,  0.0566,  0.1232,
         0.0835,  0.0577,  0.0916,  0.0989, -0.0174,  0.0853, -0.0611,  0.0371,
         0.0112, -0.0128,  0.0551,  0.1192,  0.1221,  0.1161, -0.0579,  0.0832,
         0.1388,  0.1265,  0.1321,  0.1003,  0.0979,  0.1417,  0.0520,  0.0766,
         0.1193,  0.1255,  0.1177,  0.0951,  0.1045,  0.0893,  0.0701,  0.1060,
         0.0783,  0.0809,  0.1059,  0.0520,  0.1236,  0.1138,  0.0932,  0.0976,
         0.0961,  0.1439,  0.1155,  0.1225,  0.1489,  0.0977,  0.0865,  0.0765,
         0.0963,  0.0502,  0.0881,  0.1004,  0.1249,  0.0994,  0.0657,  0.1033,
         0.1181,  0.0782,  0.1100,  0.0902,  0.0859,  0.1499,  0.1165,  0.0907,
         0.1280,  0.1335,  0.1128,  0.0974,  0.1065,  0.0636,  0.1235,  0.1083,
         0.1122,  0.1057,  0.0829,  0.0896,  0.1074,  0.1404,  0.0724,  0.0725,
         0.1104,  0.0893,  0.1244,  0.0797,  0.1118,  0.1022,  0.0995,  0.0407,
         0.0889,  0.0578,  0.0866,  0.0801,  0.0715,  0.0774,  0.1309,  0.1048,
         0.0598,  0.0882,  0.1044,  0.1039,  0.1021,  0.0809,  0.0873,  0.1227,
         0.1040,  0.1107,  0.0499,  0.0996,  0.1093,  0.0396,  0.0974,  0.1164,
         0.1012,  0.0623,  0.0852,  0.0845,  0.0935,  0.1364,  0.1076,  0.0976,
         0.1149,  0.1288,  0.1238,  0.0804,  0.1340,  0.1084,  0.1036,  0.1043,
         0.1086,  0.0792,  0.0746,  0.0654,  0.1032,  0.1129,  0.1072,  0.1511,
         0.1028,  0.0725,  0.0920,  0.0606,  0.1036,  0.0817,  0.0592,  0.1209],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.6615, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:05,155 :: INFO :: Epoch 40: loss tensor(171.0488, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0229,  0.0152, -0.0103,  0.0293,  0.0288, -0.0024,  0.0256,  0.0391,
         0.0075,  0.0203, -0.0049,  0.0012,  0.0367,  0.0418,  0.0231,  0.0226,
        -0.0018,  0.0472,  0.0131,  0.0273,  0.0106,  0.0133,  0.0516, -0.0128,
        -0.0004,  0.0298,  0.0007,  0.0265,  0.0515, -0.0030,  0.0381,  0.0484,
         0.0353,  0.0170, -0.0132, -0.0429,  0.0200,  0.0190,  0.0036,  0.0130,
        -0.0230,  0.0098,  0.0400,  0.0122,  0.0201,  0.0073,  0.0236, -0.0136,
         0.0150,  0.0052,  0.0025,  0.0106, -0.0008,  0.0116,  0.0031,  0.0703,
         0.0566,  0.0568, -0.0012, -0.0256,  0.0203,  0.0227, -0.0192,  0.0266,
         0.0228,  0.0152,  0.0377,  0.0114,  0.0344, -0.0232,  0.0165,  0.0111,
         0.0163,  0.0432,  0.0466, -0.0237,  0.0396, -0.0006,  0.0373,  0.0422,
         0.0363,  0.0029,  0.0445,  0.0440, -0.0147,  0.0178,  0.0261, -0.0041,
         0.0083, -0.0292,  0.0185,  0.0539,  0.0073,  0.0426,  0.0011,  0.0488,
         0.0034, -0.0015, -0.0170,  0.0214,  0.0067,  0.0399,  0.0335, -0.0023,
        -0.0128, -0.0381,  0.0252,  0.0062,  0.0406,  0.0518, -0.0140,  0.0302,
         0.0175,  0.0198, -0.0207, -0.0290,  0.0059,  0.0200,  0.0097,  0.0098,
        -0.0013,  0.0123,  0.0226, -0.0009,  0.0355, -0.0095, -0.0147,  0.0548,
         0.1406,  0.0865,  0.0871,  0.0958,  0.1023,  0.1534,  0.0761,  0.0152,
        -0.0086,  0.0634,  0.0732, -0.0354,  0.1378, -0.0317,  0.1346,  0.1565,
         0.1153,  0.1223,  0.0922,  0.1391,  0.0754,  0.0512, -0.0265,  0.1619,
         0.0971, -0.0598, -0.0162, -0.0405, -0.0280, -0.0686,  0.0923, -0.0776,
         0.1254, -0.0801,  0.1299, -0.0132,  0.0978,  0.0586, -0.0517, -0.0018,
        -0.0277,  0.0898,  0.1668,  0.0930, -0.0753,  0.1226,  0.1313,  0.0058,
         0.0694,  0.1058, -0.0073, -0.0033,  0.1081, -0.0232,  0.0862, -0.0688,
         0.1035,  0.1511, -0.0571,  0.1392,  0.1813, -0.0683,  0.0044,  0.0111,
         0.1017, -0.0593,  0.0981,  0.1349,  0.1616,  0.0535,  0.0957,  0.1257,
         0.1308,  0.1661,  0.0672,  0.1079,  0.0904,  0.1632, -0.0430, -0.0778,
         0.0773,  0.0462,  0.1565,  0.1021,  0.0801, -0.0369,  0.0767,  0.1656,
         0.0695, -0.0537,  0.1215, -0.0679,  0.1543,  0.0272,  0.1610,  0.1426,
         0.1210,  0.1691,  0.1023,  0.1551,  0.0923,  0.1231,  0.0816,  0.0654,
         0.1079,  0.1044,  0.1029,  0.1152, -0.0058,  0.0084,  0.0571,  0.1341,
         0.0948,  0.0712,  0.1101,  0.1119, -0.0126,  0.0983, -0.0647,  0.0475,
         0.0192, -0.0060,  0.0730,  0.1336,  0.1397,  0.1347, -0.0597,  0.0936,
         0.1633,  0.1490,  0.1575,  0.1103,  0.1187,  0.1657,  0.0670,  0.0920,
         0.1444,  0.1508,  0.1394,  0.1126,  0.1172,  0.0969,  0.0853,  0.1244,
         0.0997,  0.0982,  0.1196,  0.0669,  0.1479,  0.1329,  0.1086,  0.1114,
         0.1117,  0.1701,  0.1305,  0.1458,  0.1745,  0.1047,  0.1042,  0.0927,
         0.1082,  0.0661,  0.1082,  0.1178,  0.1518,  0.1137,  0.0785,  0.1206,
         0.1328,  0.0988,  0.1163,  0.1005,  0.1086,  0.1750,  0.1421,  0.1155,
         0.1521,  0.1579,  0.1250,  0.1204,  0.1203,  0.0824,  0.1463,  0.1326,
         0.1338,  0.1272,  0.1051,  0.1005,  0.1273,  0.1657,  0.0864,  0.0935,
         0.1245,  0.1134,  0.1406,  0.0970,  0.1345,  0.1173,  0.1236,  0.0593,
         0.1047,  0.0694,  0.1015,  0.0968,  0.0888,  0.0933,  0.1535,  0.1118,
         0.0672,  0.1042,  0.1276,  0.1121,  0.1179,  0.0955,  0.1028,  0.1437,
         0.1207,  0.1256,  0.0589,  0.1201,  0.1288,  0.0543,  0.1119,  0.1359,
         0.1195,  0.0752,  0.1034,  0.0970,  0.1176,  0.1589,  0.1232,  0.1140,
         0.1370,  0.1545,  0.1481,  0.1058,  0.1559,  0.1194,  0.1221,  0.1271,
         0.1233,  0.0957,  0.0922,  0.0883,  0.1135,  0.1268,  0.1327,  0.1752,
         0.1142,  0.0725,  0.1021,  0.0824,  0.1168,  0.0998,  0.0769,  0.1377],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.8263, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:05,155 :: INFO :: ----- frontend -----
2023-04-11 14:36:05,171 :: INFO :: Environment 0
2023-04-11 14:36:20,155 :: INFO :: Epoch 5: loss tensor(867.4779, device='cuda:0'), U.norm 14.6512451171875, V.norm 18.335725784301758, MLP.norm 1.881217360496521
2023-04-11 14:36:20,249 :: INFO :: Epoch 10: loss tensor(853.1010, device='cuda:0'), U.norm 12.049667358398438, V.norm 17.845590591430664, MLP.norm 2.6881327629089355
2023-04-11 14:36:20,327 :: INFO :: Epoch 15: loss tensor(832.8370, device='cuda:0'), U.norm 10.39042854309082, V.norm 17.62509536743164, MLP.norm 3.738251209259033
2023-04-11 14:36:20,421 :: INFO :: Epoch 20: loss tensor(807.6191, device='cuda:0'), U.norm 9.201498031616211, V.norm 17.485361099243164, MLP.norm 4.859912872314453
2023-04-11 14:36:20,421 :: INFO :: Environment 1
2023-04-11 14:36:34,624 :: INFO :: Epoch 5: loss tensor(790.1326, device='cuda:0'), U.norm 14.646727561950684, V.norm 18.199363708496094, MLP.norm 1.8353337049484253
2023-04-11 14:36:34,717 :: INFO :: Epoch 10: loss tensor(779.4006, device='cuda:0'), U.norm 12.0400972366333, V.norm 17.654314041137695, MLP.norm 2.548527956008911
2023-04-11 14:36:34,811 :: INFO :: Epoch 15: loss tensor(762.9784, device='cuda:0'), U.norm 10.37524700164795, V.norm 17.408733367919922, MLP.norm 3.50042986869812
2023-04-11 14:36:34,889 :: INFO :: Epoch 20: loss tensor(743.2811, device='cuda:0'), U.norm 9.179858207702637, V.norm 17.254140853881836, MLP.norm 4.513216972351074
2023-04-11 14:36:34,889 :: INFO :: Environment 2
2023-04-11 14:36:49,264 :: INFO :: Epoch 5: loss tensor(867.1854, device='cuda:0'), U.norm 14.649201393127441, V.norm 18.339313507080078, MLP.norm 1.8726075887680054
2023-04-11 14:36:49,342 :: INFO :: Epoch 10: loss tensor(853.4922, device='cuda:0'), U.norm 12.045860290527344, V.norm 17.848207473754883, MLP.norm 2.665830612182617
2023-04-11 14:36:49,436 :: INFO :: Epoch 15: loss tensor(833.8452, device='cuda:0'), U.norm 10.385004997253418, V.norm 17.625732421875, MLP.norm 3.715772867202759
2023-04-11 14:36:49,515 :: INFO :: Epoch 20: loss tensor(809.1098, device='cuda:0'), U.norm 9.194503784179688, V.norm 17.481609344482422, MLP.norm 4.816987991333008
2023-04-11 14:36:49,515 :: INFO :: Environment 3
2023-04-11 14:37:01,811 :: INFO :: Epoch 5: loss tensor(842.1672, device='cuda:0'), U.norm 14.64866828918457, V.norm 18.30278778076172, MLP.norm 1.8715590238571167
2023-04-11 14:37:01,921 :: INFO :: Epoch 10: loss tensor(828.3147, device='cuda:0'), U.norm 12.044986724853516, V.norm 17.798677444458008, MLP.norm 2.6583030223846436
2023-04-11 14:37:02,014 :: INFO :: Epoch 15: loss tensor(808.8510, device='cuda:0'), U.norm 10.383768081665039, V.norm 17.571340560913086, MLP.norm 3.6767709255218506
2023-04-11 14:37:02,125 :: INFO :: Epoch 20: loss tensor(785.2296, device='cuda:0'), U.norm 9.192596435546875, V.norm 17.42459487915039, MLP.norm 4.737098693847656
2023-04-11 14:37:02,125 :: INFO :: Environment 4
2023-04-11 14:37:15,468 :: INFO :: Epoch 5: loss tensor(844.5430, device='cuda:0'), U.norm 14.648178100585938, V.norm 18.296794891357422, MLP.norm 1.8686424493789673
2023-04-11 14:37:15,561 :: INFO :: Epoch 10: loss tensor(831.2016, device='cuda:0'), U.norm 12.04350757598877, V.norm 17.789663314819336, MLP.norm 2.638491153717041
2023-04-11 14:37:15,655 :: INFO :: Epoch 15: loss tensor(811.8282, device='cuda:0'), U.norm 10.381245613098145, V.norm 17.560016632080078, MLP.norm 3.654783010482788
2023-04-11 14:37:15,749 :: INFO :: Epoch 20: loss tensor(788.2235, device='cuda:0'), U.norm 9.188879013061523, V.norm 17.412944793701172, MLP.norm 4.745960235595703
2023-04-11 14:37:15,749 :: INFO :: Environment 5
2023-04-11 14:37:30,063 :: INFO :: Epoch 5: loss tensor(860.7850, device='cuda:0'), U.norm 14.650106430053711, V.norm 18.32836151123047, MLP.norm 1.8761662244796753
2023-04-11 14:37:30,155 :: INFO :: Epoch 10: loss tensor(847.8452, device='cuda:0'), U.norm 12.047295570373535, V.norm 17.834775924682617, MLP.norm 2.6703073978424072
2023-04-11 14:37:30,233 :: INFO :: Epoch 15: loss tensor(828.9939, device='cuda:0'), U.norm 10.387002944946289, V.norm 17.61174774169922, MLP.norm 3.7093913555145264
2023-04-11 14:37:30,327 :: INFO :: Epoch 20: loss tensor(805.5172, device='cuda:0'), U.norm 9.196952819824219, V.norm 17.468788146972656, MLP.norm 4.819164276123047
2023-04-11 14:37:30,327 :: INFO :: Environment 6
2023-04-11 14:37:43,077 :: INFO :: Epoch 5: loss tensor(825.5038, device='cuda:0'), U.norm 14.648138999938965, V.norm 18.257549285888672, MLP.norm 1.8619695901870728
2023-04-11 14:37:43,186 :: INFO :: Epoch 10: loss tensor(813.0729, device='cuda:0'), U.norm 12.04360580444336, V.norm 17.737010955810547, MLP.norm 2.6255075931549072
2023-04-11 14:37:43,296 :: INFO :: Epoch 15: loss tensor(795.1753, device='cuda:0'), U.norm 10.381319046020508, V.norm 17.50526237487793, MLP.norm 3.628584861755371
2023-04-11 14:37:43,421 :: INFO :: Epoch 20: loss tensor(773.3159, device='cuda:0'), U.norm 9.188830375671387, V.norm 17.35702896118164, MLP.norm 4.692885398864746
2023-04-11 14:37:43,436 :: INFO :: Environment 7
2023-04-11 14:37:56,717 :: INFO :: Epoch 5: loss tensor(835.9232, device='cuda:0'), U.norm 14.647627830505371, V.norm 18.287508010864258, MLP.norm 1.869094729423523
2023-04-11 14:37:56,811 :: INFO :: Epoch 10: loss tensor(822.9033, device='cuda:0'), U.norm 12.04224681854248, V.norm 17.778467178344727, MLP.norm 2.6512551307678223
2023-04-11 14:37:56,905 :: INFO :: Epoch 15: loss tensor(803.8011, device='cuda:0'), U.norm 10.378817558288574, V.norm 17.54844093322754, MLP.norm 3.6694705486297607
2023-04-11 14:37:56,999 :: INFO :: Epoch 20: loss tensor(781.3701, device='cuda:0'), U.norm 9.184982299804688, V.norm 17.40264320373535, MLP.norm 4.73938512802124
2023-04-11 14:37:56,999 :: INFO :: Environment 8
2023-04-11 14:38:10,671 :: INFO :: Epoch 5: loss tensor(834.1295, device='cuda:0'), U.norm 14.650970458984375, V.norm 18.276355743408203, MLP.norm 1.860849142074585
2023-04-11 14:38:10,765 :: INFO :: Epoch 10: loss tensor(821.6655, device='cuda:0'), U.norm 12.048829078674316, V.norm 17.760719299316406, MLP.norm 2.6194746494293213
2023-04-11 14:38:10,890 :: INFO :: Epoch 15: loss tensor(803.3708, device='cuda:0'), U.norm 10.38896656036377, V.norm 17.528621673583984, MLP.norm 3.6146528720855713
2023-04-11 14:38:11,030 :: INFO :: Epoch 20: loss tensor(781.8036, device='cuda:0'), U.norm 9.19910717010498, V.norm 17.379695892333984, MLP.norm 4.647181510925293
2023-04-11 14:38:11,030 :: INFO :: Environment 9
2023-04-11 14:38:25,436 :: INFO :: Epoch 5: loss tensor(935.1064, device='cuda:0'), U.norm 14.64990234375, V.norm 18.45781135559082, MLP.norm 1.8955483436584473
2023-04-11 14:38:25,530 :: INFO :: Epoch 10: loss tensor(918.5028, device='cuda:0'), U.norm 12.048521041870117, V.norm 18.008472442626953, MLP.norm 2.7406105995178223
2023-04-11 14:38:25,639 :: INFO :: Epoch 15: loss tensor(894.9691, device='cuda:0'), U.norm 10.390390396118164, V.norm 17.802396774291992, MLP.norm 3.8310720920562744
2023-04-11 14:38:25,719 :: INFO :: Epoch 20: loss tensor(866.3409, device='cuda:0'), U.norm 9.202865600585938, V.norm 17.66798973083496, MLP.norm 4.973869323730469
2023-04-11 14:38:25,735 :: INFO :: Ite = 1, Delta = 4135
2023-04-11 14:38:25,735 :: INFO :: ----- backend -----
2023-04-11 14:38:27,702 :: INFO :: Epoch 5: loss tensor(356.8416, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-5.8756e-03, -3.8951e-03, -6.3913e-03, -2.5317e-03, -9.7716e-03,
        -1.0132e-02, -5.9785e-03,  1.2966e-03, -7.0441e-03, -2.0819e-03,
        -1.0237e-02, -5.4217e-03, -8.0190e-03, -1.4008e-03,  3.4296e-04,
         1.8207e-03, -3.8707e-03, -3.1617e-03,  1.4686e-03, -1.5900e-03,
        -5.6274e-03, -1.3398e-03,  2.7169e-04, -9.9384e-03, -5.3918e-03,
        -7.5852e-03, -1.1184e-03, -3.7681e-03, -2.7951e-03, -3.6520e-03,
        -6.2415e-03,  4.4045e-03, -3.1081e-03, -4.6841e-03, -8.1600e-03,
        -9.4794e-03,  7.6755e-04, -6.8805e-03, -7.9334e-03,  1.8923e-03,
        -9.3151e-03, -8.7034e-03,  3.5510e-03, -3.0720e-03, -6.6342e-03,
         1.3899e-03, -2.5404e-03, -7.2886e-03, -4.7764e-03, -2.3726e-03,
        -7.0446e-03, -9.1618e-04,  1.7008e-03, -4.6281e-03, -7.3401e-03,
         6.0408e-03, -3.2158e-03,  7.2010e-04, -3.9408e-03, -1.0194e-02,
        -2.2378e-03, -1.7130e-03, -8.8539e-03, -7.2610e-03, -2.4614e-03,
        -5.1409e-03, -3.9779e-06,  2.5840e-03,  3.6266e-03, -8.9047e-03,
         5.9838e-04, -8.4381e-03,  1.1573e-02, -6.2046e-03, -2.2437e-03,
        -6.8583e-03,  1.0499e-02, -7.4864e-03, -1.3772e-03,  1.1480e-02,
        -8.2936e-03, -1.0268e-02, -3.0247e-05, -2.7004e-03, -9.4073e-03,
        -6.1282e-03,  4.8927e-03, -5.2748e-03, -6.8429e-03, -9.9026e-03,
        -8.3542e-03,  8.1940e-04, -5.8768e-03,  2.0413e-03, -4.5371e-03,
        -5.8306e-03, -3.9215e-03, -4.9226e-03, -9.9221e-03, -5.8181e-03,
        -2.0786e-03, -4.5561e-03, -4.8100e-03, -1.0577e-02, -9.3409e-03,
        -1.0057e-02,  3.9373e-03, -3.7736e-03, -4.0887e-03, -5.9330e-04,
        -7.6238e-03,  2.3540e-03, -7.4507e-03,  9.8647e-04, -1.0483e-02,
        -9.9833e-03, -4.3401e-03, -9.6729e-03, -2.5448e-03, -6.6956e-03,
        -5.9313e-03, -7.3735e-03, -2.1634e-03, -3.6732e-03, -3.9371e-03,
        -5.9962e-03, -6.3630e-03, -4.4431e-03,  6.5353e-03,  2.9837e-03,
        -8.1991e-03,  1.4244e-02,  5.4451e-03,  9.8165e-03, -5.6147e-03,
        -1.1282e-02, -1.2203e-02,  1.1087e-02, -3.7361e-03, -1.1975e-02,
         1.9589e-03, -1.1683e-02, -4.1805e-03, -8.5737e-03, -7.1978e-03,
         8.4071e-03,  2.1331e-03, -1.1615e-02, -9.2997e-03, -1.1957e-02,
        -1.1925e-02,  6.3501e-04,  1.4510e-03, -1.1864e-02, -1.1343e-02,
        -1.2245e-02, -1.1532e-02, -1.2231e-02,  1.2982e-02, -1.3237e-02,
         1.1319e-02, -1.2775e-02,  1.0484e-02, -1.0711e-02,  1.0651e-02,
         1.2778e-02, -1.2026e-02, -1.1117e-02, -1.1737e-02, -2.5363e-03,
         6.8695e-03,  1.0609e-02, -1.2577e-02, -2.8597e-03,  2.0714e-03,
        -1.1973e-02,  1.2735e-02,  7.0668e-03, -1.0987e-02, -1.0836e-02,
        -3.5081e-03, -1.2179e-02,  9.6777e-03, -1.2254e-02,  3.3172e-03,
         6.1964e-03, -1.1973e-02, -1.2121e-03,  1.2052e-02, -1.2245e-02,
        -1.2943e-02, -1.1264e-02,  1.0890e-02, -1.2236e-02,  2.9657e-03,
         7.9165e-03, -1.0108e-02,  1.2977e-02,  4.6806e-03, -9.9570e-03,
        -3.0985e-03, -8.7028e-03,  8.1255e-03,  1.5120e-02, -7.5630e-03,
        -1.0410e-02, -1.2088e-02, -1.2611e-02,  1.3014e-02, -9.9604e-03,
         4.4671e-03,  9.9760e-03,  1.1940e-02, -1.2076e-02, -2.4736e-03,
         4.3517e-04, -1.0574e-02, -1.2068e-02,  1.3261e-02, -1.2301e-02,
        -2.7714e-03, -1.0260e-02,  1.2835e-02,  3.2902e-03,  4.8642e-03,
        -2.3185e-03,  2.2561e-03, -4.7473e-03,  1.1327e-03,  2.6533e-03,
         1.0911e-04,  1.0554e-02,  1.4171e-02,  1.1864e-02,  1.3531e-02,
        -1.2050e-02, -1.1522e-02, -1.1327e-02,  9.9704e-03,  7.6670e-03,
        -6.8751e-03, -1.0529e-02, -1.1252e-02,  1.3651e-02, -1.1709e-02,
        -8.3465e-03, -1.2155e-02, -9.1069e-03, -1.0565e-02, -1.1171e-02,
        -1.2408e-02,  1.3510e-02, -9.5588e-03, -9.8089e-03, -1.2439e-02,
        -6.6287e-03, -1.7337e-03, -1.1428e-02, -1.0560e-02,  5.5507e-03,
         5.4297e-03,  5.8889e-03, -1.2330e-02, -8.6437e-03, -8.6202e-03,
        -4.0191e-03, -1.0220e-02, -1.2709e-02, -5.8649e-03, -3.3108e-03,
        -1.2494e-02, -7.7197e-03, -9.3088e-03, -1.2515e-02, -3.3506e-03,
        -1.2232e-02, -6.6483e-04, -1.6950e-03, -1.1549e-02, -8.0160e-03,
        -1.0066e-02, -5.8743e-03, -6.1357e-03, -1.1883e-02, -5.9743e-03,
         1.1711e-03, -1.0391e-02, -3.6217e-03,  2.0464e-03, -7.5709e-03,
        -9.3050e-03, -5.8506e-03, -6.6114e-03, -5.3347e-03, -1.3356e-02,
         3.5155e-03, -3.8422e-03, -1.2763e-02, -3.8248e-03,  3.4447e-03,
        -1.3334e-02, -1.2898e-03, -1.0818e-02, -1.2919e-02,  5.7047e-03,
        -1.1508e-02, -6.6689e-03, -1.1485e-02, -8.0846e-03, -1.1838e-02,
        -1.1965e-02, -5.3206e-03, -7.7775e-03, -4.4292e-03, -1.2258e-02,
        -1.2990e-03, -9.5943e-04, -1.2207e-02, -6.1102e-03, -1.2812e-02,
        -1.4169e-03, -1.1671e-02,  1.0048e-03, -1.1166e-02, -2.4804e-03,
        -1.0668e-02, -1.2866e-02, -6.9750e-03, -1.2033e-02, -9.6170e-03,
        -4.8574e-03, -1.2123e-02, -1.1238e-02, -1.2142e-02, -2.3863e-03,
        -6.4786e-03, -1.0154e-02, -4.4573e-03, -5.0909e-03, -8.4442e-03,
        -8.2741e-03, -1.0381e-02, -8.2320e-03,  3.6494e-03, -4.0415e-03,
        -8.7271e-03, -8.6228e-03, -1.3245e-02, -1.1455e-02, -1.1687e-02,
        -1.1992e-02, -1.2408e-02, -8.9203e-03, -1.1929e-02, -5.6839e-03,
        -1.0952e-02, -1.3026e-02,  4.8330e-03, -1.1094e-02, -4.0883e-03,
        -1.1385e-02, -1.2404e-02, -1.0092e-02, -1.0057e-02,  1.7624e-03,
        -1.5619e-03,  4.4380e-04, -1.2058e-02, -1.2642e-02, -9.4844e-03,
        -1.0594e-02, -1.2105e-02, -1.9016e-03, -3.1895e-03, -1.2573e-03,
        -5.4307e-03, -9.4192e-03, -6.0390e-03, -7.9941e-03, -1.3393e-02,
         2.2222e-03, -9.1004e-03, -8.7541e-03, -1.1304e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(2.5031, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:29,811 :: INFO :: Epoch 10: loss tensor(347.8445, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-2.3051e-03,  1.8094e-03, -6.9183e-03,  3.9799e-03, -9.5844e-03,
        -1.5356e-02, -4.0913e-03,  9.9809e-03, -5.8839e-03,  4.2609e-03,
        -1.5265e-02, -2.4893e-03, -4.2104e-03,  3.6228e-03,  6.0237e-03,
         9.8116e-03,  1.4956e-03,  3.1169e-03,  6.7573e-03,  5.3425e-03,
        -1.3944e-03,  5.2632e-03,  6.5312e-03, -1.3605e-02, -3.5696e-03,
        -3.5128e-03,  5.1303e-03,  1.9978e-03,  1.5741e-03,  1.4603e-03,
        -2.0977e-03,  1.2786e-02,  1.3605e-04, -1.3260e-03, -1.0528e-02,
        -1.4951e-02,  7.3922e-03, -1.7077e-03, -5.7412e-03,  6.2258e-03,
        -1.1618e-02, -9.0169e-03,  9.9874e-03,  2.7251e-03, -4.3697e-03,
         7.9928e-03,  3.7431e-03, -8.5677e-03, -1.5163e-03,  3.6913e-03,
        -6.6452e-03,  6.4695e-03,  8.6058e-03,  2.6153e-04, -8.4351e-03,
         1.6579e-02,  1.7947e-03,  8.1750e-03, -3.3375e-03, -1.6799e-02,
         5.1136e-03,  4.2492e-03, -1.1374e-02, -5.8847e-03,  1.7124e-03,
        -7.9723e-04,  5.3353e-03,  7.6396e-03,  1.2743e-02, -1.2045e-02,
         7.9592e-03, -1.0852e-02,  2.0311e-02, -8.2962e-04,  5.4947e-03,
        -7.8845e-03,  1.6757e-02, -8.5035e-03,  5.6323e-03,  1.8890e-02,
        -5.9042e-03, -1.6913e-02,  9.3526e-03,  4.2666e-03, -1.3546e-02,
        -4.1141e-03,  1.2214e-02, -5.3980e-03, -4.2541e-03, -1.3757e-02,
        -5.9900e-03,  1.1060e-02, -8.4085e-04,  1.0932e-02, -5.1681e-03,
        -1.0600e-03,  2.6211e-03, -3.3936e-03, -1.3362e-02, -3.6881e-03,
        -5.6033e-04,  3.0576e-03,  1.2831e-03, -1.4951e-02, -1.2546e-02,
        -1.6234e-02,  1.0324e-02, -7.3253e-04,  7.3126e-04,  7.9032e-03,
        -7.1658e-03,  1.0456e-02, -1.0058e-02,  6.5569e-03, -1.5340e-02,
        -1.4744e-02, -1.1930e-03, -1.2246e-02,  3.0053e-03, -5.3411e-03,
        -4.5478e-03, -4.9439e-03,  2.5715e-03, -1.6711e-03,  3.2801e-03,
        -4.6608e-03, -4.8631e-03,  2.5476e-03,  2.1767e-02,  1.7400e-02,
        -1.8530e-03,  2.9840e-02,  1.7735e-02,  2.5559e-02,  6.3668e-03,
        -1.4706e-02, -2.1231e-02,  2.6065e-02,  1.0423e-02, -1.8928e-02,
         1.7646e-02, -1.8729e-02,  1.0310e-02, -1.0990e-03,  2.6332e-03,
         2.2787e-02,  1.7504e-02, -1.7609e-02, -5.0944e-03, -1.5640e-02,
        -2.0267e-02,  1.6673e-02,  1.6618e-02, -2.0787e-02, -1.4645e-02,
        -2.1516e-02, -1.7984e-02, -2.2015e-02,  2.5236e-02, -2.4326e-02,
         2.6336e-02, -2.3158e-02,  2.3887e-02, -1.2426e-02,  2.3990e-02,
         2.2709e-02, -2.0308e-02, -1.4653e-02, -1.8931e-02,  1.1654e-02,
         2.4246e-02,  2.4549e-02, -2.2690e-02,  1.0835e-02,  1.8341e-02,
        -2.0419e-02,  2.4318e-02,  2.2160e-02, -1.3084e-02, -1.2073e-02,
         9.6133e-03, -2.0718e-02,  2.4417e-02, -2.1952e-02,  1.8002e-02,
         2.1769e-02, -2.0691e-02,  1.5309e-02,  2.5337e-02, -2.1721e-02,
        -2.2918e-02, -1.5693e-02,  2.4848e-02, -2.1729e-02,  1.9962e-02,
         2.2638e-02, -6.0540e-03,  2.3743e-02,  2.0892e-02, -6.4138e-03,
         1.0582e-02,  8.5944e-04,  2.3445e-02,  3.2301e-02,  1.0606e-03,
        -8.1231e-03, -1.9174e-02, -2.2652e-02,  2.6560e-02, -8.1126e-03,
         2.0796e-02,  2.4051e-02,  1.7607e-02, -2.0174e-02,  1.3158e-02,
         1.5823e-02, -1.1306e-02, -1.9983e-02,  2.6930e-02, -2.1990e-02,
         1.3373e-02, -1.6228e-03,  2.7338e-02,  1.9857e-02,  1.5952e-02,
         1.2897e-02,  1.6881e-02,  8.0415e-03,  1.5625e-02,  1.8889e-02,
         1.4783e-02,  2.4191e-02,  2.8677e-02,  2.1074e-02,  2.7485e-02,
        -1.8153e-02, -1.6776e-02, -1.5608e-02,  2.2292e-02,  2.2359e-02,
         3.8857e-03, -1.2016e-02, -1.5811e-02,  2.7464e-02, -1.8826e-02,
        -2.5939e-03, -2.1906e-02, -2.3689e-03, -1.2750e-02, -1.6443e-02,
        -2.1811e-02,  2.7590e-02, -5.7986e-03, -5.8457e-03, -2.2773e-02,
         4.2425e-03,  1.4847e-02, -1.4765e-02, -2.8597e-03,  2.2895e-02,
         2.2385e-02,  2.2370e-02, -1.1036e-02,  3.1943e-03, -2.8458e-03,
         1.1802e-02, -6.7263e-03, -1.4493e-02,  9.5465e-03,  1.2976e-02,
        -1.7081e-02,  6.3282e-03,  1.5412e-04, -1.7777e-02,  1.2910e-02,
        -1.1324e-02,  1.5440e-02,  1.4281e-02, -5.8895e-03,  5.1571e-03,
        -3.1461e-05,  8.7496e-03,  9.1427e-03, -1.6197e-02,  7.7452e-03,
         1.8281e-02, -2.7361e-03,  1.2039e-02,  1.9037e-02,  5.2397e-03,
        -5.4347e-03,  9.1229e-03,  7.5434e-03,  1.0140e-02, -1.9436e-02,
         2.0711e-02,  1.2506e-02, -1.5435e-02,  1.2250e-02,  2.0619e-02,
        -2.0020e-02,  1.4357e-02, -6.3890e-03, -1.6417e-02,  2.2234e-02,
        -9.5116e-03,  7.3180e-03, -6.7540e-03,  5.5324e-03, -1.5327e-02,
        -1.6302e-02,  9.5924e-03,  4.3639e-03,  1.1503e-02, -1.1288e-02,
         1.5467e-02,  1.5280e-02, -1.2891e-02,  7.9158e-03, -1.7345e-02,
         1.5104e-02, -1.2632e-02,  1.7971e-02, -1.1628e-02,  1.3776e-02,
        -4.2801e-03, -1.4421e-02,  6.6580e-03, -1.3090e-02, -6.6774e-04,
         1.0567e-02, -1.7327e-02, -6.1547e-03, -8.4381e-03,  1.3003e-02,
         8.5671e-03, -1.8560e-03,  1.1138e-02,  1.0064e-02,  5.4090e-03,
         3.9202e-03, -7.3010e-04,  6.6680e-04,  2.0373e-02,  1.2038e-02,
         4.0739e-03,  3.1256e-03, -1.6525e-02, -1.2469e-02, -7.1605e-03,
        -8.9111e-03, -1.8243e-02,  2.6658e-03, -6.5497e-03,  9.1813e-03,
        -4.8367e-03, -1.9649e-02,  2.1975e-02, -7.7905e-03,  1.2019e-02,
        -6.9024e-03, -1.6329e-02, -4.4289e-03,  5.1760e-04,  1.8794e-02,
         1.4931e-02,  1.6887e-02, -1.0695e-02, -1.4008e-02,  2.6057e-03,
        -3.1800e-03, -1.1794e-02,  1.4761e-02,  1.2952e-02,  1.5151e-02,
         9.0955e-03,  1.5248e-03,  8.0676e-03,  5.2713e-03, -2.0313e-02,
         1.9368e-02,  1.8591e-03,  1.7322e-03, -8.9439e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.8912, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:31,593 :: INFO :: Epoch 15: loss tensor(343.4918, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.8846e-04,  3.9155e-03, -9.3151e-03,  7.0725e-03, -7.3712e-03,
        -2.0020e-02, -4.7072e-03,  1.4815e-02, -5.8850e-03,  6.7040e-03,
        -1.9173e-02, -2.7121e-03,  1.4106e-03,  3.8528e-03,  6.8234e-03,
         1.2827e-02,  3.7721e-03,  5.7602e-03,  6.2310e-03,  9.4216e-03,
         1.1878e-03,  7.2917e-03,  8.3277e-03, -1.6421e-02, -4.8637e-03,
         3.3438e-04,  7.3689e-03,  4.3916e-03,  3.0774e-03,  2.5798e-03,
         6.4145e-04,  1.5862e-02, -1.3117e-03, -1.1838e-03, -1.5775e-02,
        -2.1419e-02,  7.9426e-03,  3.7272e-03, -3.4905e-03,  4.2002e-03,
        -1.4278e-02, -8.5640e-03,  1.0239e-02,  5.2906e-03, -4.3308e-03,
         8.8929e-03,  6.1488e-03, -1.2201e-02, -2.5531e-03,  5.2111e-03,
        -8.4578e-03,  9.0189e-03,  1.0189e-02,  2.7265e-03, -1.1141e-02,
         2.3487e-02,  2.5068e-03,  1.2361e-02, -8.1357e-03, -2.4068e-02,
         8.9119e-03,  5.6925e-03, -1.5493e-02, -6.3166e-03,  8.4347e-04,
         3.4721e-04,  4.9000e-03,  6.1004e-03,  1.7156e-02, -1.6228e-02,
         9.9272e-03, -1.4911e-02,  2.3546e-02,  4.0889e-03,  9.8642e-03,
        -1.1689e-02,  1.6768e-02, -1.2387e-02,  7.8385e-03,  1.9795e-02,
        -1.2818e-03, -2.3484e-02,  1.5755e-02,  9.2192e-03, -1.9232e-02,
        -3.8245e-03,  1.3883e-02, -9.3792e-03, -3.5301e-03, -1.7176e-02,
        -3.2025e-03,  1.8883e-02,  3.5434e-03,  1.5432e-02, -1.1627e-02,
         1.5382e-03,  7.1014e-03, -6.3864e-03, -1.6319e-02, -4.8191e-03,
        -5.3898e-03,  8.6975e-03,  5.1027e-03, -1.7461e-02, -1.5425e-02,
        -2.3266e-02,  1.0958e-02, -2.6813e-03,  2.4408e-03,  1.3031e-02,
        -8.1225e-03,  1.4328e-02, -1.4603e-02,  6.8071e-03, -1.9999e-02,
        -2.0168e-02, -2.1451e-03, -1.4268e-02,  4.0144e-03, -5.8956e-03,
        -5.8085e-03, -2.5763e-03,  2.0216e-03, -4.8801e-03,  8.4687e-03,
        -6.2397e-03, -5.6786e-03,  7.2360e-03,  3.7872e-02,  3.1854e-02,
         1.1527e-02,  4.6096e-02,  3.5601e-02,  4.3494e-02,  2.1559e-02,
        -1.1007e-02, -2.7311e-02,  3.9543e-02,  2.7450e-02, -1.9875e-02,
         3.4891e-02, -2.1466e-02,  2.8345e-02,  1.4390e-02,  1.8841e-02,
         3.6832e-02,  3.3813e-02, -1.6388e-02,  8.4889e-03, -6.6316e-03,
        -2.5466e-02,  3.5433e-02,  3.3156e-02, -2.7292e-02, -8.9958e-03,
        -2.7104e-02, -1.9943e-02, -2.9754e-02,  3.4940e-02, -3.3188e-02,
         4.1481e-02, -3.1469e-02,  3.6422e-02, -5.2678e-03,  3.5592e-02,
         2.6604e-02, -2.5318e-02, -1.1147e-02, -2.0443e-02,  2.8091e-02,
         4.3816e-02,  3.7663e-02, -3.0733e-02,  2.6557e-02,  3.6630e-02,
        -2.5843e-02,  3.3236e-02,  3.7376e-02, -6.0992e-03, -6.9503e-03,
         2.5044e-02, -2.5424e-02,  3.8332e-02, -2.9549e-02,  3.2590e-02,
         3.8336e-02, -2.6960e-02,  3.4591e-02,  4.3055e-02, -2.8845e-02,
        -2.9072e-02, -1.3416e-02,  3.7490e-02, -2.8781e-02,  3.8610e-02,
         3.7712e-02,  8.4328e-03,  2.9867e-02,  3.8664e-02,  6.7287e-03,
         2.6499e-02,  1.8288e-02,  3.8921e-02,  5.1280e-02,  1.6263e-02,
         5.1873e-03, -2.0183e-02, -3.0653e-02,  3.8659e-02,  1.2569e-03,
         3.8580e-02,  3.7014e-02,  1.9629e-02, -2.4668e-02,  3.1868e-02,
         3.3413e-02, -4.4607e-03, -2.3647e-02,  3.9679e-02, -2.9526e-02,
         3.2530e-02,  1.6609e-02,  4.3228e-02,  3.8166e-02,  3.0176e-02,
         3.1085e-02,  3.2194e-02,  2.4485e-02,  3.0871e-02,  3.7013e-02,
         3.0652e-02,  3.6330e-02,  4.3051e-02,  3.2581e-02,  4.0635e-02,
        -1.6376e-02, -1.5481e-02, -1.3653e-02,  3.1522e-02,  3.7109e-02,
         1.9637e-02, -5.6134e-03, -1.3504e-02,  4.0527e-02, -2.1965e-02,
         9.5763e-03, -2.9538e-02,  1.3223e-02, -8.4730e-03, -1.6085e-02,
        -2.7968e-02,  4.1272e-02,  6.0844e-03,  8.5434e-03, -3.0993e-02,
         1.9836e-02,  3.4583e-02, -8.4279e-03,  1.4383e-02,  4.2158e-02,
         4.1247e-02,  4.0667e-02,  4.4347e-03,  2.2353e-02,  1.1906e-02,
         3.0926e-02,  8.4030e-03, -2.6781e-03,  2.8862e-02,  3.2208e-02,
        -1.0656e-02,  2.5856e-02,  1.8509e-02, -1.3274e-02,  3.2380e-02,
         4.0845e-03,  3.4089e-02,  3.3719e-02,  1.1303e-02,  2.4686e-02,
         1.8935e-02,  2.7713e-02,  2.8694e-02, -1.0524e-02,  2.5811e-02,
         3.7525e-02,  1.5628e-02,  3.1478e-02,  3.7780e-02,  2.4402e-02,
         9.7429e-03,  2.8638e-02,  2.6574e-02,  2.9380e-02, -1.4962e-02,
         3.9887e-02,  3.2220e-02, -4.4252e-03,  3.1351e-02,  3.9765e-02,
        -1.8194e-02,  3.2886e-02,  8.8528e-03, -9.5491e-03,  4.0648e-02,
         3.8260e-03,  2.6404e-02,  9.6566e-03,  2.4866e-02, -5.6089e-03,
        -1.0162e-02,  2.8873e-02,  2.3183e-02,  3.1058e-02,  3.8971e-03,
         3.4778e-02,  3.3991e-02, -2.0194e-03,  2.7305e-02, -1.1676e-02,
         3.4429e-02, -1.1866e-03,  3.7303e-02,  3.9764e-04,  3.3208e-02,
         1.2740e-02, -3.6267e-03,  2.5703e-02,  3.5912e-05,  1.7925e-02,
         3.0065e-02, -9.9662e-03,  1.1289e-02,  8.5614e-03,  3.1932e-02,
         2.7849e-02,  1.6697e-02,  3.0572e-02,  2.9302e-02,  2.4604e-02,
         2.3234e-02,  1.8091e-02,  1.9117e-02,  3.8918e-02,  3.1389e-02,
         2.2541e-02,  2.1829e-02, -7.2851e-03, -2.1316e-03,  1.0112e-02,
         7.6836e-03, -1.5858e-02,  2.1849e-02,  1.0357e-02,  2.8390e-02,
         1.3092e-02, -1.9016e-02,  4.0976e-02,  7.9167e-03,  3.1491e-02,
         9.1568e-03, -1.0383e-02,  1.1355e-02,  1.9254e-02,  3.8303e-02,
         3.4385e-02,  3.6004e-02,  3.2779e-03, -2.1499e-03,  2.1934e-02,
         1.5274e-02,  6.1875e-04,  3.4328e-02,  3.2248e-02,  3.4577e-02,
         2.7423e-02,  2.0258e-02,  2.6394e-02,  2.4457e-02, -1.9580e-02,
         3.8686e-02,  2.0937e-02,  2.0485e-02,  6.3826e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(5.1756, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:33,514 :: INFO :: Epoch 20: loss tensor(333.4193, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0049,  0.0076, -0.0084,  0.0120, -0.0034, -0.0217, -0.0009,  0.0195,
        -0.0036,  0.0095, -0.0189, -0.0028,  0.0078,  0.0056,  0.0086,  0.0154,
         0.0058,  0.0095,  0.0058,  0.0153,  0.0043,  0.0092,  0.0119, -0.0139,
        -0.0054,  0.0045,  0.0092,  0.0085,  0.0106,  0.0043,  0.0050,  0.0191,
        -0.0006,  0.0010, -0.0185, -0.0256,  0.0086,  0.0091, -0.0006,  0.0034,
        -0.0149, -0.0062,  0.0117,  0.0079, -0.0034,  0.0098,  0.0082, -0.0135,
        -0.0018,  0.0066, -0.0075,  0.0117,  0.0107,  0.0052, -0.0083,  0.0320,
         0.0045,  0.0192, -0.0098, -0.0276,  0.0124,  0.0071, -0.0171, -0.0053,
         0.0021,  0.0019,  0.0063,  0.0055,  0.0208, -0.0183,  0.0113, -0.0152,
         0.0253,  0.0094,  0.0154, -0.0137,  0.0171, -0.0135,  0.0105,  0.0201,
         0.0061, -0.0247,  0.0218,  0.0168, -0.0226, -0.0007,  0.0156, -0.0108,
        -0.0019, -0.0190,  0.0006,  0.0272,  0.0080,  0.0207, -0.0142,  0.0071,
         0.0114, -0.0077, -0.0166, -0.0042, -0.0083,  0.0139,  0.0105, -0.0174,
        -0.0127, -0.0280,  0.0116, -0.0034,  0.0051,  0.0180, -0.0062,  0.0179,
        -0.0127,  0.0078, -0.0220, -0.0237, -0.0018, -0.0131,  0.0041, -0.0033,
        -0.0057,  0.0012,  0.0019, -0.0075,  0.0135, -0.0080, -0.0037,  0.0134,
         0.0540,  0.0460,  0.0271,  0.0617,  0.0570,  0.0626,  0.0370, -0.0023,
        -0.0305,  0.0513,  0.0446, -0.0154,  0.0529, -0.0204,  0.0472,  0.0333,
         0.0369,  0.0504,  0.0498, -0.0085,  0.0257,  0.0100, -0.0279,  0.0554,
         0.0500, -0.0327,  0.0026, -0.0287, -0.0183, -0.0360,  0.0434, -0.0401,
         0.0560, -0.0382,  0.0493,  0.0075,  0.0464,  0.0298, -0.0277, -0.0029,
        -0.0162,  0.0445,  0.0642,  0.0502, -0.0373,  0.0427,  0.0555, -0.0287,
         0.0405,  0.0524,  0.0064,  0.0014,  0.0407, -0.0264,  0.0515, -0.0357,
         0.0467,  0.0555, -0.0320,  0.0547,  0.0642, -0.0347, -0.0314, -0.0059,
         0.0493, -0.0340,  0.0571,  0.0531,  0.0272,  0.0330,  0.0565,  0.0239,
         0.0430,  0.0381,  0.0540,  0.0704,  0.0334,  0.0234, -0.0156, -0.0375,
         0.0492,  0.0139,  0.0574,  0.0493,  0.0273, -0.0263,  0.0509,  0.0521,
         0.0065, -0.0238,  0.0525, -0.0356,  0.0522,  0.0363,  0.0606,  0.0567,
         0.0503,  0.0503,  0.0471,  0.0423,  0.0465,  0.0559,  0.0465,  0.0469,
         0.0562,  0.0506,  0.0533, -0.0072, -0.0086, -0.0076,  0.0364,  0.0521,
         0.0362,  0.0060, -0.0054,  0.0533, -0.0217,  0.0238, -0.0355,  0.0312,
         0.0003, -0.0110, -0.0307,  0.0550,  0.0218,  0.0268, -0.0372,  0.0362,
         0.0555,  0.0040,  0.0340,  0.0616,  0.0606,  0.0599,  0.0240,  0.0427,
         0.0302,  0.0508,  0.0275,  0.0147,  0.0487,  0.0517,  0.0027,  0.0462,
         0.0392, -0.0020,  0.0523,  0.0230,  0.0536,  0.0538,  0.0308,  0.0451,
         0.0393,  0.0481,  0.0489,  0.0021,  0.0452,  0.0568,  0.0360,  0.0516,
         0.0563,  0.0447,  0.0284,  0.0488,  0.0472,  0.0488, -0.0030,  0.0592,
         0.0525,  0.0132,  0.0501,  0.0590, -0.0097,  0.0525,  0.0281,  0.0040,
         0.0597,  0.0218,  0.0460,  0.0289,  0.0447,  0.0119,  0.0031,  0.0493,
         0.0432,  0.0514,  0.0235,  0.0544,  0.0529,  0.0145,  0.0473,  0.0009,
         0.0541,  0.0166,  0.0570,  0.0181,  0.0534,  0.0320,  0.0135,  0.0459,
         0.0184,  0.0380,  0.0502,  0.0059,  0.0315,  0.0285,  0.0517,  0.0474,
         0.0366,  0.0506,  0.0498,  0.0442,  0.0435,  0.0383,  0.0396,  0.0579,
         0.0512,  0.0407,  0.0415,  0.0087,  0.0137,  0.0301,  0.0274, -0.0070,
         0.0421,  0.0290,  0.0483,  0.0331, -0.0124,  0.0604,  0.0270,  0.0516,
         0.0284,  0.0023,  0.0303,  0.0399,  0.0585,  0.0542,  0.0557,  0.0217,
         0.0146,  0.0424,  0.0358,  0.0185,  0.0542,  0.0517,  0.0552,  0.0469,
         0.0401,  0.0442,  0.0442, -0.0129,  0.0583,  0.0413,  0.0409,  0.0252],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.4178, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:35,655 :: INFO :: Epoch 25: loss tensor(330.2258, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0140,  0.0145, -0.0022,  0.0204,  0.0023, -0.0194,  0.0084,  0.0258,
         0.0018,  0.0143, -0.0135, -0.0008,  0.0146,  0.0112,  0.0132,  0.0197,
         0.0086,  0.0160,  0.0075,  0.0246,  0.0092,  0.0135,  0.0196, -0.0058,
        -0.0035,  0.0099,  0.0123,  0.0160,  0.0246,  0.0081,  0.0121,  0.0247,
         0.0053,  0.0068, -0.0168, -0.0265,  0.0118,  0.0149,  0.0033,  0.0061,
        -0.0126, -0.0016,  0.0172,  0.0115,  0.0004,  0.0125,  0.0119, -0.0103,
         0.0022,  0.0095, -0.0025,  0.0163,  0.0119,  0.0084,  0.0008,  0.0437,
         0.0097,  0.0301, -0.0049, -0.0265,  0.0167,  0.0101, -0.0154, -0.0011,
         0.0074,  0.0056,  0.0114,  0.0084,  0.0258, -0.0178,  0.0154, -0.0093,
         0.0279,  0.0157,  0.0236, -0.0127,  0.0206, -0.0100,  0.0159,  0.0224,
         0.0167, -0.0198,  0.0288,  0.0277, -0.0226,  0.0060,  0.0197, -0.0070,
         0.0017, -0.0188,  0.0063,  0.0379,  0.0134,  0.0283, -0.0101,  0.0170,
         0.0161, -0.0055, -0.0135, -0.0006, -0.0072,  0.0196,  0.0187, -0.0148,
        -0.0042, -0.0295,  0.0153, -0.0015,  0.0098,  0.0255,  0.0003,  0.0228,
        -0.0025,  0.0118, -0.0208, -0.0242,  0.0020, -0.0086,  0.0055,  0.0036,
        -0.0024,  0.0074,  0.0037, -0.0074,  0.0195, -0.0080,  0.0025,  0.0225,
         0.0698,  0.0595,  0.0426,  0.0757,  0.0779,  0.0820,  0.0519,  0.0088,
        -0.0315,  0.0597,  0.0612, -0.0074,  0.0706, -0.0167,  0.0659,  0.0534,
         0.0548,  0.0636,  0.0647,  0.0036,  0.0433,  0.0285, -0.0284,  0.0755,
         0.0662, -0.0374,  0.0161, -0.0275, -0.0144, -0.0413,  0.0519, -0.0454,
         0.0699, -0.0438,  0.0625,  0.0218,  0.0567,  0.0351, -0.0282,  0.0076,
        -0.0086,  0.0603,  0.0844,  0.0622, -0.0428,  0.0584,  0.0741, -0.0297,
         0.0473,  0.0669,  0.0206,  0.0111,  0.0559, -0.0248,  0.0635, -0.0407,
         0.0601,  0.0726, -0.0360,  0.0745,  0.0863, -0.0397, -0.0301,  0.0044,
         0.0605, -0.0381,  0.0737,  0.0683,  0.0467,  0.0352,  0.0738,  0.0418,
         0.0591,  0.0581,  0.0683,  0.0882,  0.0503,  0.0430, -0.0076, -0.0433,
         0.0584,  0.0276,  0.0761,  0.0610,  0.0403, -0.0262,  0.0693,  0.0709,
         0.0191, -0.0214,  0.0652, -0.0407,  0.0717,  0.0556,  0.0782,  0.0750,
         0.0717,  0.0696,  0.0611,  0.0606,  0.0617,  0.0745,  0.0616,  0.0555,
         0.0678,  0.0703,  0.0652,  0.0060,  0.0014,  0.0007,  0.0400,  0.0670,
         0.0523,  0.0193,  0.0058,  0.0657, -0.0193,  0.0384, -0.0404,  0.0489,
         0.0110, -0.0033, -0.0305,  0.0685,  0.0388,  0.0458, -0.0418,  0.0520,
         0.0766,  0.0195,  0.0543,  0.0803,  0.0796,  0.0797,  0.0440,  0.0627,
         0.0498,  0.0708,  0.0471,  0.0333,  0.0677,  0.0702,  0.0185,  0.0663,
         0.0600,  0.0126,  0.0715,  0.0423,  0.0729,  0.0736,  0.0500,  0.0649,
         0.0593,  0.0688,  0.0685,  0.0185,  0.0651,  0.0751,  0.0563,  0.0715,
         0.0732,  0.0649,  0.0476,  0.0686,  0.0685,  0.0673,  0.0116,  0.0777,
         0.0723,  0.0326,  0.0674,  0.0774,  0.0028,  0.0724,  0.0485,  0.0207,
         0.0790,  0.0410,  0.0645,  0.0483,  0.0639,  0.0315,  0.0193,  0.0699,
         0.0634,  0.0714,  0.0439,  0.0733,  0.0713,  0.0328,  0.0670,  0.0167,
         0.0730,  0.0364,  0.0762,  0.0373,  0.0736,  0.0509,  0.0326,  0.0661,
         0.0378,  0.0578,  0.0699,  0.0244,  0.0517,  0.0485,  0.0716,  0.0657,
         0.0553,  0.0703,  0.0700,  0.0625,  0.0633,  0.0580,  0.0601,  0.0770,
         0.0706,  0.0567,  0.0602,  0.0267,  0.0312,  0.0502,  0.0471,  0.0055,
         0.0620,  0.0463,  0.0684,  0.0528, -0.0020,  0.0795,  0.0459,  0.0712,
         0.0479,  0.0182,  0.0500,  0.0610,  0.0783,  0.0734,  0.0751,  0.0408,
         0.0319,  0.0625,  0.0564,  0.0381,  0.0732,  0.0703,  0.0762,  0.0667,
         0.0594,  0.0598,  0.0631, -0.0022,  0.0771,  0.0616,  0.0614,  0.0439],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.6196, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:37,498 :: INFO :: Epoch 30: loss tensor(329.2366, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 2.5405e-02,  2.3563e-02,  7.0788e-03,  3.0519e-02,  9.0039e-03,
        -1.3711e-02,  2.0736e-02,  3.3430e-02,  9.7672e-03,  2.1165e-02,
        -4.3821e-03,  3.2629e-03,  2.1916e-02,  1.9478e-02,  1.9684e-02,
         2.5330e-02,  1.2728e-02,  2.4343e-02,  1.1074e-02,  3.5684e-02,
         1.5173e-02,  1.9786e-02,  2.9857e-02,  5.3912e-03,  1.1426e-03,
         1.5790e-02,  1.6787e-02,  2.5484e-02,  4.1264e-02,  1.3514e-02,
         2.0215e-02,  3.1975e-02,  1.4581e-02,  1.5422e-02, -1.2000e-02,
        -2.4201e-02,  1.7219e-02,  2.1257e-02,  8.3309e-03,  1.2127e-02,
        -8.3692e-03,  5.2504e-03,  2.5915e-02,  1.6589e-02,  5.7406e-03,
         1.7316e-02,  1.7352e-02, -4.1644e-03,  9.0891e-03,  1.4152e-02,
         4.9765e-03,  2.2576e-02,  1.4568e-02,  1.2548e-02,  1.3014e-02,
         5.6812e-02,  1.7568e-02,  4.3415e-02,  4.6650e-03, -2.1503e-02,
         2.2007e-02,  1.4991e-02, -1.0480e-02,  5.0454e-03,  1.5436e-02,
         1.1701e-02,  1.9642e-02,  1.4713e-02,  3.2213e-02, -1.4866e-02,
         2.1678e-02,  2.5696e-04,  3.2007e-02,  2.3480e-02,  3.3755e-02,
        -9.1441e-03,  2.6842e-02, -3.0177e-03,  2.3629e-02,  2.7244e-02,
         2.8860e-02, -1.0799e-02,  3.6590e-02,  4.0517e-02, -1.9457e-02,
         1.5449e-02,  2.5914e-02,  5.9464e-04,  6.6892e-03, -1.6380e-02,
         1.2972e-02,  4.9610e-02,  1.9454e-02,  3.7447e-02, -1.1714e-03,
         2.8943e-02,  2.1308e-02, -3.0709e-05, -7.9352e-03,  4.9792e-03,
        -2.2982e-03,  2.5894e-02,  2.8384e-02, -1.0050e-02,  7.5542e-03,
        -2.8126e-02,  2.1385e-02,  2.6062e-03,  1.5986e-02,  3.4562e-02,
         9.8538e-03,  2.9300e-02,  1.1424e-02,  1.8055e-02, -1.6877e-02,
        -2.2132e-02,  8.5267e-03, -2.0299e-03,  8.0828e-03,  1.2670e-02,
         3.0999e-03,  1.5068e-02,  8.0655e-03, -4.5474e-03,  2.6658e-02,
        -6.1526e-03,  1.1688e-02,  3.2929e-02,  8.5189e-02,  7.2531e-02,
         5.8004e-02,  8.7922e-02,  9.6037e-02,  1.0066e-01,  6.6152e-02,
         2.1082e-02, -3.0603e-02,  6.6464e-02,  7.6862e-02,  2.7436e-03,
         8.7729e-02, -1.1171e-02,  8.4287e-02,  7.4099e-02,  7.2327e-02,
         7.6472e-02,  7.8145e-02,  1.8202e-02,  6.0770e-02,  4.6820e-02,
        -2.7394e-02,  9.5181e-02,  8.1831e-02, -4.1713e-02,  2.9713e-02,
        -2.3689e-02, -8.7850e-03, -4.5870e-02,  6.0028e-02, -4.8762e-02,
         8.2779e-02, -4.8037e-02,  7.5615e-02,  3.6579e-02,  6.6598e-02,
         4.1510e-02, -2.7260e-02,  1.9231e-02,  1.4410e-03,  7.5347e-02,
         1.0412e-01,  7.3721e-02, -4.7369e-02,  7.3822e-02,  9.2206e-02,
        -2.9069e-02,  5.2691e-02,  8.0887e-02,  3.4951e-02,  2.1777e-02,
         7.0706e-02, -2.1193e-02,  7.4242e-02, -4.4913e-02,  7.2974e-02,
         8.9753e-02, -3.9234e-02,  9.3644e-02,  1.0799e-01, -4.4054e-02,
        -2.5490e-02,  1.6184e-02,  7.1251e-02, -4.1339e-02,  8.8031e-02,
         8.3482e-02,  6.6051e-02,  3.7248e-02,  9.0329e-02,  5.9867e-02,
         7.5019e-02,  7.7950e-02,  8.1888e-02,  1.0400e-01,  6.6881e-02,
         6.2885e-02,  2.3811e-03, -4.8298e-02,  6.6294e-02,  4.1456e-02,
         9.4544e-02,  7.2202e-02,  5.3216e-02, -2.4701e-02,  8.6545e-02,
         8.9908e-02,  3.2586e-02, -1.7117e-02,  7.8089e-02, -4.5172e-02,
         9.0573e-02,  7.3706e-02,  9.5453e-02,  9.2885e-02,  9.1489e-02,
         8.8632e-02,  7.4467e-02,  7.8996e-02,  7.6126e-02,  9.2484e-02,
         7.6049e-02,  6.2959e-02,  7.8346e-02,  8.7696e-02,  7.6588e-02,
         2.1388e-02,  1.3210e-02,  1.0405e-02,  4.4609e-02,  8.1371e-02,
         6.7712e-02,  3.3785e-02,  1.9057e-02,  7.7809e-02, -1.4973e-02,
         5.2721e-02, -4.4431e-02,  6.6139e-02,  2.2613e-02,  6.2486e-03,
        -2.7516e-02,  8.2129e-02,  5.6141e-02,  6.4625e-02, -4.5268e-02,
         6.7385e-02,  9.7804e-02,  3.6446e-02,  7.5002e-02,  9.7722e-02,
         9.8280e-02,  9.9600e-02,  6.2980e-02,  8.1879e-02,  6.9941e-02,
         9.0965e-02,  6.6644e-02,  5.1497e-02,  8.5234e-02,  8.6903e-02,
         3.4366e-02,  8.5657e-02,  8.0639e-02,  2.8017e-02,  8.9455e-02,
         6.1063e-02,  9.2506e-02,  9.2695e-02,  6.7807e-02,  8.3474e-02,
         7.8099e-02,  8.9834e-02,  8.6857e-02,  3.6318e-02,  8.5517e-02,
         9.1544e-02,  7.5663e-02,  9.0760e-02,  8.8660e-02,  8.4665e-02,
         6.6486e-02,  8.7692e-02,  9.0049e-02,  8.4295e-02,  2.5920e-02,
         9.5096e-02,  9.0913e-02,  5.1747e-02,  8.2612e-02,  9.4476e-02,
         1.7604e-02,  9.2634e-02,  6.9220e-02,  3.8850e-02,  9.8464e-02,
         6.0927e-02,  8.0880e-02,  6.7160e-02,  8.1454e-02,  5.1329e-02,
         3.6485e-02,  9.0643e-02,  8.3298e-02,  9.0793e-02,  6.3737e-02,
         9.0773e-02,  8.9284e-02,  5.2040e-02,  8.5540e-02,  3.3858e-02,
         9.0603e-02,  5.6822e-02,  9.4097e-02,  5.6233e-02,  9.3598e-02,
         6.8703e-02,  5.2564e-02,  8.6008e-02,  5.6136e-02,  7.6402e-02,
         8.8534e-02,  4.2307e-02,  7.1290e-02,  6.7391e-02,  9.1242e-02,
         8.1620e-02,  7.1511e-02,  8.9081e-02,  9.0044e-02,  7.8642e-02,
         8.1854e-02,  7.6632e-02,  7.9671e-02,  9.5847e-02,  8.9226e-02,
         6.9655e-02,  7.6989e-02,  4.5286e-02,  4.8944e-02,  6.9328e-02,
         6.5166e-02,  1.9036e-02,  8.1017e-02,  6.1518e-02,  8.7991e-02,
         7.1102e-02,  1.0993e-02,  9.8043e-02,  6.3184e-02,  8.9808e-02,
         6.7076e-02,  3.6034e-02,  6.9906e-02,  8.2142e-02,  9.7726e-02,
         9.0951e-02,  9.4185e-02,  5.9995e-02,  4.7764e-02,  8.1809e-02,
         7.5856e-02,  5.8592e-02,  9.0586e-02,  8.7901e-02,  9.7230e-02,
         8.6670e-02,  7.6666e-02,  7.1177e-02,  8.0301e-02,  1.1104e-02,
         9.4860e-02,  8.1058e-02,  8.1455e-02,  6.1212e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(8.7739, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:39,718 :: INFO :: Epoch 35: loss tensor(326.1851, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0369,  0.0326,  0.0162,  0.0407,  0.0153, -0.0061,  0.0329,  0.0409,
         0.0192,  0.0278,  0.0063,  0.0075,  0.0292,  0.0277,  0.0262,  0.0313,
         0.0172,  0.0329,  0.0149,  0.0465,  0.0208,  0.0266,  0.0403,  0.0172,
         0.0063,  0.0215,  0.0216,  0.0342,  0.0574,  0.0192,  0.0278,  0.0389,
         0.0247,  0.0241, -0.0061, -0.0206,  0.0231,  0.0269,  0.0132,  0.0198,
        -0.0032,  0.0135,  0.0356,  0.0215,  0.0106,  0.0231,  0.0229,  0.0037,
         0.0172,  0.0190,  0.0120,  0.0291,  0.0180,  0.0168,  0.0256,  0.0692,
         0.0273,  0.0558,  0.0151, -0.0150,  0.0273,  0.0202, -0.0035,  0.0113,
         0.0235,  0.0177,  0.0291,  0.0219,  0.0390, -0.0111,  0.0279,  0.0105,
         0.0361,  0.0317,  0.0436, -0.0045,  0.0342,  0.0052,  0.0317,  0.0326,
         0.0403,  0.0003,  0.0439,  0.0533, -0.0151,  0.0251,  0.0321,  0.0089,
         0.0121, -0.0130,  0.0195,  0.0607,  0.0254,  0.0466,  0.0092,  0.0406,
         0.0262,  0.0059, -0.0024,  0.0106,  0.0041,  0.0319,  0.0378, -0.0040,
         0.0185, -0.0248,  0.0281,  0.0075,  0.0225,  0.0429,  0.0199,  0.0362,
         0.0251,  0.0250, -0.0121, -0.0200,  0.0149,  0.0058,  0.0109,  0.0212,
         0.0089,  0.0226,  0.0135, -0.0008,  0.0337, -0.0033,  0.0213,  0.0428,
         0.1000,  0.0846,  0.0725,  0.0982,  0.1106,  0.1183,  0.0793,  0.0332,
        -0.0284,  0.0695,  0.0916,  0.0133,  0.1042, -0.0049,  0.1018,  0.0950,
         0.0889,  0.0887,  0.0896,  0.0337,  0.0767,  0.0639, -0.0255,  0.1144,
         0.0964, -0.0457,  0.0421, -0.0185, -0.0025, -0.0500,  0.0672, -0.0511,
         0.0947, -0.0516,  0.0883,  0.0498,  0.0759,  0.0476, -0.0255,  0.0308,
         0.0124,  0.0893,  0.1229,  0.0844, -0.0514,  0.0887,  0.1095, -0.0274,
         0.0576,  0.0942,  0.0479,  0.0320,  0.0847, -0.0165,  0.0837, -0.0486,
         0.0850,  0.1066, -0.0420,  0.1116,  0.1287, -0.0480, -0.0184,  0.0279,
         0.0812, -0.0439,  0.0998,  0.0982,  0.0851,  0.0382,  0.1056,  0.0774,
         0.0904,  0.0974,  0.0943,  0.1173,  0.0822,  0.0825,  0.0123, -0.0528,
         0.0729,  0.0547,  0.1123,  0.0827,  0.0629, -0.0224,  0.1025,  0.1087,
         0.0458, -0.0119,  0.0905, -0.0491,  0.1089,  0.0901,  0.1122,  0.1103,
         0.1083,  0.1073,  0.0868,  0.0973,  0.0893,  0.1096,  0.0891,  0.0685,
         0.0875,  0.1005,  0.0872,  0.0374,  0.0252,  0.0205,  0.0460,  0.0949,
         0.0821,  0.0481,  0.0331,  0.0892, -0.0097,  0.0664, -0.0479,  0.0817,
         0.0339,  0.0163, -0.0222,  0.0957,  0.0735,  0.0828, -0.0479,  0.0818,
         0.1189,  0.0537,  0.0958,  0.1139,  0.1164,  0.1192,  0.0806,  0.1000,
         0.0901,  0.1109,  0.0859,  0.0690,  0.1013,  0.1016,  0.0496,  0.1041,
         0.1008,  0.0435,  0.1060,  0.0787,  0.1118,  0.1110,  0.0841,  0.1008,
         0.0956,  0.1110,  0.1040,  0.0549,  0.1061,  0.1064,  0.0940,  0.1093,
         0.1023,  0.1036,  0.0853,  0.1057,  0.1115,  0.0994,  0.0394,  0.1115,
         0.1084,  0.0707,  0.0948,  0.1101,  0.0336,  0.1129,  0.0900,  0.0578,
         0.1179,  0.0810,  0.0947,  0.0856,  0.0974,  0.0710,  0.0541,  0.1111,
         0.1028,  0.1099,  0.0826,  0.1066,  0.1066,  0.0717,  0.1029,  0.0516,
         0.1068,  0.0774,  0.1106,  0.0748,  0.1130,  0.0852,  0.0727,  0.1053,
         0.0732,  0.0939,  0.1061,  0.0593,  0.0898,  0.0849,  0.1106,  0.0953,
         0.0852,  0.1068,  0.1096,  0.0927,  0.0992,  0.0939,  0.0984,  0.1144,
         0.1069,  0.0801,  0.0918,  0.0636,  0.0661,  0.0873,  0.0818,  0.0330,
         0.0991,  0.0753,  0.1069,  0.0879,  0.0261,  0.1158,  0.0788,  0.1075,
         0.0858,  0.0547,  0.0899,  0.1031,  0.1166,  0.1071,  0.1125,  0.0790,
         0.0618,  0.1000,  0.0943,  0.0791,  0.1066,  0.1040,  0.1181,  0.1064,
         0.0922,  0.0791,  0.0957,  0.0262,  0.1112,  0.0995,  0.1009,  0.0772],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.8694, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:41,967 :: INFO :: Epoch 40: loss tensor(320.3560, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 4.7258e-02,  4.1120e-02,  2.5329e-02,  5.0574e-02,  2.2088e-02,
         3.9123e-03,  4.4739e-02,  4.8161e-02,  2.9368e-02,  3.4555e-02,
         1.7399e-02,  1.2403e-02,  3.6697e-02,  3.6214e-02,  3.2645e-02,
         3.6928e-02,  2.2284e-02,  4.0989e-02,  1.9256e-02,  5.7206e-02,
         2.6903e-02,  3.3538e-02,  5.0404e-02,  2.8454e-02,  1.1706e-02,
         2.7165e-02,  2.7124e-02,  4.2237e-02,  7.2621e-02,  2.4914e-02,
         3.5068e-02,  4.5285e-02,  3.4887e-02,  3.2251e-02,  6.1154e-04,
        -1.6188e-02,  2.8765e-02,  3.2694e-02,  1.8377e-02,  2.8149e-02,
         2.7299e-03,  2.2521e-02,  4.5997e-02,  2.6071e-02,  1.5595e-02,
         2.9775e-02,  2.9080e-02,  1.2299e-02,  2.6374e-02,  2.4228e-02,
         1.8445e-02,  3.5410e-02,  2.2523e-02,  2.1503e-02,  3.7659e-02,
         8.0532e-02,  3.8285e-02,  6.8080e-02,  2.5410e-02, -7.7456e-03,
         3.2727e-02,  2.6236e-02,  4.4903e-03,  1.7431e-02,  3.1460e-02,
         2.3883e-02,  3.9316e-02,  2.9222e-02,  4.5596e-02, -6.3878e-03,
         3.4202e-02,  2.0584e-02,  4.0390e-02,  4.0885e-02,  5.2276e-02,
         9.6497e-05,  4.2261e-02,  1.3490e-02,  3.9213e-02,  3.8468e-02,
         5.0497e-02,  1.2802e-02,  5.1355e-02,  6.5572e-02, -9.8646e-03,
         3.3952e-02,  3.8145e-02,  1.7535e-02,  1.7904e-02, -8.7649e-03,
         2.5258e-02,  7.1176e-02,  3.1171e-02,  5.5002e-02,  2.0084e-02,
         5.1589e-02,  3.0820e-02,  1.1928e-02,  3.9142e-03,  1.6142e-02,
         1.1716e-02,  3.7662e-02,  4.5906e-02,  3.4066e-03,  2.9080e-02,
        -1.9600e-02,  3.5016e-02,  1.2792e-02,  2.9232e-02,  5.0933e-02,
         3.1136e-02,  4.2817e-02,  3.7399e-02,  3.2458e-02, -6.9801e-03,
        -1.6799e-02,  2.1869e-02,  1.4777e-02,  1.5116e-02,  2.9315e-02,
         1.5406e-02,  2.9740e-02,  1.9703e-02,  4.2642e-03,  4.0543e-02,
         9.8623e-04,  3.0840e-02,  5.1850e-02,  1.1350e-01,  9.5166e-02,
         8.5492e-02,  1.0600e-01,  1.2086e-01,  1.3342e-01,  9.0629e-02,
         4.4178e-02, -2.5255e-02,  7.2158e-02,  1.0475e-01,  2.2752e-02,
         1.1934e-01,  1.2512e-03,  1.1822e-01,  1.1525e-01,  1.0428e-01,
         1.0007e-01,  9.8787e-02,  4.9547e-02,  9.0698e-02,  7.9254e-02,
        -2.3081e-02,  1.3265e-01,  1.0939e-01, -4.9625e-02,  5.2524e-02,
        -1.2760e-02,  3.3558e-03, -5.4053e-02,  7.3042e-02, -5.2331e-02,
         1.0480e-01, -5.4317e-02,  1.0029e-01,  6.1056e-02,  8.4209e-02,
         5.2673e-02, -2.3754e-02,  4.0923e-02,  2.2572e-02,  1.0164e-01,
         1.4065e-01,  9.3644e-02, -5.5284e-02,  1.0257e-01,  1.2580e-01,
        -2.4972e-02,  6.0235e-02,  1.0615e-01,  5.8499e-02,  4.0188e-02,
         9.7608e-02, -1.1168e-02,  9.0561e-02, -5.2206e-02,  9.5814e-02,
         1.2286e-01, -4.4825e-02,  1.2806e-01,  1.4805e-01, -5.1842e-02,
        -9.2811e-03,  3.8371e-02,  9.0221e-02, -4.6195e-02,  1.0806e-01,
         1.1240e-01,  1.0328e-01,  3.8719e-02,  1.1937e-01,  9.3886e-02,
         1.0487e-01,  1.1594e-01,  1.0443e-01,  1.2775e-01,  9.5274e-02,
         1.0156e-01,  2.0862e-02, -5.7023e-02,  7.7335e-02,  6.6777e-02,
         1.2899e-01,  9.2243e-02,  6.8735e-02, -1.9944e-02,  1.1642e-01,
         1.2706e-01,  5.7914e-02, -7.0582e-03,  1.0235e-01, -5.3022e-02,
         1.2622e-01,  1.0449e-01,  1.2790e-01,  1.2653e-01,  1.2198e-01,
         1.2492e-01,  9.7686e-02,  1.1517e-01,  1.0059e-01,  1.2552e-01,
         9.9994e-02,  7.1771e-02,  9.4472e-02,  1.0867e-01,  9.6721e-02,
         5.3089e-02,  3.6300e-02,  2.9525e-02,  4.5148e-02,  1.0728e-01,
         9.5024e-02,  6.1435e-02,  4.7232e-02,  9.9356e-02, -4.3075e-03,
         7.9238e-02, -5.1224e-02,  9.4587e-02,  4.3736e-02,  2.5727e-02,
        -1.4825e-02,  1.0895e-01,  9.0338e-02,  9.9905e-02, -4.9857e-02,
         9.4801e-02,  1.3964e-01,  7.1371e-02,  1.1646e-01,  1.2847e-01,
         1.3401e-01,  1.3866e-01,  9.6683e-02,  1.1660e-01,  1.1033e-01,
         1.3068e-01,  1.0447e-01,  8.5312e-02,  1.1540e-01,  1.1408e-01,
         6.3939e-02,  1.2119e-01,  1.2017e-01,  5.8804e-02,  1.2077e-01,
         9.5484e-02,  1.3088e-01,  1.2863e-01,  9.9051e-02,  1.1635e-01,
         1.1174e-01,  1.3215e-01,  1.1952e-01,  7.3718e-02,  1.2676e-01,
         1.1858e-01,  1.1119e-01,  1.2683e-01,  1.1401e-01,  1.2180e-01,
         1.0343e-01,  1.2258e-01,  1.3290e-01,  1.1267e-01,  5.1383e-02,
         1.2659e-01,  1.2421e-01,  8.8852e-02,  1.0319e-01,  1.2401e-01,
         5.0273e-02,  1.3312e-01,  1.1061e-01,  7.7043e-02,  1.3712e-01,
         1.0107e-01,  1.0570e-01,  1.0362e-01,  1.1145e-01,  8.9704e-02,
         7.2079e-02,  1.3145e-01,  1.2182e-01,  1.2827e-01,  1.0109e-01,
         1.2045e-01,  1.2333e-01,  9.1452e-02,  1.1878e-01,  6.9180e-02,
         1.2122e-01,  9.7827e-02,  1.2538e-01,  9.2495e-02,  1.3197e-01,
         1.0012e-01,  9.2918e-02,  1.2423e-01,  8.8772e-02,  1.0981e-01,
         1.2224e-01,  7.5292e-02,  1.0718e-01,  1.0083e-01,  1.2960e-01,
         1.0548e-01,  9.5758e-02,  1.2337e-01,  1.2876e-01,  1.0343e-01,
         1.1489e-01,  1.0941e-01,  1.1592e-01,  1.3256e-01,  1.2334e-01,
         8.8108e-02,  1.0484e-01,  8.1482e-02,  8.3022e-02,  1.0402e-01,
         9.6272e-02,  4.7704e-02,  1.1581e-01,  8.7069e-02,  1.2547e-01,
         1.0257e-01,  4.2400e-02,  1.3311e-01,  9.2654e-02,  1.2378e-01,
         1.0398e-01,  7.4004e-02,  1.0952e-01,  1.2407e-01,  1.3468e-01,
         1.2134e-01,  1.3013e-01,  9.7721e-02,  7.4059e-02,  1.1684e-01,
         1.1164e-01,  9.9603e-02,  1.2026e-01,  1.1863e-01,  1.3862e-01,
         1.2594e-01,  1.0556e-01,  8.2703e-02,  1.0875e-01,  4.2083e-02,
         1.2578e-01,  1.1675e-01,  1.1971e-01,  9.1472e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.8959, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:41,967 :: INFO :: ----- frontend -----
2023-04-11 14:38:41,967 :: INFO :: Environment 0
2023-04-11 14:38:56,859 :: INFO :: Epoch 5: loss tensor(871.9537, device='cuda:0'), U.norm 14.65007209777832, V.norm 18.348159790039062, MLP.norm 1.8738207817077637
2023-04-11 14:38:56,952 :: INFO :: Epoch 10: loss tensor(858.0969, device='cuda:0'), U.norm 12.047375679016113, V.norm 17.86322784423828, MLP.norm 2.6654584407806396
2023-04-11 14:38:57,061 :: INFO :: Epoch 15: loss tensor(837.8220, device='cuda:0'), U.norm 10.387129783630371, V.norm 17.64420509338379, MLP.norm 3.7001216411590576
2023-04-11 14:38:57,171 :: INFO :: Epoch 20: loss tensor(812.7770, device='cuda:0'), U.norm 9.197229385375977, V.norm 17.503385543823242, MLP.norm 4.786757946014404
2023-04-11 14:38:57,171 :: INFO :: Environment 1
2023-04-11 14:39:10,796 :: INFO :: Epoch 5: loss tensor(892.0544, device='cuda:0'), U.norm 14.649955749511719, V.norm 18.37648582458496, MLP.norm 1.8853497505187988
2023-04-11 14:39:10,890 :: INFO :: Epoch 10: loss tensor(877.4917, device='cuda:0'), U.norm 12.047818183898926, V.norm 17.900484085083008, MLP.norm 2.7056422233581543
2023-04-11 14:39:10,967 :: INFO :: Epoch 15: loss tensor(856.6741, device='cuda:0'), U.norm 10.388382911682129, V.norm 17.683725357055664, MLP.norm 3.7759764194488525
2023-04-11 14:39:11,062 :: INFO :: Epoch 20: loss tensor(831.0859, device='cuda:0'), U.norm 9.19904899597168, V.norm 17.54302215576172, MLP.norm 4.892043590545654
2023-04-11 14:39:11,062 :: INFO :: Environment 2
2023-04-11 14:39:24,717 :: INFO :: Epoch 5: loss tensor(808.9810, device='cuda:0'), U.norm 14.64728832244873, V.norm 18.234901428222656, MLP.norm 1.865416169166565
2023-04-11 14:39:24,796 :: INFO :: Epoch 10: loss tensor(796.8972, device='cuda:0'), U.norm 12.041679382324219, V.norm 17.705604553222656, MLP.norm 2.6223936080932617
2023-04-11 14:39:24,905 :: INFO :: Epoch 15: loss tensor(779.1282, device='cuda:0'), U.norm 10.377819061279297, V.norm 17.46865463256836, MLP.norm 3.618049383163452
2023-04-11 14:39:24,999 :: INFO :: Epoch 20: loss tensor(757.0125, device='cuda:0'), U.norm 9.183448791503906, V.norm 17.318092346191406, MLP.norm 4.669224739074707
2023-04-11 14:39:24,999 :: INFO :: Environment 3
2023-04-11 14:39:39,624 :: INFO :: Epoch 5: loss tensor(854.7393, device='cuda:0'), U.norm 14.647916793823242, V.norm 18.317607879638672, MLP.norm 1.8776838779449463
2023-04-11 14:39:39,718 :: INFO :: Epoch 10: loss tensor(841.0697, device='cuda:0'), U.norm 12.043427467346191, V.norm 17.82198143005371, MLP.norm 2.6753368377685547
2023-04-11 14:39:39,796 :: INFO :: Epoch 15: loss tensor(821.4424, device='cuda:0'), U.norm 10.381027221679688, V.norm 17.597923278808594, MLP.norm 3.7081551551818848
2023-04-11 14:39:39,890 :: INFO :: Epoch 20: loss tensor(797.9224, device='cuda:0'), U.norm 9.188231468200684, V.norm 17.45454216003418, MLP.norm 4.793680191040039
2023-04-11 14:39:39,890 :: INFO :: Environment 4
2023-04-11 14:39:53,796 :: INFO :: Epoch 5: loss tensor(808.0692, device='cuda:0'), U.norm 14.64615535736084, V.norm 18.2281551361084, MLP.norm 1.8581500053405762
2023-04-11 14:39:53,890 :: INFO :: Epoch 10: loss tensor(796.5109, device='cuda:0'), U.norm 12.03886890411377, V.norm 17.692949295043945, MLP.norm 2.6056885719299316
2023-04-11 14:39:53,983 :: INFO :: Epoch 15: loss tensor(779.1539, device='cuda:0'), U.norm 10.37314224243164, V.norm 17.451335906982422, MLP.norm 3.5874011516571045
2023-04-11 14:39:54,061 :: INFO :: Epoch 20: loss tensor(758.2098, device='cuda:0'), U.norm 9.176689147949219, V.norm 17.295629501342773, MLP.norm 4.62274694442749
2023-04-11 14:39:54,077 :: INFO :: Environment 5
2023-04-11 14:40:07,311 :: INFO :: Epoch 5: loss tensor(922.3793, device='cuda:0'), U.norm 14.648239135742188, V.norm 18.440532684326172, MLP.norm 1.9003721475601196
2023-04-11 14:40:07,436 :: INFO :: Epoch 10: loss tensor(904.9771, device='cuda:0'), U.norm 12.045121192932129, V.norm 17.98641586303711, MLP.norm 2.7632336616516113
2023-04-11 14:40:07,530 :: INFO :: Epoch 15: loss tensor(880.5332, device='cuda:0'), U.norm 10.384700775146484, V.norm 17.779932022094727, MLP.norm 3.8631250858306885
2023-04-11 14:40:07,655 :: INFO :: Epoch 20: loss tensor(850.8412, device='cuda:0'), U.norm 9.194611549377441, V.norm 17.646038055419922, MLP.norm 5.004215717315674
2023-04-11 14:40:07,655 :: INFO :: Environment 6
2023-04-11 14:40:21,733 :: INFO :: Epoch 5: loss tensor(887.7643, device='cuda:0'), U.norm 14.651345252990723, V.norm 18.382646560668945, MLP.norm 1.8801169395446777
2023-04-11 14:40:21,811 :: INFO :: Epoch 10: loss tensor(873.0331, device='cuda:0'), U.norm 12.05070686340332, V.norm 17.910247802734375, MLP.norm 2.6903843879699707
2023-04-11 14:40:21,890 :: INFO :: Epoch 15: loss tensor(851.6454, device='cuda:0'), U.norm 10.393327713012695, V.norm 17.698421478271484, MLP.norm 3.736555337905884
2023-04-11 14:40:21,983 :: INFO :: Epoch 20: loss tensor(825.2469, device='cuda:0'), U.norm 9.206603050231934, V.norm 17.56308937072754, MLP.norm 4.839842319488525
2023-04-11 14:40:21,983 :: INFO :: Environment 7
2023-04-11 14:40:35,547 :: INFO :: Epoch 5: loss tensor(824.7815, device='cuda:0'), U.norm 14.649168968200684, V.norm 18.260757446289062, MLP.norm 1.8575187921524048
2023-04-11 14:40:35,640 :: INFO :: Epoch 10: loss tensor(812.7148, device='cuda:0'), U.norm 12.045165061950684, V.norm 17.74173927307129, MLP.norm 2.6187593936920166
2023-04-11 14:40:35,717 :: INFO :: Epoch 15: loss tensor(795.2211, device='cuda:0'), U.norm 10.383101463317871, V.norm 17.507944107055664, MLP.norm 3.617598533630371
2023-04-11 14:40:35,811 :: INFO :: Epoch 20: loss tensor(774.4431, device='cuda:0'), U.norm 9.190771102905273, V.norm 17.35738754272461, MLP.norm 4.670670032501221
2023-04-11 14:40:35,811 :: INFO :: Environment 8
2023-04-11 14:40:49,265 :: INFO :: Epoch 5: loss tensor(800.9776, device='cuda:0'), U.norm 14.649097442626953, V.norm 18.210405349731445, MLP.norm 1.8461579084396362
2023-04-11 14:40:49,358 :: INFO :: Epoch 10: loss tensor(789.8535, device='cuda:0'), U.norm 12.044869422912598, V.norm 17.666568756103516, MLP.norm 2.583378314971924
2023-04-11 14:40:49,467 :: INFO :: Epoch 15: loss tensor(772.7527, device='cuda:0'), U.norm 10.382999420166016, V.norm 17.42137908935547, MLP.norm 3.5688788890838623
2023-04-11 14:40:49,577 :: INFO :: Epoch 20: loss tensor(752.1366, device='cuda:0'), U.norm 9.190940856933594, V.norm 17.2650146484375, MLP.norm 4.620466709136963
2023-04-11 14:40:49,577 :: INFO :: Environment 9
2023-04-11 14:41:01,655 :: INFO :: Epoch 5: loss tensor(830.2570, device='cuda:0'), U.norm 14.646376609802246, V.norm 18.276906967163086, MLP.norm 1.875023365020752
2023-04-11 14:41:01,733 :: INFO :: Epoch 10: loss tensor(818.0978, device='cuda:0'), U.norm 12.039774894714355, V.norm 17.763010025024414, MLP.norm 2.6412851810455322
2023-04-11 14:41:01,811 :: INFO :: Epoch 15: loss tensor(800.8668, device='cuda:0'), U.norm 10.374897003173828, V.norm 17.53166389465332, MLP.norm 3.630338668823242
2023-04-11 14:41:01,889 :: INFO :: Epoch 20: loss tensor(780.1610, device='cuda:0'), U.norm 9.179505348205566, V.norm 17.383066177368164, MLP.norm 4.676366806030273
2023-04-11 14:41:01,889 :: INFO :: Ite = 1, Delta = 4195
2023-04-11 14:41:01,905 :: INFO :: ----- backend -----
2023-04-11 14:41:04,155 :: INFO :: Epoch 5: loss tensor(133.1255, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0097, -0.0110, -0.0122, -0.0054, -0.0115, -0.0142, -0.0113, -0.0054,
        -0.0133, -0.0101, -0.0133, -0.0123, -0.0105, -0.0083, -0.0063, -0.0077,
        -0.0106, -0.0067, -0.0052, -0.0101, -0.0102, -0.0067, -0.0086, -0.0136,
        -0.0103, -0.0111, -0.0070, -0.0101, -0.0095, -0.0081, -0.0102,  0.0013,
        -0.0083, -0.0089, -0.0132, -0.0139, -0.0045, -0.0095, -0.0130, -0.0055,
        -0.0140, -0.0128, -0.0088, -0.0107, -0.0114, -0.0096, -0.0084, -0.0134,
        -0.0101, -0.0074, -0.0121, -0.0099, -0.0094, -0.0074, -0.0132,  0.0006,
        -0.0107, -0.0049, -0.0123, -0.0138, -0.0063, -0.0021, -0.0136, -0.0115,
        -0.0084, -0.0085, -0.0002, -0.0108, -0.0032, -0.0135,  0.0012, -0.0122,
         0.0104, -0.0099, -0.0059, -0.0133,  0.0050, -0.0124, -0.0061,  0.0126,
        -0.0106, -0.0130, -0.0055, -0.0079, -0.0137, -0.0073, -0.0057, -0.0120,
        -0.0108, -0.0132, -0.0125, -0.0062, -0.0077, -0.0064, -0.0128, -0.0092,
        -0.0074, -0.0121, -0.0134, -0.0116, -0.0104, -0.0061, -0.0093, -0.0127,
        -0.0121, -0.0147, -0.0071, -0.0111, -0.0078, -0.0058, -0.0111, -0.0071,
        -0.0116, -0.0073, -0.0134, -0.0134, -0.0095, -0.0136, -0.0057, -0.0108,
        -0.0115, -0.0110, -0.0083, -0.0106, -0.0113, -0.0097, -0.0130, -0.0068,
         0.0060, -0.0007, -0.0015,  0.0182,  0.0012,  0.0140, -0.0081, -0.0124,
        -0.0139,  0.0142, -0.0111, -0.0139,  0.0150, -0.0137,  0.0028, -0.0117,
         0.0010,  0.0161,  0.0073, -0.0077, -0.0004, -0.0114, -0.0136, -0.0079,
         0.0112, -0.0151, -0.0133, -0.0140, -0.0138, -0.0148,  0.0180, -0.0152,
         0.0154, -0.0150,  0.0145, -0.0124,  0.0154,  0.0166, -0.0142, -0.0132,
        -0.0134, -0.0079,  0.0132,  0.0124, -0.0149,  0.0046,  0.0102, -0.0138,
         0.0174,  0.0091, -0.0125, -0.0130,  0.0047, -0.0135,  0.0151, -0.0149,
         0.0105,  0.0133, -0.0146,  0.0066,  0.0174, -0.0150, -0.0152, -0.0119,
         0.0169, -0.0145,  0.0038,  0.0136, -0.0108,  0.0172, -0.0046, -0.0015,
         0.0094,  0.0005, -0.0037,  0.0172,  0.0010, -0.0067, -0.0142, -0.0149,
         0.0180, -0.0116,  0.0187,  0.0186,  0.0142, -0.0137, -0.0047,  0.0095,
        -0.0093, -0.0144,  0.0158, -0.0147, -0.0061, -0.0137,  0.0096, -0.0045,
         0.0065,  0.0037,  0.0048,  0.0035,  0.0064,  0.0160,  0.0055,  0.0155,
         0.0188,  0.0121,  0.0161, -0.0073, -0.0129, -0.0127,  0.0114,  0.0129,
        -0.0058, -0.0070, -0.0074,  0.0114, -0.0132,  0.0009, -0.0148, -0.0090,
        -0.0118, -0.0130, -0.0148,  0.0185, -0.0036,  0.0041, -0.0143, -0.0030,
        -0.0118, -0.0132, -0.0135, -0.0014, -0.0092,  0.0042, -0.0151, -0.0135,
        -0.0133, -0.0101, -0.0122, -0.0137, -0.0118, -0.0062, -0.0137, -0.0126,
        -0.0143, -0.0132, -0.0097, -0.0144, -0.0084, -0.0070, -0.0125, -0.0119,
        -0.0116, -0.0122, -0.0122, -0.0134, -0.0105, -0.0078, -0.0145, -0.0103,
        -0.0097, -0.0141, -0.0107, -0.0110, -0.0135, -0.0105, -0.0133, -0.0108,
        -0.0109, -0.0142, -0.0011, -0.0075, -0.0145, -0.0131, -0.0143, -0.0146,
        -0.0024, -0.0136, -0.0083, -0.0137, -0.0061, -0.0150, -0.0136, -0.0090,
        -0.0088, -0.0090, -0.0132, -0.0094, -0.0098, -0.0137, -0.0123, -0.0139,
        -0.0084, -0.0143, -0.0020, -0.0135, -0.0084, -0.0129, -0.0139, -0.0131,
        -0.0127, -0.0129, -0.0118, -0.0139, -0.0145, -0.0141, -0.0062, -0.0079,
        -0.0123, -0.0123, -0.0131, -0.0112, -0.0113, -0.0137, -0.0140,  0.0034,
        -0.0082, -0.0086, -0.0135, -0.0134, -0.0133, -0.0146, -0.0126, -0.0139,
        -0.0121, -0.0137, -0.0099, -0.0137, -0.0142, -0.0083, -0.0122, -0.0100,
        -0.0133, -0.0145, -0.0135, -0.0127, -0.0018, -0.0080, -0.0079, -0.0140,
        -0.0130, -0.0124, -0.0141, -0.0137, -0.0083, -0.0060, -0.0098, -0.0097,
        -0.0100, -0.0016, -0.0103, -0.0145, -0.0092, -0.0138, -0.0140, -0.0125],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.9699, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:06,733 :: INFO :: Epoch 10: loss tensor(126.8229, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-7.8603e-03, -1.1950e-02, -1.9523e-02, -1.1704e-03, -1.1541e-02,
        -2.3199e-02, -1.4555e-02,  2.7721e-04, -1.9520e-02, -9.2529e-03,
        -2.1336e-02, -1.9940e-02, -7.3071e-03, -8.8456e-03, -6.2939e-03,
        -8.0667e-03, -1.2539e-02, -4.0172e-03, -7.0902e-03, -1.0153e-02,
        -1.2878e-02, -6.5108e-03, -7.0592e-03, -2.2077e-02, -1.5939e-02,
        -1.0703e-02, -4.1142e-03, -9.5204e-03, -8.9588e-03, -9.1380e-03,
        -1.1018e-02,  6.8014e-03, -1.0468e-02, -9.6374e-03, -2.2840e-02,
        -2.5077e-02,  4.4148e-05, -6.7940e-03, -1.8256e-02, -7.1211e-03,
        -2.2804e-02, -1.5232e-02, -9.6454e-03, -1.3818e-02, -1.4001e-02,
        -1.2686e-02, -4.7197e-03, -2.2460e-02, -1.3777e-02, -6.5449e-03,
        -1.7843e-02, -1.2486e-02, -1.2486e-02, -6.2246e-03, -2.1148e-02,
         8.9698e-03, -1.2231e-02,  1.0443e-03, -1.9605e-02, -2.3814e-02,
        -2.5267e-03,  1.8406e-03, -2.2654e-02, -1.4375e-02, -9.6008e-03,
        -6.8487e-03,  4.2589e-03, -1.7162e-02,  2.4753e-03, -2.2127e-02,
         7.8225e-03, -1.8264e-02,  1.7751e-02, -5.6837e-03,  1.0809e-03,
        -2.1820e-02,  5.3932e-03, -2.0071e-02, -9.7715e-04,  1.4923e-02,
        -9.6709e-03, -2.2292e-02,  2.1226e-03, -2.8356e-03, -2.2975e-02,
        -5.2804e-03, -5.8571e-03, -1.8491e-02, -1.1426e-02, -2.0798e-02,
        -1.5574e-02,  2.4741e-03, -2.7223e-03, -1.4978e-03, -2.1795e-02,
        -7.1502e-03, -2.7326e-03, -1.9762e-02, -1.9519e-02, -1.8264e-02,
        -1.6889e-02,  9.2772e-04, -8.5795e-03, -1.7350e-02, -1.8352e-02,
        -2.6053e-02, -8.1280e-03, -1.6419e-02, -7.0457e-03,  2.5211e-05,
        -1.6406e-02, -3.3598e-03, -1.7355e-02, -6.7206e-03, -2.2322e-02,
        -2.2687e-02, -1.1589e-02, -2.0833e-02, -4.3510e-03, -1.4080e-02,
        -1.6806e-02, -1.2572e-02, -1.1203e-02, -1.5765e-02, -1.3211e-02,
        -1.3666e-02, -2.0077e-02, -1.1679e-04,  2.6797e-02,  1.8890e-02,
         1.8423e-02,  3.7601e-02,  2.0987e-02,  3.0230e-02,  6.9892e-03,
        -1.0389e-02, -1.9298e-02,  2.6216e-02, -3.4471e-03, -2.1745e-02,
         3.6016e-02, -2.0560e-02,  2.4332e-02, -1.1386e-02,  2.2648e-02,
         3.5816e-02,  2.6754e-02,  9.5821e-03,  2.0980e-02, -1.4091e-03,
        -1.9788e-02,  7.9723e-03,  3.2932e-02, -2.6012e-02, -1.8283e-02,
        -2.1948e-02, -1.8753e-02, -2.6054e-02,  3.3411e-02, -2.7763e-02,
         3.5868e-02, -2.7299e-02,  3.0962e-02, -1.4865e-02,  3.3784e-02,
         2.4410e-02, -2.3486e-02, -1.5375e-02, -1.8685e-02,  7.6169e-03,
         3.4337e-02,  3.1731e-02, -2.6611e-02,  2.5579e-02,  3.2043e-02,
        -1.8636e-02,  3.0968e-02,  3.0090e-02, -1.3287e-02, -1.9019e-02,
         2.5048e-02, -1.8012e-02,  3.2252e-02, -2.6160e-02,  3.0561e-02,
         3.3824e-02, -2.4906e-02,  2.7840e-02,  3.6978e-02, -2.6148e-02,
        -2.6698e-02, -9.6908e-03,  3.5896e-02, -2.4861e-02,  2.4556e-02,
         3.3041e-02,  4.2462e-04,  2.7518e-02,  1.3937e-02,  1.9464e-02,
         3.0801e-02,  2.1831e-02,  1.3439e-02,  3.6056e-02,  2.2332e-02,
         1.1820e-02, -2.2181e-02, -2.6444e-02,  3.5765e-02, -6.3346e-03,
         3.8942e-02,  3.6877e-02,  2.0371e-02, -2.0712e-02,  1.5082e-02,
         3.0644e-02,  3.3806e-03, -2.4135e-02,  3.4672e-02, -2.5951e-02,
         1.3426e-02, -1.5132e-02,  2.8671e-02,  1.3903e-02,  2.3211e-02,
         2.4985e-02,  2.5265e-02,  2.3980e-02,  2.5148e-02,  3.7877e-02,
         2.6908e-02,  3.3399e-02,  3.5346e-02,  2.4899e-02,  3.5810e-02,
         1.0421e-02, -1.4463e-02, -1.4748e-02,  2.0264e-02,  3.1072e-02,
         1.2615e-02,  9.5860e-03,  9.0921e-03,  3.1501e-02, -1.5757e-02,
         2.1123e-02, -2.5907e-02,  2.6636e-03, -9.2813e-03, -1.6532e-02,
        -2.3709e-02,  3.7194e-02,  1.5422e-02,  2.6147e-02, -2.4595e-02,
         1.7047e-02,  3.9232e-03, -1.5602e-02, -1.0065e-02,  2.0809e-02,
         7.2001e-03,  2.6459e-02, -1.4221e-02, -4.3244e-03, -1.7791e-02,
         4.9253e-03, -1.0290e-02, -1.8492e-02,  7.7518e-04,  1.3209e-02,
        -1.9718e-02, -9.3234e-04, -1.7715e-02, -1.8650e-02,  7.5021e-03,
        -1.3838e-02,  1.0180e-02,  1.2856e-02, -1.2178e-02, -4.1152e-03,
        -1.4653e-03, -4.8891e-03, -4.5946e-03, -1.5549e-02,  3.6291e-03,
         1.0641e-02, -8.2290e-03,  6.7386e-03,  4.2264e-03, -6.5063e-03,
         2.6325e-04,  6.8696e-03, -1.2188e-02,  2.2396e-03, -1.8911e-02,
         4.0589e-04,  2.9331e-03, -2.2108e-02,  2.0864e-02,  1.1380e-02,
        -2.1060e-02, -1.0292e-02, -1.7025e-02, -2.2968e-02,  1.9085e-02,
        -1.6801e-02,  8.6669e-03, -1.8569e-02,  1.4395e-02, -1.3570e-02,
        -1.8734e-02,  8.5022e-03,  1.0450e-02,  8.4097e-03, -1.2738e-02,
         8.0475e-03,  6.0301e-03, -1.6712e-02, -1.6461e-03, -1.9247e-02,
         1.0195e-02, -2.1409e-02,  2.0325e-02, -1.8042e-02,  1.0195e-02,
        -1.4708e-02, -1.5312e-02, -6.9679e-03, -1.0054e-02, -9.9093e-03,
         1.7725e-03, -1.8450e-02, -1.0511e-02, -1.1177e-02,  1.4181e-02,
         1.0735e-02, -1.1471e-02,  1.4288e-03, -1.2630e-02,  1.5957e-03,
         3.0679e-03, -4.7211e-03, -1.1988e-02,  2.6111e-02,  1.1339e-02,
         7.4835e-03, -1.6509e-02, -1.6694e-02, -1.8603e-02, -1.1077e-02,
        -6.8756e-03, -1.9660e-02,  1.2339e-03, -1.6508e-02,  5.7231e-03,
        -1.0869e-02, -2.1148e-02,  8.8813e-03, -1.0807e-02,  6.7727e-03,
        -1.3256e-02, -2.0539e-02, -1.5072e-02, -3.5191e-03,  2.0320e-02,
         8.8128e-03,  1.1767e-02, -2.0448e-02, -1.3652e-02,  1.1315e-03,
        -1.0472e-02, -9.4537e-03,  9.7634e-03,  1.5038e-02,  7.1121e-03,
         5.2037e-03,  4.2921e-03,  2.0173e-02,  5.9264e-03, -2.2559e-02,
         8.2000e-03, -3.4820e-03, -4.4051e-03, -6.6738e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.7531, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:09,202 :: INFO :: Epoch 15: loss tensor(125.2907, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0016, -0.0063, -0.0202,  0.0075, -0.0052, -0.0268, -0.0075,  0.0093,
        -0.0183, -0.0033, -0.0207, -0.0236,  0.0015, -0.0021, -0.0010, -0.0038,
        -0.0100,  0.0054, -0.0061, -0.0033, -0.0098, -0.0021,  0.0016, -0.0232,
        -0.0171, -0.0052,  0.0010, -0.0017,  0.0043, -0.0076, -0.0036,  0.0145,
        -0.0029, -0.0033, -0.0275, -0.0326,  0.0067,  0.0010, -0.0189, -0.0015,
        -0.0278, -0.0122, -0.0041, -0.0124, -0.0108, -0.0099,  0.0030, -0.0259,
        -0.0111, -0.0028, -0.0167, -0.0093, -0.0134, -0.0019, -0.0204,  0.0195,
        -0.0066,  0.0124, -0.0186, -0.0277,  0.0030,  0.0061, -0.0263, -0.0110,
        -0.0036, -0.0026,  0.0149, -0.0168,  0.0089, -0.0262,  0.0140, -0.0149,
         0.0196,  0.0035,  0.0118, -0.0248,  0.0107, -0.0210,  0.0085,  0.0168,
         0.0002, -0.0221,  0.0109,  0.0097, -0.0273,  0.0036, -0.0018, -0.0183,
        -0.0069, -0.0240, -0.0127,  0.0156,  0.0042,  0.0087, -0.0226,  0.0041,
         0.0044, -0.0218, -0.0201, -0.0183, -0.0159,  0.0097, -0.0009, -0.0167,
        -0.0157, -0.0338, -0.0044, -0.0173, -0.0017,  0.0101, -0.0150,  0.0042,
        -0.0127, -0.0011, -0.0264, -0.0282, -0.0077, -0.0227, -0.0026, -0.0098,
        -0.0171, -0.0086, -0.0099, -0.0156, -0.0087, -0.0148, -0.0204,  0.0116,
         0.0467,  0.0391,  0.0403,  0.0555,  0.0424,  0.0470,  0.0269,  0.0015,
        -0.0167,  0.0318,  0.0136, -0.0240,  0.0570, -0.0221,  0.0466,  0.0006,
         0.0453,  0.0548,  0.0438,  0.0333,  0.0441,  0.0211, -0.0200,  0.0313,
         0.0542, -0.0344, -0.0158, -0.0240, -0.0167, -0.0351,  0.0453, -0.0385,
         0.0551, -0.0377,  0.0469, -0.0089,  0.0496,  0.0288, -0.0292, -0.0092,
        -0.0164,  0.0283,  0.0567,  0.0493, -0.0362,  0.0468,  0.0540, -0.0158,
         0.0399,  0.0500, -0.0051, -0.0204,  0.0451, -0.0141,  0.0466, -0.0351,
         0.0496,  0.0549, -0.0324,  0.0491,  0.0599, -0.0352, -0.0348,  0.0023,
         0.0532, -0.0320,  0.0434,  0.0524,  0.0222,  0.0315,  0.0362,  0.0423,
         0.0520,  0.0450,  0.0328,  0.0540,  0.0445,  0.0353, -0.0250, -0.0362,
         0.0504,  0.0083,  0.0595,  0.0536,  0.0338, -0.0222,  0.0377,  0.0528,
         0.0228, -0.0304,  0.0527, -0.0349,  0.0375, -0.0046,  0.0491,  0.0358,
         0.0420,  0.0473,  0.0451,  0.0457,  0.0414,  0.0596,  0.0476,  0.0485,
         0.0470,  0.0413,  0.0539,  0.0339, -0.0065, -0.0088,  0.0263,  0.0486,
         0.0344,  0.0312,  0.0313,  0.0500, -0.0103,  0.0422, -0.0342,  0.0229,
         0.0026, -0.0123, -0.0267,  0.0555,  0.0371,  0.0495, -0.0315,  0.0387,
         0.0292, -0.0065,  0.0087,  0.0455,  0.0313,  0.0510,  0.0035,  0.0189,
        -0.0120,  0.0286,  0.0053, -0.0129,  0.0239,  0.0378, -0.0172,  0.0230,
        -0.0075, -0.0144,  0.0323,  0.0021,  0.0343,  0.0385,  0.0013,  0.0172,
         0.0217,  0.0171,  0.0166, -0.0048,  0.0275,  0.0344,  0.0136,  0.0317,
         0.0259,  0.0158,  0.0226,  0.0323,  0.0054,  0.0248, -0.0159,  0.0225,
         0.0269, -0.0233,  0.0445,  0.0358, -0.0198,  0.0073, -0.0050, -0.0250,
         0.0438, -0.0084,  0.0318, -0.0137,  0.0392,  0.0043, -0.0132,  0.0333,
         0.0362,  0.0333,  0.0022,  0.0326,  0.0291, -0.0079,  0.0217, -0.0155,
         0.0353, -0.0190,  0.0454, -0.0117,  0.0352, -0.0048, -0.0035,  0.0140,
         0.0070,  0.0078,  0.0263, -0.0110,  0.0104,  0.0081,  0.0395,  0.0345,
         0.0022,  0.0259,  0.0023,  0.0241,  0.0275,  0.0180,  0.0065,  0.0509,
         0.0361,  0.0297, -0.0081, -0.0086, -0.0149,  0.0091,  0.0136, -0.0167,
         0.0257, -0.0094,  0.0304,  0.0074, -0.0214,  0.0324,  0.0045,  0.0316,
         0.0004, -0.0177, -0.0032,  0.0203,  0.0451,  0.0331,  0.0368, -0.0178,
        -0.0018,  0.0259,  0.0098,  0.0094,  0.0347,  0.0394,  0.0323,  0.0294,
         0.0276,  0.0437,  0.0304, -0.0245,  0.0327,  0.0204,  0.0190,  0.0128],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.4100, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:11,686 :: INFO :: Epoch 20: loss tensor(124.0249, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.6717e-02,  5.2482e-03, -1.2695e-02,  2.2053e-02,  5.4758e-03,
        -2.3156e-02,  7.5015e-03,  2.2037e-02, -1.0846e-02,  6.6743e-03,
        -1.0871e-02, -2.2322e-02,  1.2800e-02,  1.1839e-02,  1.0637e-02,
         3.9535e-03, -2.3179e-03,  1.9903e-02,  2.1894e-03,  1.0755e-02,
        -1.0043e-03,  9.1566e-03,  1.6548e-02, -1.5543e-02, -1.1816e-02,
         3.7729e-03,  1.0042e-02,  1.1962e-02,  2.4971e-02, -4.1950e-05,
         8.5283e-03,  2.6309e-02,  1.3755e-02,  9.7533e-03, -2.5399e-02,
        -3.4999e-02,  1.7893e-02,  1.1403e-02, -1.5288e-02,  1.2311e-02,
        -2.8254e-02, -3.5132e-03,  8.3163e-03, -6.4011e-03, -2.2732e-03,
        -1.4129e-03,  1.4221e-02, -2.1996e-02, -2.5949e-04,  5.5988e-03,
        -8.0230e-03, -9.4392e-04, -1.0134e-02,  6.8525e-03, -8.5819e-03,
         3.4616e-02,  5.8139e-03,  2.8794e-02, -7.8461e-03, -2.5235e-02,
         1.2277e-02,  1.4754e-02, -2.2910e-02, -1.4609e-03,  9.7662e-03,
         6.1091e-03,  3.1854e-02, -7.7252e-03,  1.9742e-02, -2.4646e-02,
         2.5434e-02, -2.6649e-03,  2.5861e-02,  1.6702e-02,  2.6038e-02,
        -2.0023e-02,  2.4414e-02, -1.3394e-02,  2.1835e-02,  2.6143e-02,
         1.4320e-02, -1.0544e-02,  2.2603e-02,  2.7863e-02, -2.5173e-02,
         1.8163e-02,  8.5795e-03, -8.9566e-03,  2.0464e-03, -2.0665e-02,
        -5.2537e-03,  3.1117e-02,  1.4787e-02,  2.2549e-02, -1.2057e-02,
         2.0439e-02,  1.4161e-02, -1.4485e-02, -1.5161e-02, -1.1406e-02,
        -5.3664e-03,  2.0400e-02,  1.0774e-02, -9.7705e-03, -3.1374e-03,
        -3.5985e-02,  6.8088e-03, -1.2925e-02,  8.6343e-03,  2.2726e-02,
        -4.5678e-03,  1.5280e-02,  2.4554e-04,  1.0806e-02, -2.5581e-02,
        -2.7585e-02,  3.3609e-03, -1.9404e-02,  3.5539e-03,  1.4618e-03,
        -9.8510e-03,  1.4091e-03, -3.2911e-04, -8.0921e-03,  1.2621e-03,
        -1.0664e-02, -1.2470e-02,  2.6526e-02,  6.4975e-02,  5.8525e-02,
         6.1899e-02,  7.0024e-02,  5.9718e-02,  6.3850e-02,  4.6310e-02,
         1.7856e-02, -7.1602e-03,  3.5244e-02,  3.2005e-02, -2.1604e-02,
         7.7211e-02, -1.9133e-02,  6.7933e-02,  2.2134e-02,  6.7390e-02,
         7.2952e-02,  5.6936e-02,  5.7853e-02,  6.6674e-02,  4.4622e-02,
        -1.5447e-02,  5.5176e-02,  7.4327e-02, -4.1249e-02, -7.9217e-03,
        -2.0563e-02, -9.5019e-03, -4.2279e-02,  5.5996e-02, -4.7387e-02,
         7.2665e-02, -4.6243e-02,  6.3039e-02,  1.7085e-03,  6.3897e-02,
         3.5660e-02, -3.1949e-02,  2.6493e-03, -8.4842e-03,  4.8280e-02,
         7.8057e-02,  6.4928e-02, -4.3932e-02,  6.7307e-02,  7.4795e-02,
        -6.7569e-03,  4.6593e-02,  6.8437e-02,  7.6871e-03, -1.7821e-02,
         6.4020e-02, -3.7371e-03,  5.8184e-02, -4.2042e-02,  6.7243e-02,
         7.5712e-02, -3.7509e-02,  6.8660e-02,  8.2992e-02, -4.2561e-02,
        -3.8627e-02,  1.8905e-02,  6.9295e-02, -3.6433e-02,  5.7679e-02,
         7.1343e-02,  4.6620e-02,  3.4123e-02,  5.7283e-02,  6.4936e-02,
         7.2608e-02,  6.8286e-02,  5.1653e-02,  6.9100e-02,  6.5721e-02,
         5.9945e-02, -2.3753e-02, -4.4244e-02,  6.1848e-02,  2.6091e-02,
         7.9746e-02,  6.8761e-02,  4.6381e-02, -1.9138e-02,  5.8304e-02,
         7.5282e-02,  4.3345e-02, -3.3648e-02,  7.0280e-02, -4.1933e-02,
         6.1849e-02,  1.0835e-02,  6.9629e-02,  5.8376e-02,  5.8511e-02,
         6.9851e-02,  6.2891e-02,  6.7921e-02,  5.4812e-02,  8.0422e-02,
         6.6294e-02,  6.0440e-02,  5.6352e-02,  5.4224e-02,  7.0229e-02,
         5.8220e-02,  7.0700e-03,  2.1733e-03,  3.0887e-02,  6.5314e-02,
         5.5031e-02,  5.3155e-02,  5.4407e-02,  6.7352e-02,  1.0871e-03,
         6.2885e-02, -4.0450e-02,  4.4232e-02,  1.8595e-02, -2.4521e-03,
        -2.3071e-02,  7.3613e-02,  5.9109e-02,  7.2447e-02, -3.4808e-02,
         5.9164e-02,  5.5860e-02,  9.8352e-03,  3.3097e-02,  6.8980e-02,
         5.5169e-02,  7.5983e-02,  2.6527e-02,  4.3898e-02,  2.1429e-03,
         5.3143e-02,  2.6868e-02, -1.1637e-03,  4.6933e-02,  6.1235e-02,
        -8.9169e-03,  4.7940e-02,  1.0426e-02, -3.8358e-03,  5.6509e-02,
         2.3035e-02,  5.8809e-02,  6.4045e-02,  1.9888e-02,  4.0230e-02,
         4.6061e-02,  4.3712e-02,  3.9245e-02,  1.3452e-02,  5.3623e-02,
         5.6086e-02,  3.8165e-02,  5.6918e-02,  4.5872e-02,  4.1110e-02,
         4.6294e-02,  5.7484e-02,  3.0542e-02,  4.7546e-02, -7.3013e-03,
         4.4767e-02,  5.0822e-02, -1.9059e-02,  6.5699e-02,  5.9284e-02,
        -1.2451e-02,  3.1156e-02,  1.6315e-02, -2.1420e-02,  6.8258e-02,
         8.3942e-03,  5.3528e-02, -2.3448e-03,  6.2944e-02,  2.7139e-02,
         6.9774e-04,  5.9116e-02,  6.2247e-02,  5.8604e-02,  2.3207e-02,
         5.6434e-02,  5.1422e-02,  9.5678e-03,  4.6123e-02, -4.9670e-03,
         5.9663e-02, -7.7103e-03,  6.9447e-02,  1.3296e-03,  6.0398e-02,
         1.0698e-02,  1.6159e-02,  3.8599e-02,  2.7919e-02,  2.9794e-02,
         5.0933e-02,  3.7283e-03,  3.5002e-02,  3.0959e-02,  6.5475e-02,
         5.6294e-02,  1.9252e-02,  5.0811e-02,  2.4147e-02,  4.5200e-02,
         5.1663e-02,  4.1831e-02,  2.9028e-02,  7.5498e-02,  5.9779e-02,
         4.9676e-02,  5.0069e-03,  6.3355e-03, -4.9807e-03,  3.2818e-02,
         3.6433e-02, -7.6839e-03,  5.0822e-02,  1.4833e-03,  5.5739e-02,
         2.8772e-02, -1.5693e-02,  5.5696e-02,  2.3362e-02,  5.6503e-02,
         2.0435e-02, -6.3641e-03,  1.7108e-02,  4.7181e-02,  6.9222e-02,
         5.7024e-02,  6.1700e-02, -8.4223e-03,  1.5095e-02,  5.1346e-02,
         3.4104e-02,  3.3829e-02,  5.8683e-02,  6.2365e-02,  5.8762e-02,
         5.5363e-02,  5.0166e-02,  6.3848e-02,  5.3949e-02, -2.1559e-02,
         5.6243e-02,  4.5583e-02,  4.4680e-02,  3.4441e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.9793, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:14,530 :: INFO :: Epoch 25: loss tensor(122.1127, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0299,  0.0147, -0.0030,  0.0347,  0.0153, -0.0156,  0.0219,  0.0322,
        -0.0014,  0.0142,  0.0012, -0.0200,  0.0224,  0.0247,  0.0205,  0.0091,
         0.0056,  0.0325,  0.0094,  0.0234,  0.0072,  0.0188,  0.0295, -0.0045,
        -0.0065,  0.0112,  0.0173,  0.0238,  0.0446,  0.0060,  0.0187,  0.0354,
         0.0290,  0.0219, -0.0206, -0.0346,  0.0257,  0.0206, -0.0108,  0.0249,
        -0.0261,  0.0055,  0.0195,  0.0005,  0.0053,  0.0061,  0.0231, -0.0149,
         0.0110,  0.0124,  0.0004,  0.0063, -0.0057,  0.0136,  0.0055,  0.0470,
         0.0186,  0.0435,  0.0041, -0.0197,  0.0185,  0.0224, -0.0165,  0.0077,
         0.0219,  0.0130,  0.0464,  0.0019,  0.0278, -0.0213,  0.0346,  0.0106,
         0.0311,  0.0299,  0.0367, -0.0135,  0.0372, -0.0045,  0.0324,  0.0364,
         0.0270,  0.0045,  0.0310,  0.0442, -0.0202,  0.0310,  0.0163,  0.0021,
         0.0099, -0.0162,  0.0021,  0.0431,  0.0221,  0.0338,  0.0008,  0.0347,
         0.0205, -0.0060, -0.0090, -0.0041,  0.0066,  0.0273,  0.0194, -0.0011,
         0.0106, -0.0353,  0.0166, -0.0077,  0.0178,  0.0332,  0.0064,  0.0254,
         0.0129,  0.0222, -0.0222, -0.0254,  0.0130, -0.0131,  0.0088,  0.0118,
        -0.0023,  0.0099,  0.0091, -0.0001,  0.0115, -0.0061, -0.0033,  0.0392,
         0.0814,  0.0759,  0.0819,  0.0805,  0.0714,  0.0788,  0.0639,  0.0352,
         0.0067,  0.0387,  0.0492, -0.0157,  0.0958, -0.0129,  0.0876,  0.0464,
         0.0878,  0.0895,  0.0664,  0.0815,  0.0877,  0.0665, -0.0072,  0.0772,
         0.0927, -0.0471,  0.0023, -0.0125,  0.0010, -0.0478,  0.0642, -0.0544,
         0.0881, -0.0532,  0.0773,  0.0155,  0.0762,  0.0399, -0.0322,  0.0170,
         0.0031,  0.0665,  0.0976,  0.0783, -0.0499,  0.0865,  0.0940,  0.0058,
         0.0518,  0.0848,  0.0220, -0.0130,  0.0812,  0.0103,  0.0659, -0.0470,
         0.0828,  0.0955, -0.0409,  0.0860,  0.1043, -0.0487, -0.0390,  0.0366,
         0.0834, -0.0385,  0.0670,  0.0891,  0.0699,  0.0347,  0.0764,  0.0863,
         0.0919,  0.0905,  0.0681,  0.0804,  0.0852,  0.0835, -0.0192, -0.0508,
         0.0697,  0.0437,  0.0986,  0.0818,  0.0547, -0.0132,  0.0759,  0.0969,
         0.0627, -0.0343,  0.0864, -0.0471,  0.0846,  0.0270,  0.0881,  0.0799,
         0.0706,  0.0911,  0.0782,  0.0893,  0.0652,  0.0996,  0.0826,  0.0686,
         0.0644,  0.0618,  0.0843,  0.0814,  0.0230,  0.0158,  0.0329,  0.0797,
         0.0736,  0.0742,  0.0766,  0.0832,  0.0152,  0.0822, -0.0447,  0.0644,
         0.0355,  0.0106, -0.0149,  0.0908,  0.0801,  0.0941, -0.0352,  0.0779,
         0.0825,  0.0295,  0.0593,  0.0900,  0.0773,  0.1007,  0.0492,  0.0678,
         0.0212,  0.0769,  0.0496,  0.0135,  0.0679,  0.0823,  0.0024,  0.0718,
         0.0311,  0.0101,  0.0786,  0.0445,  0.0824,  0.0886,  0.0390,  0.0617,
         0.0690,  0.0711,  0.0604,  0.0351,  0.0798,  0.0747,  0.0620,  0.0809,
         0.0625,  0.0660,  0.0688,  0.0814,  0.0573,  0.0679,  0.0039,  0.0648,
         0.0730, -0.0108,  0.0839,  0.0802, -0.0003,  0.0564,  0.0404, -0.0131,
         0.0918,  0.0291,  0.0719,  0.0131,  0.0848,  0.0502,  0.0184,  0.0845,
         0.0876,  0.0829,  0.0452,  0.0778,  0.0718,  0.0317,  0.0691,  0.0101,
         0.0820,  0.0090,  0.0917,  0.0177,  0.0849,  0.0275,  0.0391,  0.0631,
         0.0482,  0.0514,  0.0742,  0.0210,  0.0593,  0.0530,  0.0909,  0.0744,
         0.0353,  0.0742,  0.0476,  0.0633,  0.0744,  0.0642,  0.0515,  0.0993,
         0.0818,  0.0667,  0.0183,  0.0243,  0.0088,  0.0560,  0.0582,  0.0050,
         0.0746,  0.0120,  0.0803,  0.0491, -0.0052,  0.0780,  0.0413,  0.0800,
         0.0427,  0.0106,  0.0407,  0.0742,  0.0921,  0.0787,  0.0852,  0.0054,
         0.0324,  0.0755,  0.0581,  0.0598,  0.0805,  0.0827,  0.0851,  0.0815,
         0.0704,  0.0800,  0.0753, -0.0144,  0.0777,  0.0698,  0.0698,  0.0553],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.4430, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:17,140 :: INFO :: Epoch 30: loss tensor(121.2446, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 4.1244e-02,  2.0634e-02,  7.4022e-03,  4.4896e-02,  2.3132e-02,
        -7.3479e-03,  3.4677e-02,  4.0244e-02,  7.2681e-03,  1.9573e-02,
         1.3944e-02, -1.6738e-02,  2.9726e-02,  3.5362e-02,  2.8986e-02,
         1.3262e-02,  1.1479e-02,  4.1869e-02,  1.4311e-02,  3.4592e-02,
         1.3601e-02,  2.6165e-02,  4.0834e-02,  7.0816e-03, -9.5078e-04,
         1.7581e-02,  2.1206e-02,  3.3125e-02,  6.2073e-02,  1.0528e-02,
         2.6908e-02,  4.1831e-02,  4.1477e-02,  3.1874e-02, -1.4742e-02,
        -3.2579e-02,  3.0039e-02,  2.7753e-02, -6.3286e-03,  3.4732e-02,
        -2.2440e-02,  1.4011e-02,  2.8841e-02,  5.7410e-03,  1.0580e-02,
         1.1793e-02,  2.9095e-02, -6.4873e-03,  2.0713e-02,  1.7581e-02,
         7.4813e-03,  1.2399e-02, -1.6547e-03,  1.7338e-02,  1.9437e-02,
         5.6346e-02,  2.9405e-02,  5.6330e-02,  1.5204e-02, -1.2056e-02,
         2.2811e-02,  2.7757e-02, -9.3112e-03,  1.5102e-02,  3.1491e-02,
         1.7353e-02,  5.7618e-02,  1.0830e-02,  3.3305e-02, -1.7166e-02,
         4.0846e-02,  2.2969e-02,  3.4381e-02,  4.2015e-02,  4.5468e-02,
        -6.5291e-03,  4.8342e-02,  4.6555e-03,  3.9817e-02,  4.5612e-02,
         3.7262e-02,  1.9997e-02,  3.6402e-02,  5.9047e-02, -1.4642e-02,
         4.1792e-02,  2.2193e-02,  1.2858e-02,  1.5779e-02, -1.1634e-02,
         8.8341e-03,  5.2509e-02,  2.6126e-02,  4.1644e-02,  1.3385e-02,
         4.5544e-02,  2.4913e-02,  2.2387e-03, -3.1442e-03,  2.5580e-03,
         1.7716e-02,  3.1097e-02,  2.6411e-02,  6.7318e-03,  2.3859e-02,
        -3.2605e-02,  2.4271e-02, -3.3275e-03,  2.4752e-02,  4.1158e-02,
         1.8386e-02,  3.2761e-02,  2.5194e-02,  3.2047e-02, -1.7691e-02,
        -2.3151e-02,  2.0918e-02, -5.1978e-03,  1.1571e-02,  2.1361e-02,
         3.9129e-03,  1.6319e-02,  1.5827e-02,  5.7289e-03,  1.9112e-02,
        -1.7529e-03,  5.4856e-03,  4.8939e-02,  9.6261e-02,  9.0546e-02,
         1.0004e-01,  8.7575e-02,  7.8950e-02,  9.1553e-02,  7.8388e-02,
         5.1775e-02,  2.2797e-02,  4.1354e-02,  6.4193e-02, -7.9521e-03,
         1.1231e-01, -4.9406e-03,  1.0517e-01,  7.1611e-02,  1.0647e-01,
         1.0393e-01,  7.2364e-02,  1.0438e-01,  1.0652e-01,  8.6139e-02,
         2.6921e-03,  9.7754e-02,  1.0858e-01, -5.1985e-02,  1.3108e-02,
        -1.3919e-03,  1.2528e-02, -5.2010e-02,  6.9566e-02, -5.9744e-02,
         1.0129e-01, -5.8961e-02,  8.9144e-02,  2.9727e-02,  8.5993e-02,
         4.0461e-02, -3.0895e-02,  3.1459e-02,  1.6425e-02,  8.2086e-02,
         1.1501e-01,  8.8465e-02, -5.4742e-02,  1.0396e-01,  1.1136e-01,
         2.0216e-02,  5.5059e-02,  9.8244e-02,  3.5840e-02, -7.4393e-03,
         9.6026e-02,  2.5335e-02,  6.9837e-02, -5.0779e-02,  9.5590e-02,
         1.1380e-01, -4.3145e-02,  1.0102e-01,  1.2376e-01, -5.3641e-02,
        -3.6049e-02,  5.3428e-02,  9.4882e-02, -3.9170e-02,  7.1909e-02,
         1.0518e-01,  9.2089e-02,  3.3101e-02,  9.3297e-02,  1.0615e-01,
         1.0948e-01,  1.1167e-01,  8.1100e-02,  8.8105e-02,  1.0210e-01,
         1.0601e-01, -1.2500e-02, -5.6353e-02,  7.3550e-02,  5.9607e-02,
         1.1567e-01,  9.1927e-02,  5.9287e-02, -5.8855e-03,  9.0719e-02,
         1.1738e-01,  7.9537e-02, -3.3171e-02,  1.0010e-01, -5.1157e-02,
         1.0649e-01,  4.1726e-02,  1.0456e-01,  1.0018e-01,  7.9118e-02,
         1.1080e-01,  9.0655e-02,  1.0943e-01,  7.2325e-02,  1.1693e-01,
         9.5899e-02,  7.2289e-02,  7.1210e-02,  6.5031e-02,  9.5589e-02,
         1.0331e-01,  3.8983e-02,  2.9434e-02,  3.4122e-02,  9.1570e-02,
         8.9490e-02,  9.3405e-02,  9.7487e-02,  9.7381e-02,  3.0017e-02,
         9.9561e-02, -4.7337e-02,  8.2345e-02,  5.1322e-02,  2.4786e-02,
        -3.1648e-03,  1.0658e-01,  9.9702e-02,  1.1430e-01, -3.3231e-02,
         9.3968e-02,  1.0822e-01,  5.0423e-02,  8.5322e-02,  1.0804e-01,
         9.8505e-02,  1.2460e-01,  7.0287e-02,  8.9629e-02,  4.2879e-02,
         1.0044e-01,  7.2363e-02,  2.8917e-02,  8.6216e-02,  1.0054e-01,
         1.5418e-02,  9.3849e-02,  5.2327e-02,  2.5562e-02,  9.7615e-02,
         6.4715e-02,  1.0569e-01,  1.1206e-01,  5.7551e-02,  8.0968e-02,
         9.0159e-02,  9.8106e-02,  7.9265e-02,  5.7890e-02,  1.0550e-01,
         8.9869e-02,  8.4130e-02,  1.0280e-01,  7.5924e-02,  8.9089e-02,
         9.0677e-02,  1.0310e-01,  8.4251e-02,  8.4874e-02,  1.5814e-02,
         8.2823e-02,  9.2487e-02, -1.4241e-05,  9.8460e-02,  9.8048e-02,
         1.4602e-02,  8.1287e-02,  6.5743e-02, -1.1025e-03,  1.1519e-01,
         5.1260e-02,  8.6947e-02,  3.0427e-02,  1.0446e-01,  7.2716e-02,
         3.8300e-02,  1.0904e-01,  1.1222e-01,  1.0610e-01,  6.6774e-02,
         9.5773e-02,  9.0710e-02,  5.5223e-02,  8.9722e-02,  2.7464e-02,
         1.0195e-01,  2.8880e-02,  1.1172e-01,  3.5118e-02,  1.0835e-01,
         4.3834e-02,  6.2804e-02,  8.6968e-02,  6.7138e-02,  7.0908e-02,
         9.5362e-02,  3.8456e-02,  8.1805e-02,  7.2915e-02,  1.1537e-01,
         8.8009e-02,  4.9177e-02,  9.5243e-02,  7.1067e-02,  7.7300e-02,
         9.4525e-02,  8.4237e-02,  7.2102e-02,  1.2233e-01,  1.0139e-01,
         8.1555e-02,  3.0501e-02,  4.3116e-02,  2.4837e-02,  7.7343e-02,
         7.8073e-02,  1.9557e-02,  9.6195e-02,  2.1830e-02,  1.0363e-01,
         6.7252e-02,  8.5835e-03,  9.9557e-02,  5.6712e-02,  1.0132e-01,
         6.4990e-02,  3.0494e-02,  6.5011e-02,  1.0088e-01,  1.1402e-01,
         9.7670e-02,  1.0694e-01,  2.1616e-02,  4.8476e-02,  9.7755e-02,
         8.0399e-02,  8.5554e-02,  9.9659e-02,  9.9696e-02,  1.1078e-01,
         1.0721e-01,  8.7937e-02,  9.2356e-02,  9.3395e-02, -4.0825e-03,
         9.6379e-02,  9.2291e-02,  9.3712e-02,  7.4366e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.7999, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:19,624 :: INFO :: Epoch 35: loss tensor(119.3798, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0476,  0.0234,  0.0150,  0.0520,  0.0286, -0.0003,  0.0432,  0.0458,
         0.0136,  0.0228,  0.0239, -0.0134,  0.0341,  0.0425,  0.0342,  0.0146,
         0.0141,  0.0477,  0.0160,  0.0420,  0.0177,  0.0289,  0.0481,  0.0163,
         0.0022,  0.0206,  0.0223,  0.0395,  0.0757,  0.0124,  0.0321,  0.0441,
         0.0502,  0.0380, -0.0093, -0.0305,  0.0319,  0.0333, -0.0028,  0.0405,
        -0.0185,  0.0193,  0.0340,  0.0090,  0.0128,  0.0155,  0.0316,  0.0018,
         0.0279,  0.0193,  0.0118,  0.0144,  0.0006,  0.0175,  0.0309,  0.0623,
         0.0364,  0.0675,  0.0233, -0.0054,  0.0237,  0.0310, -0.0033,  0.0189,
         0.0374,  0.0188,  0.0642,  0.0160,  0.0357, -0.0142,  0.0442,  0.0319,
         0.0345,  0.0523,  0.0502, -0.0026,  0.0557,  0.0117,  0.0432,  0.0520,
         0.0438,  0.0334,  0.0388,  0.0697, -0.0105,  0.0478,  0.0254,  0.0203,
         0.0194, -0.0088,  0.0134,  0.0590,  0.0277,  0.0449,  0.0222,  0.0528,
         0.0255,  0.0074,  0.0013,  0.0068,  0.0251,  0.0321,  0.0296,  0.0141,
         0.0343, -0.0294,  0.0279, -0.0014,  0.0291,  0.0461,  0.0275,  0.0356,
         0.0340,  0.0381, -0.0137, -0.0223,  0.0259,  0.0028,  0.0119,  0.0285,
         0.0079,  0.0195,  0.0200,  0.0085,  0.0242,  0.0009,  0.0122,  0.0548,
         0.1087,  0.1022,  0.1160,  0.0910,  0.0822,  0.1018,  0.0895,  0.0667,
         0.0392,  0.0431,  0.0753,  0.0004,  0.1266,  0.0038,  0.1204,  0.0957,
         0.1233,  0.1159,  0.0752,  0.1258,  0.1231,  0.1027,  0.0134,  0.1150,
         0.1220, -0.0561,  0.0229,  0.0105,  0.0243, -0.0553,  0.0720, -0.0641,
         0.1112, -0.0639,  0.0985,  0.0425,  0.0927,  0.0387, -0.0287,  0.0446,
         0.0292,  0.0941,  0.1303,  0.0955, -0.0586,  0.1196,  0.1267,  0.0352,
         0.0565,  0.1086,  0.0478, -0.0019,  0.1083,  0.0401,  0.0709, -0.0537,
         0.1055,  0.1303, -0.0445,  0.1135,  0.1409, -0.0578, -0.0312,  0.0686,
         0.1033, -0.0387,  0.0732,  0.1194,  0.1117,  0.0307,  0.1070,  0.1241,
         0.1252,  0.1307,  0.0906,  0.0924,  0.1162,  0.1264, -0.0049, -0.0611,
         0.0734,  0.0730,  0.1304,  0.0991,  0.0612,  0.0018,  0.1016,  0.1363,
         0.0942, -0.0309,  0.1114, -0.0543,  0.1252,  0.0535,  0.1179,  0.1177,
         0.0850,  0.1280,  0.1001,  0.1279,  0.0761,  0.1320,  0.1056,  0.0723,
         0.0764,  0.0648,  0.1038,  0.1236,  0.0538,  0.0424,  0.0344,  0.1006,
         0.1021,  0.1107,  0.1169,  0.1093,  0.0445,  0.1146, -0.0489,  0.0971,
         0.0652,  0.0386,  0.0098,  0.1207,  0.1175,  0.1328, -0.0297,  0.1073,
         0.1332,  0.0717,  0.1108,  0.1229,  0.1180,  0.1475,  0.0883,  0.1086,
         0.0652,  0.1230,  0.0948,  0.0441,  0.1016,  0.1151,  0.0287,  0.1136,
         0.0735,  0.0414,  0.1135,  0.0824,  0.1280,  0.1333,  0.0746,  0.0977,
         0.1084,  0.1239,  0.0953,  0.0810,  0.1309,  0.1012,  0.1036,  0.1226,
         0.0869,  0.1093,  0.1108,  0.1223,  0.1107,  0.0989,  0.0272,  0.0982,
         0.1087,  0.0130,  0.1091,  0.1118,  0.0313,  0.1056,  0.0909,  0.0134,
         0.1373,  0.0740,  0.0978,  0.0483,  0.1211,  0.0944,  0.0584,  0.1326,
         0.1351,  0.1285,  0.0878,  0.1100,  0.1073,  0.0790,  0.1073,  0.0461,
         0.1190,  0.0502,  0.1289,  0.0526,  0.1311,  0.0592,  0.0868,  0.1093,
         0.0838,  0.0870,  0.1138,  0.0552,  0.1018,  0.0899,  0.1382,  0.0975,
         0.0596,  0.1134,  0.0941,  0.0865,  0.1115,  0.1014,  0.0907,  0.1441,
         0.1187,  0.0949,  0.0410,  0.0623,  0.0415,  0.0958,  0.0949,  0.0347,
         0.1150,  0.0310,  0.1251,  0.0821,  0.0245,  0.1200,  0.0690,  0.1201,
         0.0871,  0.0517,  0.0889,  0.1269,  0.1351,  0.1135,  0.1262,  0.0394,
         0.0626,  0.1174,  0.0998,  0.1104,  0.1152,  0.1133,  0.1358,  0.1320,
         0.1022,  0.1007,  0.1073,  0.0085,  0.1114,  0.1124,  0.1151,  0.0921],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.0509, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:22,218 :: INFO :: Epoch 40: loss tensor(119.4157, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 5.2258e-02,  2.5886e-02,  2.2224e-02,  5.7203e-02,  3.2748e-02,
         6.9685e-03,  4.9967e-02,  4.9221e-02,  1.9263e-02,  2.5301e-02,
         3.3163e-02, -1.0389e-02,  3.8085e-02,  4.8632e-02,  3.8352e-02,
         1.5544e-02,  1.6866e-02,  5.1985e-02,  1.8198e-02,  4.8314e-02,
         2.1030e-02,  3.1056e-02,  5.3619e-02,  2.4622e-02,  5.2534e-03,
         2.3075e-02,  2.2405e-02,  4.5241e-02,  8.7794e-02,  1.3209e-02,
         3.7086e-02,  4.4925e-02,  5.7398e-02,  4.2469e-02, -4.2985e-03,
        -2.8465e-02,  3.3516e-02,  3.7921e-02,  6.3776e-04,  4.5186e-02,
        -1.3957e-02,  2.3986e-02,  3.8371e-02,  1.1372e-02,  1.4742e-02,
         1.7864e-02,  3.3741e-02,  1.0897e-02,  3.3932e-02,  2.0826e-02,
         1.5713e-02,  1.5578e-02,  2.9769e-03,  1.7135e-02,  4.1249e-02,
         6.6695e-02,  4.1758e-02,  7.7936e-02,  3.0112e-02,  1.8292e-03,
         2.3741e-02,  3.3124e-02,  1.8093e-03,  2.2106e-02,  4.1212e-02,
         1.8589e-02,  6.9679e-02,  2.0009e-02,  3.7360e-02, -1.1407e-02,
         4.6212e-02,  3.8531e-02,  3.3422e-02,  6.3201e-02,  5.3482e-02,
         7.3102e-05,  6.1031e-02,  1.7640e-02,  4.4384e-02,  5.7272e-02,
         4.9090e-02,  4.5690e-02,  3.9892e-02,  7.9456e-02, -6.9695e-03,
         5.2154e-02,  2.6807e-02,  2.5574e-02,  2.2370e-02, -6.6883e-03,
         1.7013e-02,  6.4714e-02,  2.8232e-02,  4.7089e-02,  2.9936e-02,
         5.7640e-02,  2.5992e-02,  1.1580e-02,  5.2646e-03,  1.0459e-02,
         3.0951e-02,  3.1649e-02,  3.1302e-02,  2.1125e-02,  4.3271e-02,
        -2.5856e-02,  3.0164e-02, -6.2550e-04,  3.2791e-02,  4.9045e-02,
         3.5592e-02,  3.8221e-02,  4.1610e-02,  4.2650e-02, -1.0123e-02,
        -2.1203e-02,  2.9157e-02,  1.1118e-02,  1.1856e-02,  3.4088e-02,
         1.1024e-02,  2.1998e-02,  2.3775e-02,  1.1360e-02,  2.6781e-02,
         3.4698e-03,  1.7821e-02,  5.9766e-02,  1.1877e-01,  1.1116e-01,
         1.2968e-01,  9.1428e-02,  8.2231e-02,  1.0978e-01,  9.7195e-02,
         7.9287e-02,  5.4647e-02,  4.3893e-02,  8.3553e-02,  8.7152e-03,
         1.3838e-01,  1.2438e-02,  1.3314e-01,  1.1907e-01,  1.3789e-01,
         1.2529e-01,  7.5510e-02,  1.4537e-01,  1.3671e-01,  1.1629e-01,
         2.3774e-02,  1.2903e-01,  1.3231e-01, -5.9502e-02,  3.1189e-02,
         2.2430e-02,  3.5566e-02, -5.7786e-02,  7.2434e-02, -6.7579e-02,
         1.1814e-01, -6.7908e-02,  1.0536e-01,  5.3080e-02,  9.7059e-02,
         3.5503e-02, -2.5711e-02,  5.5950e-02,  4.0789e-02,  1.0326e-01,
         1.4340e-01,  9.9243e-02, -6.1744e-02,  1.3307e-01,  1.3969e-01,
         4.9475e-02,  5.6594e-02,  1.1592e-01,  5.7334e-02,  3.5654e-03,
         1.1794e-01,  5.3866e-02,  6.9609e-02, -5.5703e-02,  1.1221e-01,
         1.4468e-01, -4.5016e-02,  1.2335e-01,  1.5600e-01, -6.1240e-02,
        -2.4651e-02,  8.1024e-02,  1.0893e-01, -3.7628e-02,  7.1810e-02,
         1.3144e-01,  1.2825e-01,  2.7716e-02,  1.1809e-01,  1.3994e-01,
         1.3891e-01,  1.4742e-01,  9.7444e-02,  9.4077e-02,  1.2700e-01,
         1.4466e-01,  2.7511e-03, -6.5072e-02,  7.0484e-02,  8.3913e-02,
         1.4255e-01,  1.0309e-01,  6.2003e-02,  9.6015e-03,  1.0880e-01,
         1.5360e-01,  1.0604e-01, -2.7785e-02,  1.1980e-01, -5.6758e-02,
         1.4105e-01,  6.2214e-02,  1.2829e-01,  1.3315e-01,  8.8248e-02,
         1.4282e-01,  1.0667e-01,  1.4446e-01,  7.6963e-02,  1.4470e-01,
         1.1169e-01,  6.8993e-02,  8.0989e-02,  6.2216e-02,  1.0892e-01,
         1.4159e-01,  6.6398e-02,  5.4316e-02,  3.3420e-02,  1.0652e-01,
         1.1149e-01,  1.2560e-01,  1.3439e-01,  1.1946e-01,  5.7905e-02,
         1.2752e-01, -4.9687e-02,  1.0839e-01,  7.7159e-02,  5.1484e-02,
         2.2645e-02,  1.3298e-01,  1.3314e-01,  1.4933e-01, -2.5166e-02,
         1.1747e-01,  1.5721e-01,  9.2773e-02,  1.3549e-01,  1.3489e-01,
         1.3527e-01,  1.6946e-01,  1.0369e-01,  1.2500e-01,  8.7634e-02,
         1.4457e-01,  1.1699e-01,  5.9392e-02,  1.1462e-01,  1.2676e-01,
         4.2148e-02,  1.3139e-01,  9.4120e-02,  5.7682e-02,  1.2636e-01,
         9.7455e-02,  1.4885e-01,  1.5268e-01,  9.1014e-02,  1.1111e-01,
         1.2451e-01,  1.4887e-01,  1.0854e-01,  1.0411e-01,  1.5528e-01,
         1.0852e-01,  1.2038e-01,  1.4029e-01,  9.5040e-02,  1.2811e-01,
         1.3031e-01,  1.3949e-01,  1.3594e-01,  1.1084e-01,  3.8117e-02,
         1.1149e-01,  1.2213e-01,  2.6947e-02,  1.1616e-01,  1.2237e-01,
         4.8958e-02,  1.2880e-01,  1.1600e-01,  2.9396e-02,  1.5829e-01,
         9.6582e-02,  1.0491e-01,  6.6284e-02,  1.3513e-01,  1.1507e-01,
         7.8476e-02,  1.5513e-01,  1.5714e-01,  1.4949e-01,  1.0817e-01,
         1.2117e-01,  1.2204e-01,  1.0250e-01,  1.2193e-01,  6.5230e-02,
         1.3324e-01,  7.1787e-02,  1.4343e-01,  6.9852e-02,  1.5285e-01,
         7.3404e-02,  1.1022e-01,  1.3012e-01,  9.8077e-02,  1.0019e-01,
         1.2963e-01,  7.0787e-02,  1.1935e-01,  1.0390e-01,  1.5966e-01,
         1.0344e-01,  6.7218e-02,  1.2904e-01,  1.1618e-01,  9.1276e-02,
         1.2544e-01,  1.1551e-01,  1.0707e-01,  1.6433e-01,  1.3382e-01,
         1.0728e-01,  5.0479e-02,  8.1299e-02,  5.8011e-02,  1.1226e-01,
         1.0937e-01,  4.9990e-02,  1.3125e-01,  3.9608e-02,  1.4480e-01,
         9.3685e-02,  4.1762e-02,  1.3943e-01,  7.9659e-02,  1.3681e-01,
         1.0875e-01,  7.3475e-02,  1.1184e-01,  1.5201e-01,  1.5519e-01,
         1.2706e-01,  1.4358e-01,  5.7753e-02,  7.5511e-02,  1.3471e-01,
         1.1698e-01,  1.3449e-01,  1.2665e-01,  1.2422e-01,  1.5988e-01,
         1.5583e-01,  1.1372e-01,  1.0504e-01,  1.1744e-01,  2.2693e-02,
         1.2341e-01,  1.3022e-01,  1.3455e-01,  1.0826e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(13.1994, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:22,249 :: INFO :: mask tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], device='cuda:0')
2023-04-11 14:41:44,405 :: INFO :: Epoch 5: loss tensor(20.5272, device='cuda:0', grad_fn=<AddBackward0>), U.norm 12.451095581054688, V.norm 16.635616302490234, MLP.norm 6.552561283111572
2023-04-11 14:42:16,264 :: INFO :: ----- val -----
2023-04-11 14:42:16,264 :: INFO :: Precision [0.009426864350520026, 0.005744633768532913, 0.0048329276388580835]
2023-04-11 14:42:16,264 :: INFO :: Recall [0.0027266490566328658, 0.009389824011991555, 0.016412702456379897]
2023-04-11 14:42:16,264 :: INFO :: ndcg [0.009426864350520026, 0.01804877382597124, 0.023854171709903293]
2023-04-11 14:42:36,843 :: INFO :: ----- test -----
2023-04-11 14:42:36,843 :: INFO :: Precision [0.005595283585348545, 0.0034281347235111423, 0.0031142529614061353]
2023-04-11 14:42:36,843 :: INFO :: Recall [0.002122106873555476, 0.00789561392346932, 0.015154842209854408]
2023-04-11 14:42:36,843 :: INFO :: ndcg [0.005595283585348545, 0.010882521903631525, 0.015257342939821214]
2023-04-11 14:42:41,623 :: INFO :: Epoch 10: loss tensor(30.1840, device='cuda:0', grad_fn=<AddBackward0>), U.norm 20.899818420410156, V.norm 20.37004280090332, MLP.norm 12.201753616333008
2023-04-11 14:43:03,280 :: INFO :: ----- val -----
2023-04-11 14:43:03,280 :: INFO :: Precision [0.010223500774507635, 0.004275282142066866, 0.005067492808143308]
2023-04-11 14:43:03,280 :: INFO :: Recall [0.0027537201021814502, 0.007056563676442436, 0.017333767603987523]
2023-04-11 14:43:03,280 :: INFO :: ndcg [0.010223500774507635, 0.015546402968639558, 0.02448778829895055]
2023-04-11 14:43:24,717 :: INFO :: ----- test -----
2023-04-11 14:43:24,717 :: INFO :: Precision [0.0059228123805884604, 0.0027949123860472984, 0.0034581581964080277]
2023-04-11 14:43:24,717 :: INFO :: Recall [0.002352384311240523, 0.0070003287672525455, 0.0174043846505559]
2023-04-11 14:43:24,717 :: INFO :: ndcg [0.0059228123805884604, 0.009755928400204832, 0.0161831181696122]
2023-04-11 14:43:30,577 :: INFO :: Epoch 15: loss tensor(36.5848, device='cuda:0', grad_fn=<AddBackward0>), U.norm 28.136281967163086, V.norm 29.10418128967285, MLP.norm 14.205952644348145
2023-04-11 14:43:52,639 :: INFO :: ----- val -----
2023-04-11 14:43:52,639 :: INFO :: Precision [0.010931622040274397, 0.0051604337242753305, 0.004930294312901005]
2023-04-11 14:43:52,639 :: INFO :: Recall [0.0029201163058646647, 0.008552236004498037, 0.016435412916368432]
2023-04-11 14:43:52,639 :: INFO :: ndcg [0.010931622040274397, 0.017445488921638636, 0.024438368773926144]
2023-04-11 14:44:13,811 :: INFO :: ----- test -----
2023-04-11 14:44:13,811 :: INFO :: Precision [0.005977400513128446, 0.003340793711447164, 0.0032288880397400997]
2023-04-11 14:44:13,811 :: INFO :: Recall [0.002153181936018715, 0.007950462889912462, 0.015818125346110695]
2023-04-11 14:44:13,811 :: INFO :: ndcg [0.005977400513128446, 0.01068450297328572, 0.015454248789522217]
2023-04-11 14:44:20,202 :: INFO :: Epoch 20: loss tensor(37.0261, device='cuda:0', grad_fn=<AddBackward0>), U.norm 34.83544158935547, V.norm 46.73408889770508, MLP.norm 14.36330509185791
2023-04-11 14:44:42,155 :: INFO :: ----- val -----
2023-04-11 14:44:42,155 :: INFO :: Precision [0.011108652356716087, 0.006319982296968375, 0.006373091391900708]
2023-04-11 14:44:42,155 :: INFO :: Recall [0.002989084366645073, 0.010154574750012243, 0.021926026327784873]
2023-04-11 14:44:42,155 :: INFO :: ndcg [0.011108652356716087, 0.020342184648813565, 0.02957306666955446]
2023-04-11 14:45:03,077 :: INFO :: ----- test -----
2023-04-11 14:45:03,077 :: INFO :: Precision [0.0066324581036082755, 0.004143239259784923, 0.004181450952562797]
2023-04-11 14:45:03,077 :: INFO :: Recall [0.0025716909521586047, 0.009592924184797527, 0.02006861355519425]
2023-04-11 14:45:03,077 :: INFO :: ndcg [0.0066324581036082755, 0.012895974663466406, 0.01930151028078666]
2023-04-11 14:45:08,608 :: INFO :: Epoch 25: loss tensor(35.2542, device='cuda:0', grad_fn=<AddBackward0>), U.norm 42.134132385253906, V.norm 74.08811950683594, MLP.norm 14.143243789672852
2023-04-11 14:45:30,374 :: INFO :: ----- val -----
2023-04-11 14:45:30,374 :: INFO :: Precision [0.011241425094047356, 0.009524231024562804, 0.007731799070590608]
2023-04-11 14:45:30,374 :: INFO :: Recall [0.0030572157702780435, 0.01539080025936558, 0.02647992217661038]
2023-04-11 14:45:30,374 :: INFO :: ndcg [0.011241425094047356, 0.027274186974225012, 0.036015232338668074]
2023-04-11 14:45:50,733 :: INFO :: ----- test -----
2023-04-11 14:45:50,733 :: INFO :: Precision [0.007014575031388176, 0.005971941699874351, 0.0050739669195915185]
2023-04-11 14:45:50,733 :: INFO :: Recall [0.002697840067196068, 0.01373663567269854, 0.024409650503749804]
2023-04-11 14:45:50,733 :: INFO :: ndcg [0.007014575031388176, 0.017174939468364365, 0.023591769240013317]
2023-04-11 14:45:56,249 :: INFO :: Epoch 30: loss tensor(33.7505, device='cuda:0', grad_fn=<AddBackward0>), U.norm 50.409847259521484, V.norm 108.38743591308594, MLP.norm 13.657752990722656
2023-04-11 14:46:17,936 :: INFO :: ----- val -----
2023-04-11 14:46:17,936 :: INFO :: Precision [0.012126576676255808, 0.011666297853507158, 0.008816109758795907]
2023-04-11 14:46:17,936 :: INFO :: Recall [0.0031335180825596983, 0.02036949815161827, 0.03010272261793727]
2023-04-11 14:46:17,936 :: INFO :: ndcg [0.012126576676255808, 0.03303117155720429, 0.04150471530424993]
2023-04-11 14:46:39,061 :: INFO :: ----- test -----
2023-04-11 14:46:39,061 :: INFO :: Precision [0.00742398602543807, 0.007533162290517861, 0.005709918663682319]
2023-04-11 14:46:39,061 :: INFO :: Recall [0.0029850558832732964, 0.017797657611506083, 0.026970783842767434]
2023-04-11 14:46:39,061 :: INFO :: ndcg [0.00742398602543807, 0.021129973751593492, 0.02698387661686974]
2023-04-11 14:46:44,967 :: INFO :: Epoch 35: loss tensor(34.9929, device='cuda:0', grad_fn=<AddBackward0>), U.norm 59.454586029052734, V.norm 145.64039611816406, MLP.norm 13.176545143127441
2023-04-11 14:47:07,124 :: INFO :: ----- val -----
2023-04-11 14:47:07,124 :: INFO :: Precision [0.014427970789997788, 0.01321531312237186, 0.01011285682673122]
2023-04-11 14:47:07,124 :: INFO :: Recall [0.003904226040867172, 0.023522660871181406, 0.03460105174265675]
2023-04-11 14:47:07,124 :: INFO :: ndcg [0.014427970789997788, 0.03874598771091059, 0.04828248092062977]
2023-04-11 14:47:28,373 :: INFO :: ----- test -----
2023-04-11 14:47:28,373 :: INFO :: Precision [0.00892515967028768, 0.008870571537747438, 0.006722528522299005]
2023-04-11 14:47:28,373 :: INFO :: Recall [0.003528396951622332, 0.021545631162565238, 0.031793399897824655]
2023-04-11 14:47:28,373 :: INFO :: ndcg [0.00892515967028768, 0.025826465594379663, 0.03224597293242081]
2023-04-11 14:47:34,140 :: INFO :: Epoch 40: loss tensor(35.0245, device='cuda:0', grad_fn=<AddBackward0>), U.norm 68.70018768310547, V.norm 182.2181854248047, MLP.norm 12.828021049499512
2023-04-11 14:47:56,483 :: INFO :: ----- val -----
2023-04-11 14:47:56,483 :: INFO :: Precision [0.01531312237220624, 0.014188979862801111, 0.011391900863022391]
2023-04-11 14:47:56,483 :: INFO :: Recall [0.00459965194554772, 0.025610548991999812, 0.03807728053121988]
2023-04-11 14:47:56,483 :: INFO :: ndcg [0.01531312237220624, 0.04123892938427431, 0.052832160832616014]
2023-04-11 14:48:17,686 :: INFO :: ----- test -----
2023-04-11 14:48:17,686 :: INFO :: Precision [0.010289862983787324, 0.00988045198973712, 0.007486762377858885]
2023-04-11 14:48:17,686 :: INFO :: Recall [0.004499376705098081, 0.024145673200859294, 0.0345864687256831]
2023-04-11 14:48:17,686 :: INFO :: ndcg [0.010289862983787324, 0.029024808607862784, 0.036027314327961246]
2023-04-11 14:48:23,890 :: INFO :: Epoch 45: loss tensor(37.9771, device='cuda:0', grad_fn=<AddBackward0>), U.norm 77.82238006591797, V.norm 216.4127960205078, MLP.norm 12.258986473083496
2023-04-11 14:48:46,312 :: INFO :: ----- val -----
2023-04-11 14:48:46,312 :: INFO :: Precision [0.01597698605886258, 0.015074131445009513, 0.012259349413586825]
2023-04-11 14:48:46,312 :: INFO :: Recall [0.005234345136861629, 0.027218380564027433, 0.0411436155546918]
2023-04-11 14:48:46,312 :: INFO :: ndcg [0.01597698605886258, 0.04362761722219858, 0.056052513193374495]
2023-04-11 14:49:06,233 :: INFO :: ----- test -----
2023-04-11 14:49:06,233 :: INFO :: Precision [0.01119056717069709, 0.010568262459740905, 0.008040831923139858]
2023-04-11 14:49:06,233 :: INFO :: Recall [0.005083288006401595, 0.025941714506890583, 0.037349580694985195]
2023-04-11 14:49:06,233 :: INFO :: ndcg [0.01119056717069709, 0.030955727295022314, 0.038683666853379074]
2023-04-11 14:49:11,468 :: INFO :: Epoch 50: loss tensor(39.2168, device='cuda:0', grad_fn=<AddBackward0>), U.norm 86.4638671875, V.norm 246.8034210205078, MLP.norm 11.89857006072998
2023-04-11 14:49:31,952 :: INFO :: ----- val -----
2023-04-11 14:49:31,952 :: INFO :: Precision [0.017348971011285682, 0.015950431511395836, 0.013020579774286233]
2023-04-11 14:49:31,952 :: INFO :: Recall [0.006097745681961666, 0.029272314744996128, 0.044361457244435444]
2023-04-11 14:49:31,952 :: INFO :: ndcg [0.017348971011285682, 0.045952319470624316, 0.05916319507247214]
2023-04-11 14:49:52,593 :: INFO :: ----- test -----
2023-04-11 14:49:52,609 :: INFO :: Precision [0.012637152683006714, 0.011212402423712698, 0.008633113161198828]
2023-04-11 14:49:52,609 :: INFO :: Recall [0.005868611685678997, 0.02819480567053259, 0.04116016609432302]
2023-04-11 14:49:52,609 :: INFO :: ndcg [0.012637152683006714, 0.03316566984698877, 0.04165139962386801]
2023-04-11 14:49:57,859 :: INFO :: Epoch 55: loss tensor(40.3019, device='cuda:0', grad_fn=<AddBackward0>), U.norm 94.45633697509766, V.norm 272.6502380371094, MLP.norm 11.594002723693848
2023-04-11 14:50:18,155 :: INFO :: ----- val -----
2023-04-11 14:50:18,155 :: INFO :: Precision [0.01712768311573357, 0.016100907280371262, 0.01341447222836908]
2023-04-11 14:50:18,155 :: INFO :: Recall [0.00614402484806916, 0.029692553805563682, 0.046068248524515104]
2023-04-11 14:50:18,155 :: INFO :: ndcg [0.01712768311573357, 0.046218678942762846, 0.06047135124809206]
2023-04-11 14:50:38,233 :: INFO :: ----- test -----
2023-04-11 14:50:38,233 :: INFO :: Precision [0.012882799279436651, 0.011452590206888623, 0.00886784213112081]
2023-04-11 14:50:38,233 :: INFO :: Recall [0.006111414414321132, 0.02878901239626683, 0.042256728606771154]
2023-04-11 14:50:38,233 :: INFO :: ndcg [0.012882799279436651, 0.03370611254654156, 0.042557091692899084]
2023-04-11 14:50:43,484 :: INFO :: Epoch 60: loss tensor(41.9557, device='cuda:0', grad_fn=<AddBackward0>), U.norm 101.7737808227539, V.norm 294.065673828125, MLP.norm 11.308511734008789
2023-04-11 14:51:03,421 :: INFO :: ----- val -----
2023-04-11 14:51:03,421 :: INFO :: Precision [0.018366895330825403, 0.016269086080990862, 0.01380836468245189]
2023-04-11 14:51:03,421 :: INFO :: Recall [0.0067645048469838695, 0.030125487671012148, 0.048195524997415244]
2023-04-11 14:51:03,421 :: INFO :: ndcg [0.018366895330825403, 0.04731801871175586, 0.06229340115672276]
2023-04-11 14:51:23,577 :: INFO :: ----- test -----
2023-04-11 14:51:23,577 :: INFO :: Precision [0.013865385665156394, 0.011769201375620523, 0.009138053387193798]
2023-04-11 14:51:23,577 :: INFO :: Recall [0.00659959041127838, 0.02992150520708868, 0.04388309715342678]
2023-04-11 14:51:23,577 :: INFO :: ndcg [0.013865385665156394, 0.03494429803331103, 0.04405595561880293]
2023-04-11 14:51:28,468 :: INFO :: Epoch 65: loss tensor(44.2964, device='cuda:0', grad_fn=<AddBackward0>), U.norm 108.51793670654297, V.norm 311.9453125, MLP.norm 11.079216003417969
2023-04-11 14:51:48,906 :: INFO :: ----- val -----
2023-04-11 14:51:48,906 :: INFO :: Precision [0.019207789333923433, 0.01637530427085587, 0.013905731356494823]
2023-04-11 14:51:48,906 :: INFO :: Recall [0.007344992460358769, 0.030680667701796848, 0.04964430932229283]
2023-04-11 14:51:48,906 :: INFO :: ndcg [0.019207789333923433, 0.04814999790744107, 0.06323675438050981]
2023-04-11 14:52:09,109 :: INFO :: ----- test -----
2023-04-11 14:52:09,109 :: INFO :: Precision [0.014793383918336154, 0.012063977291336427, 0.009468311589060781]
2023-04-11 14:52:09,109 :: INFO :: Recall [0.007494774107752659, 0.0312797347035379, 0.046569535231859804]
2023-04-11 14:52:09,109 :: INFO :: ndcg [0.014793383918336154, 0.036068942541621664, 0.04571882054859854]
2023-04-11 14:52:14,202 :: INFO :: Epoch 70: loss tensor(43.1571, device='cuda:0', grad_fn=<AddBackward0>), U.norm 114.66687774658203, V.norm 326.2845153808594, MLP.norm 10.879875183105469
2023-04-11 14:52:34,687 :: INFO :: ----- val -----
2023-04-11 14:52:34,687 :: INFO :: Precision [0.01880947112192963, 0.016516928524009215, 0.014007523788448824]
2023-04-11 14:52:34,687 :: INFO :: Recall [0.00726323982905101, 0.031004268501112858, 0.05077432149753089]
2023-04-11 14:52:34,687 :: INFO :: ndcg [0.01880947112192963, 0.04829600615307544, 0.06371712468917368]
2023-04-11 14:52:55,468 :: INFO :: ----- test -----
2023-04-11 14:52:55,468 :: INFO :: Precision [0.015311971177466018, 0.012184071182924389, 0.009716687592117766]
2023-04-11 14:52:55,468 :: INFO :: Recall [0.007784778079605754, 0.031844744533056926, 0.04865125843793864]
2023-04-11 14:52:55,468 :: INFO :: ndcg [0.015311971177466018, 0.036834187174716405, 0.047085929043108106]
2023-04-11 14:53:01,515 :: INFO :: Epoch 75: loss tensor(43.0374, device='cuda:0', grad_fn=<AddBackward0>), U.norm 120.33415985107422, V.norm 338.60260009765625, MLP.norm 10.635605812072754
2023-04-11 14:53:21,905 :: INFO :: ----- val -----
2023-04-11 14:53:21,905 :: INFO :: Precision [0.01996016817880062, 0.016463819429076713, 0.014087187430847598]
2023-04-11 14:53:21,905 :: INFO :: Recall [0.007724121004435652, 0.031136529788346023, 0.05194604344313953]
2023-04-11 14:53:21,905 :: INFO :: ndcg [0.01996016817880062, 0.04852327592285259, 0.06426816477138006]
2023-04-11 14:53:42,421 :: INFO :: ----- test -----
2023-04-11 14:53:42,421 :: INFO :: Precision [0.01561220590643594, 0.012347835580544338, 0.009861346143348748]
2023-04-11 14:53:42,421 :: INFO :: Recall [0.008338540824383615, 0.032501431793949966, 0.049747270189299926]
2023-04-11 14:53:42,421 :: INFO :: ndcg [0.01561220590643594, 0.03738487618456219, 0.047806608224545785]
2023-04-11 14:53:47,843 :: INFO :: Epoch 80: loss tensor(46.9132, device='cuda:0', grad_fn=<AddBackward0>), U.norm 125.48605346679688, V.norm 348.5749206542969, MLP.norm 10.392645835876465
2023-04-11 14:54:08,327 :: INFO :: ----- val -----
2023-04-11 14:54:08,327 :: INFO :: Precision [0.019517592387696393, 0.016401858818322124, 0.0139322859039611]
2023-04-11 14:54:08,327 :: INFO :: Recall [0.007833635976198915, 0.03153825484571379, 0.05154096572383927]
2023-04-11 14:54:08,327 :: INFO :: ndcg [0.019517592387696393, 0.04824568165456126, 0.06373075668514083]
2023-04-11 14:54:28,843 :: INFO :: ----- test -----
2023-04-11 14:54:28,843 :: INFO :: Precision [0.015694088105245918, 0.012495223538402286, 0.009940498935531747]
2023-04-11 14:54:28,843 :: INFO :: Recall [0.008653805943641426, 0.03325126472817624, 0.050740250600422795]
2023-04-11 14:54:28,843 :: INFO :: ndcg [0.015694088105245918, 0.03800610480479287, 0.048356259343949846]
2023-04-11 14:54:33,874 :: INFO :: Epoch 85: loss tensor(48.1768, device='cuda:0', grad_fn=<AddBackward0>), U.norm 130.2052764892578, V.norm 356.3014831542969, MLP.norm 10.340903282165527
2023-04-11 14:54:54,249 :: INFO :: ----- val -----
2023-04-11 14:54:54,249 :: INFO :: Precision [0.020048683337021464, 0.016481522460720887, 0.014060632883381347]
2023-04-11 14:54:54,249 :: INFO :: Recall [0.007822563183338115, 0.03159642462627701, 0.052454661941832674]
2023-04-11 14:54:54,249 :: INFO :: ndcg [0.020048683337021464, 0.04876497915145394, 0.06460147502406687]
2023-04-11 14:55:14,859 :: INFO :: ----- test -----
2023-04-11 14:55:14,859 :: INFO :: Precision [0.015475735575085976, 0.012511599978164282, 0.009948687155412745]
2023-04-11 14:55:14,859 :: INFO :: Recall [0.008435028839117282, 0.03354393432150361, 0.0514004319171631]
2023-04-11 14:55:14,859 :: INFO :: ndcg [0.015475735575085976, 0.03808248010314654, 0.04857223077365428]
2023-04-11 14:55:20,952 :: INFO :: Epoch 90: loss tensor(47.9197, device='cuda:0', grad_fn=<AddBackward0>), U.norm 134.5854034423828, V.norm 362.6773986816406, MLP.norm 10.129890441894531
2023-04-11 14:55:41,030 :: INFO :: ----- val -----
2023-04-11 14:55:41,030 :: INFO :: Precision [0.020181456074352733, 0.016481522460720884, 0.014144722283691156]
2023-04-11 14:55:41,030 :: INFO :: Recall [0.007891974524800794, 0.03176613050450694, 0.05245872095027936]
2023-04-11 14:55:41,030 :: INFO :: ndcg [0.020181456074352733, 0.04907403755657224, 0.06486104552050911]
2023-04-11 14:56:01,093 :: INFO :: ----- test -----
2023-04-11 14:56:01,093 :: INFO :: Precision [0.015694088105245918, 0.012533435231180275, 0.010016922321087746]
2023-04-11 14:56:01,093 :: INFO :: Recall [0.008635606291370697, 0.033786416605855005, 0.051885513111580926]
2023-04-11 14:56:01,093 :: INFO :: ndcg [0.015694088105245918, 0.0382649268897233, 0.04894499458215072]
2023-04-11 14:56:06,171 :: INFO :: Epoch 95: loss tensor(44.8375, device='cuda:0', grad_fn=<AddBackward0>), U.norm 138.6709442138672, V.norm 368.07611083984375, MLP.norm 9.985416412353516
2023-04-11 14:56:26,437 :: INFO :: ----- val -----
2023-04-11 14:56:26,437 :: INFO :: Precision [0.01965036512502766, 0.01649037397654297, 0.014109316220402818]
2023-04-11 14:56:26,437 :: INFO :: Recall [0.0077563481414647415, 0.03210398366786978, 0.052873320847548186]
2023-04-11 14:56:26,437 :: INFO :: ndcg [0.01965036512502766, 0.04896165525898405, 0.06473119383544071]
2023-04-11 14:56:46,499 :: INFO :: ----- test -----
2023-04-11 14:56:46,499 :: INFO :: Precision [0.015803264370325892, 0.012467929472132296, 0.01006605164037374]
2023-04-11 14:56:46,499 :: INFO :: Recall [0.008975675458373005, 0.033781962006103144, 0.052364317430254756]
2023-04-11 14:56:46,499 :: INFO :: ndcg [0.015803264370325892, 0.03831460665467213, 0.049204416921215625]
2023-04-11 14:56:51,234 :: INFO :: Epoch 100: loss tensor(50.0672, device='cuda:0', grad_fn=<AddBackward0>), U.norm 142.61447143554688, V.norm 372.25592041015625, MLP.norm 10.004242897033691
2023-04-11 14:57:11,405 :: INFO :: ----- val -----
2023-04-11 14:57:11,405 :: INFO :: Precision [0.020535516707236114, 0.016658552777162564, 0.014078335915025508]
2023-04-11 14:57:11,405 :: INFO :: Recall [0.008129902862419333, 0.03223376562488129, 0.05259627367043654]
2023-04-11 14:57:11,405 :: INFO :: ndcg [0.020535516707236114, 0.04960101092920426, 0.06496775812234615]
2023-04-11 14:57:31,343 :: INFO :: ----- test -----
2023-04-11 14:57:31,343 :: INFO :: Precision [0.016212675364375786, 0.01255527048419627, 0.010090616300016737]
2023-04-11 14:57:31,343 :: INFO :: Recall [0.009158287857367943, 0.03418187723757809, 0.05346251291391485]
2023-04-11 14:57:31,343 :: INFO :: ndcg [0.016212675364375786, 0.0386695984450583, 0.04954369213906383]
2023-04-11 14:57:36,468 :: INFO :: Epoch 105: loss tensor(47.7438, device='cuda:0', grad_fn=<AddBackward0>), U.norm 146.2319793701172, V.norm 375.4200134277344, MLP.norm 10.00153636932373
2023-04-11 14:57:56,358 :: INFO :: ----- val -----
2023-04-11 14:57:56,358 :: INFO :: Precision [0.01996016817880062, 0.016570037618941727, 0.013865899535295444]
2023-04-11 14:57:56,358 :: INFO :: Recall [0.007989934967217976, 0.03220798961410077, 0.05249626955590878]
2023-04-11 14:57:56,358 :: INFO :: ndcg [0.01996016817880062, 0.049130776283937555, 0.06416077285638232]
2023-04-11 14:58:16,344 :: INFO :: ----- test -----
2023-04-11 14:58:16,344 :: INFO :: Precision [0.015803264370325892, 0.012478847098640295, 0.010085157486762739]
2023-04-11 14:58:16,344 :: INFO :: Recall [0.00906646390723873, 0.03381348271928959, 0.0535570048837557]
2023-04-11 14:58:16,344 :: INFO :: ndcg [0.015803264370325892, 0.0383244626119424, 0.04940440481595701]
2023-04-11 14:58:21,390 :: INFO :: Epoch 110: loss tensor(49.6100, device='cuda:0', grad_fn=<AddBackward0>), U.norm 149.69161987304688, V.norm 377.9937438964844, MLP.norm 9.862857818603516
2023-04-11 14:58:40,937 :: INFO :: ----- val -----
2023-04-11 14:58:40,937 :: INFO :: Precision [0.02026997123257358, 0.016587740650585887, 0.01391015711440588]
2023-04-11 14:58:40,937 :: INFO :: Recall [0.007842068278081094, 0.0324259512532078, 0.05264912998175385]
2023-04-11 14:58:40,937 :: INFO :: ndcg [0.02026997123257358, 0.049416343432677055, 0.06455607590421525]
2023-04-11 14:59:00,687 :: INFO :: ----- test -----
2023-04-11 14:59:00,687 :: INFO :: Precision [0.01607620503302582, 0.012664446749276236, 0.010167039685572738]
2023-04-11 14:59:00,687 :: INFO :: Recall [0.009388380286254296, 0.03487252145040133, 0.054339914335517284]
2023-04-11 14:59:00,687 :: INFO :: ndcg [0.01607620503302582, 0.03898051237417681, 0.05002294877884972]
2023-04-11 14:59:05,749 :: INFO :: Epoch 115: loss tensor(47.1706, device='cuda:0', grad_fn=<AddBackward0>), U.norm 153.02284240722656, V.norm 380.3173522949219, MLP.norm 9.578078269958496
2023-04-11 14:59:25,187 :: INFO :: ----- val -----
2023-04-11 14:59:25,187 :: INFO :: Precision [0.019827395441469352, 0.016561186103119633, 0.013848196503651282]
2023-04-11 14:59:25,187 :: INFO :: Recall [0.00796817034521475, 0.032415118364525826, 0.0529490307084878]
2023-04-11 14:59:25,187 :: INFO :: ndcg [0.019827395441469352, 0.04908999358620891, 0.06419993241748494]
2023-04-11 14:59:45,484 :: INFO :: ----- test -----
2023-04-11 14:59:45,484 :: INFO :: Precision [0.015830558436595882, 0.012582564550466264, 0.010210710191604739]
2023-04-11 14:59:45,484 :: INFO :: Recall [0.009395545268834765, 0.03457017657130291, 0.05467715509870187]
2023-04-11 14:59:45,484 :: INFO :: ndcg [0.015830558436595882, 0.03855138628644079, 0.04990170760238748]
2023-04-11 14:59:50,405 :: INFO :: Epoch 120: loss tensor(51.4396, device='cuda:0', grad_fn=<AddBackward0>), U.norm 156.2386474609375, V.norm 382.19427490234375, MLP.norm 9.52497386932373
2023-04-11 15:00:10,187 :: INFO :: ----- val -----
2023-04-11 15:00:10,187 :: INFO :: Precision [0.020225713653463154, 0.016623146713874225, 0.013963266209338404]
2023-04-11 15:00:10,187 :: INFO :: Recall [0.007914911235827406, 0.0328466905154467, 0.053378957808843566]
2023-04-11 15:00:10,187 :: INFO :: ndcg [0.020225713653463154, 0.049565030997492994, 0.06474795888002402]
2023-04-11 15:00:30,312 :: INFO :: ----- test -----
2023-04-11 15:00:30,312 :: INFO :: Precision [0.01607620503302582, 0.01259348217697426, 0.010216169004858736]
2023-04-11 15:00:30,312 :: INFO :: Recall [0.009242525202063945, 0.03466121170732128, 0.054774013588998666]
2023-04-11 15:00:30,312 :: INFO :: ndcg [0.01607620503302582, 0.03871913316784146, 0.05004548618599855]
2023-04-11 15:00:35,483 :: INFO :: Epoch 125: loss tensor(50.3174, device='cuda:0', grad_fn=<AddBackward0>), U.norm 159.2787628173828, V.norm 383.67626953125, MLP.norm 9.450550079345703
2023-04-11 15:00:55,327 :: INFO :: ----- val -----
2023-04-11 15:00:55,327 :: INFO :: Precision [0.02009294091613189, 0.016623146713874225, 0.013857048019473373]
2023-04-11 15:00:55,327 :: INFO :: Recall [0.008021900394992908, 0.03279684862474301, 0.05338362534345375]
2023-04-11 15:00:55,327 :: INFO :: ndcg [0.02009294091613189, 0.0494413744072963, 0.06454617080684515]
2023-04-11 15:01:15,342 :: INFO :: ----- test -----
2023-04-11 15:01:15,342 :: INFO :: Precision [0.015912440635405863, 0.012697199628800226, 0.010251651291009727]
2023-04-11 15:01:15,342 :: INFO :: Recall [0.009308519780281238, 0.03511696039270859, 0.055291453793732945]
2023-04-11 15:01:15,342 :: INFO :: ndcg [0.015912440635405863, 0.03902652033052808, 0.050211188424118196]
2023-04-11 15:01:20,483 :: INFO :: Epoch 130: loss tensor(50.9750, device='cuda:0', grad_fn=<AddBackward0>), U.norm 162.25509643554688, V.norm 384.57415771484375, MLP.norm 9.488091468811035
2023-04-11 15:01:40,874 :: INFO :: ----- val -----
2023-04-11 15:01:40,874 :: INFO :: Precision [0.020314228811684, 0.01645496791325463, 0.013830493472007101]
2023-04-11 15:01:40,874 :: INFO :: Recall [0.008043947911831955, 0.03249591685337372, 0.053508346410497384]
2023-04-11 15:01:40,874 :: INFO :: ndcg [0.020314228811684, 0.0491069940551393, 0.06445079792522397]
2023-04-11 15:02:00,733 :: INFO :: ----- test -----
2023-04-11 15:02:00,733 :: INFO :: Precision [0.016021616900485834, 0.012828211146896185, 0.01031442764343072]
2023-04-11 15:02:00,733 :: INFO :: Recall [0.009451888019920673, 0.03553199840706468, 0.055679133252563205]
2023-04-11 15:02:00,733 :: INFO :: ndcg [0.016021616900485834, 0.03926677116822631, 0.05044036130735607]
2023-04-11 15:02:06,077 :: INFO :: Epoch 135: loss tensor(50.4606, device='cuda:0', grad_fn=<AddBackward0>), U.norm 165.07130432128906, V.norm 385.20953369140625, MLP.norm 9.393136024475098
2023-04-11 15:02:26,249 :: INFO :: ----- val -----
2023-04-11 15:02:26,249 :: INFO :: Precision [0.01947333480858597, 0.016401858818322124, 0.0137375525558752]
2023-04-11 15:02:26,249 :: INFO :: Recall [0.007903242866833922, 0.03247778802738697, 0.05364398404780182]
2023-04-11 15:02:26,249 :: INFO :: ndcg [0.01947333480858597, 0.04870308533492207, 0.06390415348307514]
2023-04-11 15:02:46,452 :: INFO :: ----- test -----
2023-04-11 15:02:46,452 :: INFO :: Precision [0.015912440635405863, 0.012910093345706159, 0.010251651291009727]
2023-04-11 15:02:46,452 :: INFO :: Recall [0.009324717747000088, 0.035844171965971575, 0.05561471781325716]
2023-04-11 15:02:46,452 :: INFO :: ndcg [0.015912440635405863, 0.03931011590834003, 0.05021674726308336]
2023-04-11 15:02:51,796 :: INFO :: Epoch 140: loss tensor(53.0610, device='cuda:0', grad_fn=<AddBackward0>), U.norm 167.84657287597656, V.norm 385.6908874511719, MLP.norm 9.32092571258545
2023-04-11 15:03:12,031 :: INFO :: ----- val -----
2023-04-11 15:03:12,031 :: INFO :: Precision [0.01876521354281921, 0.01641071033414421, 0.013746404071697292]
2023-04-11 15:03:12,031 :: INFO :: Recall [0.008128250701774243, 0.03273557721680303, 0.053973993710724075]
2023-04-11 15:03:12,031 :: INFO :: ndcg [0.01876521354281921, 0.04843844423442129, 0.06383154968687715]
2023-04-11 15:03:32,124 :: INFO :: ----- test -----
2023-04-11 15:03:32,124 :: INFO :: Precision [0.01550302964135597, 0.01281729352038819, 0.010254380697636721]
2023-04-11 15:03:32,124 :: INFO :: Recall [0.009508140092655397, 0.03553430937911757, 0.05594778920255676]
2023-04-11 15:03:32,124 :: INFO :: ndcg [0.01550302964135597, 0.03908854242279604, 0.0501164350660469]
2023-04-11 15:03:36,718 :: INFO :: Epoch 145: loss tensor(51.9074, device='cuda:0', grad_fn=<AddBackward0>), U.norm 170.47872924804688, V.norm 385.9593200683594, MLP.norm 9.408304214477539
2023-04-11 15:03:56,671 :: INFO :: ----- val -----
2023-04-11 15:03:56,671 :: INFO :: Precision [0.018189865014383712, 0.016331046691745454, 0.013799513166629812]
2023-04-11 15:03:56,671 :: INFO :: Recall [0.007923026136071694, 0.032588964937791585, 0.05396078761423936]
2023-04-11 15:03:56,671 :: INFO :: ndcg [0.018189865014383712, 0.04812999276351581, 0.06367707458816009]
2023-04-11 15:04:15,562 :: INFO :: ----- test -----
2023-04-11 15:04:15,562 :: INFO :: Precision [0.014929854249686118, 0.012784540640864199, 0.010287133577160722]
2023-04-11 15:04:15,562 :: INFO :: Recall [0.00924236412235134, 0.03555783270097628, 0.05630858337995888]
2023-04-11 15:04:15,562 :: INFO :: ndcg [0.014929854249686118, 0.03888964266934764, 0.05001930079294082]
2023-04-11 15:04:20,312 :: INFO :: Epoch 150: loss tensor(50.9957, device='cuda:0', grad_fn=<AddBackward0>), U.norm 173.01145935058594, V.norm 386.2641296386719, MLP.norm 9.530111312866211
2023-04-11 15:04:39,374 :: INFO :: ----- val -----
2023-04-11 15:04:39,374 :: INFO :: Precision [0.017526001327727372, 0.016251383049346698, 0.01379508740871877]
2023-04-11 15:04:39,390 :: INFO :: Recall [0.007710193331502036, 0.03233836993108188, 0.054081542420190865]
2023-04-11 15:04:39,390 :: INFO :: ndcg [0.017526001327727372, 0.04767618059733761, 0.06340336431771419]
2023-04-11 15:04:59,687 :: INFO :: ----- test -----
2023-04-11 15:04:59,687 :: INFO :: Precision [0.014684207653256181, 0.012773623014356202, 0.010330804083192717]
2023-04-11 15:04:59,687 :: INFO :: Recall [0.009198763425704198, 0.03568940058934543, 0.05643318876236897]
2023-04-11 15:04:59,687 :: INFO :: ndcg [0.014684207653256181, 0.03885680678608399, 0.050108596154888205]
2023-04-11 15:05:05,296 :: INFO :: Epoch 155: loss tensor(48.9829, device='cuda:0', grad_fn=<AddBackward0>), U.norm 175.53123474121094, V.norm 386.3345031738281, MLP.norm 9.397186279296875
2023-04-11 15:05:26,390 :: INFO :: ----- val -----
2023-04-11 15:05:26,390 :: INFO :: Precision [0.01743748616950653, 0.016375304270855876, 0.013812790440362939]
2023-04-11 15:05:26,390 :: INFO :: Recall [0.0077597950931197306, 0.032671338826497934, 0.05430647330703477]
2023-04-11 15:05:26,390 :: INFO :: ndcg [0.01743748616950653, 0.048013009356440525, 0.06345637419157744]
2023-04-11 15:05:47,686 :: INFO :: ----- test -----
2023-04-11 15:05:47,686 :: INFO :: Precision [0.014165620394126317, 0.012789999454118198, 0.010281674763906713]
2023-04-11 15:05:47,686 :: INFO :: Recall [0.008896309330739594, 0.0357332243953833, 0.05629686297460226]
2023-04-11 15:05:47,686 :: INFO :: ndcg [0.014165620394126317, 0.03873622182644314, 0.04991501874189487]
2023-04-11 15:05:53,280 :: INFO :: Epoch 160: loss tensor(54.1189, device='cuda:0', grad_fn=<AddBackward0>), U.norm 177.9518280029297, V.norm 386.19171142578125, MLP.norm 9.372292518615723
2023-04-11 15:06:13,407 :: INFO :: ----- val -----
2023-04-11 15:06:13,407 :: INFO :: Precision [0.017570258906837798, 0.01628678911263503, 0.013843770745740246]
2023-04-11 15:06:13,407 :: INFO :: Recall [0.00795497548745923, 0.032757510036604626, 0.05421489915208673]
2023-04-11 15:06:13,407 :: INFO :: ndcg [0.017570258906837798, 0.04789846332793798, 0.06380339126685997]
2023-04-11 15:06:34,267 :: INFO :: ----- test -----
2023-04-11 15:06:34,267 :: INFO :: Precision [0.014220208526666302, 0.012708117255308224, 0.010352639336208717]
2023-04-11 15:06:34,267 :: INFO :: Recall [0.009021179683924812, 0.03531231166220179, 0.056342628091104574]
2023-04-11 15:06:34,267 :: INFO :: ndcg [0.014220208526666302, 0.03860804697732012, 0.050098037832011694]
2023-04-11 15:06:40,593 :: INFO :: Epoch 165: loss tensor(48.3447, device='cuda:0', grad_fn=<AddBackward0>), U.norm 180.30213928222656, V.norm 385.99951171875, MLP.norm 9.379304885864258
2023-04-11 15:07:01,609 :: INFO :: ----- val -----
2023-04-11 15:07:01,609 :: INFO :: Precision [0.01717194069484399, 0.016339898207567535, 0.013768532861252503]
2023-04-11 15:07:01,609 :: INFO :: Recall [0.0076237980346840805, 0.032907991555903175, 0.053868467607939625]
2023-04-11 15:07:01,609 :: INFO :: ndcg [0.01717194069484399, 0.04776840272164941, 0.06322226387635832]
2023-04-11 15:07:22,499 :: INFO :: ----- test -----
2023-04-11 15:07:22,499 :: INFO :: Precision [0.014438561056826246, 0.012768164201102203, 0.01040176865549471]
2023-04-11 15:07:22,499 :: INFO :: Recall [0.009139563844712715, 0.03557892418192341, 0.056960977828538296]
2023-04-11 15:07:22,499 :: INFO :: ndcg [0.014438561056826246, 0.03879417980409194, 0.05031358625620914]
2023-04-11 15:07:28,249 :: INFO :: Epoch 170: loss tensor(48.5050, device='cuda:0', grad_fn=<AddBackward0>), U.norm 182.62088012695312, V.norm 385.6714172363281, MLP.norm 9.30756950378418
2023-04-11 15:07:49,124 :: INFO :: ----- val -----
2023-04-11 15:07:49,124 :: INFO :: Precision [0.017570258906837798, 0.016295640628457116, 0.013733126797964165]
2023-04-11 15:07:49,124 :: INFO :: Recall [0.007600731696187174, 0.032843867086600566, 0.05377695095884877]
2023-04-11 15:07:49,124 :: INFO :: ndcg [0.017570258906837798, 0.04776177433144647, 0.06321666550714954]
2023-04-11 15:08:09,874 :: INFO :: ----- test -----
2023-04-11 15:08:09,874 :: INFO :: Precision [0.014957148315956112, 0.012789999454118198, 0.010352639336208716]
2023-04-11 15:08:09,874 :: INFO :: Recall [0.009472756257167094, 0.035888677674847394, 0.05663528186164292]
2023-04-11 15:08:09,874 :: INFO :: ndcg [0.014957148315956112, 0.03906736848342941, 0.05037789741119012]
2023-04-11 15:08:14,686 :: INFO :: Epoch 175: loss tensor(50.6579, device='cuda:0', grad_fn=<AddBackward0>), U.norm 184.92498779296875, V.norm 385.56268310546875, MLP.norm 9.409402847290039
2023-04-11 15:08:34,358 :: INFO :: ----- val -----
2023-04-11 15:08:34,358 :: INFO :: Precision [0.01810134985616287, 0.016224828501880444, 0.01371984952423104]
2023-04-11 15:08:34,358 :: INFO :: Recall [0.007724927203113117, 0.03297137135154108, 0.05370354138866472]
2023-04-11 15:08:34,358 :: INFO :: ndcg [0.01810134985616287, 0.04786696668373854, 0.06344385759235115]
2023-04-11 15:08:54,296 :: INFO :: ----- test -----
2023-04-11 15:08:54,296 :: INFO :: Precision [0.015202794912386047, 0.012773623014356202, 0.010322615863311714]
2023-04-11 15:08:54,296 :: INFO :: Recall [0.009620819194703457, 0.03574463747403554, 0.05691030775564622]
2023-04-11 15:08:54,296 :: INFO :: ndcg [0.015202794912386047, 0.03902112416890802, 0.05029226221241469]
2023-04-11 15:10:26,436 :: INFO :: Epoch 180: loss tensor(51.1983, device='cuda:0', grad_fn=<AddBackward0>), U.norm 187.1385955810547, V.norm 385.2073059082031, MLP.norm 9.367634773254395
2023-04-11 15:10:49,014 :: INFO :: ----- val -----
2023-04-11 15:10:49,014 :: INFO :: Precision [0.01810134985616287, 0.01631334366010128, 0.013613631334365996]
2023-04-11 15:10:49,014 :: INFO :: Recall [0.007474067135524645, 0.03296997354268496, 0.053453033306641716]
2023-04-11 15:10:49,014 :: INFO :: ndcg [0.01810134985616287, 0.04802187819638341, 0.06300779717905444]
2023-04-11 15:11:11,483 :: INFO :: ----- test -----
2023-04-11 15:11:11,483 :: INFO :: Precision [0.015311971177466018, 0.012877340466182169, 0.010363556962716711]
2023-04-11 15:11:11,483 :: INFO :: Recall [0.009499604080737328, 0.036146965008029656, 0.0573636856757228]
2023-04-11 15:11:11,483 :: INFO :: ndcg [0.015311971177466018, 0.03929329546746559, 0.050565906620103115]
2023-04-11 15:11:17,671 :: INFO :: Epoch 185: loss tensor(49.0611, device='cuda:0', grad_fn=<AddBackward0>), U.norm 189.3159942626953, V.norm 384.8448486328125, MLP.norm 9.454532623291016
2023-04-11 15:11:40,030 :: INFO :: ----- val -----
2023-04-11 15:11:40,030 :: INFO :: Precision [0.01765877406505864, 0.016277937596812946, 0.013423323744191158]
2023-04-11 15:11:40,030 :: INFO :: Recall [0.007280686878979709, 0.03282295439730526, 0.052778828776579745]
2023-04-11 15:11:40,030 :: INFO :: ndcg [0.01765877406505864, 0.047772575734158954, 0.06248239206948535]
2023-04-11 15:12:02,061 :: INFO :: ----- test -----
2023-04-11 15:12:02,061 :: INFO :: Precision [0.015584911840165947, 0.012773623014356202, 0.010232545444620725]
2023-04-11 15:12:02,061 :: INFO :: Recall [0.00946762936537378, 0.03581347917451654, 0.05683649850468885]
2023-04-11 15:12:02,061 :: INFO :: ndcg [0.015584911840165947, 0.039259035812486194, 0.050264792846912695]
2023-04-11 15:12:08,296 :: INFO :: Epoch 190: loss tensor(53.2147, device='cuda:0', grad_fn=<AddBackward0>), U.norm 191.49917602539062, V.norm 384.470458984375, MLP.norm 9.516301155090332
2023-04-11 15:12:29,733 :: INFO :: ----- val -----
2023-04-11 15:12:29,733 :: INFO :: Precision [0.018057092277052444, 0.016269086080990862, 0.01331267979641508]
2023-04-11 15:12:29,733 :: INFO :: Recall [0.007445331312267649, 0.03285348076988392, 0.052234681717404335]
2023-04-11 15:12:29,733 :: INFO :: ndcg [0.018057092277052444, 0.04789376736098715, 0.06233166167495102]
2023-04-11 15:12:51,499 :: INFO :: ----- test -----
2023-04-11 15:12:51,499 :: INFO :: Precision [0.015748676237785905, 0.012888258092690166, 0.010262568917517723]
2023-04-11 15:12:51,499 :: INFO :: Recall [0.009902905193768663, 0.036296383533214224, 0.05727302063882705]
2023-04-11 15:12:51,499 :: INFO :: ndcg [0.015748676237785905, 0.039572864321804936, 0.05049894242653938]
2023-04-11 15:12:58,311 :: INFO :: Epoch 195: loss tensor(51.2598, device='cuda:0', grad_fn=<AddBackward0>), U.norm 193.6625518798828, V.norm 384.0536193847656, MLP.norm 9.377906799316406
2023-04-11 15:13:20,593 :: INFO :: ----- val -----
2023-04-11 15:13:20,593 :: INFO :: Precision [0.019340562071254702, 0.016348749723389622, 0.013436601017924285]
2023-04-11 15:13:20,593 :: INFO :: Recall [0.007872117959062066, 0.03275229806160269, 0.05271533088766888]
2023-04-11 15:13:20,593 :: INFO :: ndcg [0.019340562071254702, 0.04847201483788064, 0.06317933318432284]
2023-04-11 15:13:43,015 :: INFO :: ----- test -----
2023-04-11 15:13:43,015 :: INFO :: Precision [0.016376439761995744, 0.012964681478246143, 0.010295321797041727]
2023-04-11 15:13:43,015 :: INFO :: Recall [0.009946683781584167, 0.036439640985887055, 0.05670000235798056]
2023-04-11 15:13:43,015 :: INFO :: ndcg [0.016376439761995744, 0.039913016964958005, 0.050913252944205976]
2023-04-11 15:13:49,155 :: INFO :: Epoch 200: loss tensor(48.7703, device='cuda:0', grad_fn=<AddBackward0>), U.norm 195.77000427246094, V.norm 383.4802551269531, MLP.norm 9.521916389465332
2023-04-11 15:14:11,077 :: INFO :: ----- val -----
2023-04-11 15:14:11,077 :: INFO :: Precision [0.01942907722947555, 0.016233680017702527, 0.013626908608099134]
2023-04-11 15:14:11,077 :: INFO :: Recall [0.007879610773591833, 0.032573887045143565, 0.05315562365474958]
2023-04-11 15:14:11,077 :: INFO :: ndcg [0.01942907722947555, 0.04827395001704033, 0.06367286374646199]
2023-04-11 15:14:33,217 :: INFO :: ----- test -----
2023-04-11 15:14:33,217 :: INFO :: Precision [0.01588514656913587, 0.012866422839674171, 0.010388121622359723]
2023-04-11 15:14:33,217 :: INFO :: Recall [0.009703075075450212, 0.036132967066333704, 0.05724886351254896]
2023-04-11 15:14:33,217 :: INFO :: ndcg [0.01588514656913587, 0.03952898525330097, 0.050922603338694555]
2023-04-11 15:14:40,765 :: INFO :: Epoch 205: loss tensor(53.6538, device='cuda:0', grad_fn=<AddBackward0>), U.norm 197.87020874023438, V.norm 383.04522705078125, MLP.norm 9.55610179901123
2023-04-11 15:15:03,546 :: INFO :: ----- val -----
2023-04-11 15:15:03,546 :: INFO :: Precision [0.01898650143837132, 0.016242531533524614, 0.013671166187209564]
2023-04-11 15:15:03,546 :: INFO :: Recall [0.007891612901106243, 0.032733892541070474, 0.05333034285066062]
2023-04-11 15:15:03,546 :: INFO :: ndcg [0.01898650143837132, 0.048012340379611815, 0.06361069763951437]
2023-04-11 15:15:25,874 :: INFO :: ----- test -----
2023-04-11 15:15:25,874 :: INFO :: Precision [0.01607620503302582, 0.013052022490310114, 0.010579180086249702]
2023-04-11 15:15:25,874 :: INFO :: Recall [0.009831974378944841, 0.03670620201279254, 0.058849145774933984]
2023-04-11 15:15:25,874 :: INFO :: ndcg [0.01607620503302582, 0.03992294654114846, 0.051480989358469724]
2023-04-11 15:15:32,889 :: INFO :: Epoch 210: loss tensor(50.2458, device='cuda:0', grad_fn=<AddBackward0>), U.norm 199.90997314453125, V.norm 382.63470458984375, MLP.norm 9.502634048461914
2023-04-11 15:15:54,155 :: INFO :: ----- val -----
2023-04-11 15:15:54,155 :: INFO :: Precision [0.019340562071254702, 0.016286789112635036, 0.013702146492586864]
2023-04-11 15:15:54,155 :: INFO :: Recall [0.007995670508541403, 0.032855374456457025, 0.0536847404770471]
2023-04-11 15:15:54,155 :: INFO :: ndcg [0.019340562071254702, 0.04831831704269498, 0.06396295872918102]
2023-04-11 15:16:16,124 :: INFO :: ----- test -----
2023-04-11 15:16:16,124 :: INFO :: Precision [0.016294557563185763, 0.012953763851738145, 0.010551886019979712]
2023-04-11 15:16:16,124 :: INFO :: Recall [0.010074328357856495, 0.03640511496911583, 0.058321744182040246]
2023-04-11 15:16:16,124 :: INFO :: ndcg [0.016294557563185763, 0.039920257414752976, 0.05158850495031286]
2023-04-11 15:16:23,171 :: INFO :: Epoch 215: loss tensor(49.2480, device='cuda:0', grad_fn=<AddBackward0>), U.norm 201.97779846191406, V.norm 382.279296875, MLP.norm 9.66431713104248
2023-04-11 15:16:45,265 :: INFO :: ----- val -----
2023-04-11 15:16:45,265 :: INFO :: Precision [0.01845541048904625, 0.016233680017702527, 0.013759681345430421]
2023-04-11 15:16:45,265 :: INFO :: Recall [0.00780211000401317, 0.03282147995335375, 0.05358505539223644]
2023-04-11 15:16:45,265 :: INFO :: ndcg [0.01845541048904625, 0.047926722905869704, 0.0637787682315814]
2023-04-11 15:17:06,702 :: INFO :: ----- test -----
2023-04-11 15:17:06,702 :: INFO :: Precision [0.015939734701675857, 0.013008351984278129, 0.010521862547082709]
2023-04-11 15:17:06,702 :: INFO :: Recall [0.010039364292377659, 0.03679808433704049, 0.058253822448670736]
2023-04-11 15:17:06,702 :: INFO :: ndcg [0.015939734701675857, 0.03980625199979614, 0.051312077410166454]
2023-04-11 15:17:12,983 :: INFO :: Epoch 220: loss tensor(51.4988, device='cuda:0', grad_fn=<AddBackward0>), U.norm 203.99037170410156, V.norm 381.9180908203125, MLP.norm 9.524710655212402
2023-04-11 15:17:33,623 :: INFO :: ----- val -----
2023-04-11 15:17:33,623 :: INFO :: Precision [0.01898650143837132, 0.016357601239211702, 0.013777384377074594]
2023-04-11 15:17:33,623 :: INFO :: Recall [0.007687222514706562, 0.032981842594605466, 0.05367526267243179]
2023-04-11 15:17:33,623 :: INFO :: ndcg [0.01898650143837132, 0.048436544790262465, 0.0640571087681439]
2023-04-11 15:17:55,565 :: INFO :: ----- test -----
2023-04-11 15:17:55,565 :: INFO :: Precision [0.016485616027075715, 0.013079316556580105, 0.010611932965773702]
2023-04-11 15:17:55,565 :: INFO :: Recall [0.010221077287972417, 0.03697334878224063, 0.05844389502676562]
2023-04-11 15:17:55,565 :: INFO :: ndcg [0.016485616027075715, 0.04023174053282782, 0.05189592391457146]
2023-04-11 15:18:02,155 :: INFO :: Epoch 225: loss tensor(48.5984, device='cuda:0', grad_fn=<AddBackward0>), U.norm 205.93017578125, V.norm 381.35516357421875, MLP.norm 9.500960350036621
2023-04-11 15:18:24,843 :: INFO :: ----- val -----
2023-04-11 15:18:24,843 :: INFO :: Precision [0.01911927417570259, 0.01643726488161046, 0.01364461163974331]
2023-04-11 15:18:24,843 :: INFO :: Recall [0.00768991755269702, 0.032995043330239913, 0.053430822442659974]
2023-04-11 15:18:24,843 :: INFO :: ndcg [0.01911927417570259, 0.048672337423308704, 0.064009164423855]
2023-04-11 15:18:46,608 :: INFO :: ----- test -----
2023-04-11 15:18:46,608 :: INFO :: Precision [0.016485616027075715, 0.013101151809596097, 0.010652874065178697]
2023-04-11 15:18:46,608 :: INFO :: Recall [0.010112411063524765, 0.03708783750483252, 0.05909206671392188]
2023-04-11 15:18:46,608 :: INFO :: ndcg [0.016485616027075715, 0.04022532563966734, 0.05198364347186739]
2023-04-11 15:18:52,780 :: INFO :: Epoch 230: loss tensor(51.7355, device='cuda:0', grad_fn=<AddBackward0>), U.norm 207.8987274169922, V.norm 380.8987731933594, MLP.norm 9.641303062438965
2023-04-11 15:19:15,092 :: INFO :: ----- val -----
2023-04-11 15:19:15,092 :: INFO :: Precision [0.019384819650365124, 0.01642841336578837, 0.013587076786899747]
2023-04-11 15:19:15,092 :: INFO :: Recall [0.007797449260250856, 0.03291745621921232, 0.05319026491132367]
2023-04-11 15:19:15,092 :: INFO :: ndcg [0.019384819650365124, 0.04865649440919274, 0.06374362096448263]
2023-04-11 15:19:36,717 :: INFO :: ----- test -----
2023-04-11 15:19:36,717 :: INFO :: Precision [0.01626726349691577, 0.013030187237294122, 0.010581909492876707]
2023-04-11 15:19:36,717 :: INFO :: Recall [0.009954393925222562, 0.0368261688660528, 0.058696190022723735]
2023-04-11 15:19:36,717 :: INFO :: ndcg [0.01626726349691577, 0.04001652878408702, 0.05166803945513375]
2023-04-11 15:19:44,171 :: INFO :: Epoch 235: loss tensor(52.5690, device='cuda:0', grad_fn=<AddBackward0>), U.norm 209.8702392578125, V.norm 380.3409729003906, MLP.norm 9.682829856872559
2023-04-11 15:20:05,252 :: INFO :: ----- val -----
2023-04-11 15:20:05,252 :: INFO :: Precision [0.019075016596592165, 0.016552334587297546, 0.013383491922991765]
2023-04-11 15:20:05,252 :: INFO :: Recall [0.007661961910172481, 0.033251194767041546, 0.05247060573224188]
2023-04-11 15:20:05,252 :: INFO :: ndcg [0.019075016596592165, 0.048817139086617715, 0.06322779730212907]
2023-04-11 15:20:26,108 :: INFO :: ----- test -----
2023-04-11 15:20:26,108 :: INFO :: Precision [0.016485616027075715, 0.013101151809596095, 0.010587368306130709]
2023-04-11 15:20:26,108 :: INFO :: Recall [0.010089378443224708, 0.036826792076370116, 0.0586771924490309]
2023-04-11 15:20:26,108 :: INFO :: ndcg [0.016485616027075715, 0.04030501778156977, 0.051889249795701205]
2023-04-11 15:20:32,530 :: INFO :: Epoch 240: loss tensor(50.0902, device='cuda:0', grad_fn=<AddBackward0>), U.norm 211.82200622558594, V.norm 379.8705749511719, MLP.norm 9.590601921081543
2023-04-11 15:20:54,296 :: INFO :: ----- val -----
2023-04-11 15:20:54,296 :: INFO :: Precision [0.01880947112192963, 0.01639300730250004, 0.013795087408718779]
2023-04-11 15:20:54,296 :: INFO :: Recall [0.007758267647659896, 0.032946104996877675, 0.05365230672706229]
2023-04-11 15:20:54,296 :: INFO :: ndcg [0.01880947112192963, 0.04843291649129653, 0.06417284651240944]
2023-04-11 15:21:16,202 :: INFO :: ----- test -----
2023-04-11 15:21:16,202 :: INFO :: Precision [0.01662208635842568, 0.013232163327692056, 0.0106610622850597]
2023-04-11 15:21:16,202 :: INFO :: Recall [0.010199768762551107, 0.037151102253155856, 0.05913750557988335]
2023-04-11 15:21:16,202 :: INFO :: ndcg [0.01662208635842568, 0.04064485594370693, 0.0521621825558877]
2023-04-11 15:21:22,436 :: INFO :: Epoch 245: loss tensor(52.7369, device='cuda:0', grad_fn=<AddBackward0>), U.norm 213.7471923828125, V.norm 379.4072265625, MLP.norm 9.570721626281738
2023-04-11 15:21:44,217 :: INFO :: ----- val -----
2023-04-11 15:21:44,217 :: INFO :: Precision [0.01916353175481301, 0.01633989820756753, 0.013834919229918165]
2023-04-11 15:21:44,217 :: INFO :: Recall [0.007995888167364249, 0.03282861499082863, 0.05398761234217045]
2023-04-11 15:21:44,217 :: INFO :: ndcg [0.01916353175481301, 0.04858510266907286, 0.06441389222372937]
2023-04-11 15:22:05,874 :: INFO :: ----- test -----
2023-04-11 15:22:05,874 :: INFO :: Precision [0.016185381298105792, 0.013139363502374083, 0.010669250504940693]
2023-04-11 15:22:05,874 :: INFO :: Recall [0.009892675940804615, 0.03671292379985767, 0.05917584655441105]
2023-04-11 15:22:05,874 :: INFO :: ndcg [0.016185381298105792, 0.04045740703866048, 0.05207039512873832]
2023-04-11 15:22:13,092 :: INFO :: Epoch 250: loss tensor(49.2547, device='cuda:0', grad_fn=<AddBackward0>), U.norm 215.7296142578125, V.norm 378.9424133300781, MLP.norm 9.796409606933594
2023-04-11 15:22:35,717 :: INFO :: ----- val -----
2023-04-11 15:22:35,717 :: INFO :: Precision [0.018411152909935828, 0.016631998229696306, 0.013932285903961113]
2023-04-11 15:22:35,717 :: INFO :: Recall [0.007878549926885118, 0.033186106906054974, 0.054348560363177915]
2023-04-11 15:22:35,717 :: INFO :: ndcg [0.018411152909935828, 0.04876666947500299, 0.06448286943928261]
2023-04-11 15:22:58,264 :: INFO :: ----- test -----
2023-04-11 15:22:58,264 :: INFO :: Precision [0.015748676237785905, 0.013144822315628081, 0.01074021507724269]
2023-04-11 15:22:58,264 :: INFO :: Recall [0.009738386378467582, 0.03676452104354193, 0.059033045154054546]
2023-04-11 15:22:58,264 :: INFO :: ndcg [0.015748676237785905, 0.04024318204682274, 0.05213334770169748]
2023-04-11 15:23:04,155 :: INFO :: Epoch 255: loss tensor(50.7064, device='cuda:0', grad_fn=<AddBackward0>), U.norm 217.64466857910156, V.norm 378.44097900390625, MLP.norm 9.758299827575684
2023-04-11 15:23:25,968 :: INFO :: ----- val -----
2023-04-11 15:23:25,968 :: INFO :: Precision [0.01880947112192963, 0.016658552777162553, 0.014056207125470328]
2023-04-11 15:23:25,968 :: INFO :: Recall [0.007881823385558429, 0.03329771070131901, 0.05452511393394909]
2023-04-11 15:23:25,968 :: INFO :: ndcg [0.01880947112192963, 0.04887084049167303, 0.06476414416439122]
2023-04-11 15:23:48,499 :: INFO :: ----- test -----
2023-04-11 15:23:48,499 :: INFO :: Precision [0.015939734701675857, 0.013166657568644075, 0.010781156176647686]
2023-04-11 15:23:48,499 :: INFO :: Recall [0.009939245211757224, 0.0367689562459956, 0.05917249224901058]
2023-04-11 15:23:48,499 :: INFO :: ndcg [0.015939734701675857, 0.040302415053930646, 0.052216709015730736]
2023-04-11 15:23:54,796 :: INFO :: Epoch 260: loss tensor(54.1365, device='cuda:0', grad_fn=<AddBackward0>), U.norm 219.55540466308594, V.norm 377.9219970703125, MLP.norm 9.662453651428223
2023-04-11 15:24:17,342 :: INFO :: ----- val -----
2023-04-11 15:24:17,342 :: INFO :: Precision [0.018720955963708784, 0.01670281035627298, 0.014034078335915112]
2023-04-11 15:24:17,342 :: INFO :: Recall [0.007721171001626558, 0.03343313938537898, 0.05434180480909843]
2023-04-11 15:24:17,342 :: INFO :: ndcg [0.018720955963708784, 0.04884378578861664, 0.06457814274205448]
2023-04-11 15:24:39,265 :: INFO :: ----- test -----
2023-04-11 15:24:39,265 :: INFO :: Precision [0.015530323707625962, 0.013155739942136079, 0.010786614989901685]
2023-04-11 15:24:39,265 :: INFO :: Recall [0.009569848586765317, 0.03676067762910022, 0.05909641373664498]
2023-04-11 15:24:39,265 :: INFO :: ndcg [0.015530323707625962, 0.0400500968632879, 0.05201768125542941]
2023-04-11 15:24:45,983 :: INFO :: Epoch 265: loss tensor(51.0338, device='cuda:0', grad_fn=<AddBackward0>), U.norm 221.49624633789062, V.norm 377.4306945800781, MLP.norm 9.56605339050293
2023-04-11 15:25:08,171 :: INFO :: ----- val -----
2023-04-11 15:25:08,171 :: INFO :: Precision [0.018012834697942022, 0.01684443460942632, 0.013963266209338422]
2023-04-11 15:25:08,171 :: INFO :: Recall [0.007717069522925584, 0.03364415928974626, 0.05366286879120763]
2023-04-11 15:25:08,171 :: INFO :: ndcg [0.018012834697942022, 0.04907515183561933, 0.06438128524191633]
2023-04-11 15:25:29,655 :: INFO :: ----- test -----
2023-04-11 15:25:29,655 :: INFO :: Precision [0.014902560183416125, 0.013253998580708049, 0.01076205033025869]
2023-04-11 15:25:29,655 :: INFO :: Recall [0.009504670105052094, 0.03700542437213868, 0.059313478579732806]
2023-04-11 15:25:29,655 :: INFO :: ndcg [0.014902560183416125, 0.04016313903460478, 0.051990075266959423]
2023-04-11 15:25:36,640 :: INFO :: Epoch 270: loss tensor(50.0576, device='cuda:0', grad_fn=<AddBackward0>), U.norm 223.3822784423828, V.norm 377.00897216796875, MLP.norm 9.741865158081055
2023-04-11 15:25:58,296 :: INFO :: ----- val -----
2023-04-11 15:25:58,296 :: INFO :: Precision [0.01761451648594822, 0.016782473998671735, 0.014233237441912051]
2023-04-11 15:25:58,296 :: INFO :: Recall [0.007480796026080779, 0.033218512293544195, 0.05468297991550666]
2023-04-11 15:25:58,296 :: INFO :: ndcg [0.01761451648594822, 0.04876609443838512, 0.06489312714505968]
2023-04-11 15:26:20,124 :: INFO :: ----- test -----
2023-04-11 15:26:20,124 :: INFO :: Precision [0.014902560183416125, 0.01334679840602602, 0.010887603035100677]
2023-04-11 15:26:20,124 :: INFO :: Recall [0.009349757165478102, 0.03734463967174147, 0.05950363799500986]
2023-04-11 15:26:20,124 :: INFO :: ndcg [0.014902560183416125, 0.04033111163361198, 0.05232770944691181]
2023-04-11 15:26:26,108 :: INFO :: Epoch 275: loss tensor(49.9569, device='cuda:0', grad_fn=<AddBackward0>), U.norm 225.2993621826172, V.norm 376.6065673828125, MLP.norm 9.731372833251953
2023-04-11 15:26:47,545 :: INFO :: ----- val -----
2023-04-11 15:26:47,545 :: INFO :: Precision [0.018411152909935828, 0.016738216419561316, 0.014109316220402845]
2023-04-11 15:26:47,545 :: INFO :: Recall [0.007547759127760045, 0.033420759202690026, 0.05438440331659633]
2023-04-11 15:26:47,545 :: INFO :: ndcg [0.018411152909935828, 0.04888017012752878, 0.06484157837757358]
2023-04-11 15:27:09,405 :: INFO :: ----- test -----
2023-04-11 15:27:09,405 :: INFO :: Precision [0.01542114744254599, 0.013204869261422064, 0.010827556089306693]
2023-04-11 15:27:09,405 :: INFO :: Recall [0.009537476355326228, 0.03690425207867087, 0.05930003535033006]
2023-04-11 15:27:09,405 :: INFO :: ndcg [0.01542114744254599, 0.04009511193991386, 0.05227568574717713]
2023-04-11 15:27:15,358 :: INFO :: Epoch 280: loss tensor(50.9535, device='cuda:0', grad_fn=<AddBackward0>), U.norm 227.1473388671875, V.norm 376.05224609375, MLP.norm 9.737614631652832
2023-04-11 15:27:36,936 :: INFO :: ----- val -----
2023-04-11 15:27:36,952 :: INFO :: Precision [0.018189865014383712, 0.016782473998671735, 0.014184554104890564]
2023-04-11 15:27:36,952 :: INFO :: Recall [0.007712407509185955, 0.03331489802997387, 0.05456725790570296]
2023-04-11 15:27:36,952 :: INFO :: ndcg [0.018189865014383712, 0.04908355706648773, 0.06502437803546063]
2023-04-11 15:27:58,265 :: INFO :: ----- test -----
2023-04-11 15:27:58,265 :: INFO :: Precision [0.015530323707625962, 0.013204869261422064, 0.01080026202303669]
2023-04-11 15:27:58,265 :: INFO :: Recall [0.009657906007253234, 0.037032684669012154, 0.05895528158689691]
2023-04-11 15:27:58,265 :: INFO :: ndcg [0.015530323707625962, 0.0401452851387536, 0.052178767214652644]
2023-04-11 15:28:04,296 :: INFO :: Epoch 285: loss tensor(52.6119, device='cuda:0', grad_fn=<AddBackward0>), U.norm 229.00733947753906, V.norm 375.5523376464844, MLP.norm 9.656249046325684
2023-04-11 15:28:25,577 :: INFO :: ----- val -----
2023-04-11 15:28:25,577 :: INFO :: Precision [0.01965036512502766, 0.016817880061960076, 0.014091613188758664]
2023-04-11 15:28:25,577 :: INFO :: Recall [0.008425683437169373, 0.03338431185228036, 0.05467686466703646]
2023-04-11 15:28:25,577 :: INFO :: ndcg [0.01965036512502766, 0.04936817417632622, 0.06517588495327163]
2023-04-11 15:28:46,046 :: INFO :: ----- test -----
2023-04-11 15:28:46,046 :: INFO :: Precision [0.015530323707625962, 0.01318303400840607, 0.010830285495933685]
2023-04-11 15:28:46,046 :: INFO :: Recall [0.009399551745662215, 0.036842388993837405, 0.05888970842068273]
2023-04-11 15:28:46,046 :: INFO :: ndcg [0.015530323707625962, 0.03977405932610216, 0.05183672726653711]
2023-04-11 15:28:52,186 :: INFO :: Epoch 290: loss tensor(50.5160, device='cuda:0', grad_fn=<AddBackward0>), U.norm 230.86026000976562, V.norm 375.01190185546875, MLP.norm 9.856731414794922
2023-04-11 15:29:13,139 :: INFO :: ----- val -----
2023-04-11 15:29:13,139 :: INFO :: Precision [0.019517592387696393, 0.01690639522018091, 0.014122593494135968]
2023-04-11 15:29:13,139 :: INFO :: Recall [0.0081515650932347, 0.03359181348914595, 0.05435334613302413]
2023-04-11 15:29:13,139 :: INFO :: ndcg [0.019517592387696393, 0.04981129499573095, 0.06558534487844503]
2023-04-11 15:29:34,217 :: INFO :: ----- test -----
2023-04-11 15:29:34,217 :: INFO :: Precision [0.015694088105245918, 0.013128445875866089, 0.010936732354386679]
2023-04-11 15:29:34,217 :: INFO :: Recall [0.009686570935784847, 0.037059846601352114, 0.0596915859199138]
2023-04-11 15:29:34,217 :: INFO :: ndcg [0.015694088105245918, 0.04001383322097927, 0.05258652253859035]
2023-04-11 15:29:40,905 :: INFO :: Epoch 295: loss tensor(51.1765, device='cuda:0', grad_fn=<AddBackward0>), U.norm 232.67779541015625, V.norm 374.43536376953125, MLP.norm 9.657573699951172
2023-04-11 15:30:02,186 :: INFO :: ----- val -----
2023-04-11 15:30:02,186 :: INFO :: Precision [0.01965036512502766, 0.016809028546137986, 0.014122593494135974]
2023-04-11 15:30:02,186 :: INFO :: Recall [0.007828928540543005, 0.03369318842081428, 0.054198671279997924]
2023-04-11 15:30:02,186 :: INFO :: ndcg [0.01965036512502766, 0.049790574133526985, 0.06564955381908397]
2023-04-11 15:30:23,514 :: INFO :: ----- test -----
2023-04-11 15:30:23,514 :: INFO :: Precision [0.015694088105245918, 0.013226704514438059, 0.010994049893553676]
2023-04-11 15:30:23,514 :: INFO :: Recall [0.009375618953412481, 0.037285593459813964, 0.06018590455024484]
2023-04-11 15:30:23,514 :: INFO :: ndcg [0.015694088105245918, 0.0401933136521725, 0.05272344917808113]
2023-04-11 15:30:29,015 :: INFO :: Epoch 300: loss tensor(51.0712, device='cuda:0', grad_fn=<AddBackward0>), U.norm 234.51368713378906, V.norm 374.0047302246094, MLP.norm 9.793054580688477
2023-04-11 15:30:49,827 :: INFO :: ----- val -----
2023-04-11 15:30:49,827 :: INFO :: Precision [0.020181456074352733, 0.01685328612524841, 0.014188979862801609]
2023-04-11 15:30:49,827 :: INFO :: Recall [0.008158993530767008, 0.0335846564790553, 0.0544128921308532]
2023-04-11 15:30:49,827 :: INFO :: ndcg [0.020181456074352733, 0.05018326748067875, 0.0660149190087149]
2023-04-11 15:31:10,889 :: INFO :: ----- test -----
2023-04-11 15:31:10,889 :: INFO :: Precision [0.016594792292155686, 0.013188492821660068, 0.010887603035100689]
2023-04-11 15:31:10,889 :: INFO :: Recall [0.01025832273542845, 0.03698001558228411, 0.05922531800817805]
2023-04-11 15:31:10,889 :: INFO :: ndcg [0.016594792292155686, 0.040692363380798714, 0.0530056838812997]
2023-04-11 15:31:17,328 :: INFO :: Epoch 305: loss tensor(53.6595, device='cuda:0', grad_fn=<AddBackward0>), U.norm 236.2530059814453, V.norm 373.4435729980469, MLP.norm 9.742780685424805
2023-04-11 15:31:37,858 :: INFO :: ----- val -----
2023-04-11 15:31:37,858 :: INFO :: Precision [0.020845319761009073, 0.016941801283469247, 0.014414693516264817]
2023-04-11 15:31:37,858 :: INFO :: Recall [0.008497773849032622, 0.03361074245252592, 0.054864950313996824]
2023-04-11 15:31:37,858 :: INFO :: ndcg [0.020845319761009073, 0.05053617782492143, 0.06690480704682344]
2023-04-11 15:31:57,640 :: INFO :: ----- test -----
2023-04-11 15:31:57,640 :: INFO :: Precision [0.017249849882635514, 0.013292210273486035, 0.010863038375457683]
2023-04-11 15:31:57,640 :: INFO :: Recall [0.010274171524750134, 0.037118237423980585, 0.0592728442845062]
2023-04-11 15:31:57,640 :: INFO :: ndcg [0.017249849882635514, 0.04094251874817901, 0.053004891891040476]
2023-04-11 15:32:02,452 :: INFO :: Epoch 310: loss tensor(51.8880, device='cuda:0', grad_fn=<AddBackward0>), U.norm 238.05641174316406, V.norm 373.0608825683594, MLP.norm 9.66321849822998
2023-04-11 15:32:22,827 :: INFO :: ----- val -----
2023-04-11 15:32:22,827 :: INFO :: Precision [0.020491259128125692, 0.016888692188536742, 0.014374861695065431]
2023-04-11 15:32:22,842 :: INFO :: Recall [0.008371325219596607, 0.03360746487036459, 0.054517155293019516]
2023-04-11 15:32:22,842 :: INFO :: ndcg [0.020491259128125692, 0.05068078749406393, 0.06681790658139782]
2023-04-11 15:32:43,202 :: INFO :: ----- test -----
2023-04-11 15:32:43,202 :: INFO :: Precision [0.018041377804465308, 0.013303127899994036, 0.010833014902560695]
2023-04-11 15:32:43,202 :: INFO :: Recall [0.010697730451811317, 0.03699361048520517, 0.05902236277878386]
2023-04-11 15:32:43,202 :: INFO :: ndcg [0.018041377804465308, 0.04152971672165373, 0.05367699744351686]
2023-04-11 15:32:48,046 :: INFO :: Epoch 315: loss tensor(52.6158, device='cuda:0', grad_fn=<AddBackward0>), U.norm 239.84242248535156, V.norm 372.7072448730469, MLP.norm 9.71044921875
2023-04-11 15:33:09,561 :: INFO :: ----- val -----
2023-04-11 15:33:09,561 :: INFO :: Precision [0.020668289444567382, 0.01710112856826676, 0.014582872316884432]
2023-04-11 15:33:09,561 :: INFO :: Recall [0.008413649070311517, 0.03382704445786303, 0.054876426246424886]
2023-04-11 15:33:09,561 :: INFO :: ndcg [0.020668289444567382, 0.05121397986870879, 0.06759181355973808]
2023-04-11 15:33:31,171 :: INFO :: ----- test -----
2023-04-11 15:33:31,171 :: INFO :: Precision [0.01785031934057536, 0.013406845351820003, 0.011004967520061675]
2023-04-11 15:33:31,171 :: INFO :: Recall [0.0105593528835795, 0.03735484386018408, 0.05952505670709058]
2023-04-11 15:33:31,171 :: INFO :: ndcg [0.01785031934057536, 0.04163459333190621, 0.053995758908929783]
2023-04-11 15:33:36,592 :: INFO :: Epoch 320: loss tensor(52.2757, device='cuda:0', grad_fn=<AddBackward0>), U.norm 241.63351440429688, V.norm 372.3243408203125, MLP.norm 9.730937957763672
2023-04-11 15:33:57,936 :: INFO :: ----- val -----
2023-04-11 15:33:57,936 :: INFO :: Precision [0.021420668289444566, 0.01687098915689257, 0.014728922327948873]
2023-04-11 15:33:57,936 :: INFO :: Recall [0.008407175631773465, 0.03336308689065177, 0.05560569511026816]
2023-04-11 15:33:57,936 :: INFO :: ndcg [0.021420668289444566, 0.05118938793947766, 0.06833463652363925]
2023-04-11 15:34:19,108 :: INFO :: ----- test -----
2023-04-11 15:34:19,108 :: INFO :: Precision [0.0180686718707353, 0.013516021616899965, 0.010966755827283679]
2023-04-11 15:34:19,108 :: INFO :: Recall [0.010645752585132867, 0.03783477332253856, 0.05949432346706906]
2023-04-11 15:34:19,108 :: INFO :: ndcg [0.0180686718707353, 0.0420745314550519, 0.05405610960971341]
2023-04-11 15:34:24,467 :: INFO :: Epoch 325: loss tensor(47.7335, device='cuda:0', grad_fn=<AddBackward0>), U.norm 243.43502807617188, V.norm 371.80078125, MLP.norm 9.780468940734863
2023-04-11 15:34:44,280 :: INFO :: ----- val -----
2023-04-11 15:34:44,280 :: INFO :: Precision [0.0212878955521133, 0.016932949767647164, 0.014821863244080786]
2023-04-11 15:34:44,280 :: INFO :: Recall [0.008551416149437305, 0.03343258455964437, 0.05598617720276388]
2023-04-11 15:34:44,280 :: INFO :: ndcg [0.0212878955521133, 0.05110792153177079, 0.06847946988162287]
2023-04-11 15:35:04,530 :: INFO :: ----- test -----
2023-04-11 15:35:04,530 :: INFO :: Precision [0.018205142202085266, 0.013603362628963937, 0.010994049893553673]
2023-04-11 15:35:04,530 :: INFO :: Recall [0.010687052773138658, 0.03805892747099467, 0.059555461782926374]
2023-04-11 15:35:04,530 :: INFO :: ndcg [0.018205142202085266, 0.04229937449547201, 0.05421757213537469]
2023-04-11 15:35:10,390 :: INFO :: Epoch 330: loss tensor(51.1397, device='cuda:0', grad_fn=<AddBackward0>), U.norm 245.16139221191406, V.norm 371.2503662109375, MLP.norm 9.649040222167969
2023-04-11 15:35:32,717 :: INFO :: ----- val -----
2023-04-11 15:35:32,717 :: INFO :: Precision [0.021022350077450763, 0.01683558309360424, 0.014675813233016356]
2023-04-11 15:35:32,717 :: INFO :: Recall [0.008326578431917416, 0.03294592028147234, 0.05511987445137398]
2023-04-11 15:35:32,717 :: INFO :: ndcg [0.021022350077450763, 0.05082839563637003, 0.06792331522443366]
2023-04-11 15:35:54,389 :: INFO :: ----- test -----
2023-04-11 15:35:54,389 :: INFO :: Precision [0.017631966810415414, 0.013423221791581997, 0.010931273541132682]
2023-04-11 15:35:54,389 :: INFO :: Recall [0.010513761823402728, 0.037531708989807636, 0.059400705943973515]
2023-04-11 15:35:54,389 :: INFO :: ndcg [0.017631966810415414, 0.04164373248154443, 0.053767539971721255]
2023-04-11 15:35:59,936 :: INFO :: Epoch 335: loss tensor(50.1785, device='cuda:0', grad_fn=<AddBackward0>), U.norm 246.92471313476562, V.norm 370.9394836425781, MLP.norm 9.612980842590332
2023-04-11 15:36:22,186 :: INFO :: ----- val -----
2023-04-11 15:36:22,186 :: INFO :: Precision [0.02111086523567161, 0.016950652799291335, 0.01472007081212678]
2023-04-11 15:36:22,186 :: INFO :: Recall [0.008178710564562746, 0.033259660469494634, 0.055472122693131266]
2023-04-11 15:36:22,186 :: INFO :: ndcg [0.02111086523567161, 0.05094422832620898, 0.06804683314299269]
2023-04-11 15:36:43,765 :: INFO :: ----- test -----
2023-04-11 15:36:43,765 :: INFO :: Precision [0.018041377804465308, 0.013450515857851987, 0.010999508706807676]
2023-04-11 15:36:43,765 :: INFO :: Recall [0.010736339419881741, 0.037550105244865066, 0.05955012884732865]
2023-04-11 15:36:43,765 :: INFO :: ndcg [0.018041377804465308, 0.04175943912191337, 0.053957864249785124]
2023-04-11 15:36:50,296 :: INFO :: Epoch 340: loss tensor(48.0446, device='cuda:0', grad_fn=<AddBackward0>), U.norm 248.65249633789062, V.norm 370.5161437988281, MLP.norm 9.685633659362793
2023-04-11 15:37:11,952 :: INFO :: ----- val -----
2023-04-11 15:37:11,952 :: INFO :: Precision [0.02026997123257358, 0.01679132551449382, 0.014799734454525551]
2023-04-11 15:37:11,952 :: INFO :: Recall [0.007967024007805162, 0.03326235080606972, 0.05570896298621643]
2023-04-11 15:37:11,952 :: INFO :: ndcg [0.02026997123257358, 0.049936727194298765, 0.06743709793328063]
2023-04-11 15:37:33,436 :: INFO :: ----- test -----
2023-04-11 15:37:33,436 :: INFO :: Precision [0.0176865549429554, 0.013597903815709942, 0.01098586167367267]
2023-04-11 15:37:33,436 :: INFO :: Recall [0.010657273275528187, 0.038312553111107255, 0.05968230861029281]
2023-04-11 15:37:33,436 :: INFO :: ndcg [0.0176865549429554, 0.04167338654016571, 0.05363291535925022]
2023-04-11 15:37:39,811 :: INFO :: Epoch 345: loss tensor(51.9630, device='cuda:0', grad_fn=<AddBackward0>), U.norm 250.43194580078125, V.norm 370.14666748046875, MLP.norm 9.541475296020508
2023-04-11 15:38:02,577 :: INFO :: ----- val -----
2023-04-11 15:38:02,577 :: INFO :: Precision [0.020402743969904845, 0.01695950431511342, 0.014795308696614507]
2023-04-11 15:38:02,577 :: INFO :: Recall [0.008138140049689071, 0.033356121287098514, 0.05566173409863063]
2023-04-11 15:38:02,577 :: INFO :: ndcg [0.020402743969904845, 0.05047927837875038, 0.06776956652225379]
2023-04-11 15:38:24,561 :: INFO :: ----- test -----
2023-04-11 15:38:24,561 :: INFO :: Precision [0.017604672744145424, 0.013450515857851989, 0.010934002947759678]
2023-04-11 15:38:24,561 :: INFO :: Recall [0.01048292131798368, 0.037831439061362285, 0.05917214902612033]
2023-04-11 15:38:24,561 :: INFO :: ndcg [0.017604672744145424, 0.041399599970694534, 0.053381460404276874]
2023-04-11 15:38:31,140 :: INFO :: Epoch 350: loss tensor(48.2953, device='cuda:0', grad_fn=<AddBackward0>), U.norm 252.129638671875, V.norm 369.84716796875, MLP.norm 9.652373313903809
2023-04-11 15:38:52,546 :: INFO :: ----- val -----
2023-04-11 15:38:52,546 :: INFO :: Precision [0.021243637973002875, 0.017163089179021356, 0.014795308696614518]
2023-04-11 15:38:52,546 :: INFO :: Recall [0.008489929159499868, 0.03338891829523863, 0.05568190168397963]
2023-04-11 15:38:52,546 :: INFO :: ndcg [0.021243637973002875, 0.051172681608995046, 0.06822372269647939]
2023-04-11 15:39:14,296 :: INFO :: ----- test -----
2023-04-11 15:39:14,296 :: INFO :: Precision [0.01858725912986517, 0.013657950761503923, 0.011007696926688678]
2023-04-11 15:39:14,296 :: INFO :: Recall [0.01091798846684166, 0.03850112180938759, 0.059800821203552734]
2023-04-11 15:39:14,296 :: INFO :: ndcg [0.01858725912986517, 0.04221303679250242, 0.0541688756119803]
2023-04-11 15:39:20,592 :: INFO :: Epoch 355: loss tensor(49.5667, device='cuda:0', grad_fn=<AddBackward0>), U.norm 253.87484741210938, V.norm 369.5245666503906, MLP.norm 9.571731567382812
2023-04-11 15:39:42,217 :: INFO :: ----- val -----
2023-04-11 15:39:42,217 :: INFO :: Precision [0.021907501659659216, 0.01704801947333426, 0.014755476875415126]
2023-04-11 15:39:42,217 :: INFO :: Recall [0.008800363243602888, 0.03301517602702916, 0.055554489053462074]
2023-04-11 15:39:42,217 :: INFO :: ndcg [0.021907501659659216, 0.051528578731727136, 0.06837804665272451]
2023-04-11 15:40:03,233 :: INFO :: ----- test -----
2023-04-11 15:40:03,233 :: INFO :: Precision [0.01858725912986517, 0.01351056280364597, 0.010983132267045677]
2023-04-11 15:40:03,233 :: INFO :: Recall [0.010973056253876685, 0.037703134936712936, 0.05923918380669527]
2023-04-11 15:40:03,233 :: INFO :: ndcg [0.01858725912986517, 0.041931983179656636, 0.053998996107516475]
2023-04-11 15:40:08,092 :: INFO :: Epoch 360: loss tensor(49.2831, device='cuda:0', grad_fn=<AddBackward0>), U.norm 255.590576171875, V.norm 369.2062072753906, MLP.norm 9.567127227783203
2023-04-11 15:40:28,436 :: INFO :: ----- val -----
2023-04-11 15:40:28,436 :: INFO :: Precision [0.02195175923876964, 0.017145386147377182, 0.014808585970347649]
2023-04-11 15:40:28,436 :: INFO :: Recall [0.008497603882295538, 0.03343223117908141, 0.05551144159709245]
2023-04-11 15:40:28,436 :: INFO :: ndcg [0.02195175923876964, 0.05175521025280357, 0.06892872218553973]
2023-04-11 15:40:48,061 :: INFO :: ----- test -----
2023-04-11 15:40:48,061 :: INFO :: Precision [0.018832905726295104, 0.013466892297613987, 0.011056826245974672]
2023-04-11 15:40:48,061 :: INFO :: Recall [0.010966096643707528, 0.037388510570898544, 0.05955360807640795]
2023-04-11 15:40:48,061 :: INFO :: ndcg [0.018832905726295104, 0.04179130494980673, 0.05416911699070038]
2023-04-11 15:40:53,561 :: INFO :: Epoch 365: loss tensor(51.0200, device='cuda:0', grad_fn=<AddBackward0>), U.norm 257.3426208496094, V.norm 368.7695617675781, MLP.norm 9.605810165405273
2023-04-11 15:41:14,327 :: INFO :: ----- val -----
2023-04-11 15:41:14,327 :: INFO :: Precision [0.021996016817880062, 0.0171896437264876, 0.014866120823191203]
2023-04-11 15:41:14,327 :: INFO :: Recall [0.008416830956284383, 0.033644606324924956, 0.05579396746346382]
2023-04-11 15:41:14,327 :: INFO :: ndcg [0.021996016817880062, 0.05170260457291403, 0.06897612285990207]
2023-04-11 15:41:35,342 :: INFO :: ----- test -----
2023-04-11 15:41:35,342 :: INFO :: Precision [0.018259730334625253, 0.013636115508487931, 0.01105409683934768]
2023-04-11 15:41:35,342 :: INFO :: Recall [0.010684178276740857, 0.03796803677288493, 0.05944221130843672]
2023-04-11 15:41:35,342 :: INFO :: ndcg [0.018259730334625253, 0.04183007752136211, 0.05400432779707603]
2023-04-11 15:41:41,827 :: INFO :: Epoch 370: loss tensor(53.2314, device='cuda:0', grad_fn=<AddBackward0>), U.norm 258.988037109375, V.norm 368.36199951171875, MLP.norm 9.530654907226562
2023-04-11 15:42:03,030 :: INFO :: ----- val -----
2023-04-11 15:42:03,030 :: INFO :: Precision [0.022748395662757246, 0.017225049789775935, 0.0148661208231912]
2023-04-11 15:42:03,030 :: INFO :: Recall [0.008593351959315673, 0.033606799869114974, 0.05595268775716163]
2023-04-11 15:42:03,030 :: INFO :: ndcg [0.022748395662757246, 0.05229366104674439, 0.06923458677626713]
2023-04-11 15:42:24,499 :: INFO :: ----- test -----
2023-04-11 15:42:24,499 :: INFO :: Precision [0.018287024400895247, 0.013603362628963946, 0.011070473279109675]
2023-04-11 15:42:24,499 :: INFO :: Recall [0.010866783161772184, 0.0377806598571113, 0.05972441908193498]
2023-04-11 15:42:24,499 :: INFO :: ndcg [0.018287024400895247, 0.041894412045132466, 0.05419874994484483]
2023-04-11 15:42:31,046 :: INFO :: Epoch 375: loss tensor(54.4489, device='cuda:0', grad_fn=<AddBackward0>), U.norm 260.63525390625, V.norm 368.1255187988281, MLP.norm 9.591556549072266
2023-04-11 15:42:51,889 :: INFO :: ----- val -----
2023-04-11 15:42:51,889 :: INFO :: Precision [0.02212878955521133, 0.017348971011285113, 0.014879398096924337]
2023-04-11 15:42:51,889 :: INFO :: Recall [0.008473240294556265, 0.03385816909151615, 0.0559390241311928]
2023-04-11 15:42:51,889 :: INFO :: ndcg [0.02212878955521133, 0.05221365165593868, 0.0690348511630466]
2023-04-11 15:43:12,780 :: INFO :: ----- test -----
2023-04-11 15:43:12,780 :: INFO :: Precision [0.018778317593755117, 0.013761668213329893, 0.011051367432720687]
2023-04-11 15:43:12,795 :: INFO :: Recall [0.011128188832179435, 0.0385327962284788, 0.05996827677228993]
2023-04-11 15:43:12,795 :: INFO :: ndcg [0.018778317593755117, 0.042485078011035925, 0.054472798425230035]
2023-04-11 15:43:18,467 :: INFO :: Epoch 380: loss tensor(50.2936, device='cuda:0', grad_fn=<AddBackward0>), U.norm 262.3133239746094, V.norm 367.7578430175781, MLP.norm 9.54625129699707
2023-04-11 15:43:39,045 :: INFO :: ----- val -----
2023-04-11 15:43:39,045 :: INFO :: Precision [0.022527107767205134, 0.0174640407169722, 0.014901526886479553]
2023-04-11 15:43:39,045 :: INFO :: Recall [0.008669421753185165, 0.03420602624354506, 0.05605661356549927]
2023-04-11 15:43:39,045 :: INFO :: ndcg [0.022527107767205134, 0.05273704969003284, 0.06924653976775422]
2023-04-11 15:44:00,624 :: INFO :: ----- test -----
2023-04-11 15:44:00,624 :: INFO :: Precision [0.018396200665975217, 0.013816256345869875, 0.011015885146569685]
2023-04-11 15:44:00,624 :: INFO :: Recall [0.010796480704298367, 0.03856063645033063, 0.05950407952572273]
2023-04-11 15:44:00,624 :: INFO :: ndcg [0.018396200665975217, 0.04240349626323993, 0.054235203964671415]
2023-04-11 15:44:06,983 :: INFO :: Epoch 385: loss tensor(50.7848, device='cuda:0', grad_fn=<AddBackward0>), U.norm 263.96661376953125, V.norm 367.4115295410156, MLP.norm 9.554244995117188
2023-04-11 15:44:27,390 :: INFO :: ----- val -----
2023-04-11 15:44:27,390 :: INFO :: Precision [0.0212878955521133, 0.017623368001769713, 0.015038725381721891]
2023-04-11 15:44:27,390 :: INFO :: Recall [0.008341706022725465, 0.034386851371650255, 0.05629029399070192]
2023-04-11 15:44:27,390 :: INFO :: ndcg [0.0212878955521133, 0.05247925379156766, 0.06922351792081712]
2023-04-11 15:44:48,467 :: INFO :: ----- test -----
2023-04-11 15:44:48,467 :: INFO :: Precision [0.018177848135815272, 0.01377258583983789, 0.011114143785141677]
2023-04-11 15:44:48,467 :: INFO :: Recall [0.010914171103254119, 0.038643044956571435, 0.06033872391794872]
2023-04-11 15:44:48,467 :: INFO :: ndcg [0.018177848135815272, 0.04226604911376149, 0.05445628042464704]
2023-04-11 15:44:53,968 :: INFO :: Epoch 390: loss tensor(51.0799, device='cuda:0', grad_fn=<AddBackward0>), U.norm 265.6565246582031, V.norm 367.1252746582031, MLP.norm 9.608142852783203
2023-04-11 15:45:14,952 :: INFO :: ----- val -----
2023-04-11 15:45:14,952 :: INFO :: Precision [0.02080106218189865, 0.01740208010621761, 0.015087408718743376]
2023-04-11 15:45:14,952 :: INFO :: Recall [0.008267784261541735, 0.033940349498700235, 0.056530282555314885]
2023-04-11 15:45:14,952 :: INFO :: ndcg [0.02080106218189865, 0.051803524595012564, 0.06913979739412682]
2023-04-11 15:45:35,561 :: INFO :: ----- test -----
2023-04-11 15:45:35,561 :: INFO :: Precision [0.01850537693105519, 0.013554233309677961, 0.01110868497188767]
2023-04-11 15:45:35,561 :: INFO :: Recall [0.010984444375310013, 0.03784481309516453, 0.060228592506054154]
2023-04-11 15:45:35,561 :: INFO :: ndcg [0.01850537693105519, 0.0418771375855061, 0.05452741300829974]
2023-04-11 15:45:41,921 :: INFO :: Epoch 395: loss tensor(54.4475, device='cuda:0', grad_fn=<AddBackward0>), U.norm 267.2991638183594, V.norm 366.6736755371094, MLP.norm 9.47018051147461
2023-04-11 15:46:03,749 :: INFO :: ----- val -----
2023-04-11 15:46:03,749 :: INFO :: Precision [0.022261562292542596, 0.017534852843548873, 0.015109537508298567]
2023-04-11 15:46:03,749 :: INFO :: Recall [0.008724756276371497, 0.034229363806251537, 0.05632585295561473]
2023-04-11 15:46:03,749 :: INFO :: ndcg [0.022261562292542596, 0.052774324145470536, 0.06988934818860199]
2023-04-11 15:46:25,952 :: INFO :: ----- test -----
2023-04-11 15:46:25,952 :: INFO :: Precision [0.01842349473224521, 0.01355423330967796, 0.01118783776407066]
2023-04-11 15:46:25,952 :: INFO :: Recall [0.010814434493353087, 0.037932474875776036, 0.06067802975506599]
2023-04-11 15:46:25,952 :: INFO :: ndcg [0.01842349473224521, 0.04191071244486753, 0.05461980397942682]
2023-04-11 15:46:32,655 :: INFO :: Epoch 400: loss tensor(54.1930, device='cuda:0', grad_fn=<AddBackward0>), U.norm 268.9515075683594, V.norm 366.3333740234375, MLP.norm 9.522387504577637
2023-04-11 15:46:55,327 :: INFO :: ----- val -----
2023-04-11 15:46:55,327 :: INFO :: Precision [0.02212878955521133, 0.017340119495463022, 0.015131666297853785]
2023-04-11 15:46:55,327 :: INFO :: Recall [0.008645613698830344, 0.033915316142625884, 0.05594597579814338]
2023-04-11 15:46:55,327 :: INFO :: ndcg [0.02212878955521133, 0.052327110155576596, 0.06970421041204226]
2023-04-11 15:47:16,561 :: INFO :: ----- test -----
2023-04-11 15:47:16,561 :: INFO :: Precision [0.01861455319613516, 0.013630656695233933, 0.011174190730935658]
2023-04-11 15:47:16,561 :: INFO :: Recall [0.011174644100451174, 0.038339619821960325, 0.060065949389801615]
2023-04-11 15:47:16,561 :: INFO :: ndcg [0.01861455319613516, 0.04224261704414067, 0.05473233816025544]
2023-04-11 15:47:22,108 :: INFO :: Epoch 405: loss tensor(53.4431, device='cuda:0', grad_fn=<AddBackward0>), U.norm 270.54339599609375, V.norm 366.1209716796875, MLP.norm 9.44996166229248
2023-04-11 15:47:43,171 :: INFO :: ----- val -----
2023-04-11 15:47:43,171 :: INFO :: Precision [0.02164195618499668, 0.01729586191635261, 0.015074131445010226]
2023-04-11 15:47:43,171 :: INFO :: Recall [0.00864962905841365, 0.033737584829161234, 0.0560231615577868]
2023-04-11 15:47:43,171 :: INFO :: ndcg [0.02164195618499668, 0.051957103878207386, 0.06943238399697142]
2023-04-11 15:48:03,827 :: INFO :: ----- test -----
2023-04-11 15:48:03,827 :: INFO :: Precision [0.01896937605764507, 0.013690703641027912, 0.011125061411649678]
2023-04-11 15:48:03,827 :: INFO :: Recall [0.011476365324438895, 0.03849271078998388, 0.06046485623741248]
2023-04-11 15:48:03,827 :: INFO :: ndcg [0.01896937605764507, 0.042710192804758094, 0.05499220637905378]
2023-04-11 15:48:09,905 :: INFO :: Epoch 410: loss tensor(50.1055, device='cuda:0', grad_fn=<AddBackward0>), U.norm 272.1696472167969, V.norm 365.95391845703125, MLP.norm 9.45373821258545
2023-04-11 15:48:31,905 :: INFO :: ----- val -----
2023-04-11 15:48:31,905 :: INFO :: Precision [0.020668289444567382, 0.017287010400530524, 0.015021022350077717]
2023-04-11 15:48:31,905 :: INFO :: Recall [0.008311783253870003, 0.0339730153973564, 0.05616817846033328]
2023-04-11 15:48:31,905 :: INFO :: ndcg [0.020668289444567382, 0.051624667518682714, 0.06898566594679471]
2023-04-11 15:48:53,702 :: INFO :: ----- test -----
2023-04-11 15:48:53,702 :: INFO :: Precision [0.018259730334625253, 0.013625197881979934, 0.01112233200502266]
2023-04-11 15:48:53,702 :: INFO :: Recall [0.011002309392970325, 0.037974208853214036, 0.06030619946778201]
2023-04-11 15:48:53,702 :: INFO :: ndcg [0.018259730334625253, 0.04213651935259213, 0.05447618828483242]
2023-04-11 15:49:00,514 :: INFO :: Epoch 415: loss tensor(49.9097, device='cuda:0', grad_fn=<AddBackward0>), U.norm 273.81085205078125, V.norm 365.6802062988281, MLP.norm 9.495132446289062
2023-04-11 15:49:22,765 :: INFO :: ----- val -----
2023-04-11 15:49:22,765 :: INFO :: Precision [0.0212878955521133, 0.017295861916352608, 0.014883823854835375]
2023-04-11 15:49:22,765 :: INFO :: Recall [0.00893812423365695, 0.03394412851228595, 0.05586847193139972]
2023-04-11 15:49:22,765 :: INFO :: ndcg [0.0212878955521133, 0.05176513477819997, 0.06871715858075518]
2023-04-11 15:49:44,749 :: INFO :: ----- test -----
2023-04-11 15:49:44,749 :: INFO :: Precision [0.017140673617555544, 0.013510562803645971, 0.011081390905617675]
2023-04-11 15:49:44,749 :: INFO :: Recall [0.010498779759435469, 0.03803733407108414, 0.05995460989350498]
2023-04-11 15:49:44,749 :: INFO :: ndcg [0.017140673617555544, 0.041393350360740495, 0.0539121403658746]
2023-04-11 15:49:51,124 :: INFO :: Epoch 420: loss tensor(52.0961, device='cuda:0', grad_fn=<AddBackward0>), U.norm 275.4289855957031, V.norm 365.3628234863281, MLP.norm 9.522089004516602
2023-04-11 15:50:13,202 :: INFO :: ----- val -----
2023-04-11 15:50:13,202 :: INFO :: Precision [0.019738880283248505, 0.01734897101128511, 0.014945784465589971]
2023-04-11 15:50:13,202 :: INFO :: Recall [0.008199652843345881, 0.034257911531449865, 0.05575903202911524]
2023-04-11 15:50:13,202 :: INFO :: ndcg [0.019738880283248505, 0.051398655707917934, 0.06844590862388801]
2023-04-11 15:50:34,905 :: INFO :: ----- test -----
2023-04-11 15:50:34,905 :: INFO :: Precision [0.01700420328620558, 0.013810797532615875, 0.011141437851411666]
2023-04-11 15:50:34,905 :: INFO :: Recall [0.01023080297689468, 0.03849708703269572, 0.06062340438873615]
2023-04-11 15:50:34,905 :: INFO :: ndcg [0.01700420328620558, 0.04212744966437043, 0.05429222839950849]
2023-04-11 15:50:41,218 :: INFO :: Epoch 425: loss tensor(47.3486, device='cuda:0', grad_fn=<AddBackward0>), U.norm 277.0647277832031, V.norm 364.99859619140625, MLP.norm 9.523869514465332
2023-04-11 15:51:03,092 :: INFO :: ----- val -----
2023-04-11 15:51:03,092 :: INFO :: Precision [0.01978313786235893, 0.01752600132772679, 0.014892675370657435]
2023-04-11 15:51:03,092 :: INFO :: Recall [0.008261210962204547, 0.03461607200683602, 0.055893822590811874]
2023-04-11 15:51:03,092 :: INFO :: ndcg [0.01978313786235893, 0.05189370063473633, 0.06834662399206477]
2023-04-11 15:51:25,061 :: INFO :: ----- test -----
2023-04-11 15:51:25,061 :: INFO :: Precision [0.016976909219935585, 0.013696162454281914, 0.011291555215896653]
2023-04-11 15:51:25,061 :: INFO :: Recall [0.010206162500400935, 0.038389012832253384, 0.0615901423670116]
2023-04-11 15:51:25,061 :: INFO :: ndcg [0.016976909219935585, 0.041755934297387574, 0.054588190388368416]
2023-04-11 15:51:31,327 :: INFO :: Epoch 430: loss tensor(49.3568, device='cuda:0', grad_fn=<AddBackward0>), U.norm 278.6853332519531, V.norm 364.7541198730469, MLP.norm 9.486503601074219
2023-04-11 15:51:53,139 :: INFO :: ----- val -----
2023-04-11 15:51:53,139 :: INFO :: Precision [0.020668289444567382, 0.017410931622039702, 0.014888249612746403]
2023-04-11 15:51:53,139 :: INFO :: Recall [0.008596656908405088, 0.03406417739055612, 0.05533341379724031]
2023-04-11 15:51:53,139 :: INFO :: ndcg [0.020668289444567382, 0.05186115402485727, 0.06859194440370045]
2023-04-11 15:52:14,999 :: INFO :: ----- test -----
2023-04-11 15:52:14,999 :: INFO :: Precision [0.017550084611605437, 0.013723456520551904, 0.011187837764070657]
2023-04-11 15:52:14,999 :: INFO :: Recall [0.010460655431213531, 0.03829199599757228, 0.06074144366958027]
2023-04-11 15:52:14,999 :: INFO :: ndcg [0.017550084611605437, 0.042182169873249534, 0.05463619763831691]
2023-04-11 15:52:21,202 :: INFO :: Epoch 435: loss tensor(48.2207, device='cuda:0', grad_fn=<AddBackward0>), U.norm 280.2718811035156, V.norm 364.6578369140625, MLP.norm 9.42332935333252
2023-04-11 15:52:43,124 :: INFO :: ----- val -----
2023-04-11 15:52:43,124 :: INFO :: Precision [0.02062403186545696, 0.017543704359370957, 0.014998893560522485]
2023-04-11 15:52:43,124 :: INFO :: Recall [0.008382328258028636, 0.03480763435057419, 0.05543178119312401]
2023-04-11 15:52:43,124 :: INFO :: ndcg [0.02062403186545696, 0.052192066625960314, 0.0688215021777281]
2023-04-11 15:53:04,655 :: INFO :: ----- test -----
2023-04-11 15:53:04,655 :: INFO :: Precision [0.017140673617555544, 0.013919973797695839, 0.011215131830340666]
2023-04-11 15:53:04,655 :: INFO :: Recall [0.010091041012660857, 0.038663922883537136, 0.0610532753020011]
2023-04-11 15:53:04,655 :: INFO :: ndcg [0.017140673617555544, 0.04234724674040202, 0.054649135026532215]
2023-04-11 15:53:10,686 :: INFO :: Epoch 440: loss tensor(49.9722, device='cuda:0', grad_fn=<AddBackward0>), U.norm 281.8433837890625, V.norm 364.51239013671875, MLP.norm 9.509734153747559
2023-04-11 15:53:32,889 :: INFO :: ----- val -----
2023-04-11 15:53:32,889 :: INFO :: Precision [0.02093383491922992, 0.017649922549235968, 0.01498561628678935]
2023-04-11 15:53:32,889 :: INFO :: Recall [0.008657550646550428, 0.03430788025918051, 0.05567691349248957]
2023-04-11 15:53:32,889 :: INFO :: ndcg [0.02093383491922992, 0.05215229276371497, 0.06874725400633604]
2023-04-11 15:53:54,452 :: INFO :: ----- test -----
2023-04-11 15:53:54,452 :: INFO :: Precision [0.017904907473115343, 0.013745291773567899, 0.011185108357443662]
2023-04-11 15:53:54,452 :: INFO :: Recall [0.010486416724647734, 0.038119389306580626, 0.06013484042446878]
2023-04-11 15:53:54,452 :: INFO :: ndcg [0.017904907473115343, 0.04227152655538332, 0.054764016767180845]
2023-04-11 15:53:59,921 :: INFO :: Epoch 445: loss tensor(50.6044, device='cuda:0', grad_fn=<AddBackward0>), U.norm 283.45989990234375, V.norm 364.31231689453125, MLP.norm 9.536885261535645
2023-04-11 15:54:20,363 :: INFO :: ----- val -----
2023-04-11 15:54:20,363 :: INFO :: Precision [0.021509183447665413, 0.01760566497012555, 0.015065279929188134]
2023-04-11 15:54:20,363 :: INFO :: Recall [0.008744861252583025, 0.034128354025276024, 0.05604752633081538]
2023-04-11 15:54:20,363 :: INFO :: ndcg [0.021509183447665413, 0.05263035212367239, 0.06952655795413619]
2023-04-11 15:54:40,722 :: INFO :: ----- test -----
2023-04-11 15:54:40,722 :: INFO :: Precision [0.018751023527485124, 0.013788962279599883, 0.01130520224903165]
2023-04-11 15:54:40,722 :: INFO :: Recall [0.01095275602430217, 0.038500849656464144, 0.06108530125640519]
2023-04-11 15:54:40,722 :: INFO :: ndcg [0.018751023527485124, 0.04279263184787828, 0.05552148326945005]
2023-04-11 15:54:45,941 :: INFO :: Epoch 450: loss tensor(51.5865, device='cuda:0', grad_fn=<AddBackward0>), U.norm 284.995361328125, V.norm 364.1639404296875, MLP.norm 9.288921356201172
2023-04-11 15:55:05,722 :: INFO :: ----- val -----
2023-04-11 15:55:05,722 :: INFO :: Precision [0.0212878955521133, 0.017711883159990553, 0.01517149811905317]
2023-04-11 15:55:05,722 :: INFO :: Recall [0.00853524187036812, 0.03451745015603977, 0.05682437438253616]
2023-04-11 15:55:05,722 :: INFO :: ndcg [0.0212878955521133, 0.052487245081530944, 0.06971647626730747]
2023-04-11 15:55:25,691 :: INFO :: ----- test -----
2023-04-11 15:55:25,691 :: INFO :: Precision [0.01823243626835526, 0.01385992685190186, 0.011228778863475654]
2023-04-11 15:55:25,691 :: INFO :: Recall [0.010458114037938901, 0.03868135647385124, 0.06088441082153278]
2023-04-11 15:55:25,691 :: INFO :: ndcg [0.01823243626835526, 0.04271748127935257, 0.05514614117404551]
2023-04-11 15:55:30,628 :: INFO :: Epoch 455: loss tensor(48.5644, device='cuda:0', grad_fn=<AddBackward0>), U.norm 286.5619812011719, V.norm 363.925048828125, MLP.norm 9.360671997070312
2023-04-11 15:55:51,159 :: INFO :: ----- val -----
2023-04-11 15:55:51,159 :: INFO :: Precision [0.02093383491922992, 0.017782695286567222, 0.014976764770967281]
2023-04-11 15:55:51,159 :: INFO :: Recall [0.008503297222357969, 0.034857704284405946, 0.05596385559719252]
2023-04-11 15:55:51,159 :: INFO :: ndcg [0.02093383491922992, 0.052444622371567926, 0.0689495734014875]
2023-04-11 15:56:11,300 :: INFO :: ----- test -----
2023-04-11 15:56:11,300 :: INFO :: Precision [0.017631966810415414, 0.013745291773567897, 0.01123696708335665]
2023-04-11 15:56:11,300 :: INFO :: Recall [0.010084922924870298, 0.038467220916850016, 0.061015738067640164]
2023-04-11 15:56:11,300 :: INFO :: ndcg [0.017631966810415414, 0.042248874166884755, 0.05483477546298459]
2023-04-11 15:56:16,550 :: INFO :: Epoch 460: loss tensor(51.6028, device='cuda:0', grad_fn=<AddBackward0>), U.norm 288.1144104003906, V.norm 363.78033447265625, MLP.norm 9.323652267456055
2023-04-11 15:56:36,363 :: INFO :: ----- val -----
2023-04-11 15:56:36,363 :: INFO :: Precision [0.02062403186545696, 0.017605664970125542, 0.014901526886479532]
2023-04-11 15:56:36,363 :: INFO :: Recall [0.008200237359465767, 0.0345896031297395, 0.05559783150355333]
2023-04-11 15:56:36,363 :: INFO :: ndcg [0.02062403186545696, 0.05214678588325848, 0.06866510047229615]
2023-04-11 15:56:56,707 :: INFO :: ----- test -----
2023-04-11 15:56:56,707 :: INFO :: Precision [0.017058791418745566, 0.013838091598885867, 0.01123150827010265]
2023-04-11 15:56:56,707 :: INFO :: Recall [0.009979375037647651, 0.0386330426247865, 0.06096259266052665]
2023-04-11 15:56:56,707 :: INFO :: ndcg [0.017058791418745566, 0.04222051677147744, 0.05469771024098185]
2023-04-11 15:57:01,987 :: INFO :: Epoch 465: loss tensor(51.2604, device='cuda:0', grad_fn=<AddBackward0>), U.norm 289.69207763671875, V.norm 363.5775451660156, MLP.norm 9.284123420715332
2023-04-11 15:57:22,597 :: INFO :: ----- val -----
2023-04-11 15:57:22,597 :: INFO :: Precision [0.02097809249834034, 0.017800398318211397, 0.015060854171277067]
2023-04-11 15:57:22,597 :: INFO :: Recall [0.008563418450189047, 0.03495706804228894, 0.0555771702339452]
2023-04-11 15:57:22,597 :: INFO :: ndcg [0.02097809249834034, 0.05262735440024395, 0.06898155885127984]
2023-04-11 15:57:43,300 :: INFO :: ----- test -----
2023-04-11 15:57:43,300 :: INFO :: Precision [0.01757737867787543, 0.013930891424203838, 0.011144167258038653]
2023-04-11 15:57:43,300 :: INFO :: Recall [0.010424188183754535, 0.039102594332511544, 0.05991779990734631]
2023-04-11 15:57:43,300 :: INFO :: ndcg [0.01757737867787543, 0.04272789632803832, 0.054790517351960354]
2023-04-11 15:57:48,362 :: INFO :: Epoch 470: loss tensor(50.7749, device='cuda:0', grad_fn=<AddBackward0>), U.norm 291.2275390625, V.norm 363.4023742675781, MLP.norm 9.382354736328125
2023-04-11 15:58:08,847 :: INFO :: ----- val -----
2023-04-11 15:58:08,847 :: INFO :: Precision [0.020402743969904845, 0.01769418012834639, 0.015175923876964193]
2023-04-11 15:58:08,847 :: INFO :: Recall [0.008309251326417745, 0.03479560646226746, 0.05635160106940329]
2023-04-11 15:58:08,847 :: INFO :: ndcg [0.020402743969904845, 0.052062610318211924, 0.06907759575601947]
2023-04-11 15:58:29,769 :: INFO :: ----- test -----
2023-04-11 15:58:29,769 :: INFO :: Precision [0.01700420328620558, 0.01400731480975982, 0.011258802336372643]
2023-04-11 15:58:29,769 :: INFO :: Recall [0.010022005769726498, 0.039728638232858736, 0.060690141690832734]
2023-04-11 15:58:29,769 :: INFO :: ndcg [0.01700420328620558, 0.04267003686466728, 0.054888928566393186]
2023-04-11 15:58:35,097 :: INFO :: Epoch 475: loss tensor(49.0042, device='cuda:0', grad_fn=<AddBackward0>), U.norm 292.7118225097656, V.norm 363.2809143066406, MLP.norm 9.319555282592773
2023-04-11 15:58:55,722 :: INFO :: ----- val -----
2023-04-11 15:58:55,722 :: INFO :: Precision [0.021155122814782032, 0.01745518920115012, 0.015224607213985667]
2023-04-11 15:58:55,722 :: INFO :: Recall [0.008666229725961866, 0.0341642456031431, 0.05661911352592131]
2023-04-11 15:58:55,722 :: INFO :: ndcg [0.021155122814782032, 0.05199203265132973, 0.0694556091314053]
2023-04-11 15:59:16,550 :: INFO :: ----- test -----
2023-04-11 15:59:16,550 :: INFO :: Precision [0.017440908346525466, 0.013876303291663854, 0.011313390468912646]
2023-04-11 15:59:16,550 :: INFO :: Recall [0.010209196589593773, 0.03923726703307493, 0.061065488542996225]
2023-04-11 15:59:16,550 :: INFO :: ndcg [0.017440908346525466, 0.0425218259857848, 0.05501994780721307]
2023-04-11 15:59:21,722 :: INFO :: Epoch 480: loss tensor(50.8570, device='cuda:0', grad_fn=<AddBackward0>), U.norm 294.1730651855469, V.norm 363.1572570800781, MLP.norm 9.327128410339355
2023-04-11 15:59:42,034 :: INFO :: ----- val -----
2023-04-11 15:59:42,034 :: INFO :: Precision [0.020668289444567382, 0.017384377074573448, 0.015206904182341491]
2023-04-11 15:59:42,034 :: INFO :: Recall [0.008340430092240125, 0.03414285651851921, 0.0562436932353633]
2023-04-11 15:59:42,034 :: INFO :: ndcg [0.020668289444567382, 0.05161658538173563, 0.06901222165568942]
2023-04-11 16:00:02,269 :: INFO :: ----- test -----
2023-04-11 16:00:02,269 :: INFO :: Precision [0.01746820241279546, 0.013881762104917855, 0.01136251978819864]
2023-04-11 16:00:02,269 :: INFO :: Recall [0.010299861727477643, 0.03912226972881677, 0.06104410522997134]
2023-04-11 16:00:02,269 :: INFO :: ndcg [0.01746820241279546, 0.04255102690109648, 0.055264998979113175]
2023-04-11 16:00:07,378 :: INFO :: Epoch 485: loss tensor(50.8426, device='cuda:0', grad_fn=<AddBackward0>), U.norm 295.6375427246094, V.norm 363.0789794921875, MLP.norm 9.281221389770508
2023-04-11 16:00:27,567 :: INFO :: ----- val -----
2023-04-11 16:00:27,567 :: INFO :: Precision [0.020535516707236114, 0.017534852843548877, 0.015215755698163585]
2023-04-11 16:00:27,567 :: INFO :: Recall [0.0080233785655214, 0.033888720108402146, 0.056560582531701785]
2023-04-11 16:00:27,567 :: INFO :: ndcg [0.020535516707236114, 0.05193826179251067, 0.06929803131311203]
2023-04-11 16:00:48,300 :: INFO :: ----- test -----
2023-04-11 16:00:48,300 :: INFO :: Precision [0.017277143948905508, 0.013936350237457837, 0.011337955128555639]
2023-04-11 16:00:48,300 :: INFO :: Recall [0.010316660575408105, 0.039211390994978376, 0.06139052294096483]
2023-04-11 16:00:48,300 :: INFO :: ndcg [0.017277143948905508, 0.042687394537949196, 0.05521150399804096]
2023-04-11 16:00:53,737 :: INFO :: Epoch 490: loss tensor(49.6209, device='cuda:0', grad_fn=<AddBackward0>), U.norm 297.1181945800781, V.norm 362.99310302734375, MLP.norm 9.330217361450195
2023-04-11 16:01:13,628 :: INFO :: ----- val -----
2023-04-11 16:01:13,628 :: INFO :: Precision [0.02177472892232795, 0.017526001327726793, 0.015127240539942723]
2023-04-11 16:01:13,628 :: INFO :: Recall [0.008648495253702057, 0.03405725255367724, 0.05586981265187065]
2023-04-11 16:01:13,628 :: INFO :: ndcg [0.02177472892232795, 0.05232663846297481, 0.06919999051147979]
2023-04-11 16:01:33,565 :: INFO :: ----- test -----
2023-04-11 16:01:33,565 :: INFO :: Precision [0.01795949560565533, 0.013936350237457837, 0.011297014029150643]
2023-04-11 16:01:33,565 :: INFO :: Recall [0.01033455764433764, 0.039054843199146566, 0.06079917548724851]
2023-04-11 16:01:33,565 :: INFO :: ndcg [0.01795949560565533, 0.04279093777779296, 0.05521871376880081]
2023-04-11 16:01:38,425 :: INFO :: Epoch 495: loss tensor(52.1188, device='cuda:0', grad_fn=<AddBackward0>), U.norm 298.6814270019531, V.norm 362.8705139160156, MLP.norm 9.334085464477539
2023-04-11 16:01:59,128 :: INFO :: ----- val -----
2023-04-11 16:01:59,128 :: INFO :: Precision [0.02177472892232795, 0.0176322195175918, 0.015361805709227988]
2023-04-11 16:01:59,128 :: INFO :: Recall [0.008750787528541977, 0.03428672411830063, 0.056499456007047213]
2023-04-11 16:01:59,128 :: INFO :: ndcg [0.02177472892232795, 0.052827319377832714, 0.06991039175618605]
2023-04-11 16:02:20,253 :: INFO :: ----- test -----
2023-04-11 16:02:20,253 :: INFO :: Precision [0.017522790545335443, 0.013925432610949839, 0.011307931655658646]
2023-04-11 16:02:20,253 :: INFO :: Recall [0.010045583142832864, 0.03873661302311907, 0.06081531118256472]
2023-04-11 16:02:20,253 :: INFO :: ndcg [0.017522790545335443, 0.04276366239633595, 0.055208869216186396]
2023-04-11 16:02:24,316 :: INFO :: Epoch 500:
2023-04-11 16:02:44,378 :: INFO :: ----- val -----
2023-04-11 16:02:44,378 :: INFO :: Precision [0.021730471343217525, 0.01751714981190471, 0.015242310245629847]
2023-04-11 16:02:44,378 :: INFO :: Recall [0.00869414532814075, 0.034275888709278605, 0.056739175826377525]
2023-04-11 16:02:44,378 :: INFO :: ndcg [0.021730471343217525, 0.05249024931373722, 0.06962181632351477]
2023-04-11 16:03:04,691 :: INFO :: ----- test -----
2023-04-11 16:03:04,691 :: INFO :: Precision [0.018259730334625253, 0.013838091598885865, 0.01139254326109564]
2023-04-11 16:03:04,691 :: INFO :: Recall [0.010757864784032135, 0.038952670509135964, 0.06139301730280571]
2023-04-11 16:03:04,691 :: INFO :: ndcg [0.018259730334625253, 0.04278108367545381, 0.05556828820914314]
2023-04-11 16:03:04,691 :: INFO :: final:
2023-04-11 16:03:04,691 :: INFO :: ----- test -----
2023-04-11 16:03:04,691 :: INFO :: Precision [0.017522790545335443, 0.013925432610949839, 0.011307931655658646]
2023-04-11 16:03:04,691 :: INFO :: Recall [0.010045583142832864, 0.03873661302311907, 0.06081531118256472]
2023-04-11 16:03:04,691 :: INFO :: ndcg [0.017522790545335443, 0.04276366239633595, 0.055208869216186396]
2023-04-11 16:03:04,691 :: INFO :: max_epoch 495:
2023-04-23 16:09:55,071 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:09:55,071 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:09:56,896 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:27:31,567 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:27:31,568 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:27:33,692 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:28:42,541 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:28:42,541 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:28:44,390 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:28:52,539 :: INFO :: ERM mask: from pre-train <numpy.lib.npyio.NpzFile object at 0x7fdc9fc068e0>
2023-04-23 16:35:34,335 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:35:34,336 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:35:36,614 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:35:46,765 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 16:36:30,139 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:36:30,140 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:36:32,400 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:36:41,359 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 16:53:42,839 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:53:42,840 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:53:44,856 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:53:52,938 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 17:00:40,830 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 17:00:40,831 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 17:00:43,015 :: INFO :: torch.Size([76085, 384])
2023-04-23 17:00:51,618 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
