2023-05-09 10:53:25,429 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 10:53:25,429 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:02:20,820 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:02:20,820 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:03:01,289 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:03:01,289 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:03:02,867 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:04:10,132 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:04:10,132 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:04:11,726 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:07:35,351 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:07:35,351 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:07:36,835 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:15:08,679 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:15:08,679 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:15:10,163 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:16:08,038 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:16:08,038 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:16:09,570 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:17:28,945 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:17:28,945 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:17:30,429 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:18:10,319 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:18:10,319 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:18:11,820 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:31:20,710 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:31:20,710 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:31:22,444 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:41:30,054 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:41:30,054 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:41:31,663 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:42:10,023 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:42:10,023 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:42:11,507 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:44:26,992 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:44:26,992 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:44:28,585 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:46:08,226 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:46:08,226 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:46:09,804 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:49:46,538 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:49:46,538 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:49:48,101 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:50:08,320 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:05:09,648 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:05:09,648 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:05:11,273 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:05:31,867 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:14:47,726 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:14:47,726 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:14:49,288 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:15:10,054 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:17:04,726 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:17:04,726 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:17:06,273 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:17:26,617 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:30:39,867 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:30:39,867 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:30:41,507 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:31:01,804 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:31:21,913 :: INFO :: Epoch 5: loss tensor(0.9275, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.3382109999656677, V.norm 0.34504234790802, MLP.norm 0.024449408054351807
2023-05-09 12:31:44,288 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:31:44,288 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:31:45,819 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:32:05,429 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:32:23,804 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.011279151774942875, V.norm 0.011505632661283016, MLP.norm 0.0008211754029616714
2023-05-09 12:32:44,882 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:32:44,882 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-09 12:32:46,445 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:33:06,913 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:35:14,538 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:35:14,538 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:35:16,148 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:35:36,069 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:35:53,804 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.009770270437002182, V.norm 0.011513358913362026, MLP.norm 0.0005819871439598501
2023-05-09 12:36:12,819 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:36:12,819 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:36:14,398 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:36:34,398 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:42:53,976 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:42:53,976 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-09 12:42:55,570 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:43:15,835 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:43:33,585 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.009770270437002182, V.norm 0.011513358913362026, MLP.norm 0.0005819871439598501
2023-05-09 12:44:28,445 :: INFO :: ----- val -----
2023-05-09 12:44:28,445 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:44:28,445 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:44:28,445 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 12:45:23,694 :: INFO :: ----- test -----
2023-05-09 12:45:23,694 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 5.458813253998581e-06]
2023-05-09 12:45:23,694 :: INFO :: Recall [0.0, 5.458813253998581e-05, 5.458813253998581e-05]
2023-05-09 12:45:23,694 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 2.1117572314784737e-05]
2023-05-09 12:45:29,585 :: INFO :: Epoch 10: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.462642441969365e-05, V.norm 2.9048382202745415e-05, MLP.norm 1.4902383327353164e-06
2023-05-09 12:46:25,070 :: INFO :: ----- val -----
2023-05-09 12:46:25,070 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:46:25,070 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:46:25,070 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 12:47:19,679 :: INFO :: ----- test -----
2023-05-09 12:47:19,679 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 12:47:19,679 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 12:47:19,679 :: INFO :: ndcg [0.0, 0.0, 9.098022089997634e-06]
2023-05-09 12:47:25,445 :: INFO :: Epoch 15: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 6.051975987020342e-08, V.norm 7.146434910509925e-08, MLP.norm 3.6609961728828466e-09
2023-05-09 12:48:19,961 :: INFO :: ----- val -----
2023-05-09 12:48:19,961 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:48:19,961 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:48:19,961 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:49:14,194 :: INFO :: ----- test -----
2023-05-09 12:49:14,194 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 12:49:14,194 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 12:49:14,194 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.08676716406861e-05]
2023-05-09 12:49:19,898 :: INFO :: Epoch 20: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.4722492669427822e-10, V.norm 1.73727879304586e-10, MLP.norm 8.995852673887583e-12
2023-05-09 12:50:14,163 :: INFO :: ----- val -----
2023-05-09 12:50:14,163 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:50:14,163 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:50:14,163 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 12:51:10,366 :: INFO :: ----- test -----
2023-05-09 12:51:10,366 :: INFO :: Precision [0.0, 5.458813253998581e-06, 2.7294066269992907e-06]
2023-05-09 12:51:10,366 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 2.7294066269992904e-05]
2023-05-09 12:51:10,366 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.0558786157392369e-05]
2023-05-09 12:51:16,210 :: INFO :: Epoch 25: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 3.550328976122119e-13, V.norm 4.2008388954906106e-13, MLP.norm 2.159724798360494e-14
2023-05-09 12:52:08,445 :: INFO :: ----- val -----
2023-05-09 12:52:08,445 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:52:08,445 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:52:08,445 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 12:53:01,085 :: INFO :: ----- test -----
2023-05-09 12:53:01,085 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-09 12:53:01,085 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-09 12:53:01,085 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 4.9397789354581314e-05]
2023-05-09 12:53:06,945 :: INFO :: Epoch 30: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 8.551658929160401e-16, V.norm 1.0109702449647394e-15, MLP.norm 5.1804488731958706e-17
2023-05-09 12:54:00,569 :: INFO :: ----- val -----
2023-05-09 12:54:00,569 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:54:00,569 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:54:00,569 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 12:54:54,835 :: INFO :: ----- test -----
2023-05-09 12:54:54,835 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.3647033134996452e-05]
2023-05-09 12:54:54,835 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00013647033134996453]
2023-05-09 12:54:54,835 :: INFO :: ndcg [0.0, 3.596073380937646e-05, 5.3899409031058956e-05]
2023-05-09 12:55:00,744 :: INFO :: Epoch 35: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.049087263922991e-18, V.norm 2.4256051963012565e-18, MLP.norm 1.236366440561638e-19
2023-05-09 12:55:57,085 :: INFO :: ----- val -----
2023-05-09 12:55:57,085 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:55:57,085 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:55:57,085 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:56:51,744 :: INFO :: ----- test -----
2023-05-09 12:56:51,744 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 12:56:51,744 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 12:56:51,744 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 3.6392088359990535e-05]
2023-05-09 12:56:57,572 :: INFO :: Epoch 40: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.6740958743820435e-22, V.norm 1.9087629348058244e-22, MLP.norm 3.743392066509216e-23
2023-05-09 12:57:53,337 :: INFO :: ----- val -----
2023-05-09 12:57:53,337 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:57:53,337 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:57:53,337 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:58:48,228 :: INFO :: ----- test -----
2023-05-09 12:58:48,228 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 12:58:48,228 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 12:58:48,228 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 12:58:54,181 :: INFO :: Epoch 45: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-09 12:59:48,541 :: INFO :: ----- val -----
2023-05-09 12:59:48,541 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:59:48,541 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:59:48,541 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:00:42,384 :: INFO :: ----- test -----
2023-05-09 13:00:42,384 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:00:42,384 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:00:42,384 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:00:48,058 :: INFO :: Epoch 50: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-09 13:01:42,167 :: INFO :: ----- val -----
2023-05-09 13:01:42,167 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:01:42,167 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:01:42,167 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:02:37,761 :: INFO :: ----- test -----
2023-05-09 13:02:37,761 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:02:37,761 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:02:37,761 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:02:43,636 :: INFO :: Epoch 55: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 9.061725165810525e-14
2023-05-09 13:03:41,591 :: INFO :: ----- val -----
2023-05-09 13:03:41,591 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:03:41,591 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:03:41,591 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:04:38,591 :: INFO :: ----- test -----
2023-05-09 13:04:38,591 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:04:38,591 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:04:38,591 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:04:44,435 :: INFO :: Epoch 60: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011874802876263857
2023-05-09 13:05:38,748 :: INFO :: ----- val -----
2023-05-09 13:05:38,748 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:05:38,748 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:05:38,748 :: INFO :: ndcg [0.0, 0.0, 2.5586618837609017e-05]
2023-05-09 13:06:32,484 :: INFO :: ----- test -----
2023-05-09 13:06:32,484 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.637643976199574e-05]
2023-05-09 13:06:32,484 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.0001637643976199574]
2023-05-09 13:06:32,484 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 5.458326008491455e-05]
2023-05-09 13:06:38,343 :: INFO :: Epoch 65: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011640415759757161
2023-05-09 13:07:32,078 :: INFO :: ----- val -----
2023-05-09 13:07:32,078 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:07:32,078 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:07:32,078 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-09 13:08:27,156 :: INFO :: ----- test -----
2023-05-09 13:08:27,156 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:08:27,156 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:08:27,156 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:08:33,109 :: INFO :: Epoch 70: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011022815015166998
2023-05-09 13:09:27,531 :: INFO :: ----- val -----
2023-05-09 13:09:27,531 :: INFO :: Precision [0.0, 8.851515822084533e-06, 1.32772737331268e-05]
2023-05-09 13:09:27,531 :: INFO :: Recall [0.0, 4.425757911042266e-05, 0.00013277273733126798]
2023-05-09 13:09:27,531 :: INFO :: ndcg [0.0, 1.7121168720271814e-05, 4.323733698678516e-05]
2023-05-09 13:10:23,000 :: INFO :: ----- test -----
2023-05-09 13:10:23,000 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.637643976199574e-05]
2023-05-09 13:10:23,000 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.0001637643976199574]
2023-05-09 13:10:23,000 :: INFO :: ndcg [0.0, 4.5000063168771676e-05, 7.291844991954106e-05]
2023-05-09 13:10:28,968 :: INFO :: Epoch 75: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012304242700338364
2023-05-09 13:11:23,625 :: INFO :: ----- val -----
2023-05-09 13:11:23,625 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:11:23,625 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:11:23,625 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 13:12:18,172 :: INFO :: ----- test -----
2023-05-09 13:12:18,172 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 2.456465964299361e-05]
2023-05-09 13:12:18,172 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.0002456465964299361]
2023-05-09 13:12:18,172 :: INFO :: ndcg [0.0, 3.596073380937646e-05, 9.174096009636299e-05]
2023-05-09 13:12:23,999 :: INFO :: Epoch 80: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001238734694197774
2023-05-09 13:13:19,093 :: INFO :: ----- val -----
2023-05-09 13:13:19,093 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:13:19,093 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:13:19,093 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:14:12,719 :: INFO :: ----- test -----
2023-05-09 13:14:12,719 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 13:14:12,719 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 13:14:12,719 :: INFO :: ndcg [0.0, 0.0, 1.833266182361895e-05]
2023-05-09 13:14:18,360 :: INFO :: Epoch 85: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012542895274236798
2023-05-09 13:15:10,594 :: INFO :: ----- val -----
2023-05-09 13:15:10,594 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:15:10,594 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:15:10,594 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:16:03,213 :: INFO :: ----- test -----
2023-05-09 13:16:03,213 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:16:03,213 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:16:03,213 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.5110393031534075e-05]
2023-05-09 13:16:08,838 :: INFO :: Epoch 90: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0016105335671454668
2023-05-09 13:17:03,260 :: INFO :: ----- val -----
2023-05-09 13:17:03,260 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:17:03,260 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:17:03,260 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 13:17:57,526 :: INFO :: ----- test -----
2023-05-09 13:17:57,526 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:17:57,526 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 13:17:57,526 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 3.9089354184380295e-05]
2023-05-09 13:18:03,401 :: INFO :: Epoch 95: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011949132895097136
2023-05-09 13:18:58,807 :: INFO :: ----- val -----
2023-05-09 13:18:58,807 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:18:58,807 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:18:58,807 :: INFO :: ndcg [0.0, 0.0, 2.5586618837609017e-05]
2023-05-09 13:19:54,741 :: INFO :: ----- test -----
2023-05-09 13:19:54,741 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.3647033134996452e-05]
2023-05-09 13:19:54,741 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00013647033134996453]
2023-05-09 13:19:54,741 :: INFO :: ndcg [0.0, 4.619619152836695e-05, 5.15912573535344e-05]
2023-05-09 13:20:00,756 :: INFO :: Epoch 100: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001250851433724165
2023-05-09 13:20:55,943 :: INFO :: ----- val -----
2023-05-09 13:20:55,943 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:20:55,943 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:20:55,943 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:21:50,444 :: INFO :: ----- test -----
2023-05-09 13:21:50,444 :: INFO :: Precision [0.0, 2.1835253015994326e-05, 2.1835253015994322e-05]
2023-05-09 13:21:50,444 :: INFO :: Recall [0.0, 0.00010917626507997162, 0.00021835253015994323]
2023-05-09 13:21:50,444 :: INFO :: ndcg [0.0, 5.507349093307493e-05, 9.050838439175812e-05]
2023-05-09 13:21:56,146 :: INFO :: Epoch 105: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011035866336897016
2023-05-09 13:22:54,647 :: INFO :: ----- val -----
2023-05-09 13:22:54,647 :: INFO :: Precision [0.0, 0.0, 1.32772737331268e-05]
2023-05-09 13:22:54,647 :: INFO :: Recall [0.0, 0.0, 0.00013277273733126798]
2023-05-09 13:22:54,647 :: INFO :: ndcg [0.0, 0.0, 4.526992050341619e-05]
2023-05-09 13:23:50,053 :: INFO :: ----- test -----
2023-05-09 13:23:50,053 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:23:50,053 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:23:50,053 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.844854068323679e-05]
2023-05-09 13:23:56,006 :: INFO :: Epoch 110: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012372490018606186
2023-05-09 13:24:53,162 :: INFO :: ----- val -----
2023-05-09 13:24:53,162 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:24:53,162 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:24:53,162 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:25:47,852 :: INFO :: ----- test -----
2023-05-09 13:25:47,852 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:25:47,852 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:25:47,852 :: INFO :: ndcg [0.0, 0.0, 2.5042984554661586e-05]
2023-05-09 13:25:53,633 :: INFO :: Epoch 115: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014067536685615778
2023-05-09 13:26:47,602 :: INFO :: ----- val -----
2023-05-09 13:26:47,602 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:26:47,602 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:26:47,602 :: INFO :: ndcg [0.0, 0.0, 2.754583578894539e-05]
2023-05-09 13:27:41,586 :: INFO :: ----- test -----
2023-05-09 13:27:41,586 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 13:27:41,586 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 13:27:41,586 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.596073380937646e-05]
2023-05-09 13:27:47,461 :: INFO :: Epoch 120: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014542959397658706
2023-05-09 13:28:41,867 :: INFO :: ----- val -----
2023-05-09 13:28:41,867 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:28:41,867 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:28:41,867 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:29:37,820 :: INFO :: ----- test -----
2023-05-09 13:29:37,820 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:29:37,820 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:29:37,820 :: INFO :: ndcg [0.0, 0.0, 2.7385438061145575e-05]
2023-05-09 13:29:43,570 :: INFO :: Epoch 125: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001421201042830944
2023-05-09 13:30:37,476 :: INFO :: ----- val -----
2023-05-09 13:30:37,476 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:30:37,476 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:30:37,476 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:31:32,617 :: INFO :: ----- test -----
2023-05-09 13:31:32,617 :: INFO :: Precision [2.7294066269992904e-05, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 13:31:32,617 :: INFO :: Recall [2.7294066269992904e-05, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 13:31:32,617 :: INFO :: ndcg [2.7294066269992904e-05, 2.7294066269992904e-05, 5.652211579591172e-05]
2023-05-09 13:31:38,320 :: INFO :: Epoch 130: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.00179652264341712
2023-05-09 13:32:31,664 :: INFO :: ----- val -----
2023-05-09 13:32:31,664 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:32:31,664 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:32:31,664 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:33:27,370 :: INFO :: ----- test -----
2023-05-09 13:33:27,370 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 13:33:27,370 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 13:33:27,370 :: INFO :: ndcg [0.0, 0.0, 1.0558786157392369e-05]
2023-05-09 13:33:33,057 :: INFO :: Epoch 135: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0019852654077112675
2023-05-09 13:34:26,245 :: INFO :: ----- val -----
2023-05-09 13:34:26,245 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:34:26,245 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:34:26,245 :: INFO :: ndcg [0.0, 0.0, 2.8714238111045017e-05]
2023-05-09 13:35:21,463 :: INFO :: ----- test -----
2023-05-09 13:35:21,463 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.637643976199574e-05]
2023-05-09 13:35:21,463 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00015011736448496097]
2023-05-09 13:35:21,463 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 6.0757409596059037e-05]
2023-05-09 13:35:27,401 :: INFO :: Epoch 140: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017755500739440322
2023-05-09 13:36:24,231 :: INFO :: ----- val -----
2023-05-09 13:36:24,231 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:36:24,231 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:36:24,231 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 13:37:19,356 :: INFO :: ----- test -----
2023-05-09 13:37:19,356 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:37:19,356 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:37:19,356 :: INFO :: ndcg [0.0, 0.0, 2.6222416349463367e-05]
2023-05-09 13:37:25,262 :: INFO :: Epoch 145: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017130282940343022
2023-05-09 13:38:18,341 :: INFO :: ----- val -----
2023-05-09 13:38:18,341 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:38:18,341 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:38:18,341 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:39:13,075 :: INFO :: ----- test -----
2023-05-09 13:39:13,075 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:39:13,075 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 13:39:13,075 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 3.158570835667895e-05]
2023-05-09 13:39:19,059 :: INFO :: Epoch 150: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017711068503558636
2023-05-09 13:40:13,200 :: INFO :: ----- val -----
2023-05-09 13:40:13,216 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:40:13,216 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:40:13,216 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 13:41:06,622 :: INFO :: ----- test -----
2023-05-09 13:41:06,622 :: INFO :: Precision [0.0, 2.1835253015994326e-05, 1.3647033134996452e-05]
2023-05-09 13:41:06,622 :: INFO :: Recall [0.0, 0.00010917626507997162, 0.00013647033134996453]
2023-05-09 13:41:06,622 :: INFO :: ndcg [0.0, 4.960776694437291e-05, 5.8693649829812595e-05]
2023-05-09 13:41:12,356 :: INFO :: Epoch 155: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0018197373719885945
2023-05-09 13:42:06,326 :: INFO :: ----- val -----
2023-05-09 13:42:06,326 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:42:06,326 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:42:06,326 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:43:00,967 :: INFO :: ----- test -----
2023-05-09 13:43:00,967 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:43:00,967 :: INFO :: Recall [0.0, 5.458813253998581e-05, 9.552923194497516e-05]
2023-05-09 13:43:00,967 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 3.678120250685574e-05]
2023-05-09 13:43:06,702 :: INFO :: Epoch 160: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0016893809661269188
2023-05-09 13:44:00,389 :: INFO :: ----- val -----
2023-05-09 13:44:00,389 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:44:00,389 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:44:00,389 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:44:55,486 :: INFO :: ----- test -----
2023-05-09 13:44:55,486 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:44:55,486 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:44:55,486 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.9656808247390002e-05]
2023-05-09 13:45:01,235 :: INFO :: Epoch 165: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0021255980245769024
2023-05-09 13:45:57,079 :: INFO :: ----- val -----
2023-05-09 13:45:57,079 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:45:57,079 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:45:57,079 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 13:46:51,329 :: INFO :: ----- test -----
2023-05-09 13:46:51,329 :: INFO :: Precision [5.458813253998581e-05, 2.1835253015994326e-05, 3.275287952399148e-05]
2023-05-09 13:46:51,329 :: INFO :: Recall [5.458813253998581e-05, 0.00010917626507997162, 0.0003275287952399148]
2023-05-09 13:46:51,329 :: INFO :: ndcg [5.458813253998581e-05, 7.99900801919699e-05, 0.0001525794239754394]
2023-05-09 13:46:57,236 :: INFO :: Epoch 170: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001959695480763912
2023-05-09 13:47:50,566 :: INFO :: ----- val -----
2023-05-09 13:47:50,566 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:47:50,566 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:47:50,566 :: INFO :: ndcg [0.0, 0.0, 2.9087726610843255e-05]
2023-05-09 13:48:45,128 :: INFO :: ----- test -----
2023-05-09 13:48:45,128 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:48:45,128 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:48:45,128 :: INFO :: ndcg [0.0, 0.0, 2.399584170259722e-05]
2023-05-09 13:48:50,972 :: INFO :: Epoch 175: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0020478025544434786
2023-05-09 13:49:44,128 :: INFO :: ----- val -----
2023-05-09 13:49:44,144 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:49:44,144 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:49:44,144 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:50:37,550 :: INFO :: ----- test -----
2023-05-09 13:50:37,550 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 13:50:37,550 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 13:50:37,550 :: INFO :: ndcg [0.0, 0.0, 1.9169105410237195e-05]
2023-05-09 13:50:43,316 :: INFO :: Epoch 180: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0018744878470897675
2023-05-09 13:51:37,441 :: INFO :: ----- val -----
2023-05-09 13:51:37,441 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:51:37,441 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:51:37,441 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:52:32,222 :: INFO :: ----- test -----
2023-05-09 13:52:32,222 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:52:32,222 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:52:32,222 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.6942981076463772e-05]
2023-05-09 13:52:38,035 :: INFO :: Epoch 185: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0020866640843451023
2023-05-09 13:53:31,175 :: INFO :: ----- val -----
2023-05-09 13:53:31,175 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:53:31,175 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:53:31,175 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:54:23,847 :: INFO :: ----- test -----
2023-05-09 13:54:23,847 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 13:54:23,847 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 13:54:23,847 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.996569373068374e-05]
2023-05-09 13:54:29,456 :: INFO :: Epoch 190: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001873630448244512
2023-05-09 13:55:23,956 :: INFO :: ----- val -----
2023-05-09 13:55:23,972 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:55:23,972 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:55:23,972 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:56:19,316 :: INFO :: ----- test -----
2023-05-09 13:56:19,316 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:56:19,316 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 13:56:19,316 :: INFO :: ndcg [0.0, 2.231370067438001e-05, 4.002204201722247e-05]
2023-05-09 13:56:25,206 :: INFO :: Epoch 195: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001954291481524706
2023-05-09 13:57:18,800 :: INFO :: ----- val -----
2023-05-09 13:57:18,800 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:57:18,800 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:57:18,800 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:58:12,725 :: INFO :: ----- test -----
2023-05-09 13:58:12,725 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:58:12,725 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:58:12,725 :: INFO :: ndcg [0.0, 0.0, 2.4322419827661183e-05]
2023-05-09 13:58:18,491 :: INFO :: Epoch 200: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002102443715557456
2023-05-09 13:59:13,522 :: INFO :: ----- val -----
2023-05-09 13:59:13,522 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:59:13,522 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:59:13,522 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:00:06,616 :: INFO :: ----- test -----
2023-05-09 14:00:06,616 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:00:06,616 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 14:00:06,616 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 4.828362052060185e-05]
2023-05-09 14:00:12,319 :: INFO :: Epoch 205: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0021281905937939882
2023-05-09 14:01:04,025 :: INFO :: ----- val -----
2023-05-09 14:01:04,025 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:01:04,025 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:01:04,025 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 14:01:57,494 :: INFO :: ----- test -----
2023-05-09 14:01:57,494 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 14:01:57,494 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 14:01:57,494 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 1.9644669042832058e-05]
2023-05-09 14:02:03,369 :: INFO :: Epoch 210: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002356016542762518
2023-05-09 14:03:00,275 :: INFO :: ----- val -----
2023-05-09 14:03:00,275 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:03:00,275 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:03:00,291 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:03:54,434 :: INFO :: ----- test -----
2023-05-09 14:03:54,434 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.910584638899503e-05]
2023-05-09 14:03:54,434 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00019105846388995032]
2023-05-09 14:03:54,434 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 5.943215900167607e-05]
2023-05-09 14:04:00,199 :: INFO :: Epoch 215: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002370045520365238
2023-05-09 14:04:54,903 :: INFO :: ----- val -----
2023-05-09 14:04:54,903 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:04:54,903 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:04:54,903 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:05:49,668 :: INFO :: ----- test -----
2023-05-09 14:05:49,668 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:05:49,668 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 14:05:49,668 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 4.581181611998088e-05]
2023-05-09 14:05:55,324 :: INFO :: Epoch 220: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0022792594972997904
2023-05-09 14:06:50,871 :: INFO :: ----- val -----
2023-05-09 14:06:50,871 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:06:50,871 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:06:50,871 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:07:45,543 :: INFO :: ----- test -----
2023-05-09 14:07:45,543 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.910584638899503e-05]
2023-05-09 14:07:45,543 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00019105846388995032]
2023-05-09 14:07:45,543 :: INFO :: ndcg [2.7294066269992904e-05, 5.458813253998581e-05, 8.888791945267019e-05]
2023-05-09 14:07:51,293 :: INFO :: Epoch 225: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.6362226100329836e-20, MLP.norm 0.002370502334088087
2023-05-09 14:08:48,090 :: INFO :: ----- val -----
2023-05-09 14:08:48,090 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:08:48,090 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:08:48,090 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 14:09:42,968 :: INFO :: ----- test -----
2023-05-09 14:09:42,968 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:09:42,968 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:09:42,968 :: INFO :: ndcg [0.0, 0.0, 1.61060871767528e-05]
2023-05-09 14:09:48,749 :: INFO :: Epoch 230: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.0001335697118474e-11, MLP.norm 0.0023682070896029472
2023-05-09 14:10:45,593 :: INFO :: ----- val -----
2023-05-09 14:10:45,593 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:10:45,593 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:10:45,593 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:11:40,955 :: INFO :: ----- test -----
2023-05-09 14:11:40,955 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:11:40,955 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:11:40,955 :: INFO :: ndcg [0.0, 0.0, 1.6500073778689248e-05]
2023-05-09 14:11:46,830 :: INFO :: Epoch 235: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.9110630091745406e-06, MLP.norm 0.002698881784453988
2023-05-09 14:12:40,596 :: INFO :: ----- val -----
2023-05-09 14:12:40,596 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:12:40,596 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:12:40,596 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 14:13:35,148 :: INFO :: ----- test -----
2023-05-09 14:13:35,148 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:13:35,148 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00012282329821496806]
2023-05-09 14:13:35,148 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 4.6414318802527346e-05]
2023-05-09 14:13:41,117 :: INFO :: Epoch 240: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.3046620450651858e-09, MLP.norm 0.002717302879318595
2023-05-09 14:14:37,742 :: INFO :: ----- val -----
2023-05-09 14:14:37,742 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:14:37,742 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:14:37,742 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 14:15:33,304 :: INFO :: ----- test -----
2023-05-09 14:15:33,304 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 14:15:33,304 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 14:15:33,304 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 3.5995757313990405e-05]
2023-05-09 14:15:39,335 :: INFO :: Epoch 245: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.1357173207215965e-05, MLP.norm 0.0025642472319304943
2023-05-09 14:16:35,679 :: INFO :: ----- val -----
2023-05-09 14:16:35,679 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:16:35,679 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:16:35,679 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:17:33,007 :: INFO :: ----- test -----
2023-05-09 14:17:33,007 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 14:17:33,007 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 14:17:33,007 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 4.3734609475603034e-05]
2023-05-09 14:17:38,976 :: INFO :: Epoch 250: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.1096550376096275e-06, MLP.norm 0.0026513184420764446
2023-05-09 14:18:36,382 :: INFO :: ----- val -----
2023-05-09 14:18:36,382 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 14:18:36,382 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 14:18:36,382 :: INFO :: ndcg [0.0, 0.0, 2.611616826651335e-05]
2023-05-09 14:19:32,273 :: INFO :: ----- test -----
2023-05-09 14:19:32,273 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:19:32,273 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:19:32,273 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.847717290816175e-05]
2023-05-09 14:19:38,069 :: INFO :: Epoch 255: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.639742731640581e-05, MLP.norm 0.0025122377555817366
2023-05-09 14:20:34,492 :: INFO :: ----- val -----
2023-05-09 14:20:34,492 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:20:34,492 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:20:34,492 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:21:30,648 :: INFO :: ----- test -----
2023-05-09 14:21:30,648 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 14:21:30,648 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 14:21:30,648 :: INFO :: ndcg [0.0, 0.0, 2.582842974752692e-05]
2023-05-09 14:21:36,601 :: INFO :: Epoch 260: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.3206288713263348e-05, MLP.norm 0.002689681714400649
2023-05-09 14:22:33,695 :: INFO :: ----- val -----
2023-05-09 14:22:33,695 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:22:33,695 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:22:33,695 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:23:28,257 :: INFO :: ----- test -----
2023-05-09 14:23:28,257 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 14:23:28,257 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 14:23:28,257 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 3.0147106913685697e-05]
2023-05-09 14:23:34,023 :: INFO :: Epoch 265: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.83527073811274e-05, MLP.norm 0.002746578538790345
2023-05-09 14:24:27,288 :: INFO :: ----- val -----
2023-05-09 14:24:27,288 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:24:27,288 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:24:27,288 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 14:25:23,742 :: INFO :: ----- test -----
2023-05-09 14:25:23,742 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 14:25:23,742 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 14:25:23,742 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-09 14:25:29,554 :: INFO :: Epoch 270: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.501564762904309e-05, MLP.norm 0.0026715854182839394
2023-05-09 14:26:26,992 :: INFO :: ----- val -----
2023-05-09 14:26:26,992 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:26:26,992 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:26:26,992 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:27:25,429 :: INFO :: ----- test -----
2023-05-09 14:27:25,429 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-09 14:27:25,429 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-09 14:27:25,429 :: INFO :: ndcg [0.0, 0.0, 3.4208415121531715e-05]
2023-05-09 14:27:31,444 :: INFO :: Epoch 275: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.716950570582412e-05, MLP.norm 0.0030431768391281366
2023-05-09 14:28:28,804 :: INFO :: ----- val -----
2023-05-09 14:28:28,804 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:28:28,804 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:28:28,804 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 14:29:27,179 :: INFO :: ----- test -----
2023-05-09 14:29:27,179 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:29:27,179 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:29:27,179 :: INFO :: ndcg [0.0, 0.0, 1.7708341342842464e-05]
2023-05-09 14:29:33,211 :: INFO :: Epoch 280: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.874914884567261e-05, MLP.norm 0.002980073681101203
2023-05-09 14:30:29,773 :: INFO :: ----- val -----
2023-05-09 14:30:29,773 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:30:29,773 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:30:29,773 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:31:27,867 :: INFO :: ----- test -----
2023-05-09 14:31:27,867 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 14:31:27,867 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 14:31:27,867 :: INFO :: ndcg [0.0, 2.8975553022677294e-05, 3.7585872275522124e-05]
2023-05-09 14:31:33,773 :: INFO :: Epoch 285: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0002696090959943831, MLP.norm 0.0028922739438712597
2023-05-09 14:32:31,866 :: INFO :: ----- val -----
2023-05-09 14:32:31,866 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:32:31,866 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:32:31,866 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:33:30,367 :: INFO :: ----- test -----
2023-05-09 14:33:30,367 :: INFO :: Precision [0.0, 0.0, 1.3647033134996452e-05]
2023-05-09 14:33:30,367 :: INFO :: Recall [0.0, 0.0, 0.00012282329821496806]
2023-05-09 14:33:30,367 :: INFO :: ndcg [0.0, 0.0, 4.2191885882592485e-05]
2023-05-09 14:33:36,273 :: INFO :: Epoch 290: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00023252741084434092, MLP.norm 0.0027972508687525988
2023-05-09 14:34:34,085 :: INFO :: ----- val -----
2023-05-09 14:34:34,085 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:34:34,085 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:34:34,085 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 14:35:32,648 :: INFO :: ----- test -----
2023-05-09 14:35:32,648 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 14:35:32,648 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 14:35:32,648 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.329170217782851e-05]
2023-05-09 14:35:38,523 :: INFO :: Epoch 295: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00022146580158732831, MLP.norm 0.003257307456806302
2023-05-09 14:36:35,367 :: INFO :: ----- val -----
2023-05-09 14:36:35,367 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:36:35,367 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:36:35,367 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:37:32,320 :: INFO :: ----- test -----
2023-05-09 14:37:32,320 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 14:37:32,320 :: INFO :: Recall [0.0, 0.0, 6.823516567498226e-05]
2023-05-09 14:37:32,320 :: INFO :: ndcg [0.0, 0.0, 2.5204109266750434e-05]
2023-05-09 14:37:38,086 :: INFO :: Epoch 300: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0001995335624087602, MLP.norm 0.0031524167861789465
2023-05-09 14:38:36,992 :: INFO :: ----- val -----
2023-05-09 14:38:36,992 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:38:36,992 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:38:36,992 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:39:34,429 :: INFO :: ----- test -----
2023-05-09 14:39:34,429 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 14:39:34,429 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 14:39:34,429 :: INFO :: ndcg [0.0, 0.0, 9.098022089997634e-06]
2023-05-09 14:39:40,257 :: INFO :: Epoch 305: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0001882243959698826, MLP.norm 0.003152557648718357
2023-05-09 14:40:36,382 :: INFO :: ----- val -----
2023-05-09 14:40:36,382 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:40:36,382 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:40:36,382 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: ----- test -----
2023-05-09 14:41:32,711 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:41:38,523 :: INFO :: Epoch 310: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00021251854195725173, MLP.norm 0.00324290432035923
2023-05-09 14:42:35,101 :: INFO :: ----- val -----
2023-05-09 14:42:35,101 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:42:35,101 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:42:35,101 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:43:33,179 :: INFO :: ----- test -----
2023-05-09 14:43:33,179 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:43:33,179 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:43:33,179 :: INFO :: ndcg [0.0, 0.0, 1.8196044179995267e-05]
2023-05-09 14:43:39,117 :: INFO :: Epoch 315: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0002257401356473565, MLP.norm 0.0031606138218194246
2023-05-09 14:44:35,476 :: INFO :: ----- val -----
2023-05-09 14:44:35,476 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:44:35,476 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:44:35,476 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:45:33,163 :: INFO :: ----- test -----
2023-05-09 14:45:33,163 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:45:33,163 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:45:33,163 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.615688202607925e-05]
2023-05-09 14:45:33,163 :: INFO :: Epoch 315:
2023-05-09 14:46:27,632 :: INFO :: ----- val -----
2023-05-09 14:46:27,632 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:46:27,632 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:46:27,632 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:47:20,523 :: INFO :: ----- test -----
2023-05-09 14:47:20,523 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:47:20,523 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:47:20,523 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.615688202607925e-05]
2023-05-09 14:47:20,523 :: INFO :: final:
2023-05-09 14:47:20,523 :: INFO :: ----- test -----
2023-05-09 14:47:20,523 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 14:47:20,523 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 14:47:20,523 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.844854068323679e-05]
2023-05-09 14:47:20,523 :: INFO :: max_epoch 105:
