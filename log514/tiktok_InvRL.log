2023-03-25 20:54:09,050 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 20:54:09,050 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 20:54:10,572 :: INFO :: torch.Size([76085, 384])
2023-03-25 20:54:16,438 :: INFO :: ----- frontend -----
2023-03-25 20:54:16,458 :: INFO :: Environment 0
2023-03-25 21:00:46,702 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 21:00:46,702 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 21:00:47,896 :: INFO :: torch.Size([76085, 384])
2023-03-25 21:00:54,040 :: INFO :: ----- frontend -----
2023-03-25 21:00:54,053 :: INFO :: Environment 0
2023-03-25 21:03:21,232 :: INFO :: log info to logs/tiktok_InvRL.log
2023-03-25 21:03:21,233 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-03-25 21:03:22,482 :: INFO :: torch.Size([76085, 384])
2023-03-25 21:03:28,184 :: INFO :: ----- frontend -----
2023-03-25 21:03:28,207 :: INFO :: Environment 0
2023-03-25 21:07:23,231 :: INFO :: Epoch 5: loss tensor(78780.2812), U.norm 10.710773468017578, V.norm 14.95574951171875, MLP.norm 26.580310821533203
2023-03-25 21:11:36,910 :: INFO :: Epoch 10: loss tensor(74338.6562), U.norm 12.743417739868164, V.norm 14.875079154968262, MLP.norm 36.76354217529297
2023-03-25 21:15:47,489 :: INFO :: Epoch 15: loss tensor(70493.6875), U.norm 14.630451202392578, V.norm 15.798015594482422, MLP.norm 43.9197883605957
2023-03-25 21:19:36,280 :: INFO :: Epoch 20: loss tensor(66971.8828), U.norm 16.37555503845215, V.norm 17.27663230895996, MLP.norm 49.04502868652344
2023-03-25 21:19:37,248 :: INFO :: Environment 1
2023-03-25 21:23:28,571 :: INFO :: Epoch 5: loss tensor(78651.0078), U.norm 10.72774887084961, V.norm 14.990690231323242, MLP.norm 26.075809478759766
2023-03-25 21:27:12,210 :: INFO :: Epoch 10: loss tensor(74365.6719), U.norm 12.745518684387207, V.norm 14.886467933654785, MLP.norm 36.00558853149414
2023-03-25 21:30:57,238 :: INFO :: Epoch 15: loss tensor(70649.0625), U.norm 14.618178367614746, V.norm 15.746353149414062, MLP.norm 43.24523162841797
2023-03-25 21:34:48,076 :: INFO :: Epoch 20: loss tensor(67105.2656), U.norm 16.36060333251953, V.norm 17.128013610839844, MLP.norm 48.6616325378418
2023-03-25 21:34:48,751 :: INFO :: Environment 2
2023-03-25 21:38:53,005 :: INFO :: Epoch 5: loss tensor(78543.0859), U.norm 10.695173263549805, V.norm 14.975322723388672, MLP.norm 26.169532775878906
2023-03-25 21:42:37,072 :: INFO :: Epoch 10: loss tensor(74356.5938), U.norm 12.705785751342773, V.norm 14.863210678100586, MLP.norm 35.66987609863281
2023-03-25 21:46:27,194 :: INFO :: Epoch 15: loss tensor(70662.0547), U.norm 14.571556091308594, V.norm 15.71623706817627, MLP.norm 42.91804504394531
2023-03-25 21:50:14,984 :: INFO :: Epoch 20: loss tensor(67158.0469), U.norm 16.31003761291504, V.norm 17.092653274536133, MLP.norm 48.469970703125
2023-03-25 21:50:15,658 :: INFO :: Environment 3
2023-03-25 21:54:02,465 :: INFO :: Epoch 5: loss tensor(79166.6484), U.norm 10.651463508605957, V.norm 14.968517303466797, MLP.norm 25.521623611450195
2023-03-25 21:57:49,789 :: INFO :: Epoch 10: loss tensor(75044.9453), U.norm 12.65029239654541, V.norm 14.902669906616211, MLP.norm 35.48780059814453
2023-03-25 22:01:29,829 :: INFO :: Epoch 15: loss tensor(71449.8750), U.norm 14.526192665100098, V.norm 15.850362777709961, MLP.norm 42.67310333251953
2023-03-25 22:05:16,975 :: INFO :: Epoch 20: loss tensor(67950.5547), U.norm 16.275951385498047, V.norm 17.369108200073242, MLP.norm 48.1007080078125
2023-03-25 22:05:17,629 :: INFO :: Environment 4
2023-03-25 22:09:10,438 :: INFO :: Epoch 5: loss tensor(79249.6797), U.norm 10.66997241973877, V.norm 14.962930679321289, MLP.norm 26.017513275146484
2023-03-25 22:12:44,111 :: INFO :: Epoch 10: loss tensor(74988.2969), U.norm 12.673913955688477, V.norm 14.839290618896484, MLP.norm 35.81971740722656
2023-03-25 22:16:17,937 :: INFO :: Epoch 15: loss tensor(71230.5469), U.norm 14.548011779785156, V.norm 15.670289993286133, MLP.norm 43.1412239074707
2023-03-25 22:19:52,846 :: INFO :: Epoch 20: loss tensor(67631.9609), U.norm 16.28986358642578, V.norm 17.013879776000977, MLP.norm 48.53339767456055
2023-03-25 22:19:53,463 :: INFO :: Environment 5
2023-03-25 22:23:27,793 :: INFO :: Epoch 5: loss tensor(78725.2969), U.norm 10.633424758911133, V.norm 14.982197761535645, MLP.norm 25.779348373413086
2023-03-25 22:26:57,871 :: INFO :: Epoch 10: loss tensor(74560.2734), U.norm 12.61066722869873, V.norm 14.901220321655273, MLP.norm 35.46091079711914
2023-03-25 22:30:29,361 :: INFO :: Epoch 15: loss tensor(70831.9297), U.norm 14.483649253845215, V.norm 15.831314086914062, MLP.norm 42.746822357177734
2023-03-25 22:34:01,547 :: INFO :: Epoch 20: loss tensor(67252.8672), U.norm 16.23482894897461, V.norm 17.328262329101562, MLP.norm 48.07020950317383
2023-03-25 22:34:02,184 :: INFO :: Environment 6
2023-03-25 22:37:34,900 :: INFO :: Epoch 5: loss tensor(78920.0156), U.norm 10.679916381835938, V.norm 14.96037483215332, MLP.norm 25.992536544799805
2023-03-25 22:41:02,768 :: INFO :: Epoch 10: loss tensor(74568.3672), U.norm 12.694758415222168, V.norm 14.82158374786377, MLP.norm 35.8137321472168
2023-03-25 22:44:31,951 :: INFO :: Epoch 15: loss tensor(70746.7969), U.norm 14.566908836364746, V.norm 15.630630493164062, MLP.norm 43.1983757019043
2023-03-25 22:48:01,263 :: INFO :: Epoch 20: loss tensor(67151.4375), U.norm 16.30751609802246, V.norm 16.94659423828125, MLP.norm 48.50672149658203
2023-03-25 22:48:01,872 :: INFO :: Environment 7
2023-03-25 22:51:36,664 :: INFO :: Epoch 5: loss tensor(78673.5312), U.norm 10.650546073913574, V.norm 14.97412395477295, MLP.norm 25.52890396118164
2023-03-25 22:55:09,238 :: INFO :: Epoch 10: loss tensor(74481.5703), U.norm 12.640934944152832, V.norm 14.883415222167969, MLP.norm 35.21791076660156
2023-03-25 23:13:38,858 :: INFO :: Epoch 15: loss tensor(70770.7734), U.norm 14.515196800231934, V.norm 15.787445068359375, MLP.norm 42.6607780456543
2023-03-25 23:17:15,088 :: INFO :: Epoch 20: loss tensor(67187.3438), U.norm 16.26540756225586, V.norm 17.239900588989258, MLP.norm 48.15561294555664
2023-03-25 23:17:15,714 :: INFO :: Environment 8
2023-03-25 23:53:09,045 :: INFO :: Epoch 5: loss tensor(78978.3516), U.norm 10.697601318359375, V.norm 14.948690414428711, MLP.norm 26.770505905151367
2023-03-25 23:58:22,739 :: INFO :: Epoch 10: loss tensor(74511.5234), U.norm 12.708267211914062, V.norm 14.796112060546875, MLP.norm 36.64705276489258
2023-04-11 13:13:51,811 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 13:13:51,811 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 13:13:54,296 :: INFO :: torch.Size([76085, 384])
2023-04-11 13:14:10,733 :: INFO :: ----- frontend -----
2023-04-11 13:14:10,764 :: INFO :: Environment 0
2023-04-11 13:14:34,202 :: INFO :: Epoch 5: loss tensor(63971.6562, device='cuda:0'), U.norm 11.164484024047852, V.norm 14.704377174377441, MLP.norm 44.06386947631836
2023-04-11 13:14:41,780 :: INFO :: Epoch 10: loss tensor(58937.0820, device='cuda:0'), U.norm 12.902695655822754, V.norm 14.22950267791748, MLP.norm 53.26649856567383
2023-04-11 13:14:49,796 :: INFO :: Epoch 15: loss tensor(56739.9883, device='cuda:0'), U.norm 14.388713836669922, V.norm 14.677123069763184, MLP.norm 56.131980895996094
2023-04-11 13:14:56,811 :: INFO :: Epoch 20: loss tensor(55253.0508, device='cuda:0'), U.norm 15.768144607543945, V.norm 15.564013481140137, MLP.norm 57.704986572265625
2023-04-11 13:14:56,811 :: INFO :: Environment 1
2023-04-11 13:15:19,952 :: INFO :: Epoch 5: loss tensor(63968.2500, device='cuda:0'), U.norm 11.175091743469238, V.norm 14.74273681640625, MLP.norm 43.7144889831543
2023-04-11 13:15:27,328 :: INFO :: Epoch 10: loss tensor(58967.0273, device='cuda:0'), U.norm 12.91086196899414, V.norm 14.261296272277832, MLP.norm 52.98832321166992
2023-04-11 13:15:34,236 :: INFO :: Epoch 15: loss tensor(56801.2383, device='cuda:0'), U.norm 14.39350700378418, V.norm 14.683852195739746, MLP.norm 56.008304595947266
2023-04-11 13:15:41,686 :: INFO :: Epoch 20: loss tensor(55296.8398, device='cuda:0'), U.norm 15.767277717590332, V.norm 15.53143310546875, MLP.norm 57.68355941772461
2023-04-11 13:15:41,686 :: INFO :: Environment 2
2023-04-11 13:16:03,499 :: INFO :: Epoch 5: loss tensor(63700.6367, device='cuda:0'), U.norm 11.154010772705078, V.norm 14.728994369506836, MLP.norm 44.57552719116211
2023-04-11 13:16:11,124 :: INFO :: Epoch 10: loss tensor(58724.2148, device='cuda:0'), U.norm 12.894691467285156, V.norm 14.234764099121094, MLP.norm 53.52023696899414
2023-04-11 13:16:18,296 :: INFO :: Epoch 15: loss tensor(56673.9922, device='cuda:0'), U.norm 14.372533798217773, V.norm 14.658438682556152, MLP.norm 55.914241790771484
2023-04-11 13:16:24,608 :: INFO :: Epoch 20: loss tensor(55251.3203, device='cuda:0'), U.norm 15.747417449951172, V.norm 15.518694877624512, MLP.norm 57.440406799316406
2023-04-11 13:16:24,608 :: INFO :: Environment 3
2023-04-11 13:16:45,094 :: INFO :: Epoch 5: loss tensor(64509.9219, device='cuda:0'), U.norm 11.136981010437012, V.norm 14.716168403625488, MLP.norm 43.844810485839844
2023-04-11 13:16:51,640 :: INFO :: Epoch 10: loss tensor(59561.8828, device='cuda:0'), U.norm 12.872424125671387, V.norm 14.246015548706055, MLP.norm 52.81698989868164
2023-04-11 13:16:59,296 :: INFO :: Epoch 15: loss tensor(57428.4492, device='cuda:0'), U.norm 14.357925415039062, V.norm 14.691025733947754, MLP.norm 55.62257385253906
2023-04-11 13:17:06,546 :: INFO :: Epoch 20: loss tensor(55948.4688, device='cuda:0'), U.norm 15.737154960632324, V.norm 15.570406913757324, MLP.norm 57.32096862792969
2023-04-11 13:17:06,546 :: INFO :: Environment 4
2023-04-11 13:17:26,124 :: INFO :: Epoch 5: loss tensor(64294.4062, device='cuda:0'), U.norm 11.140814781188965, V.norm 14.716755867004395, MLP.norm 44.14636993408203
2023-04-11 13:17:32,733 :: INFO :: Epoch 10: loss tensor(59296.6992, device='cuda:0'), U.norm 12.881582260131836, V.norm 14.225922584533691, MLP.norm 53.078330993652344
2023-04-11 13:17:39,827 :: INFO :: Epoch 15: loss tensor(57194.2617, device='cuda:0'), U.norm 14.366961479187012, V.norm 14.644286155700684, MLP.norm 55.77897262573242
2023-04-11 13:17:46,686 :: INFO :: Epoch 20: loss tensor(55712.5625, device='cuda:0'), U.norm 15.74659538269043, V.norm 15.49077033996582, MLP.norm 57.39009094238281
2023-04-11 13:17:46,702 :: INFO :: Environment 5
2023-04-11 13:18:06,202 :: INFO :: Epoch 5: loss tensor(63683.0273, device='cuda:0'), U.norm 11.144392013549805, V.norm 14.72990608215332, MLP.norm 44.3858757019043
2023-04-11 13:18:13,030 :: INFO :: Epoch 10: loss tensor(58734.6367, device='cuda:0'), U.norm 12.883724212646484, V.norm 14.237281799316406, MLP.norm 53.27328109741211
2023-04-11 13:18:20,342 :: INFO :: Epoch 15: loss tensor(56662.8359, device='cuda:0'), U.norm 14.362588882446289, V.norm 14.6655912399292, MLP.norm 55.86144256591797
2023-04-11 13:18:27,797 :: INFO :: Epoch 20: loss tensor(55202.0156, device='cuda:0'), U.norm 15.737030982971191, V.norm 15.535058975219727, MLP.norm 57.47968292236328
2023-04-11 13:18:27,797 :: INFO :: Environment 6
2023-04-11 13:18:48,092 :: INFO :: Epoch 5: loss tensor(64087.6680, device='cuda:0'), U.norm 11.139106750488281, V.norm 14.712862014770508, MLP.norm 43.932254791259766
2023-04-11 13:18:55,202 :: INFO :: Epoch 10: loss tensor(59187.4648, device='cuda:0'), U.norm 12.867537498474121, V.norm 14.211603164672852, MLP.norm 52.865760803222656
2023-04-11 13:19:02,202 :: INFO :: Epoch 15: loss tensor(57041.7422, device='cuda:0'), U.norm 14.345762252807617, V.norm 14.6204195022583, MLP.norm 55.74172592163086
2023-04-11 13:19:09,374 :: INFO :: Epoch 20: loss tensor(55558.1875, device='cuda:0'), U.norm 15.71817398071289, V.norm 15.455787658691406, MLP.norm 57.38603591918945
2023-04-11 13:19:09,374 :: INFO :: Environment 7
2023-04-11 13:19:29,733 :: INFO :: Epoch 5: loss tensor(63755.4062, device='cuda:0'), U.norm 11.141400337219238, V.norm 14.721487998962402, MLP.norm 44.07299041748047
2023-04-11 13:19:36,108 :: INFO :: Epoch 10: loss tensor(58718.5742, device='cuda:0'), U.norm 12.87523365020752, V.norm 14.230205535888672, MLP.norm 53.1664924621582
2023-04-11 13:19:42,829 :: INFO :: Epoch 15: loss tensor(56617.5742, device='cuda:0'), U.norm 14.351407051086426, V.norm 14.657505989074707, MLP.norm 55.886566162109375
2023-04-11 13:19:49,780 :: INFO :: Epoch 20: loss tensor(55150.7695, device='cuda:0'), U.norm 15.722633361816406, V.norm 15.524603843688965, MLP.norm 57.51047134399414
2023-04-11 13:19:49,796 :: INFO :: Environment 8
2023-04-11 13:20:10,686 :: INFO :: Epoch 5: loss tensor(64221.1836, device='cuda:0'), U.norm 11.144296646118164, V.norm 14.70224666595459, MLP.norm 43.927024841308594
2023-04-11 13:20:18,358 :: INFO :: Epoch 10: loss tensor(59192.5703, device='cuda:0'), U.norm 12.877477645874023, V.norm 14.194478034973145, MLP.norm 53.00509262084961
2023-04-11 13:20:25,514 :: INFO :: Epoch 15: loss tensor(57082.4062, device='cuda:0'), U.norm 14.352745056152344, V.norm 14.59787368774414, MLP.norm 55.65082550048828
2023-04-11 13:20:32,655 :: INFO :: Epoch 20: loss tensor(55628.6758, device='cuda:0'), U.norm 15.72201919555664, V.norm 15.432330131530762, MLP.norm 57.15964889526367
2023-04-11 13:20:32,671 :: INFO :: Environment 9
2023-04-11 13:20:51,156 :: INFO :: Epoch 5: loss tensor(64255.2734, device='cuda:0'), U.norm 11.1485595703125, V.norm 14.731783866882324, MLP.norm 44.178932189941406
2023-04-11 13:20:57,374 :: INFO :: Epoch 10: loss tensor(59261.9180, device='cuda:0'), U.norm 12.889558792114258, V.norm 14.261752128601074, MLP.norm 53.191036224365234
2023-04-11 13:21:04,765 :: INFO :: Epoch 15: loss tensor(57123.6445, device='cuda:0'), U.norm 14.378288269042969, V.norm 14.710026741027832, MLP.norm 55.98915100097656
2023-04-11 13:21:11,967 :: INFO :: Epoch 20: loss tensor(55625.3516, device='cuda:0'), U.norm 15.76142406463623, V.norm 15.593192100524902, MLP.norm 57.65121841430664
2023-04-11 14:11:16,749 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:11:16,749 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:11:19,233 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:11:57,686 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:11:57,686 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:12:00,155 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:12:16,311 :: INFO :: ----- frontend -----
2023-04-11 14:12:16,327 :: INFO :: Environment 0
2023-04-11 14:12:39,764 :: INFO :: Epoch 5: loss tensor(63971.6562, device='cuda:0'), U.norm 11.164484024047852, V.norm 14.704377174377441, MLP.norm 44.06386947631836
2023-04-11 14:12:47,405 :: INFO :: Epoch 10: loss tensor(58937.0820, device='cuda:0'), U.norm 12.902695655822754, V.norm 14.22950267791748, MLP.norm 53.26649856567383
2023-04-11 14:12:53,843 :: INFO :: Epoch 15: loss tensor(56739.9883, device='cuda:0'), U.norm 14.388713836669922, V.norm 14.677123069763184, MLP.norm 56.131980895996094
2023-04-11 14:13:01,467 :: INFO :: Epoch 20: loss tensor(55253.0508, device='cuda:0'), U.norm 15.768144607543945, V.norm 15.564013481140137, MLP.norm 57.704986572265625
2023-04-11 14:13:01,467 :: INFO :: Environment 1
2023-04-11 14:13:23,827 :: INFO :: Epoch 5: loss tensor(63968.2500, device='cuda:0'), U.norm 11.175091743469238, V.norm 14.74273681640625, MLP.norm 43.7144889831543
2023-04-11 14:13:31,342 :: INFO :: Epoch 10: loss tensor(58967.0273, device='cuda:0'), U.norm 12.91086196899414, V.norm 14.261296272277832, MLP.norm 52.98832321166992
2023-04-11 14:13:38,405 :: INFO :: Epoch 15: loss tensor(56801.2383, device='cuda:0'), U.norm 14.39350700378418, V.norm 14.683852195739746, MLP.norm 56.008304595947266
2023-04-11 14:13:45,890 :: INFO :: Epoch 20: loss tensor(55296.8398, device='cuda:0'), U.norm 15.767277717590332, V.norm 15.53143310546875, MLP.norm 57.68355941772461
2023-04-11 14:13:45,905 :: INFO :: Environment 2
2023-04-11 14:14:06,546 :: INFO :: Epoch 5: loss tensor(63700.6367, device='cuda:0'), U.norm 11.154010772705078, V.norm 14.728994369506836, MLP.norm 44.57552719116211
2023-04-11 14:14:13,968 :: INFO :: Epoch 10: loss tensor(58724.2148, device='cuda:0'), U.norm 12.894691467285156, V.norm 14.234764099121094, MLP.norm 53.52023696899414
2023-04-11 14:14:21,171 :: INFO :: Epoch 15: loss tensor(56673.9922, device='cuda:0'), U.norm 14.372533798217773, V.norm 14.658438682556152, MLP.norm 55.914241790771484
2023-04-11 14:14:28,577 :: INFO :: Epoch 20: loss tensor(55251.3203, device='cuda:0'), U.norm 15.747417449951172, V.norm 15.518694877624512, MLP.norm 57.440406799316406
2023-04-11 14:14:28,577 :: INFO :: Environment 3
2023-04-11 14:14:50,389 :: INFO :: Epoch 5: loss tensor(64509.9219, device='cuda:0'), U.norm 11.136981010437012, V.norm 14.716168403625488, MLP.norm 43.844810485839844
2023-04-11 14:14:57,374 :: INFO :: Epoch 10: loss tensor(59561.8828, device='cuda:0'), U.norm 12.872424125671387, V.norm 14.246015548706055, MLP.norm 52.81698989868164
2023-04-11 14:15:04,561 :: INFO :: Epoch 15: loss tensor(57428.4492, device='cuda:0'), U.norm 14.357925415039062, V.norm 14.691025733947754, MLP.norm 55.62257385253906
2023-04-11 14:15:11,530 :: INFO :: Epoch 20: loss tensor(55948.4688, device='cuda:0'), U.norm 15.737154960632324, V.norm 15.570406913757324, MLP.norm 57.32096862792969
2023-04-11 14:15:11,546 :: INFO :: Environment 4
2023-04-11 14:15:31,623 :: INFO :: Epoch 5: loss tensor(64294.4062, device='cuda:0'), U.norm 11.140814781188965, V.norm 14.716755867004395, MLP.norm 44.14636993408203
2023-04-11 14:15:38,655 :: INFO :: Epoch 10: loss tensor(59296.6992, device='cuda:0'), U.norm 12.881582260131836, V.norm 14.225922584533691, MLP.norm 53.078330993652344
2023-04-11 14:15:46,233 :: INFO :: Epoch 15: loss tensor(57194.2617, device='cuda:0'), U.norm 14.366961479187012, V.norm 14.644286155700684, MLP.norm 55.77897262573242
2023-04-11 14:15:52,343 :: INFO :: Epoch 20: loss tensor(55712.5625, device='cuda:0'), U.norm 15.74659538269043, V.norm 15.49077033996582, MLP.norm 57.39009094238281
2023-04-11 14:15:52,358 :: INFO :: Environment 5
2023-04-11 14:16:12,467 :: INFO :: Epoch 5: loss tensor(63683.0273, device='cuda:0'), U.norm 11.144392013549805, V.norm 14.72990608215332, MLP.norm 44.3858757019043
2023-04-11 14:16:18,749 :: INFO :: Epoch 10: loss tensor(58734.6367, device='cuda:0'), U.norm 12.883724212646484, V.norm 14.237281799316406, MLP.norm 53.27328109741211
2023-04-11 14:16:25,124 :: INFO :: Epoch 15: loss tensor(56662.8359, device='cuda:0'), U.norm 14.362588882446289, V.norm 14.6655912399292, MLP.norm 55.86144256591797
2023-04-11 14:16:31,874 :: INFO :: Epoch 20: loss tensor(55202.0156, device='cuda:0'), U.norm 15.737030982971191, V.norm 15.535058975219727, MLP.norm 57.47968292236328
2023-04-11 14:16:31,874 :: INFO :: Environment 6
2023-04-11 14:16:52,139 :: INFO :: Epoch 5: loss tensor(64087.6680, device='cuda:0'), U.norm 11.139106750488281, V.norm 14.712862014770508, MLP.norm 43.932254791259766
2023-04-11 14:16:59,624 :: INFO :: Epoch 10: loss tensor(59187.4648, device='cuda:0'), U.norm 12.867537498474121, V.norm 14.211603164672852, MLP.norm 52.865760803222656
2023-04-11 14:17:06,921 :: INFO :: Epoch 15: loss tensor(57041.7422, device='cuda:0'), U.norm 14.345762252807617, V.norm 14.6204195022583, MLP.norm 55.74172592163086
2023-04-11 14:17:13,139 :: INFO :: Epoch 20: loss tensor(55558.1875, device='cuda:0'), U.norm 15.71817398071289, V.norm 15.455787658691406, MLP.norm 57.38603591918945
2023-04-11 14:17:13,155 :: INFO :: Environment 7
2023-04-11 14:17:34,623 :: INFO :: Epoch 5: loss tensor(63755.4062, device='cuda:0'), U.norm 11.141400337219238, V.norm 14.721487998962402, MLP.norm 44.07299041748047
2023-04-11 14:17:41,373 :: INFO :: Epoch 10: loss tensor(58718.5742, device='cuda:0'), U.norm 12.87523365020752, V.norm 14.230205535888672, MLP.norm 53.1664924621582
2023-04-11 14:17:48,405 :: INFO :: Epoch 15: loss tensor(56617.5742, device='cuda:0'), U.norm 14.351407051086426, V.norm 14.657505989074707, MLP.norm 55.886566162109375
2023-04-11 14:17:55,999 :: INFO :: Epoch 20: loss tensor(55150.7695, device='cuda:0'), U.norm 15.722633361816406, V.norm 15.524603843688965, MLP.norm 57.51047134399414
2023-04-11 14:17:55,999 :: INFO :: Environment 8
2023-04-11 14:18:17,171 :: INFO :: Epoch 5: loss tensor(64221.1836, device='cuda:0'), U.norm 11.144296646118164, V.norm 14.70224666595459, MLP.norm 43.927024841308594
2023-04-11 14:18:24,561 :: INFO :: Epoch 10: loss tensor(59192.5703, device='cuda:0'), U.norm 12.877477645874023, V.norm 14.194478034973145, MLP.norm 53.00509262084961
2023-04-11 14:18:31,686 :: INFO :: Epoch 15: loss tensor(57082.4062, device='cuda:0'), U.norm 14.352745056152344, V.norm 14.59787368774414, MLP.norm 55.65082550048828
2023-04-11 14:18:38,342 :: INFO :: Epoch 20: loss tensor(55628.6758, device='cuda:0'), U.norm 15.72201919555664, V.norm 15.432330131530762, MLP.norm 57.15964889526367
2023-04-11 14:18:38,342 :: INFO :: Environment 9
2023-04-11 14:18:58,217 :: INFO :: Epoch 5: loss tensor(64255.2734, device='cuda:0'), U.norm 11.1485595703125, V.norm 14.731783866882324, MLP.norm 44.178932189941406
2023-04-11 14:19:05,046 :: INFO :: Epoch 10: loss tensor(59261.9180, device='cuda:0'), U.norm 12.889558792114258, V.norm 14.261752128601074, MLP.norm 53.191036224365234
2023-04-11 14:19:12,092 :: INFO :: Epoch 15: loss tensor(57123.6445, device='cuda:0'), U.norm 14.378288269042969, V.norm 14.710026741027832, MLP.norm 55.98915100097656
2023-04-11 14:19:19,140 :: INFO :: Epoch 20: loss tensor(55625.3516, device='cuda:0'), U.norm 15.76142406463623, V.norm 15.593192100524902, MLP.norm 57.65121841430664
2023-04-11 14:27:35,561 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-11 14:27:35,561 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-11 14:27:37,874 :: INFO :: torch.Size([76085, 384])
2023-04-11 14:27:54,280 :: INFO :: ----- frontend -----
2023-04-11 14:27:54,280 :: INFO :: Environment 0
2023-04-11 14:28:10,249 :: INFO :: Epoch 5: loss tensor(823.6648, device='cuda:0'), U.norm 14.646507263183594, V.norm 18.26340675354004, MLP.norm 1.861497402191162
2023-04-11 14:28:10,358 :: INFO :: Epoch 10: loss tensor(811.7224, device='cuda:0'), U.norm 12.039924621582031, V.norm 17.74479866027832, MLP.norm 2.613703727722168
2023-04-11 14:28:10,468 :: INFO :: Epoch 15: loss tensor(794.7003, device='cuda:0'), U.norm 10.375273704528809, V.norm 17.512922286987305, MLP.norm 3.607811212539673
2023-04-11 14:28:10,592 :: INFO :: Epoch 20: loss tensor(773.3453, device='cuda:0'), U.norm 9.180266380310059, V.norm 17.364517211914062, MLP.norm 4.65383768081665
2023-04-11 14:28:10,592 :: INFO :: Environment 1
2023-04-11 14:28:24,092 :: INFO :: Epoch 5: loss tensor(825.8454, device='cuda:0'), U.norm 14.649739265441895, V.norm 18.262313842773438, MLP.norm 1.8743987083435059
2023-04-11 14:28:24,202 :: INFO :: Epoch 10: loss tensor(813.2893, device='cuda:0'), U.norm 12.046407699584961, V.norm 17.739742279052734, MLP.norm 2.6447043418884277
2023-04-11 14:28:24,296 :: INFO :: Epoch 15: loss tensor(795.6776, device='cuda:0'), U.norm 10.385376930236816, V.norm 17.503257751464844, MLP.norm 3.6386396884918213
2023-04-11 14:28:24,389 :: INFO :: Epoch 20: loss tensor(774.0076, device='cuda:0'), U.norm 9.1942720413208, V.norm 17.351211547851562, MLP.norm 4.6785078048706055
2023-04-11 14:28:24,389 :: INFO :: Environment 2
2023-04-11 14:28:37,624 :: INFO :: Epoch 5: loss tensor(824.8957, device='cuda:0'), U.norm 14.647931098937988, V.norm 18.254480361938477, MLP.norm 1.851955533027649
2023-04-11 14:28:37,717 :: INFO :: Epoch 10: loss tensor(812.8655, device='cuda:0'), U.norm 12.042749404907227, V.norm 17.731487274169922, MLP.norm 2.6065449714660645
2023-04-11 14:28:37,811 :: INFO :: Epoch 15: loss tensor(794.7304, device='cuda:0'), U.norm 10.379347801208496, V.norm 17.496137619018555, MLP.norm 3.604365110397339
2023-04-11 14:28:37,889 :: INFO :: Epoch 20: loss tensor(772.5021, device='cuda:0'), U.norm 9.185528755187988, V.norm 17.344900131225586, MLP.norm 4.668241024017334
2023-04-11 14:28:37,905 :: INFO :: Environment 3
2023-04-11 14:28:51,905 :: INFO :: Epoch 5: loss tensor(871.4501, device='cuda:0'), U.norm 14.651307106018066, V.norm 18.35032081604004, MLP.norm 1.8692288398742676
2023-04-11 14:28:51,999 :: INFO :: Epoch 10: loss tensor(857.7139, device='cuda:0'), U.norm 12.049845695495605, V.norm 17.86731719970703, MLP.norm 2.655243158340454
2023-04-11 14:28:52,093 :: INFO :: Epoch 15: loss tensor(837.8387, device='cuda:0'), U.norm 10.391216278076172, V.norm 17.65003776550293, MLP.norm 3.6913483142852783
2023-04-11 14:28:52,186 :: INFO :: Epoch 20: loss tensor(812.9775, device='cuda:0'), U.norm 9.20314884185791, V.norm 17.510906219482422, MLP.norm 4.788106441497803
2023-04-11 14:28:52,202 :: INFO :: Environment 4
2023-04-11 14:29:07,436 :: INFO :: Epoch 5: loss tensor(889.6653, device='cuda:0'), U.norm 14.64863395690918, V.norm 18.379098892211914, MLP.norm 1.886615514755249
2023-04-11 14:29:07,577 :: INFO :: Epoch 10: loss tensor(874.8525, device='cuda:0'), U.norm 12.044795989990234, V.norm 17.900989532470703, MLP.norm 2.7152204513549805
2023-04-11 14:29:07,671 :: INFO :: Epoch 15: loss tensor(853.5810, device='cuda:0'), U.norm 10.38342571258545, V.norm 17.685237884521484, MLP.norm 3.7857394218444824
2023-04-11 14:29:07,796 :: INFO :: Epoch 20: loss tensor(827.2908, device='cuda:0'), U.norm 9.192094802856445, V.norm 17.546886444091797, MLP.norm 4.915922164916992
2023-04-11 14:29:07,811 :: INFO :: Environment 5
2023-04-11 14:29:22,014 :: INFO :: Epoch 5: loss tensor(857.9545, device='cuda:0'), U.norm 14.649359703063965, V.norm 18.32245635986328, MLP.norm 1.8930835723876953
2023-04-11 14:29:22,092 :: INFO :: Epoch 10: loss tensor(844.0444, device='cuda:0'), U.norm 12.046106338500977, V.norm 17.82572364807129, MLP.norm 2.7061805725097656
2023-04-11 14:29:22,186 :: INFO :: Epoch 15: loss tensor(824.9352, device='cuda:0'), U.norm 10.384932518005371, V.norm 17.600799560546875, MLP.norm 3.7379801273345947
2023-04-11 14:29:22,296 :: INFO :: Epoch 20: loss tensor(801.7593, device='cuda:0'), U.norm 9.193687438964844, V.norm 17.45516014099121, MLP.norm 4.81427001953125
2023-04-11 14:29:22,296 :: INFO :: Environment 6
2023-04-11 14:29:35,874 :: INFO :: Epoch 5: loss tensor(843.1569, device='cuda:0'), U.norm 14.648971557617188, V.norm 18.29017448425293, MLP.norm 1.8753410577774048
2023-04-11 14:29:35,952 :: INFO :: Epoch 10: loss tensor(829.5076, device='cuda:0'), U.norm 12.045476913452148, V.norm 17.78328514099121, MLP.norm 2.654620885848999
2023-04-11 14:29:36,046 :: INFO :: Epoch 15: loss tensor(809.8956, device='cuda:0'), U.norm 10.384222984313965, V.norm 17.555116653442383, MLP.norm 3.673460006713867
2023-04-11 14:29:36,124 :: INFO :: Epoch 20: loss tensor(785.8866, device='cuda:0'), U.norm 9.192914962768555, V.norm 17.410888671875, MLP.norm 4.746082782745361
2023-04-11 14:29:36,124 :: INFO :: Environment 7
2023-04-11 14:29:49,967 :: INFO :: Epoch 5: loss tensor(841.5601, device='cuda:0'), U.norm 14.649643898010254, V.norm 18.288536071777344, MLP.norm 1.8546231985092163
2023-04-11 14:29:50,061 :: INFO :: Epoch 10: loss tensor(828.2904, device='cuda:0'), U.norm 12.046640396118164, V.norm 17.777708053588867, MLP.norm 2.6166796684265137
2023-04-11 14:29:50,155 :: INFO :: Epoch 15: loss tensor(809.1725, device='cuda:0'), U.norm 10.38613510131836, V.norm 17.5460262298584, MLP.norm 3.6297316551208496
2023-04-11 14:29:50,233 :: INFO :: Epoch 20: loss tensor(785.8528, device='cuda:0'), U.norm 9.195725440979004, V.norm 17.39707374572754, MLP.norm 4.694681644439697
2023-04-11 14:29:50,233 :: INFO :: Environment 8
2023-04-11 14:30:02,858 :: INFO :: Epoch 5: loss tensor(862.9836, device='cuda:0'), U.norm 14.650801658630371, V.norm 18.33808708190918, MLP.norm 1.8749662637710571
2023-04-11 14:30:02,952 :: INFO :: Epoch 10: loss tensor(849.1979, device='cuda:0'), U.norm 12.049214363098145, V.norm 17.84914779663086, MLP.norm 2.662511110305786
2023-04-11 14:30:03,061 :: INFO :: Epoch 15: loss tensor(829.6909, device='cuda:0'), U.norm 10.390433311462402, V.norm 17.628742218017578, MLP.norm 3.6874279975891113
2023-04-11 14:30:03,186 :: INFO :: Epoch 20: loss tensor(805.5191, device='cuda:0'), U.norm 9.202195167541504, V.norm 17.48768424987793, MLP.norm 4.775859832763672
2023-04-11 14:30:03,186 :: INFO :: Environment 9
2023-04-11 14:30:16,921 :: INFO :: Epoch 5: loss tensor(861.4838, device='cuda:0'), U.norm 14.648391723632812, V.norm 18.33154296875, MLP.norm 1.8717362880706787
2023-04-11 14:30:17,030 :: INFO :: Epoch 10: loss tensor(847.5295, device='cuda:0'), U.norm 12.044227600097656, V.norm 17.83710289001465, MLP.norm 2.66194224357605
2023-04-11 14:30:17,155 :: INFO :: Epoch 15: loss tensor(827.8106, device='cuda:0'), U.norm 10.382287979125977, V.norm 17.61410140991211, MLP.norm 3.695319414138794
2023-04-11 14:30:17,249 :: INFO :: Epoch 20: loss tensor(802.9005, device='cuda:0'), U.norm 9.190075874328613, V.norm 17.470415115356445, MLP.norm 4.7812676429748535
2023-04-11 14:30:17,265 :: INFO :: Ite = 1, Delta = 4088
2023-04-11 14:30:17,265 :: INFO :: ----- backend -----
2023-04-11 14:30:19,749 :: INFO :: Epoch 5: loss tensor(358.5045, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-9.7077e-03, -8.9274e-03, -9.5873e-03, -2.9930e-03, -1.0691e-02,
        -1.3297e-02, -1.0193e-02,  2.4610e-03, -1.2176e-02, -4.1106e-03,
        -1.1943e-02, -1.0227e-02, -9.6882e-03, -8.9737e-03, -5.8937e-03,
         2.2194e-03, -6.0350e-03, -2.3322e-03, -1.1809e-03, -3.5393e-03,
        -7.5479e-03, -3.3291e-03, -6.2313e-03, -1.2107e-02, -9.0905e-03,
        -1.0031e-02, -8.7161e-03, -9.0560e-03, -5.0403e-03, -4.7319e-03,
        -5.9888e-03,  5.1729e-03, -1.0846e-02, -7.2196e-03, -1.2938e-02,
        -1.2731e-02, -7.7023e-03, -9.0036e-03, -1.1768e-02,  5.9445e-04,
        -1.2630e-02, -1.2226e-02, -6.3192e-03, -1.0368e-02, -8.9768e-03,
        -3.9176e-03, -8.8967e-03, -1.3369e-02, -1.0972e-02, -8.8519e-03,
        -1.0814e-02, -5.6119e-03, -1.0373e-02, -5.5862e-03, -9.6758e-03,
         4.5665e-03, -9.2722e-03, -4.1692e-03, -1.2492e-02, -1.3252e-02,
         2.0338e-04, -9.3821e-03, -1.2462e-02, -1.0714e-02, -6.9623e-03,
        -1.1304e-02,  2.4726e-03, -1.1424e-02,  9.1204e-03, -1.2244e-02,
         5.5565e-04, -1.2252e-02,  9.0925e-03, -1.1722e-02, -2.4576e-03,
        -1.2088e-02,  1.0092e-02, -1.2116e-02, -1.8987e-03,  1.0170e-02,
        -8.6450e-03, -1.3507e-02,  8.7030e-04, -3.2832e-03, -1.2976e-02,
        -5.2915e-03,  1.0393e-02, -1.1799e-02, -1.1605e-02, -1.2302e-02,
        -1.0137e-02, -6.1142e-03, -8.0108e-03,  2.2023e-05, -1.2970e-02,
        -9.0809e-03, -2.3395e-03, -8.7797e-03, -1.2889e-02, -9.0783e-03,
        -1.2738e-02, -6.8938e-03, -9.5014e-03, -1.2013e-02, -1.1912e-02,
        -1.3156e-02, -3.9416e-05, -1.1393e-02, -3.9579e-03, -2.2500e-03,
        -1.1543e-02, -2.8294e-03, -1.1231e-02, -9.2374e-03, -1.2250e-02,
        -1.2779e-02, -1.0253e-02, -1.2516e-02, -4.4213e-03, -9.8096e-03,
        -1.0652e-02, -9.5246e-03, -8.3418e-03, -1.2519e-02, -1.0491e-02,
        -1.0834e-02, -1.2540e-02, -8.2321e-03,  6.3135e-03,  5.7454e-03,
         6.1856e-03,  1.4750e-02,  9.9828e-03,  9.5875e-03, -4.5753e-03,
        -9.3302e-03, -1.2222e-02,  1.4023e-02, -2.9943e-03, -1.1339e-02,
         7.7632e-03, -1.0890e-02, -2.1331e-03, -1.2785e-02,  2.0226e-03,
         1.4582e-02,  3.4958e-03, -1.1277e-02, -2.6373e-03, -1.2056e-02,
        -1.1227e-02, -1.9740e-03,  1.0986e-02, -1.2708e-02, -9.4959e-03,
        -1.2830e-02, -1.0597e-02, -1.2863e-02,  1.4340e-02, -1.3756e-02,
         1.4451e-02, -1.3761e-02,  1.3898e-02, -4.6768e-03,  1.0954e-02,
         1.3014e-02, -1.1808e-02, -9.4979e-03, -1.0543e-02, -2.1121e-03,
         7.5102e-03,  9.3992e-03, -1.3228e-02,  6.7666e-03,  1.0492e-02,
        -1.1282e-02,  1.4379e-02,  8.6038e-03, -7.5482e-03, -1.1087e-02,
         2.5626e-03, -1.1292e-02,  1.0570e-02, -1.2933e-02,  8.5019e-03,
         9.7553e-03, -1.2072e-02,  1.2200e-03,  9.2848e-03, -1.2987e-02,
        -1.3523e-02, -7.8680e-03,  1.4679e-02, -1.2364e-02,  3.5341e-03,
         1.0308e-02, -1.0118e-02,  1.3980e-02,  8.1071e-03, -5.4160e-03,
         7.9801e-03, -8.2218e-03,  9.1644e-03,  1.5228e-02,  3.7453e-03,
        -1.0813e-02, -1.1321e-02, -1.3432e-02,  1.1889e-02, -7.5252e-03,
         7.8059e-03,  1.4524e-02,  1.4738e-02, -1.0944e-02,  4.1885e-03,
         1.2495e-03, -7.1217e-03, -1.2368e-02,  1.4426e-02, -1.2733e-02,
        -5.0396e-03, -8.3130e-03,  1.0820e-02,  6.4322e-03,  6.5894e-03,
        -1.4731e-03,  2.3734e-03, -1.6014e-03,  3.7208e-03,  1.0473e-02,
         8.9073e-03,  1.1126e-02,  1.4790e-02,  1.3277e-02,  1.4390e-02,
        -1.1501e-02, -9.7913e-03, -8.7492e-03,  9.7263e-03,  9.1051e-03,
        -3.5435e-03, -4.7225e-03, -9.5119e-03,  1.4781e-02, -9.5534e-03,
         5.7928e-03, -1.2641e-02, -3.3478e-03, -6.4365e-03, -8.8643e-03,
        -1.3052e-02,  1.4460e-02, -7.5370e-03, -7.1744e-03, -1.2910e-02,
        -1.2309e-03, -6.8823e-03, -9.3571e-03, -1.3060e-02,  8.6787e-03,
         4.8648e-03, -9.4839e-04, -1.0003e-02, -7.3854e-03, -7.1208e-04,
        -2.3293e-03, -2.5716e-03, -9.7977e-03, -1.1807e-03, -2.5167e-03,
        -1.0742e-02, -6.9484e-03, -2.6394e-03, -1.1007e-02, -1.9695e-03,
        -7.9503e-03,  5.4073e-04,  1.3331e-04, -5.6899e-03, -3.5629e-03,
        -4.7938e-03, -1.0499e-03, -1.5935e-05, -1.2174e-02, -1.5666e-03,
         3.0631e-03, -8.8007e-03, -7.0870e-03,  8.3157e-03, -7.5275e-03,
        -1.0534e-02, -6.6797e-03, -1.3991e-03,  2.5257e-03, -1.1127e-02,
         4.8054e-03,  3.3348e-03, -7.5451e-03,  5.4774e-03,  4.7290e-03,
        -1.3200e-02,  7.1420e-03, -3.6988e-03, -1.2276e-02,  3.9152e-04,
        -1.2061e-02, -2.9125e-03, -7.0597e-03, -2.8788e-03, -8.6245e-03,
        -8.6544e-03, -6.3556e-04, -3.2481e-03,  4.6289e-03, -4.5156e-03,
         1.2253e-03, -1.7224e-03, -1.0997e-02, -2.1803e-03, -1.3098e-02,
         3.9803e-03, -6.5327e-03,  2.3674e-03, -1.0007e-02, -2.9085e-03,
        -5.9196e-03, -1.3224e-02, -8.6628e-03, -8.5675e-03, -3.6545e-03,
         1.5943e-03, -9.0077e-03, -9.0827e-03, -8.5317e-03,  1.3856e-03,
        -6.6023e-04, -6.8351e-03, -2.0411e-03,  2.5199e-03, -3.7928e-03,
         5.0434e-04, -5.9504e-03, -4.0200e-03,  2.7830e-03, -4.4730e-03,
         4.8331e-04, -2.0027e-04, -8.8600e-03, -1.0660e-02, -5.5064e-03,
        -9.2072e-03, -1.0157e-02, -6.1153e-03, -7.5162e-03, -4.3362e-03,
        -3.3825e-03, -1.2503e-02,  7.5526e-03, -6.5824e-03, -4.7266e-04,
        -9.9564e-03, -1.2555e-02, -4.8138e-03, -5.5322e-03,  9.2895e-03,
         7.2010e-03, -4.4763e-03, -1.1544e-02, -8.5679e-03, -6.5641e-03,
        -7.2200e-03, -1.3274e-02,  3.1105e-03,  1.7319e-03, -8.0847e-04,
        -8.0059e-03, -2.8195e-03, -2.2448e-03, -1.0532e-04, -1.3342e-02,
         3.7387e-03, -7.2024e-03, -9.4215e-03,  3.6195e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(2.5681, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:22,061 :: INFO :: Epoch 10: loss tensor(350.5648, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-3.9143e-03, -3.7580e-03, -7.2848e-03,  7.9325e-03, -9.9659e-03,
        -1.9396e-02, -6.1773e-03,  1.5641e-02, -1.5558e-02,  7.2653e-03,
        -1.4876e-02, -1.0096e-02, -5.2462e-03, -3.0141e-03, -6.8403e-04,
         1.2881e-02,  2.5749e-03,  9.2504e-03,  2.9432e-03,  7.9439e-03,
         2.3549e-04,  5.6517e-03,  3.5597e-03, -1.3434e-02, -6.6885e-03,
        -5.6968e-03, -5.7699e-03, -2.6783e-03,  7.1434e-03,  3.6591e-03,
         3.8911e-03,  1.7635e-02, -1.0344e-02, -9.5141e-04, -2.1276e-02,
        -2.2498e-02, -9.2922e-04, -8.0579e-04, -1.3756e-02,  7.8521e-03,
        -1.7941e-02, -1.5338e-02,  1.7696e-03, -8.6489e-03, -4.8594e-03,
         4.3057e-03, -2.7669e-03, -2.1014e-02, -1.2503e-02, -5.2626e-03,
        -9.6734e-03,  3.3795e-03, -1.0747e-02,  2.4588e-03, -3.9810e-03,
         1.8286e-02, -2.1866e-03,  6.6004e-03, -1.8978e-02, -2.2030e-02,
         1.2350e-02, -7.2640e-03, -1.6625e-02, -9.0640e-03,  4.3232e-04,
        -1.2225e-02,  1.6288e-02, -1.5270e-02,  2.2389e-02, -1.7078e-02,
         1.1209e-02, -1.5709e-02,  1.8042e-02, -1.1214e-02,  1.0011e-02,
        -1.6651e-02,  1.6679e-02, -1.6483e-02,  9.8458e-03,  1.3944e-02,
        -1.3947e-03, -1.8579e-02,  1.4858e-02,  9.0546e-03, -2.1059e-02,
         5.9426e-03,  2.0668e-02, -1.5321e-02, -1.2264e-02, -1.7966e-02,
        -7.5952e-03,  5.3996e-03, -9.6138e-04,  1.2753e-02, -1.7614e-02,
        -1.7637e-03,  1.0425e-02, -6.8125e-03, -2.0120e-02, -5.9362e-03,
        -1.9592e-02,  1.8530e-03, -4.2749e-03, -1.4080e-02, -1.1924e-02,
        -2.2570e-02,  8.5186e-03, -1.4877e-02,  5.0730e-03,  1.0476e-02,
        -1.4528e-02,  7.4408e-03, -1.1056e-02, -7.3195e-03, -2.0092e-02,
        -1.9459e-02, -9.8574e-03, -1.6469e-02,  3.0421e-03, -7.5363e-03,
        -1.0223e-02, -4.7067e-03, -4.2923e-03, -2.0128e-02, -7.9815e-03,
        -1.3065e-02, -1.6475e-02, -3.0174e-04,  2.3010e-02,  2.2041e-02,
         2.2593e-02,  3.1008e-02,  2.7363e-02,  2.6500e-02,  9.6027e-03,
        -1.8129e-03, -1.8206e-02,  2.9312e-02,  1.2575e-02, -1.0228e-02,
         2.4537e-02, -8.9829e-03,  1.3879e-02, -1.7720e-02,  1.8462e-02,
         3.0411e-02,  1.9887e-02, -1.2289e-02,  1.2281e-02, -9.5992e-03,
        -1.2801e-02,  1.4492e-02,  2.7555e-02, -2.2356e-02, -3.2490e-04,
        -1.9120e-02, -7.2257e-03, -2.3055e-02,  2.9644e-02, -2.5867e-02,
         3.0933e-02, -2.6162e-02,  3.0165e-02,  9.0735e-03,  2.6804e-02,
         2.2907e-02, -1.5790e-02, -1.1444e-03, -5.8691e-03,  1.3544e-02,
         2.4790e-02,  2.5438e-02, -2.4215e-02,  2.3090e-02,  2.7446e-02,
        -1.4720e-02,  2.9528e-02,  2.4900e-02,  4.1549e-03, -1.1618e-02,
         1.8587e-02, -1.3454e-02,  2.6606e-02, -2.3103e-02,  2.4559e-02,
         2.6608e-02, -1.9943e-02,  1.7961e-02,  2.6453e-02, -2.3419e-02,
        -2.4626e-02,  2.0323e-03,  3.0192e-02, -2.0447e-02,  2.0187e-02,
         2.6865e-02, -8.9308e-04,  2.5539e-02,  2.4937e-02,  8.7364e-03,
         2.4381e-02,  3.3249e-03,  2.5858e-02,  3.2056e-02,  2.0232e-02,
        -5.5733e-03, -1.0086e-02, -2.5121e-02,  2.7773e-02,  3.6891e-03,
         2.4837e-02,  3.0095e-02,  2.9827e-02, -1.1234e-02,  2.1004e-02,
         1.7949e-02,  4.3488e-03, -1.7056e-02,  3.0001e-02, -2.2689e-02,
         1.0317e-02,  4.4991e-03,  2.8138e-02,  2.3518e-02,  2.3829e-02,
         1.4978e-02,  1.8495e-02,  1.4523e-02,  1.9798e-02,  2.7512e-02,
         2.5342e-02,  2.6581e-02,  3.0815e-02,  2.9480e-02,  3.0153e-02,
        -1.3541e-02, -2.6686e-03, -8.0578e-05,  2.4874e-02,  2.5394e-02,
         1.1489e-02,  9.2654e-03, -2.0625e-03,  3.0997e-02, -4.4818e-03,
         2.1963e-02, -2.1865e-02,  1.1639e-02,  5.4199e-03, -1.9546e-04,
        -2.3388e-02,  3.0476e-02,  4.1587e-03,  5.1213e-03, -2.2984e-02,
         1.4368e-02,  7.3329e-03, -2.8363e-03, -1.3926e-02,  2.5625e-02,
         2.1590e-02,  1.5134e-02,  5.3738e-04,  6.3051e-03,  1.5386e-02,
         1.3849e-02,  1.3219e-02,  7.1302e-04,  1.5077e-02,  1.3048e-02,
        -7.3472e-03,  7.1319e-03,  1.3409e-02, -5.1378e-03,  1.4108e-02,
         5.2486e-03,  1.7127e-02,  1.6573e-02,  8.5862e-03,  1.2294e-02,
         1.0325e-02,  1.5506e-02,  1.6485e-02, -1.1988e-02,  1.4501e-02,
         1.9864e-02,  3.0396e-03,  6.5913e-03,  2.5232e-02,  5.4751e-03,
        -9.1758e-04,  7.2741e-03,  1.5015e-02,  1.9278e-02, -1.0413e-02,
         2.1569e-02,  2.0191e-02,  5.6239e-03,  2.2439e-02,  2.1586e-02,
        -1.9604e-02,  2.4163e-02,  1.2101e-02, -1.4782e-02,  1.6731e-02,
        -1.1648e-02,  1.2459e-02,  6.2698e-03,  1.3005e-02,  4.1027e-03,
         1.8513e-03,  1.5634e-02,  1.2711e-02,  2.1525e-02,  1.1233e-02,
         1.7843e-02,  1.4458e-02, -5.8861e-03,  1.3647e-02, -1.6958e-02,
         2.0693e-02,  7.7758e-03,  1.8999e-02, -4.3111e-04,  1.3021e-02,
         7.3502e-03, -1.6965e-02,  3.9416e-03,  3.0906e-03,  1.2134e-02,
         1.7973e-02,  2.7762e-03,  2.6469e-03,  4.2538e-03,  1.7783e-02,
         1.5699e-02,  6.1186e-03,  1.3970e-02,  1.9431e-02,  1.1454e-02,
         1.6956e-02,  8.7950e-03,  1.1811e-02,  1.9505e-02,  1.0819e-02,
         1.6870e-02,  1.6268e-02,  2.3017e-03, -1.0430e-02,  9.0804e-03,
         1.7362e-03, -4.6011e-03,  8.5289e-03,  4.4936e-03,  1.0876e-02,
         1.2349e-02, -1.7966e-02,  2.4560e-02,  6.5651e-03,  1.5782e-02,
        -2.4616e-04, -1.7185e-02,  1.0479e-02,  9.5276e-03,  2.6429e-02,
         2.4210e-02,  1.0465e-02, -1.0130e-02,  9.1357e-04,  7.7845e-03,
         6.0557e-03, -1.8866e-02,  1.9955e-02,  1.8331e-02,  1.5636e-02,
         3.8788e-03,  1.2975e-02,  1.3595e-02,  1.6285e-02, -2.0564e-02,
         2.0520e-02,  6.6743e-03,  1.7492e-03,  2.0533e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.0746, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:24,108 :: INFO :: Epoch 15: loss tensor(341.6158, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.1584e-02,  1.1207e-02,  7.2007e-03,  2.5076e-02,  7.9842e-04,
        -1.2921e-02,  1.0641e-02,  3.1275e-02, -6.2887e-03,  2.2651e-02,
        -3.6215e-03, -2.5626e-04,  7.1240e-03,  1.3104e-02,  1.4402e-02,
         2.6410e-02,  1.5622e-02,  2.6552e-02,  1.3460e-02,  2.5077e-02,
         1.4468e-02,  1.9821e-02,  2.0217e-02, -2.7308e-04,  6.1014e-03,
         6.6812e-03,  6.2233e-03,  1.3159e-02,  2.6210e-02,  1.7683e-02,
         2.0167e-02,  3.3628e-02,  5.5441e-03,  1.4481e-02, -1.7895e-02,
        -2.6058e-02,  1.3033e-02,  1.3668e-02, -5.7950e-03,  2.4855e-02,
        -1.3373e-02, -7.6059e-03,  1.8747e-02,  1.9469e-03,  8.1862e-03,
         1.8455e-02,  1.0611e-02, -1.7501e-02,  7.8379e-06,  6.9978e-03,
         3.2918e-03,  1.8365e-02, -2.1659e-03,  1.5945e-02,  1.3739e-02,
         3.5334e-02,  1.4464e-02,  2.4405e-02, -9.9212e-03, -1.8204e-02,
         2.7020e-02,  3.7773e-03, -6.2143e-03,  3.0937e-03,  1.7068e-02,
        -2.8748e-03,  3.5252e-02, -4.2924e-03,  3.7221e-02, -9.2523e-03,
         2.5938e-02, -3.4622e-03,  2.8681e-02,  9.8400e-06,  2.6758e-02,
        -8.7094e-03,  3.0530e-02, -7.0637e-03,  2.6617e-02,  2.3926e-02,
         1.4806e-02, -5.4748e-03,  3.0859e-02,  2.6845e-02, -1.8980e-02,
         2.4413e-02,  3.4570e-02, -4.3428e-03, -2.3737e-03, -1.3927e-02,
         4.2584e-03,  2.2154e-02,  1.3222e-02,  2.9246e-02, -4.1755e-03,
         1.5635e-02,  2.5760e-02,  8.0281e-03, -1.8482e-02,  8.1143e-03,
        -9.7699e-03,  1.5602e-02,  1.0737e-02, -5.8523e-03,  2.2652e-03,
        -2.4511e-02,  2.3747e-02, -5.3134e-03,  1.9598e-02,  2.7298e-02,
        -4.5870e-03,  2.0908e-02,  4.1514e-03,  6.7486e-03, -2.0824e-02,
        -1.7365e-02,  3.9037e-03, -9.1576e-03,  1.4118e-02,  5.6144e-03,
         1.1171e-03,  9.2103e-03,  1.0023e-02, -1.6882e-02,  4.2173e-03,
        -7.0865e-03, -7.2326e-03,  1.5551e-02,  4.0461e-02,  3.9230e-02,
         4.0233e-02,  4.7420e-02,  4.5499e-02,  4.4876e-02,  2.7061e-02,
         1.4099e-02, -1.5518e-02,  4.2719e-02,  3.0382e-02,  2.6019e-03,
         4.2808e-02,  3.7270e-03,  3.2584e-02, -1.1640e-02,  3.6858e-02,
         4.6932e-02,  3.7072e-02, -1.0020e-03,  3.0599e-02,  5.9566e-03,
        -3.8814e-03,  3.3296e-02,  4.5046e-02, -2.9501e-02,  1.6563e-02,
        -1.4928e-02,  6.8025e-03, -3.0106e-02,  4.5212e-02, -3.5589e-02,
         4.7757e-02, -3.6565e-02,  4.7801e-02,  2.7009e-02,  4.2885e-02,
         3.5085e-02, -1.0307e-02,  1.5278e-02,  9.5918e-03,  3.1276e-02,
         4.3824e-02,  4.2039e-02, -3.2463e-02,  4.0695e-02,  4.5741e-02,
        -8.4879e-03,  4.3875e-02,  4.1988e-02,  2.1584e-02, -2.3112e-03,
         3.6091e-02, -4.5566e-03,  4.2988e-02, -2.9903e-02,  4.1326e-02,
         4.4988e-02, -2.3435e-02,  3.6770e-02,  4.5809e-02, -3.1205e-02,
        -3.2295e-02,  1.8966e-02,  4.5982e-02, -2.3224e-02,  3.7582e-02,
         4.4735e-02,  1.7122e-02,  3.4530e-02,  4.2727e-02,  2.7253e-02,
         4.2059e-02,  2.1819e-02,  4.3040e-02,  4.9896e-02,  3.8291e-02,
         1.1073e-02,  2.9903e-03, -3.4763e-02,  4.3616e-02,  2.0643e-02,
         4.3573e-02,  4.6051e-02,  4.6310e-02, -1.2572e-03,  3.9048e-02,
         3.6972e-02,  2.1496e-02, -1.2037e-02,  4.6640e-02, -2.9429e-02,
         2.8917e-02,  2.2843e-02,  4.6667e-02,  4.1671e-02,  4.2862e-02,
         3.3964e-02,  3.5724e-02,  3.3461e-02,  3.7046e-02,  4.5885e-02,
         4.2478e-02,  4.2008e-02,  4.6509e-02,  4.7292e-02,  4.6376e-02,
        -3.6247e-03,  1.3631e-02,  1.6122e-02,  3.8316e-02,  4.2991e-02,
         2.9259e-02,  2.7262e-02,  1.4891e-02,  4.7542e-02,  1.0160e-02,
         3.9357e-02, -2.6960e-02,  2.9771e-02,  2.2475e-02,  1.6313e-02,
        -2.9835e-02,  4.7747e-02,  2.2391e-02,  2.3563e-02, -2.9158e-02,
         3.2011e-02,  2.6421e-02,  1.3600e-02, -8.3077e-04,  4.4113e-02,
         4.0165e-02,  3.4095e-02,  1.8776e-02,  2.4876e-02,  3.4211e-02,
         3.2908e-02,  3.2087e-02,  1.8898e-02,  3.3874e-02,  3.1599e-02,
         7.7503e-03,  2.5866e-02,  3.2477e-02,  1.1543e-02,  3.2822e-02,
         2.3781e-02,  3.5948e-02,  3.5320e-02,  2.7266e-02,  3.1213e-02,
         2.9138e-02,  3.4685e-02,  3.5332e-02,  1.2670e-03,  3.3512e-02,
         3.8409e-02,  2.1123e-02,  2.4846e-02,  4.3510e-02,  2.3618e-02,
         1.7092e-02,  2.5882e-02,  3.4196e-02,  3.7848e-02,  2.6381e-03,
         3.9985e-02,  3.9048e-02,  2.4297e-02,  4.1086e-02,  4.0166e-02,
        -1.5495e-02,  4.3028e-02,  3.1347e-02, -4.9739e-03,  3.5368e-02,
         2.3184e-03,  3.0905e-02,  2.4766e-02,  3.1831e-02,  2.3085e-02,
         2.0103e-02,  3.4655e-02,  3.1703e-02,  4.0410e-02,  3.0220e-02,
         3.6605e-02,  3.3198e-02,  1.0547e-02,  3.2224e-02, -8.4505e-03,
         3.9208e-02,  2.6767e-02,  3.7640e-02,  1.7537e-02,  3.1976e-02,
         2.5664e-02, -7.6652e-03,  2.2364e-02,  2.1588e-02,  3.0978e-02,
         3.6478e-02,  2.1364e-02,  2.0963e-02,  2.2895e-02,  3.6656e-02,
         3.4211e-02,  2.4183e-02,  3.2689e-02,  3.8475e-02,  3.0027e-02,
         3.5689e-02,  2.7659e-02,  3.0798e-02,  3.8314e-02,  2.9545e-02,
         3.5121e-02,  3.4875e-02,  2.0553e-02,  1.6343e-03,  2.7518e-02,
         2.0013e-02,  1.2100e-02,  2.7080e-02,  2.2124e-02,  2.9656e-02,
         3.1218e-02, -1.2340e-02,  4.3342e-02,  2.5061e-02,  3.4484e-02,
         1.7683e-02, -1.0411e-02,  2.9347e-02,  2.8718e-02,  4.5233e-02,
         4.2781e-02,  2.8986e-02,  3.7618e-03,  1.8700e-02,  2.6391e-02,
         2.4353e-02, -1.2002e-02,  3.8618e-02,  3.7022e-02,  3.4844e-02,
         2.2491e-02,  3.1660e-02,  3.1935e-02,  3.5041e-02, -1.8602e-02,
         3.9120e-02,  2.5305e-02,  1.9617e-02,  3.9346e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(5.5809, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:26,374 :: INFO :: Epoch 20: loss tensor(334.1760, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0309,  0.0301,  0.0268,  0.0456,  0.0173,  0.0025,  0.0310,  0.0501,
         0.0100,  0.0408,  0.0149,  0.0168,  0.0228,  0.0337,  0.0349,  0.0439,
         0.0315,  0.0464,  0.0313,  0.0450,  0.0328,  0.0378,  0.0399,  0.0196,
         0.0251,  0.0227,  0.0237,  0.0331,  0.0472,  0.0355,  0.0393,  0.0533,
         0.0259,  0.0337, -0.0029, -0.0206,  0.0309,  0.0306,  0.0085,  0.0453,
         0.0003,  0.0070,  0.0384,  0.0182,  0.0258,  0.0365,  0.0274, -0.0032,
         0.0196,  0.0240,  0.0220,  0.0370,  0.0134,  0.0327,  0.0344,  0.0551,
         0.0338,  0.0455,  0.0091, -0.0034,  0.0439,  0.0212,  0.0119,  0.0206,
         0.0374,  0.0135,  0.0555,  0.0153,  0.0547,  0.0072,  0.0441,  0.0155,
         0.0440,  0.0167,  0.0467,  0.0086,  0.0496,  0.0114,  0.0463,  0.0415,
         0.0345,  0.0146,  0.0493,  0.0474, -0.0058,  0.0449,  0.0528,  0.0143,
         0.0141, -0.0014,  0.0209,  0.0416,  0.0308,  0.0477,  0.0161,  0.0356,
         0.0430,  0.0284, -0.0077,  0.0274,  0.0092,  0.0321,  0.0294,  0.0094,
         0.0224, -0.0163,  0.0436,  0.0121,  0.0379,  0.0466,  0.0138,  0.0368,
         0.0244,  0.0263, -0.0122, -0.0040,  0.0235,  0.0054,  0.0299,  0.0246,
         0.0190,  0.0279,  0.0292, -0.0028,  0.0216,  0.0076,  0.0105,  0.0350,
         0.0575,  0.0562,  0.0581,  0.0625,  0.0624,  0.0640,  0.0447,  0.0319,
        -0.0054,  0.0511,  0.0479,  0.0192,  0.0614,  0.0201,  0.0518,  0.0024,
         0.0556,  0.0635,  0.0533,  0.0162,  0.0495,  0.0250,  0.0107,  0.0525,
         0.0626, -0.0351,  0.0342, -0.0027,  0.0239, -0.0349,  0.0611, -0.0435,
         0.0641, -0.0453,  0.0659,  0.0453,  0.0589,  0.0498,  0.0007,  0.0332,
         0.0276,  0.0487,  0.0632,  0.0584, -0.0387,  0.0587,  0.0643,  0.0045,
         0.0570,  0.0589,  0.0396,  0.0112,  0.0538,  0.0105,  0.0591, -0.0341,
         0.0580,  0.0640, -0.0236,  0.0560,  0.0661, -0.0372, -0.0370,  0.0372,
         0.0618, -0.0219,  0.0535,  0.0633,  0.0366,  0.0428,  0.0601,  0.0466,
         0.0601,  0.0416,  0.0597,  0.0674,  0.0564,  0.0306,  0.0195, -0.0430,
         0.0587,  0.0385,  0.0628,  0.0620,  0.0629,  0.0133,  0.0567,  0.0568,
         0.0397, -0.0011,  0.0639, -0.0336,  0.0478,  0.0414,  0.0653,  0.0597,
         0.0617,  0.0536,  0.0525,  0.0532,  0.0539,  0.0646,  0.0592,  0.0565,
         0.0609,  0.0643,  0.0623,  0.0128,  0.0319,  0.0338,  0.0473,  0.0611,
         0.0471,  0.0460,  0.0342,  0.0638,  0.0277,  0.0571, -0.0286,  0.0481,
         0.0403,  0.0343, -0.0324,  0.0658,  0.0421,  0.0432, -0.0312,  0.0497,
         0.0469,  0.0327,  0.0180,  0.0629,  0.0592,  0.0542,  0.0384,  0.0444,
         0.0542,  0.0528,  0.0515,  0.0384,  0.0530,  0.0505,  0.0258,  0.0456,
         0.0523,  0.0304,  0.0520,  0.0433,  0.0555,  0.0547,  0.0464,  0.0506,
         0.0485,  0.0552,  0.0545,  0.0196,  0.0538,  0.0568,  0.0405,  0.0441,
         0.0615,  0.0430,  0.0363,  0.0454,  0.0546,  0.0565,  0.0196,  0.0584,
         0.0584,  0.0435,  0.0597,  0.0590, -0.0032,  0.0628,  0.0517,  0.0120,
         0.0547,  0.0211,  0.0496,  0.0440,  0.0511,  0.0432,  0.0397,  0.0546,
         0.0515,  0.0599,  0.0497,  0.0556,  0.0523,  0.0304,  0.0515,  0.0070,
         0.0580,  0.0468,  0.0567,  0.0370,  0.0517,  0.0446,  0.0092,  0.0420,
         0.0409,  0.0503,  0.0556,  0.0410,  0.0406,  0.0426,  0.0562,  0.0526,
         0.0428,  0.0519,  0.0583,  0.0487,  0.0550,  0.0472,  0.0504,  0.0578,
         0.0488,  0.0526,  0.0532,  0.0401,  0.0186,  0.0469,  0.0394,  0.0311,
         0.0465,  0.0396,  0.0494,  0.0506,  0.0014,  0.0625,  0.0442,  0.0537,
         0.0374,  0.0047,  0.0494,  0.0492,  0.0645,  0.0615,  0.0483,  0.0219,
         0.0376,  0.0458,  0.0438,  0.0031,  0.0576,  0.0559,  0.0552,  0.0427,
         0.0505,  0.0499,  0.0541, -0.0088,  0.0581,  0.0449,  0.0391,  0.0584],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.0518, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:28,702 :: INFO :: Epoch 25: loss tensor(330.1114, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0486,  0.0466,  0.0450,  0.0642,  0.0331,  0.0203,  0.0502,  0.0677,
         0.0264,  0.0575,  0.0338,  0.0331,  0.0377,  0.0525,  0.0529,  0.0586,
         0.0449,  0.0644,  0.0465,  0.0637,  0.0492,  0.0534,  0.0574,  0.0391,
         0.0427,  0.0378,  0.0388,  0.0512,  0.0673,  0.0504,  0.0567,  0.0700,
         0.0453,  0.0510,  0.0133, -0.0115,  0.0453,  0.0465,  0.0228,  0.0646,
         0.0154,  0.0229,  0.0566,  0.0330,  0.0410,  0.0528,  0.0422,  0.0128,
         0.0386,  0.0381,  0.0383,  0.0531,  0.0277,  0.0464,  0.0542,  0.0736,
         0.0531,  0.0653,  0.0280,  0.0134,  0.0582,  0.0378,  0.0308,  0.0360,
         0.0553,  0.0289,  0.0748,  0.0337,  0.0705,  0.0237,  0.0595,  0.0338,
         0.0562,  0.0342,  0.0648,  0.0245,  0.0673,  0.0294,  0.0632,  0.0592,
         0.0531,  0.0355,  0.0662,  0.0667,  0.0094,  0.0642,  0.0686,  0.0318,
         0.0293,  0.0117,  0.0369,  0.0595,  0.0463,  0.0641,  0.0362,  0.0541,
         0.0573,  0.0469,  0.0055,  0.0446,  0.0283,  0.0461,  0.0457,  0.0260,
         0.0415, -0.0032,  0.0609,  0.0275,  0.0548,  0.0639,  0.0323,  0.0511,
         0.0434,  0.0433, -0.0011,  0.0096,  0.0406,  0.0215,  0.0434,  0.0421,
         0.0358,  0.0451,  0.0482,  0.0119,  0.0381,  0.0228,  0.0276,  0.0527,
         0.0741,  0.0724,  0.0756,  0.0759,  0.0780,  0.0827,  0.0614,  0.0497,
         0.0089,  0.0562,  0.0647,  0.0365,  0.0797,  0.0373,  0.0706,  0.0194,
         0.0741,  0.0795,  0.0679,  0.0353,  0.0682,  0.0444,  0.0275,  0.0717,
         0.0796, -0.0394,  0.0513,  0.0133,  0.0414, -0.0377,  0.0750, -0.0496,
         0.0794, -0.0527,  0.0834,  0.0632,  0.0737,  0.0604,  0.0141,  0.0508,
         0.0459,  0.0654,  0.0821,  0.0736, -0.0434,  0.0763,  0.0827,  0.0205,
         0.0678,  0.0749,  0.0568,  0.0252,  0.0708,  0.0275,  0.0731, -0.0361,
         0.0737,  0.0830, -0.0212,  0.0748,  0.0860, -0.0419, -0.0384,  0.0553,
         0.0765, -0.0174,  0.0676,  0.0817,  0.0564,  0.0476,  0.0768,  0.0659,
         0.0779,  0.0617,  0.0752,  0.0831,  0.0741,  0.0507,  0.0365, -0.0498,
         0.0719,  0.0557,  0.0818,  0.0770,  0.0758,  0.0292,  0.0735,  0.0766,
         0.0575,  0.0126,  0.0808, -0.0358,  0.0666,  0.0592,  0.0837,  0.0774,
         0.0789,  0.0733,  0.0681,  0.0732,  0.0700,  0.0831,  0.0750,  0.0687,
         0.0738,  0.0783,  0.0771,  0.0314,  0.0503,  0.0512,  0.0535,  0.0784,
         0.0642,  0.0646,  0.0538,  0.0793,  0.0457,  0.0744, -0.0275,  0.0661,
         0.0577,  0.0524, -0.0314,  0.0840,  0.0620,  0.0629, -0.0296,  0.0666,
         0.0677,  0.0525,  0.0384,  0.0812,  0.0781,  0.0744,  0.0580,  0.0639,
         0.0744,  0.0728,  0.0710,  0.0576,  0.0715,  0.0684,  0.0439,  0.0652,
         0.0722,  0.0495,  0.0707,  0.0628,  0.0752,  0.0739,  0.0648,  0.0695,
         0.0675,  0.0763,  0.0732,  0.0397,  0.0747,  0.0742,  0.0600,  0.0636,
         0.0784,  0.0626,  0.0555,  0.0649,  0.0757,  0.0744,  0.0370,  0.0764,
         0.0775,  0.0627,  0.0774,  0.0774,  0.0128,  0.0831,  0.0721,  0.0318,
         0.0743,  0.0414,  0.0672,  0.0629,  0.0700,  0.0633,  0.0596,  0.0749,
         0.0715,  0.0793,  0.0691,  0.0741,  0.0708,  0.0514,  0.0705,  0.0249,
         0.0765,  0.0671,  0.0753,  0.0564,  0.0716,  0.0627,  0.0287,  0.0620,
         0.0596,  0.0696,  0.0746,  0.0605,  0.0604,  0.0621,  0.0760,  0.0699,
         0.0604,  0.0711,  0.0782,  0.0663,  0.0741,  0.0665,  0.0699,  0.0773,
         0.0678,  0.0686,  0.0707,  0.0597,  0.0370,  0.0662,  0.0584,  0.0501,
         0.0659,  0.0557,  0.0691,  0.0696,  0.0192,  0.0814,  0.0630,  0.0729,
         0.0571,  0.0235,  0.0698,  0.0701,  0.0835,  0.0798,  0.0677,  0.0411,
         0.0558,  0.0652,  0.0633,  0.0220,  0.0761,  0.0744,  0.0760,  0.0635,
         0.0686,  0.0660,  0.0728,  0.0058,  0.0766,  0.0646,  0.0591,  0.0768],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.4390, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:30,624 :: INFO :: Epoch 30: loss tensor(327.6672, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0632,  0.0591,  0.0609,  0.0806,  0.0469,  0.0382,  0.0669,  0.0827,
         0.0416,  0.0707,  0.0514,  0.0464,  0.0512,  0.0680,  0.0671,  0.0692,
         0.0556,  0.0793,  0.0565,  0.0798,  0.0628,  0.0651,  0.0724,  0.0573,
         0.0567,  0.0504,  0.0498,  0.0663,  0.0860,  0.0625,  0.0705,  0.0833,
         0.0625,  0.0654,  0.0279, -0.0025,  0.0559,  0.0614,  0.0358,  0.0818,
         0.0294,  0.0380,  0.0728,  0.0446,  0.0521,  0.0665,  0.0541,  0.0276,
         0.0559,  0.0482,  0.0493,  0.0651,  0.0402,  0.0565,  0.0727,  0.0895,
         0.0716,  0.0831,  0.0444,  0.0295,  0.0694,  0.0516,  0.0485,  0.0480,
         0.0695,  0.0417,  0.0925,  0.0488,  0.0838,  0.0387,  0.0700,  0.0493,
         0.0637,  0.0518,  0.0791,  0.0370,  0.0826,  0.0446,  0.0766,  0.0756,
         0.0699,  0.0559,  0.0804,  0.0839,  0.0225,  0.0815,  0.0799,  0.0465,
         0.0427,  0.0228,  0.0502,  0.0747,  0.0591,  0.0776,  0.0551,  0.0704,
         0.0689,  0.0614,  0.0183,  0.0581,  0.0463,  0.0566,  0.0585,  0.0424,
         0.0586,  0.0107,  0.0742,  0.0393,  0.0688,  0.0787,  0.0496,  0.0635,
         0.0597,  0.0567,  0.0093,  0.0205,  0.0545,  0.0378,  0.0535,  0.0569,
         0.0492,  0.0581,  0.0655,  0.0238,  0.0523,  0.0356,  0.0422,  0.0678,
         0.0898,  0.0876,  0.0927,  0.0873,  0.0913,  0.1002,  0.0771,  0.0670,
         0.0253,  0.0606,  0.0804,  0.0534,  0.0971,  0.0544,  0.0889,  0.0378,
         0.0921,  0.0947,  0.0803,  0.0548,  0.0863,  0.0632,  0.0449,  0.0907,
         0.0959, -0.0427,  0.0674,  0.0306,  0.0589, -0.0389,  0.0868, -0.0540,
         0.0937, -0.0588,  0.1002,  0.0802,  0.0872,  0.0655,  0.0283,  0.0679,
         0.0638,  0.0811,  0.1005,  0.0876, -0.0465,  0.0935,  0.1006,  0.0379,
         0.0758,  0.0899,  0.0733,  0.0393,  0.0872,  0.0451,  0.0852, -0.0363,
         0.0884,  0.1017, -0.0169,  0.0927,  0.1052, -0.0454, -0.0369,  0.0730,
         0.0900, -0.0102,  0.0797,  0.0997,  0.0760,  0.0480,  0.0928,  0.0847,
         0.0953,  0.0814,  0.0897,  0.0969,  0.0913,  0.0708,  0.0529, -0.0554,
         0.0832,  0.0723,  0.1003,  0.0909,  0.0840,  0.0454,  0.0895,  0.0963,
         0.0747,  0.0271,  0.0971, -0.0363,  0.0850,  0.0760,  0.1018,  0.0949,
         0.0937,  0.0927,  0.0823,  0.0929,  0.0847,  0.1011,  0.0897,  0.0783,
         0.0852,  0.0882,  0.0907,  0.0505,  0.0682,  0.0682,  0.0571,  0.0946,
         0.0804,  0.0828,  0.0733,  0.0941,  0.0636,  0.0912, -0.0241,  0.0834,
         0.0745,  0.0700, -0.0271,  0.1019,  0.0817,  0.0822, -0.0253,  0.0828,
         0.0884,  0.0726,  0.0594,  0.0985,  0.0965,  0.0947,  0.0770,  0.0826,
         0.0947,  0.0924,  0.0902,  0.0762,  0.0889,  0.0851,  0.0612,  0.0841,
         0.0917,  0.0684,  0.0885,  0.0816,  0.0948,  0.0927,  0.0823,  0.0877,
         0.0857,  0.0976,  0.0909,  0.0600,  0.0957,  0.0903,  0.0790,  0.0826,
         0.0940,  0.0818,  0.0742,  0.0837,  0.0968,  0.0910,  0.0539,  0.0935,
         0.0958,  0.0813,  0.0940,  0.0949,  0.0309,  0.1036,  0.0925,  0.0528,
         0.0937,  0.0625,  0.0833,  0.0813,  0.0883,  0.0829,  0.0793,  0.0950,
         0.0913,  0.0983,  0.0877,  0.0917,  0.0887,  0.0729,  0.0888,  0.0440,
         0.0941,  0.0872,  0.0932,  0.0752,  0.0911,  0.0797,  0.0490,  0.0816,
         0.0775,  0.0881,  0.0929,  0.0792,  0.0796,  0.0808,  0.0957,  0.0856,
         0.0761,  0.0896,  0.0981,  0.0824,  0.0925,  0.0850,  0.0884,  0.0963,
         0.0859,  0.0826,  0.0870,  0.0790,  0.0555,  0.0850,  0.0764,  0.0693,
         0.0847,  0.0698,  0.0883,  0.0875,  0.0389,  0.1000,  0.0810,  0.0913,
         0.0764,  0.0439,  0.0904,  0.0911,  0.1019,  0.0970,  0.0866,  0.0603,
         0.0733,  0.0839,  0.0824,  0.0422,  0.0935,  0.0920,  0.0968,  0.0843,
         0.0854,  0.0803,  0.0903,  0.0231,  0.0942,  0.0836,  0.0787,  0.0943],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.7337, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:32,702 :: INFO :: Epoch 35: loss tensor(320.3222, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0772,  0.0692,  0.0771,  0.0960,  0.0601,  0.0546,  0.0825,  0.0974,
         0.0549,  0.0826,  0.0680,  0.0584,  0.0632,  0.0828,  0.0799,  0.0796,
         0.0635,  0.0928,  0.0648,  0.0961,  0.0762,  0.0744,  0.0864,  0.0748,
         0.0704,  0.0622,  0.0581,  0.0806,  0.1042,  0.0732,  0.0842,  0.0948,
         0.0775,  0.0786,  0.0411,  0.0066,  0.0643,  0.0759,  0.0479,  0.0971,
         0.0436,  0.0520,  0.0865,  0.0548,  0.0615,  0.0776,  0.0650,  0.0424,
         0.0724,  0.0561,  0.0588,  0.0753,  0.0516,  0.0642,  0.0904,  0.1050,
         0.0881,  0.1014,  0.0599,  0.0446,  0.0789,  0.0651,  0.0641,  0.0591,
         0.0822,  0.0533,  0.1076,  0.0622,  0.0963,  0.0524,  0.0801,  0.0635,
         0.0709,  0.0692,  0.0926,  0.0479,  0.0971,  0.0602,  0.0877,  0.0904,
         0.0865,  0.0753,  0.0940,  0.1013,  0.0346,  0.0976,  0.0898,  0.0603,
         0.0545,  0.0324,  0.0629,  0.0893,  0.0709,  0.0890,  0.0727,  0.0851,
         0.0788,  0.0748,  0.0314,  0.0697,  0.0617,  0.0658,  0.0706,  0.0591,
         0.0754,  0.0249,  0.0865,  0.0497,  0.0813,  0.0929,  0.0671,  0.0743,
         0.0748,  0.0697,  0.0199,  0.0308,  0.0671,  0.0540,  0.0620,  0.0716,
         0.0615,  0.0709,  0.0813,  0.0331,  0.0654,  0.0485,  0.0562,  0.0825,
         0.1042,  0.1013,  0.1087,  0.0953,  0.1016,  0.1165,  0.0912,  0.0831,
         0.0419,  0.0629,  0.0946,  0.0689,  0.1133,  0.0703,  0.1060,  0.0570,
         0.1090,  0.1088,  0.0899,  0.0740,  0.1031,  0.0810,  0.0616,  0.1090,
         0.1109, -0.0455,  0.0816,  0.0480,  0.0750, -0.0391,  0.0967, -0.0569,
         0.1061, -0.0636,  0.1157,  0.0957,  0.0994,  0.0669,  0.0415,  0.0835,
         0.0805,  0.0952,  0.1179,  0.1000, -0.0487,  0.1097,  0.1176,  0.0552,
         0.0817,  0.1032,  0.0878,  0.0524,  0.1021,  0.0619,  0.0941, -0.0355,
         0.1016,  0.1195, -0.0116,  0.1092,  0.1238, -0.0480, -0.0327,  0.0895,
         0.1022, -0.0021,  0.0888,  0.1167,  0.0949,  0.0475,  0.1071,  0.1025,
         0.1117,  0.1003,  0.1025,  0.1078,  0.1071,  0.0903,  0.0675, -0.0600,
         0.0921,  0.0874,  0.1177,  0.1032,  0.0876,  0.0602,  0.1035,  0.1154,
         0.0907,  0.0407,  0.1122, -0.0358,  0.1023,  0.0909,  0.1190,  0.1115,
         0.1061,  0.1115,  0.0946,  0.1120,  0.0965,  0.1182,  0.1025,  0.0843,
         0.0952,  0.0941,  0.1025,  0.0691,  0.0848,  0.0839,  0.0585,  0.1091,
         0.0950,  0.0999,  0.0920,  0.1078,  0.0805,  0.1071, -0.0195,  0.0991,
         0.0899,  0.0866, -0.0202,  0.1192,  0.1005,  0.1008, -0.0191,  0.0974,
         0.1087,  0.0922,  0.0803,  0.1145,  0.1146,  0.1148,  0.0949,  0.1002,
         0.1151,  0.1122,  0.1085,  0.0938,  0.1050,  0.0997,  0.0771,  0.1021,
         0.1109,  0.0864,  0.1048,  0.0994,  0.1142,  0.1107,  0.0986,  0.1044,
         0.1029,  0.1187,  0.1075,  0.0801,  0.1166,  0.1045,  0.0969,  0.1007,
         0.1080,  0.1001,  0.0925,  0.1016,  0.1179,  0.1061,  0.0692,  0.1098,
         0.1132,  0.0992,  0.1087,  0.1110,  0.0497,  0.1238,  0.1129,  0.0739,
         0.1130,  0.0833,  0.0972,  0.0993,  0.1051,  0.1018,  0.0984,  0.1150,
         0.1107,  0.1170,  0.1057,  0.1078,  0.1057,  0.0941,  0.1060,  0.0633,
         0.1104,  0.1072,  0.1098,  0.0932,  0.1104,  0.0952,  0.0694,  0.1009,
         0.0938,  0.1055,  0.1101,  0.0967,  0.0978,  0.0985,  0.1149,  0.0989,
         0.0896,  0.1070,  0.1177,  0.0960,  0.1097,  0.1023,  0.1057,  0.1150,
         0.1027,  0.0944,  0.1014,  0.0977,  0.0734,  0.1029,  0.0930,  0.0876,
         0.1024,  0.0816,  0.1068,  0.1042,  0.0590,  0.1181,  0.0975,  0.1088,
         0.0956,  0.0645,  0.1107,  0.1121,  0.1200,  0.1128,  0.1046,  0.0793,
         0.0892,  0.1017,  0.1005,  0.0626,  0.1095,  0.1083,  0.1176,  0.1048,
         0.1004,  0.0921,  0.1063,  0.0414,  0.1106,  0.1017,  0.0976,  0.1109],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.9451, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:34,686 :: INFO :: Epoch 40: loss tensor(318.8211, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0887,  0.0768,  0.0926,  0.1089,  0.0712,  0.0703,  0.0959,  0.1093,
         0.0674,  0.0920,  0.0837,  0.0690,  0.0729,  0.0954,  0.0900,  0.0877,
         0.0694,  0.1033,  0.0700,  0.1105,  0.0863,  0.0800,  0.0988,  0.0908,
         0.0827,  0.0711,  0.0622,  0.0930,  0.1213,  0.0810,  0.0964,  0.1029,
         0.0909,  0.0884,  0.0529,  0.0148,  0.0702,  0.0887,  0.0579,  0.1103,
         0.0568,  0.0645,  0.0990,  0.0616,  0.0684,  0.0859,  0.0731,  0.0559,
         0.0881,  0.0613,  0.0661,  0.0815,  0.0614,  0.0671,  0.1073,  0.1190,
         0.1037,  0.1191,  0.0729,  0.0584,  0.0844,  0.0760,  0.0786,  0.0679,
         0.0921,  0.0606,  0.1217,  0.0742,  0.1066,  0.0646,  0.0870,  0.0767,
         0.0742,  0.0860,  0.1033,  0.0550,  0.1088,  0.0737,  0.0953,  0.1030,
         0.1018,  0.0944,  0.1052,  0.1174,  0.0447,  0.1119,  0.0969,  0.0723,
         0.0641,  0.0385,  0.0735,  0.1024,  0.0800,  0.0977,  0.0892,  0.0983,
         0.0859,  0.0863,  0.0433,  0.0786,  0.0760,  0.0720,  0.0796,  0.0741,
         0.0906,  0.0387,  0.0945,  0.0576,  0.0911,  0.1044,  0.0840,  0.0829,
         0.0890,  0.0798,  0.0288,  0.0374,  0.0774,  0.0702,  0.0671,  0.0840,
         0.0713,  0.0809,  0.0955,  0.0405,  0.0758,  0.0593,  0.0684,  0.0956,
         0.1176,  0.1139,  0.1234,  0.1005,  0.1088,  0.1316,  0.1037,  0.0979,
         0.0581,  0.0646,  0.1074,  0.0829,  0.1276,  0.0851,  0.1214,  0.0761,
         0.1247,  0.1214,  0.0960,  0.0922,  0.1185,  0.0976,  0.0773,  0.1260,
         0.1241, -0.0476,  0.0942,  0.0647,  0.0901, -0.0383,  0.1043, -0.0582,
         0.1174, -0.0672,  0.1287,  0.1097,  0.1102,  0.0640,  0.0541,  0.0979,
         0.0959,  0.1079,  0.1341,  0.1106, -0.0498,  0.1248,  0.1335,  0.0718,
         0.0855,  0.1147,  0.1007,  0.0651,  0.1154,  0.0777,  0.0986, -0.0336,
         0.1130,  0.1361, -0.0058,  0.1237,  0.1414, -0.0499, -0.0264,  0.1044,
         0.1126,  0.0066,  0.0951,  0.1325,  0.1127,  0.0451,  0.1201,  0.1189,
         0.1268,  0.1177,  0.1138,  0.1145,  0.1210,  0.1086,  0.0808, -0.0635,
         0.0986,  0.1011,  0.1331,  0.1134,  0.0877,  0.0739,  0.1157,  0.1333,
         0.1050,  0.0537,  0.1252, -0.0343,  0.1186,  0.1038,  0.1349,  0.1275,
         0.1154,  0.1289,  0.1052,  0.1300,  0.1048,  0.1340,  0.1130,  0.0875,
         0.1039,  0.0943,  0.1125,  0.0865,  0.1001,  0.0984,  0.0604,  0.1208,
         0.1079,  0.1158,  0.1097,  0.1208,  0.0963,  0.1220, -0.0139,  0.1133,
         0.1040,  0.1020, -0.0116,  0.1355,  0.1180,  0.1182, -0.0117,  0.1102,
         0.1285,  0.1109,  0.1006,  0.1290,  0.1321,  0.1343,  0.1117,  0.1167,
         0.1352,  0.1318,  0.1260,  0.1107,  0.1197,  0.1127,  0.0920,  0.1191,
         0.1297,  0.1036,  0.1196,  0.1162,  0.1334,  0.1278,  0.1137,  0.1198,
         0.1188,  0.1393,  0.1226,  0.0997,  0.1371,  0.1165,  0.1138,  0.1178,
         0.1207,  0.1176,  0.1099,  0.1184,  0.1388,  0.1198,  0.0830,  0.1251,
         0.1294,  0.1167,  0.1214,  0.1258,  0.0687,  0.1436,  0.1330,  0.0945,
         0.1317,  0.1035,  0.1091,  0.1170,  0.1206,  0.1200,  0.1167,  0.1347,
         0.1295,  0.1353,  0.1233,  0.1225,  0.1220,  0.1147,  0.1221,  0.0824,
         0.1253,  0.1265,  0.1251,  0.1103,  0.1292,  0.1094,  0.0895,  0.1197,
         0.1091,  0.1217,  0.1263,  0.1130,  0.1150,  0.1149,  0.1335,  0.1099,
         0.1007,  0.1233,  0.1369,  0.1070,  0.1258,  0.1183,  0.1219,  0.1329,
         0.1184,  0.1041,  0.1142,  0.1157,  0.0907,  0.1199,  0.1081,  0.1052,
         0.1191,  0.0916,  0.1243,  0.1194,  0.0790,  0.1359,  0.1128,  0.1253,
         0.1146,  0.0850,  0.1306,  0.1327,  0.1376,  0.1271,  0.1219,  0.0981,
         0.1042,  0.1183,  0.1177,  0.0825,  0.1237,  0.1231,  0.1381,  0.1246,
         0.1138,  0.1020,  0.1204,  0.0602,  0.1254,  0.1188,  0.1158,  0.1264],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.0627, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:30:34,686 :: INFO :: ----- frontend -----
2023-04-11 14:30:34,686 :: INFO :: Environment 0
2023-04-11 14:30:49,484 :: INFO :: Epoch 5: loss tensor(837.3739, device='cuda:0'), U.norm 14.64841365814209, V.norm 18.28533935546875, MLP.norm 1.8741796016693115
2023-04-11 14:30:49,577 :: INFO :: Epoch 10: loss tensor(825.1317, device='cuda:0'), U.norm 12.043856620788574, V.norm 17.775676727294922, MLP.norm 2.653385877609253
2023-04-11 14:30:49,655 :: INFO :: Epoch 15: loss tensor(808.0425, device='cuda:0'), U.norm 10.381181716918945, V.norm 17.547607421875, MLP.norm 3.6683878898620605
2023-04-11 14:30:49,765 :: INFO :: Epoch 20: loss tensor(787.5581, device='cuda:0'), U.norm 9.188216209411621, V.norm 17.401762008666992, MLP.norm 4.728372573852539
2023-04-11 14:30:49,765 :: INFO :: Environment 1
2023-04-11 14:31:02,530 :: INFO :: Epoch 5: loss tensor(861.1701, device='cuda:0'), U.norm 14.648165702819824, V.norm 18.32138442993164, MLP.norm 1.8677492141723633
2023-04-11 14:31:02,624 :: INFO :: Epoch 10: loss tensor(846.8743, device='cuda:0'), U.norm 12.044111251831055, V.norm 17.82631492614746, MLP.norm 2.6577627658843994
2023-04-11 14:31:02,766 :: INFO :: Epoch 15: loss tensor(826.2785, device='cuda:0'), U.norm 10.38238525390625, V.norm 17.5998592376709, MLP.norm 3.688204765319824
2023-04-11 14:31:02,874 :: INFO :: Epoch 20: loss tensor(801.5701, device='cuda:0'), U.norm 9.190545082092285, V.norm 17.452880859375, MLP.norm 4.758709907531738
2023-04-11 14:31:02,874 :: INFO :: Environment 2
2023-04-11 14:31:16,671 :: INFO :: Epoch 5: loss tensor(854.0235, device='cuda:0'), U.norm 14.648995399475098, V.norm 18.31740951538086, MLP.norm 1.8660005331039429
2023-04-11 14:31:16,780 :: INFO :: Epoch 10: loss tensor(840.2184, device='cuda:0'), U.norm 12.045333862304688, V.norm 17.817676544189453, MLP.norm 2.6497957706451416
2023-04-11 14:31:16,874 :: INFO :: Epoch 15: loss tensor(820.2673, device='cuda:0'), U.norm 10.384086608886719, V.norm 17.59027862548828, MLP.norm 3.668842315673828
2023-04-11 14:31:16,968 :: INFO :: Epoch 20: loss tensor(796.3965, device='cuda:0'), U.norm 9.192893981933594, V.norm 17.443634033203125, MLP.norm 4.7335357666015625
2023-04-11 14:31:16,968 :: INFO :: Environment 3
2023-04-11 14:31:31,905 :: INFO :: Epoch 5: loss tensor(811.7056, device='cuda:0'), U.norm 14.647021293640137, V.norm 18.2384033203125, MLP.norm 1.864817500114441
2023-04-11 14:31:32,014 :: INFO :: Epoch 10: loss tensor(799.7125, device='cuda:0'), U.norm 12.040950775146484, V.norm 17.708337783813477, MLP.norm 2.6159827709198
2023-04-11 14:31:32,108 :: INFO :: Epoch 15: loss tensor(782.2836, device='cuda:0'), U.norm 10.376625061035156, V.norm 17.46896743774414, MLP.norm 3.5978598594665527
2023-04-11 14:31:32,202 :: INFO :: Epoch 20: loss tensor(761.2525, device='cuda:0'), U.norm 9.181694984436035, V.norm 17.31846046447754, MLP.norm 4.632116317749023
2023-04-11 14:31:32,217 :: INFO :: Environment 4
2023-04-11 14:31:46,327 :: INFO :: Epoch 5: loss tensor(826.6943, device='cuda:0'), U.norm 14.648125648498535, V.norm 18.258237838745117, MLP.norm 1.865452527999878
2023-04-11 14:31:46,436 :: INFO :: Epoch 10: loss tensor(814.0945, device='cuda:0'), U.norm 12.043144226074219, V.norm 17.735687255859375, MLP.norm 2.6282148361206055
2023-04-11 14:31:46,514 :: INFO :: Epoch 15: loss tensor(795.7532, device='cuda:0'), U.norm 10.380337715148926, V.norm 17.500818252563477, MLP.norm 3.630688190460205
2023-04-11 14:31:46,608 :: INFO :: Epoch 20: loss tensor(773.1682, device='cuda:0'), U.norm 9.187175750732422, V.norm 17.3480224609375, MLP.norm 4.692960262298584
2023-04-11 14:31:46,608 :: INFO :: Environment 5
2023-04-11 14:32:01,108 :: INFO :: Epoch 5: loss tensor(879.9066, device='cuda:0'), U.norm 14.648436546325684, V.norm 18.36918067932129, MLP.norm 1.8873140811920166
2023-04-11 14:32:01,186 :: INFO :: Epoch 10: loss tensor(864.7968, device='cuda:0'), U.norm 12.044864654541016, V.norm 17.892934799194336, MLP.norm 2.702763557434082
2023-04-11 14:32:01,280 :: INFO :: Epoch 15: loss tensor(843.5372, device='cuda:0'), U.norm 10.383983612060547, V.norm 17.67953872680664, MLP.norm 3.747802495956421
2023-04-11 14:32:01,358 :: INFO :: Epoch 20: loss tensor(817.6461, device='cuda:0'), U.norm 9.193202018737793, V.norm 17.542457580566406, MLP.norm 4.8395233154296875
2023-04-11 14:32:01,373 :: INFO :: Environment 6
2023-04-11 14:32:15,123 :: INFO :: Epoch 5: loss tensor(921.8500, device='cuda:0'), U.norm 14.650724411010742, V.norm 18.439367294311523, MLP.norm 1.8718888759613037
2023-04-11 14:32:15,202 :: INFO :: Epoch 10: loss tensor(906.0626, device='cuda:0'), U.norm 12.04949951171875, V.norm 17.987287521362305, MLP.norm 2.6966753005981445
2023-04-11 14:32:15,311 :: INFO :: Epoch 15: loss tensor(883.6223, device='cuda:0'), U.norm 10.39145565032959, V.norm 17.7822265625, MLP.norm 3.790344715118408
2023-04-11 14:32:15,405 :: INFO :: Epoch 20: loss tensor(855.9398, device='cuda:0'), U.norm 9.204073905944824, V.norm 17.649797439575195, MLP.norm 4.943329811096191
2023-04-11 14:32:15,405 :: INFO :: Environment 7
2023-04-11 14:32:29,608 :: INFO :: Epoch 5: loss tensor(885.1347, device='cuda:0'), U.norm 14.649225234985352, V.norm 18.369796752929688, MLP.norm 1.8714370727539062
2023-04-11 14:32:29,686 :: INFO :: Epoch 10: loss tensor(871.6733, device='cuda:0'), U.norm 12.045918464660645, V.norm 17.89261817932129, MLP.norm 2.656594753265381
2023-04-11 14:32:29,780 :: INFO :: Epoch 15: loss tensor(852.6095, device='cuda:0'), U.norm 10.385329246520996, V.norm 17.6790714263916, MLP.norm 3.682591438293457
2023-04-11 14:32:29,874 :: INFO :: Epoch 20: loss tensor(828.5251, device='cuda:0'), U.norm 9.194942474365234, V.norm 17.542545318603516, MLP.norm 4.761242389678955
2023-04-11 14:32:29,874 :: INFO :: Environment 8
2023-04-11 14:32:44,202 :: INFO :: Epoch 5: loss tensor(799.2241, device='cuda:0'), U.norm 14.644018173217773, V.norm 18.219623565673828, MLP.norm 1.8431288003921509
2023-04-11 14:32:44,296 :: INFO :: Epoch 10: loss tensor(787.6042, device='cuda:0'), U.norm 12.03520393371582, V.norm 17.67987060546875, MLP.norm 2.5780513286590576
2023-04-11 14:32:44,405 :: INFO :: Epoch 15: loss tensor(769.8510, device='cuda:0'), U.norm 10.367904663085938, V.norm 17.437480926513672, MLP.norm 3.5602660179138184
2023-04-11 14:32:44,515 :: INFO :: Epoch 20: loss tensor(748.2712, device='cuda:0'), U.norm 9.16986083984375, V.norm 17.282888412475586, MLP.norm 4.615426063537598
2023-04-11 14:32:44,515 :: INFO :: Environment 9
2023-04-11 14:32:58,811 :: INFO :: Epoch 5: loss tensor(825.6633, device='cuda:0'), U.norm 14.646759033203125, V.norm 18.258115768432617, MLP.norm 1.8592575788497925
2023-04-11 14:32:58,952 :: INFO :: Epoch 10: loss tensor(813.1041, device='cuda:0'), U.norm 12.040780067443848, V.norm 17.735380172729492, MLP.norm 2.616558790206909
2023-04-11 14:32:59,046 :: INFO :: Epoch 15: loss tensor(794.9452, device='cuda:0'), U.norm 10.376705169677734, V.norm 17.499563217163086, MLP.norm 3.624211311340332
2023-04-11 14:32:59,139 :: INFO :: Epoch 20: loss tensor(773.3002, device='cuda:0'), U.norm 9.181949615478516, V.norm 17.348247528076172, MLP.norm 4.686493396759033
2023-04-11 14:32:59,139 :: INFO :: Ite = 1, Delta = 4041
2023-04-11 14:32:59,139 :: INFO :: ----- backend -----
2023-04-11 14:33:01,843 :: INFO :: Epoch 5: loss tensor(178.3159, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.0109e-02, -5.3789e-03, -1.1551e-02, -3.3010e-03, -5.3627e-03,
        -1.3667e-02, -9.4143e-03,  6.1969e-03, -9.7752e-03, -2.5624e-03,
        -1.5026e-02, -5.0446e-03, -8.2564e-03,  8.4976e-04,  6.3391e-03,
         5.8553e-03,  1.5487e-03,  2.9528e-03,  1.0903e-03, -8.6810e-03,
        -4.3216e-03, -6.1156e-03,  5.6020e-03, -1.3440e-02, -2.3170e-03,
        -2.2048e-03, -3.5426e-03, -7.2161e-03, -1.1287e-02, -7.4318e-03,
         3.1785e-04,  1.0938e-02, -7.5638e-03, -9.6145e-03, -1.1589e-02,
        -1.2844e-02,  4.6178e-03,  2.6058e-03, -6.7010e-03, -5.5813e-03,
        -1.0416e-02, -1.1726e-02, -2.7023e-03,  8.2256e-03, -7.1149e-04,
        -3.0188e-03, -6.0562e-03, -9.3504e-03, -8.8089e-03, -3.9696e-03,
        -1.0415e-02, -2.1741e-03, -2.0301e-03, -1.6262e-03, -1.3219e-02,
         8.6010e-03, -5.6215e-03, -8.3584e-03, -1.1604e-02, -1.5078e-02,
        -6.8350e-04, -4.7010e-03, -1.1199e-02, -5.8217e-03, -4.1301e-03,
        -7.7324e-03, -1.1079e-03,  4.6037e-03,  9.7140e-03, -1.1321e-02,
        -1.8737e-03, -1.1191e-02,  2.3049e-03, -4.0067e-03,  8.2381e-07,
        -1.0216e-02,  1.1110e-02, -1.2242e-02,  4.5298e-06,  9.0573e-03,
        -7.4855e-03, -1.4069e-02,  5.1840e-03, -3.2884e-03, -1.2514e-02,
        -8.6021e-03,  4.1890e-03, -8.5202e-03, -7.1130e-03, -1.3239e-02,
        -8.3758e-03,  1.4770e-03, -6.9069e-03,  4.4089e-03, -1.1697e-02,
        -8.1453e-03, -1.6659e-03, -7.0227e-03, -1.2453e-02, -8.3518e-03,
        -9.1697e-03, -4.4614e-04,  1.4041e-03, -1.2842e-02, -1.3038e-02,
        -1.2777e-02,  4.6913e-03, -6.9427e-03, -1.0620e-03,  1.0550e-03,
        -1.3403e-02,  3.0343e-03, -1.2996e-02, -2.7379e-06, -1.1871e-02,
        -9.7307e-03, -9.3538e-03, -1.1637e-02,  1.4345e-03, -1.1429e-02,
        -6.6025e-03, -5.2601e-03, -7.1819e-03, -3.5711e-03,  7.3269e-04,
        -2.1809e-03, -9.4232e-03, -3.3272e-03,  1.0197e-02,  7.0805e-03,
        -1.3818e-03,  2.0493e-02,  6.1896e-03,  1.6040e-02, -3.4731e-03,
        -1.4785e-02, -1.7640e-02,  1.7341e-02, -4.8891e-03, -1.5493e-02,
         1.3463e-02, -1.6770e-02,  3.8685e-03, -4.2104e-03,  8.1526e-04,
         2.0226e-02,  7.0827e-03, -1.3007e-02,  2.7709e-04, -1.0316e-02,
        -1.7323e-02, -2.8250e-03,  1.0242e-02, -1.8173e-02, -1.4104e-02,
        -1.7476e-02, -1.6435e-02, -1.8358e-02,  2.0172e-02, -1.5049e-02,
         2.0059e-02, -1.5412e-02,  1.6989e-02, -1.2116e-02,  1.9680e-02,
         1.5533e-02, -1.7043e-02, -1.4692e-02, -1.5975e-02, -1.7247e-03,
         1.3760e-02,  1.2885e-02, -1.7898e-02,  4.0827e-03,  1.1792e-02,
        -1.7416e-02,  1.9873e-02,  1.3968e-02, -1.3472e-02, -1.3312e-02,
         1.5879e-03, -1.7051e-02,  1.6872e-02, -1.8138e-02,  1.0332e-02,
         1.4050e-02, -1.8191e-02,  7.7799e-03,  2.0194e-02, -1.8279e-02,
        -1.4647e-02, -1.4667e-02,  1.9930e-02, -1.8294e-02,  6.2737e-03,
         1.3914e-02, -3.2734e-03,  1.9797e-02,  2.9145e-03, -4.0821e-03,
         7.9845e-03,  1.7499e-03,  9.2688e-03,  1.9024e-02,  1.6497e-03,
        -5.4646e-03, -1.5330e-02, -1.7132e-02,  1.7565e-02, -1.1426e-02,
         1.9826e-02,  1.9827e-02,  1.6880e-02, -1.7338e-02,  1.1324e-03,
         1.0137e-02, -1.2120e-02, -1.6780e-02,  1.6566e-02, -1.8449e-02,
        -1.0257e-03, -1.2197e-02,  1.5382e-02,  4.5399e-03,  8.9589e-03,
         7.2708e-03,  7.2188e-03,  2.1808e-03,  6.6102e-03,  1.6666e-02,
         7.2758e-03,  1.6988e-02,  2.0346e-02,  1.5619e-02,  2.0094e-02,
        -1.1171e-02, -1.5211e-02, -1.5033e-02,  1.5026e-02,  1.5344e-02,
        -3.6101e-03, -9.9600e-03, -1.3360e-02,  1.6915e-02, -1.6541e-02,
        -1.5138e-03, -1.8331e-02, -7.9686e-03, -1.3231e-02, -1.5291e-02,
        -1.5109e-02,  2.0246e-02, -5.9538e-03, -4.6301e-04, -1.8358e-02,
        -2.7145e-03, -6.2798e-03, -2.6251e-03, -2.4428e-03, -2.1736e-03,
        -1.3636e-03, -4.9260e-03, -1.6524e-02, -1.5237e-02, -1.4752e-03,
        -3.8777e-03,  2.6151e-03, -1.0711e-02, -4.1931e-03, -4.1576e-03,
        -6.9265e-03, -1.4195e-02, -7.6338e-03, -8.1459e-03, -8.4953e-03,
        -1.0053e-02, -7.4517e-04, -3.3869e-03, -1.1566e-03, -7.2471e-03,
        -1.1389e-02,  4.7958e-03, -5.5606e-03, -7.8290e-03, -1.5805e-03,
         4.6225e-03, -1.6387e-02, -1.2658e-02,  7.2728e-03, -1.6145e-02,
        -6.7151e-03, -1.4419e-02, -3.9815e-03, -7.7467e-03, -7.7183e-03,
         1.9788e-03, -4.5653e-03, -1.3127e-02,  2.8653e-03, -4.9272e-03,
        -1.0932e-02,  1.6260e-02,  2.0321e-03, -8.8855e-03,  2.3146e-03,
        -1.0923e-03, -6.4662e-03, -7.6068e-03, -2.6644e-03, -5.0406e-03,
        -1.1431e-03, -1.0268e-02, -2.9302e-04,  8.3295e-04, -6.8072e-03,
        -8.6749e-03, -6.0675e-03,  4.7876e-03, -1.4883e-02, -1.1096e-02,
        -6.5403e-03, -6.7512e-03, -5.2712e-03, -8.1519e-03, -2.1377e-03,
         2.3389e-03, -7.8683e-03, -1.3050e-02, -8.2588e-03, -1.3198e-02,
        -1.2880e-02, -1.0979e-02, -1.6080e-02, -1.5748e-02, -3.7666e-03,
        -1.2718e-03, -8.6313e-03, -1.4033e-02, -5.0999e-03, -4.3848e-03,
        -1.1332e-02, -1.3527e-02, -1.3732e-02, -9.9297e-04, -7.1592e-03,
         4.4571e-03, -4.7867e-03, -6.2572e-03, -7.9848e-03, -1.6487e-02,
        -1.0538e-02, -1.1104e-02, -1.4774e-02, -6.9583e-03, -6.5200e-03,
        -1.1361e-02, -8.8821e-03,  1.9815e-03, -7.1335e-03, -1.0170e-02,
        -5.2787e-03, -7.6266e-03,  1.1377e-02, -1.1341e-02, -2.0486e-03,
         3.0597e-03, -1.1625e-02, -5.1126e-03, -1.2894e-04, -1.5569e-02,
        -1.6541e-02, -1.4463e-02, -8.4726e-04, -7.0820e-03,  1.1865e-03,
         1.1203e-02,  8.4139e-04,  1.0956e-02, -7.1157e-03, -1.1946e-02,
        -9.0046e-03, -1.6086e-02, -1.5984e-02,  4.8168e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.0482, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:04,577 :: INFO :: Epoch 10: loss tensor(173.0975, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.1296e-02,  1.2261e-04, -2.0442e-02,  1.5812e-03,  4.7906e-03,
        -2.1632e-02, -9.1328e-03,  1.7332e-02, -9.8288e-03,  1.9252e-04,
        -2.4100e-02, -3.2558e-03, -1.7211e-04,  5.9756e-03,  1.2108e-02,
         1.3118e-02,  9.2451e-03,  1.1165e-02, -9.2519e-05, -1.0972e-02,
        -2.4681e-03, -8.0223e-03,  1.6838e-02, -1.9946e-02, -7.2758e-05,
         7.9527e-03, -9.2895e-04, -6.4052e-03, -1.6398e-02, -7.4651e-03,
         1.3059e-02,  2.0355e-02, -9.4520e-03, -1.3473e-02, -1.6700e-02,
        -2.0883e-02,  1.1017e-02,  1.7586e-02, -8.3538e-04, -8.2920e-03,
        -1.2245e-02, -1.3862e-02, -4.8228e-04,  1.9910e-02,  7.8187e-03,
        -1.0425e-03, -4.4480e-03, -1.1920e-02, -1.1508e-02, -2.6507e-03,
        -1.4110e-02,  2.0938e-03, -1.6400e-03,  3.4784e-03, -2.1884e-02,
         1.9990e-02, -2.2289e-03, -1.1722e-02, -2.0631e-02, -2.3821e-02,
         4.9641e-03, -6.6145e-03, -1.6509e-02, -2.5743e-03, -4.3595e-03,
        -8.5539e-03,  7.4696e-04,  6.5330e-03,  2.0339e-02, -1.5625e-02,
        -2.6299e-03, -1.5503e-02,  1.6076e-03,  3.4608e-03,  1.0554e-02,
        -1.6855e-02,  1.4087e-02, -2.1452e-02,  7.9462e-03,  1.0447e-02,
         4.1816e-03, -2.1704e-02,  2.0407e-02,  3.9468e-03, -1.8501e-02,
        -8.1377e-03,  9.1378e-03, -1.3675e-02, -5.0934e-03, -1.9446e-02,
        -3.8880e-03,  1.5361e-02, -6.0497e-03,  1.5427e-02, -2.0540e-02,
        -1.9809e-03,  3.6897e-03, -1.1322e-02, -1.7367e-02, -1.1228e-02,
        -1.4874e-02,  8.1505e-03,  1.5979e-02, -1.4604e-02, -2.2800e-02,
        -2.2232e-02,  7.9212e-03, -7.4967e-03,  5.9037e-03,  1.0079e-02,
        -2.3753e-02,  1.1283e-02, -1.9961e-02,  1.1122e-03, -1.5668e-02,
        -1.6723e-02, -1.4827e-02, -1.0095e-02,  1.6307e-03, -1.8127e-02,
        -8.1751e-03,  5.3123e-04, -8.4390e-03, -7.7085e-03,  8.6482e-03,
        -4.2756e-03, -1.2536e-02,  5.3476e-03,  3.2693e-02,  2.8951e-02,
         2.0652e-02,  4.2697e-02,  2.9465e-02,  3.7902e-02,  1.7092e-02,
        -1.0905e-02, -2.5908e-02,  3.6408e-02,  1.3408e-02, -2.0094e-02,
         3.7028e-02, -2.1753e-02,  2.7524e-02,  1.0088e-02,  2.3865e-02,
         4.2361e-02,  2.9651e-02,  4.7855e-06,  2.3165e-02,  8.3329e-03,
        -2.4218e-02,  1.7614e-02,  3.3568e-02, -3.1222e-02, -9.8568e-03,
        -2.6601e-02, -1.9598e-02, -3.2352e-02,  3.8704e-02, -2.7741e-02,
         4.2229e-02, -2.8355e-02,  3.8633e-02, -3.3754e-03,  4.0218e-02,
         2.3990e-02, -2.6584e-02, -1.1771e-02, -1.9411e-02,  1.9105e-02,
         3.7477e-02,  3.4706e-02, -3.1774e-02,  2.7056e-02,  3.5531e-02,
        -2.5461e-02,  3.6230e-02,  3.6644e-02, -6.7060e-03, -1.0938e-02,
         2.4289e-02, -2.2254e-02,  3.8689e-02, -3.1828e-02,  3.2929e-02,
         3.7456e-02, -3.0489e-02,  3.1632e-02,  4.4087e-02, -3.1777e-02,
        -2.4971e-02, -9.2662e-03,  4.1319e-02, -3.0944e-02,  2.9038e-02,
         3.6911e-02,  1.8188e-02,  3.3338e-02,  2.4166e-02,  1.7952e-02,
         3.1215e-02,  2.5557e-02,  3.0733e-02,  4.0844e-02,  2.4430e-02,
         1.5261e-02, -2.0415e-02, -3.0751e-02,  3.8587e-02,  1.6432e-03,
         4.3375e-02,  4.1500e-02,  2.5322e-02, -2.4297e-02,  2.3550e-02,
         3.4019e-02,  2.6305e-04, -2.6248e-02,  3.8139e-02, -3.2226e-02,
         2.1667e-02, -8.0965e-03,  3.8890e-02,  2.6604e-02,  3.1139e-02,
         3.1280e-02,  2.9935e-02,  2.5159e-02,  2.9137e-02,  4.0164e-02,
         3.0113e-02,  3.8240e-02,  4.0751e-02,  3.4839e-02,  4.1846e-02,
         5.0243e-03, -1.1754e-02, -1.1728e-02,  3.1232e-02,  3.8035e-02,
         1.7983e-02,  7.3857e-03, -1.9287e-03,  3.8165e-02, -1.9697e-02,
         2.0107e-02, -3.1885e-02,  1.0691e-02, -4.1603e-03, -1.3453e-02,
        -2.3672e-02,  4.2512e-02,  1.4249e-02,  2.2723e-02, -3.1680e-02,
         1.9251e-02,  1.5034e-02,  1.9768e-02,  2.0095e-02,  2.0216e-02,
         2.1539e-02,  1.7387e-02, -1.7675e-02, -7.8099e-03,  2.0563e-02,
         1.8212e-02,  2.5829e-02,  4.8292e-03,  1.8021e-02,  1.8212e-02,
         1.3413e-02, -2.5637e-03,  1.2278e-02,  1.2062e-02,  1.1987e-02,
         8.1666e-03,  2.2186e-02,  1.8907e-02,  2.1309e-02,  1.3885e-02,
         5.7793e-03,  2.8427e-02,  1.6223e-02,  1.2016e-02,  2.1643e-02,
         2.7799e-02, -1.5810e-02,  1.5726e-03,  3.0449e-02, -1.2912e-02,
         1.4139e-02, -2.3289e-03,  1.8476e-02,  1.2361e-02,  1.1979e-02,
         2.4883e-02,  1.7912e-02, -2.0754e-03,  2.5773e-02,  1.6927e-02,
         6.0568e-03,  3.9981e-02,  2.5628e-02,  1.0189e-02,  2.5587e-02,
         2.1185e-02,  1.4535e-02,  1.1691e-02,  1.9901e-02,  1.6428e-02,
         2.1908e-02,  8.8239e-03,  2.2439e-02,  2.3783e-02,  1.4141e-02,
         1.2000e-02,  1.5002e-02,  2.8521e-02, -7.6609e-03,  4.5910e-03,
         1.5147e-02,  1.4138e-02,  1.6751e-02,  1.0201e-02,  2.0098e-02,
         2.5376e-02,  1.2477e-02, -2.1444e-04,  1.2505e-02, -5.1960e-04,
         1.5049e-03,  5.5905e-03, -1.3525e-02, -1.2131e-02,  1.8013e-02,
         2.1625e-02,  1.0081e-02, -2.5490e-03,  1.5439e-02,  1.7578e-02,
         6.8755e-03,  4.0873e-04, -5.1335e-03,  2.1705e-02,  1.3671e-02,
         2.7517e-02,  1.6306e-02,  1.4742e-02,  1.1091e-02, -1.9904e-02,
         8.2801e-03,  6.6254e-03, -4.3345e-03,  1.3221e-02,  1.4315e-02,
         4.5412e-03,  1.0564e-02,  2.5284e-02,  1.3926e-02,  9.1020e-03,
         1.6165e-02,  1.2648e-02,  3.5088e-02,  6.5075e-03,  2.0900e-02,
         2.5991e-02,  4.2601e-03,  1.6077e-02,  2.2787e-02, -8.2221e-03,
        -1.7010e-02, -3.1015e-03,  2.2322e-02,  1.4067e-02,  2.4704e-02,
         3.4858e-02,  2.3646e-02,  3.3847e-02,  1.3079e-02,  3.0716e-03,
         1.1139e-02, -1.4800e-02, -8.4436e-03,  2.8109e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.8928, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:06,953 :: INFO :: Epoch 15: loss tensor(168.2888, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0013,  0.0158, -0.0235,  0.0170,  0.0217, -0.0214,  0.0058,  0.0340,
         0.0015,  0.0106, -0.0246,  0.0053,  0.0142,  0.0233,  0.0271,  0.0270,
         0.0204,  0.0285,  0.0043, -0.0052,  0.0067, -0.0007,  0.0365, -0.0142,
         0.0107,  0.0231,  0.0080,  0.0042, -0.0080,  0.0014,  0.0332,  0.0368,
         0.0018, -0.0077, -0.0092, -0.0202,  0.0236,  0.0358,  0.0110, -0.0016,
        -0.0032, -0.0088,  0.0112,  0.0342,  0.0222,  0.0090,  0.0039, -0.0024,
        -0.0050,  0.0061, -0.0056,  0.0163,  0.0033,  0.0135, -0.0215,  0.0393,
         0.0108, -0.0045, -0.0215, -0.0228,  0.0162, -0.0030, -0.0112,  0.0101,
         0.0065, -0.0022,  0.0140,  0.0173,  0.0349, -0.0109,  0.0039, -0.0040,
         0.0031,  0.0166,  0.0303, -0.0155,  0.0247, -0.0229,  0.0245,  0.0179,
         0.0269, -0.0175,  0.0399,  0.0228, -0.0126,  0.0028,  0.0202, -0.0093,
         0.0047, -0.0170,  0.0082,  0.0350,  0.0013,  0.0332, -0.0206,  0.0189,
         0.0128, -0.0061, -0.0136, -0.0061, -0.0090,  0.0207,  0.0376, -0.0063,
        -0.0240, -0.0260,  0.0185,  0.0013,  0.0187,  0.0265, -0.0290,  0.0237,
        -0.0138,  0.0119, -0.0085, -0.0173, -0.0110,  0.0020,  0.0058, -0.0161,
        -0.0023,  0.0153, -0.0020, -0.0044,  0.0229, -0.0020, -0.0059,  0.0235,
         0.0549,  0.0512,  0.0448,  0.0637,  0.0529,  0.0620,  0.0401,  0.0055,
        -0.0239,  0.0465,  0.0359, -0.0136,  0.0615, -0.0150,  0.0526,  0.0339,
         0.0488,  0.0645,  0.0512,  0.0242,  0.0481,  0.0344, -0.0211,  0.0431,
         0.0574, -0.0410,  0.0072, -0.0249, -0.0110, -0.0424,  0.0567, -0.0377,
         0.0638, -0.0386,  0.0615,  0.0164,  0.0601,  0.0373, -0.0289,  0.0034,
        -0.0099,  0.0422,  0.0628,  0.0561, -0.0421,  0.0512,  0.0602, -0.0244,
         0.0484,  0.0593,  0.0120,  0.0006,  0.0483, -0.0146,  0.0593, -0.0414,
         0.0557,  0.0621, -0.0382,  0.0567,  0.0712, -0.0417, -0.0300,  0.0088,
         0.0626, -0.0384,  0.0504,  0.0606,  0.0444,  0.0426,  0.0475,  0.0433,
         0.0554,  0.0521,  0.0525,  0.0628,  0.0486,  0.0415, -0.0150, -0.0415,
         0.0581,  0.0232,  0.0681,  0.0632,  0.0469, -0.0215,  0.0478,  0.0598,
         0.0218, -0.0287,  0.0600, -0.0420,  0.0467,  0.0086,  0.0638,  0.0504,
         0.0560,  0.0574,  0.0528,  0.0505,  0.0514,  0.0645,  0.0533,  0.0574,
         0.0585,  0.0572,  0.0633,  0.0299,  0.0055,  0.0036,  0.0407,  0.0616,
         0.0421,  0.0313,  0.0208,  0.0591, -0.0105,  0.0440, -0.0407,  0.0347,
         0.0152,  0.0012, -0.0246,  0.0656,  0.0392,  0.0483, -0.0392,  0.0435,
         0.0416,  0.0453,  0.0462,  0.0452,  0.0467,  0.0439, -0.0013,  0.0154,
         0.0469,  0.0438,  0.0512,  0.0292,  0.0432,  0.0431,  0.0380,  0.0220,
         0.0376,  0.0371,  0.0375,  0.0331,  0.0477,  0.0446,  0.0461,  0.0395,
         0.0314,  0.0547,  0.0416,  0.0377,  0.0485,  0.0518,  0.0028,  0.0260,
         0.0540,  0.0076,  0.0395,  0.0223,  0.0455,  0.0368,  0.0368,  0.0493,
         0.0439,  0.0207,  0.0494,  0.0418,  0.0312,  0.0656,  0.0520,  0.0363,
         0.0511,  0.0471,  0.0391,  0.0362,  0.0449,  0.0419,  0.0478,  0.0349,
         0.0483,  0.0490,  0.0397,  0.0373,  0.0395,  0.0546,  0.0146,  0.0292,
         0.0406,  0.0402,  0.0424,  0.0352,  0.0455,  0.0499,  0.0384,  0.0239,
         0.0380,  0.0237,  0.0261,  0.0303,  0.0073,  0.0083,  0.0440,  0.0462,
         0.0335,  0.0213,  0.0407,  0.0423,  0.0327,  0.0254,  0.0172,  0.0468,
         0.0387,  0.0509,  0.0404,  0.0403,  0.0357, -0.0061,  0.0337,  0.0318,
         0.0196,  0.0370,  0.0399,  0.0295,  0.0364,  0.0505,  0.0395,  0.0348,
         0.0415,  0.0387,  0.0609,  0.0326,  0.0462,  0.0507,  0.0292,  0.0413,
         0.0477,  0.0152,  0.0009,  0.0214,  0.0474,  0.0391,  0.0513,  0.0604,
         0.0483,  0.0567,  0.0381,  0.0278,  0.0366,  0.0041,  0.0150,  0.0529],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.6706, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:09,296 :: INFO :: Epoch 20: loss tensor(165.7211, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0165,  0.0358, -0.0198,  0.0388,  0.0406, -0.0132,  0.0283,  0.0548,
         0.0191,  0.0266, -0.0168,  0.0191,  0.0304,  0.0473,  0.0488,  0.0458,
         0.0345,  0.0509,  0.0160,  0.0087,  0.0218,  0.0150,  0.0603,  0.0017,
         0.0282,  0.0410,  0.0220,  0.0220,  0.0105,  0.0171,  0.0558,  0.0584,
         0.0222,  0.0059,  0.0079, -0.0115,  0.0405,  0.0548,  0.0261,  0.0138,
         0.0129,  0.0019,  0.0296,  0.0515,  0.0400,  0.0248,  0.0174,  0.0149,
         0.0101,  0.0203,  0.0109,  0.0364,  0.0133,  0.0274, -0.0117,  0.0632,
         0.0300,  0.0122, -0.0129, -0.0130,  0.0315,  0.0070,  0.0026,  0.0287,
         0.0254,  0.0109,  0.0344,  0.0367,  0.0529,  0.0005,  0.0181,  0.0172,
         0.0095,  0.0341,  0.0538, -0.0057,  0.0434, -0.0162,  0.0457,  0.0340,
         0.0520, -0.0028,  0.0610,  0.0472,  0.0030,  0.0206,  0.0371,  0.0048,
         0.0194, -0.0072,  0.0245,  0.0578,  0.0138,  0.0545, -0.0107,  0.0435,
         0.0252,  0.0087, -0.0031,  0.0066,  0.0077,  0.0366,  0.0610,  0.0085,
        -0.0166, -0.0233,  0.0361,  0.0158,  0.0365,  0.0468, -0.0285,  0.0402,
         0.0022,  0.0301,  0.0063, -0.0105,  0.0014,  0.0197,  0.0162, -0.0062,
         0.0111,  0.0350,  0.0126,  0.0080,  0.0417,  0.0072,  0.0094,  0.0459,
         0.0758,  0.0724,  0.0685,  0.0820,  0.0738,  0.0855,  0.0623,  0.0258,
        -0.0135,  0.0517,  0.0577, -0.0008,  0.0858, -0.0020,  0.0772,  0.0610,
         0.0734,  0.0860,  0.0700,  0.0505,  0.0726,  0.0606, -0.0110,  0.0691,
         0.0806, -0.0489,  0.0273, -0.0149,  0.0037, -0.0499,  0.0737, -0.0457,
         0.0838, -0.0471,  0.0842,  0.0377,  0.0792,  0.0542, -0.0261,  0.0226,
         0.0062,  0.0642,  0.0877,  0.0763, -0.0500,  0.0749,  0.0843, -0.0161,
         0.0568,  0.0812,  0.0333,  0.0146,  0.0718,  0.0010,  0.0777, -0.0481,
         0.0774,  0.0868, -0.0430,  0.0809,  0.0983, -0.0496, -0.0292,  0.0303,
         0.0830, -0.0420,  0.0679,  0.0841,  0.0715,  0.0506,  0.0698,  0.0687,
         0.0793,  0.0792,  0.0731,  0.0824,  0.0722,  0.0689, -0.0035, -0.0505,
         0.0752,  0.0460,  0.0929,  0.0841,  0.0655, -0.0122,  0.0707,  0.0861,
         0.0450, -0.0263,  0.0818, -0.0492,  0.0717,  0.0279,  0.0887,  0.0743,
         0.0789,  0.0839,  0.0743,  0.0764,  0.0720,  0.0885,  0.0754,  0.0733,
         0.0734,  0.0762,  0.0836,  0.0563,  0.0270,  0.0227,  0.0480,  0.0847,
         0.0655,  0.0557,  0.0459,  0.0793,  0.0050,  0.0676, -0.0464,  0.0585,
         0.0367,  0.0206, -0.0179,  0.0888,  0.0651,  0.0741, -0.0413,  0.0671,
         0.0687,  0.0711,  0.0733,  0.0693,  0.0716,  0.0713,  0.0211,  0.0409,
         0.0741,  0.0696,  0.0765,  0.0537,  0.0672,  0.0665,  0.0619,  0.0479,
         0.0632,  0.0620,  0.0622,  0.0582,  0.0733,  0.0702,  0.0697,  0.0642,
         0.0572,  0.0820,  0.0663,  0.0641,  0.0763,  0.0739,  0.0266,  0.0515,
         0.0754,  0.0323,  0.0652,  0.0481,  0.0735,  0.0602,  0.0606,  0.0728,
         0.0695,  0.0448,  0.0712,  0.0656,  0.0574,  0.0920,  0.0791,  0.0635,
         0.0768,  0.0736,  0.0620,  0.0603,  0.0692,  0.0677,  0.0738,  0.0621,
         0.0743,  0.0741,  0.0652,  0.0618,  0.0634,  0.0815,  0.0392,  0.0551,
         0.0653,  0.0668,  0.0677,  0.0603,  0.0710,  0.0733,  0.0652,  0.0495,
         0.0631,  0.0491,  0.0511,  0.0553,  0.0324,  0.0327,  0.0706,  0.0690,
         0.0550,  0.0462,  0.0665,  0.0654,  0.0583,  0.0508,  0.0417,  0.0717,
         0.0634,  0.0719,  0.0631,  0.0658,  0.0600,  0.0144,  0.0590,  0.0572,
         0.0451,  0.0585,  0.0659,  0.0544,  0.0631,  0.0755,  0.0645,  0.0603,
         0.0668,  0.0659,  0.0873,  0.0602,  0.0712,  0.0744,  0.0549,  0.0666,
         0.0717,  0.0408,  0.0248,  0.0484,  0.0716,  0.0632,  0.0788,  0.0864,
         0.0719,  0.0766,  0.0623,  0.0540,  0.0613,  0.0282,  0.0412,  0.0767],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3746, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:11,561 :: INFO :: Epoch 25: loss tensor(162.8123, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 3.3868e-02,  5.2766e-02, -1.2496e-02,  5.9085e-02,  5.6568e-02,
        -2.0763e-04,  4.9734e-02,  7.2891e-02,  3.6398e-02,  4.1358e-02,
        -4.0104e-03,  3.2703e-02,  4.5328e-02,  6.8687e-02,  6.7974e-02,
         6.1253e-02,  4.7555e-02,  7.0395e-02,  2.7991e-02,  2.4180e-02,
         3.5147e-02,  3.0674e-02,  8.0731e-02,  1.9174e-02,  4.4680e-02,
         5.5725e-02,  3.4579e-02,  3.9417e-02,  3.1287e-02,  3.2502e-02,
         7.4452e-02,  7.7245e-02,  4.3172e-02,  2.0962e-02,  2.5096e-02,
        -2.3851e-04,  5.5081e-02,  7.1495e-02,  3.9874e-02,  3.0627e-02,
         2.8464e-02,  1.5561e-02,  4.7755e-02,  6.5511e-02,  5.4954e-02,
         4.0206e-02,  3.0400e-02,  3.2332e-02,  2.8170e-02,  3.3558e-02,
         2.6294e-02,  5.3365e-02,  2.4553e-02,  3.9622e-02,  2.3576e-03,
         8.3890e-02,  5.0779e-02,  3.0391e-02, -5.6206e-04,  1.6928e-04,
         4.5056e-02,  1.8988e-02,  1.8596e-02,  4.5520e-02,  4.3316e-02,
         2.3731e-02,  5.4046e-02,  5.4624e-02,  6.8438e-02,  1.3519e-02,
         3.2412e-02,  3.7737e-02,  1.6511e-02,  5.2335e-02,  7.3429e-02,
         5.9553e-03,  6.1928e-02, -5.9637e-03,  6.2905e-02,  5.2330e-02,
         7.3986e-02,  1.6872e-02,  7.8353e-02,  7.0231e-02,  1.9676e-02,
         3.9155e-02,  5.1671e-02,  2.0854e-02,  3.3577e-02,  4.4993e-03,
         3.9140e-02,  7.7974e-02,  2.5726e-02,  7.2121e-02,  3.8967e-03,
         6.6084e-02,  3.5665e-02,  2.4383e-02,  9.2827e-03,  2.1239e-02,
         2.6829e-02,  4.9348e-02,  8.0345e-02,  2.4572e-02, -4.9897e-03,
        -1.6494e-02,  5.2739e-02,  2.9712e-02,  5.4029e-02,  6.3785e-02,
        -2.4027e-02,  5.5288e-02,  2.0197e-02,  4.7960e-02,  2.1582e-02,
        -1.9855e-03,  1.5353e-02,  3.7579e-02,  2.6738e-02,  5.7433e-03,
         2.4668e-02,  5.2178e-02,  3.0035e-02,  2.2423e-02,  5.7948e-02,
         1.8348e-02,  2.5079e-02,  6.5854e-02,  9.5711e-02,  9.2102e-02,
         9.0891e-02,  9.6803e-02,  9.1451e-02,  1.0687e-01,  8.2505e-02,
         4.6307e-02,  1.4848e-03,  5.4537e-02,  7.7997e-02,  1.4584e-02,
         1.0883e-01,  1.3716e-02,  1.0055e-01,  8.9625e-02,  9.6981e-02,
         1.0611e-01,  8.5025e-02,  7.6925e-02,  9.5802e-02,  8.5365e-02,
         2.9613e-03,  9.5174e-02,  1.0230e-01, -5.5355e-02,  4.7323e-02,
        -6.9719e-05,  2.0571e-02, -5.5148e-02,  8.7019e-02, -5.1139e-02,
         1.0181e-01, -5.3295e-02,  1.0555e-01,  5.8489e-02,  9.6284e-02,
         6.3689e-02, -1.9887e-02,  4.2174e-02,  2.4528e-02,  8.4787e-02,
         1.1138e-01,  9.3944e-02, -5.5661e-02,  9.7408e-02,  1.0719e-01,
        -3.3657e-03,  6.2152e-02,  1.0121e-01,  5.3682e-02,  2.8716e-02,
         9.3852e-02,  1.9586e-02,  9.1397e-02, -5.2526e-02,  9.7287e-02,
         1.1078e-01, -4.5335e-02,  1.0346e-01,  1.2396e-01, -5.5840e-02,
        -2.2188e-02,  5.1592e-02,  1.0146e-01, -4.2810e-02,  8.1039e-02,
         1.0647e-01,  9.8733e-02,  5.2947e-02,  9.0501e-02,  9.3290e-02,
         1.0209e-01,  1.0585e-01,  9.1965e-02,  9.8783e-02,  9.4204e-02,
         9.6108e-02,  1.0621e-02, -5.7387e-02,  8.8086e-02,  6.7534e-02,
         1.1677e-01,  1.0305e-01,  7.7477e-02,  1.4571e-04,  9.1605e-02,
         1.1210e-01,  6.7026e-02, -2.0378e-02,  1.0248e-01, -5.4220e-02,
         9.6216e-02,  4.7132e-02,  1.1304e-01,  9.8089e-02,  9.8028e-02,
         1.0997e-01,  9.3784e-02,  1.0203e-01,  8.9003e-02,  1.1151e-01,
         9.5377e-02,  8.4195e-02,  8.5325e-02,  8.9190e-02,  1.0200e-01,
         8.2453e-02,  4.8295e-02,  4.2135e-02,  5.3338e-02,  1.0625e-01,
         8.7067e-02,  7.9011e-02,  7.0768e-02,  9.7986e-02,  2.2637e-02,
         9.0026e-02, -4.9532e-02,  8.0502e-02,  5.7312e-02,  4.0539e-02,
        -4.4473e-03,  1.1140e-01,  9.0500e-02,  9.9027e-02, -3.9165e-02,
         8.9059e-02,  9.5937e-02,  9.6315e-02,  1.0047e-01,  9.1585e-02,
         9.5638e-02,  9.8420e-02,  4.3913e-02,  6.5479e-02,  1.0150e-01,
         9.5387e-02,  1.0132e-01,  7.6599e-02,  8.9029e-02,  8.7243e-02,
         8.3973e-02,  7.2823e-02,  8.8221e-02,  8.5781e-02,  8.4908e-02,
         8.2233e-02,  9.8731e-02,  9.4946e-02,  9.1354e-02,  8.7357e-02,
         8.1496e-02,  1.0940e-01,  8.9290e-02,  8.9940e-02,  1.0431e-01,
         9.2996e-02,  5.0416e-02,  7.6008e-02,  9.4372e-02,  5.6950e-02,
         9.0417e-02,  7.2658e-02,  1.0162e-01,  8.1631e-02,  8.2353e-02,
         9.4770e-02,  9.3623e-02,  6.8404e-02,  8.9757e-02,  8.7468e-02,
         8.3427e-02,  1.1836e-01,  1.0623e-01,  9.0818e-02,  1.0207e-01,
         9.9995e-02,  8.1801e-02,  8.3678e-02,  9.1685e-02,  9.3024e-02,
         9.9445e-02,  8.9030e-02,  1.0008e-01,  9.8663e-02,  9.0144e-02,
         8.4058e-02,  8.6008e-02,  1.0851e-01,  6.2553e-02,  8.0800e-02,
         8.7908e-02,  9.3488e-02,  9.1294e-02,  8.4377e-02,  9.6005e-02,
         9.4588e-02,  9.1600e-02,  7.4759e-02,  8.6743e-02,  7.3281e-02,
         7.4902e-02,  7.9140e-02,  5.7204e-02,  5.6323e-02,  9.6864e-02,
         8.8666e-02,  7.3105e-02,  6.9908e-02,  9.2141e-02,  8.5447e-02,
         8.2303e-02,  7.4772e-02,  6.4924e-02,  9.5848e-02,  8.6830e-02,
         9.0067e-02,  8.3756e-02,  9.0281e-02,  8.2965e-02,  3.5709e-02,
         8.2390e-02,  8.1759e-02,  6.9677e-02,  7.6750e-02,  9.0949e-02,
         7.7732e-02,  8.9730e-02,  1.0001e-01,  8.7731e-02,  8.4352e-02,
         9.1626e-02,  9.3213e-02,  1.1370e-01,  8.7965e-02,  9.5564e-02,
         9.6333e-02,  7.9547e-02,  9.1410e-02,  9.3945e-02,  6.5446e-02,
         4.8842e-02,  7.5975e-02,  9.3532e-02,  8.5417e-02,  1.0612e-01,
         1.1232e-01,  9.3281e-02,  9.2477e-02,  8.4451e-02,  8.0093e-02,
         8.4259e-02,  5.2087e-02,  6.6969e-02,  9.8970e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(9.9601, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:13,952 :: INFO :: Epoch 30: loss tensor(161.3350, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0503,  0.0653, -0.0024,  0.0775,  0.0699,  0.0142,  0.0692,  0.0882,
         0.0518,  0.0551,  0.0110,  0.0451,  0.0585,  0.0875,  0.0854,  0.0741,
         0.0584,  0.0866,  0.0394,  0.0403,  0.0472,  0.0442,  0.0986,  0.0371,
         0.0606,  0.0677,  0.0451,  0.0554,  0.0520,  0.0468,  0.0907,  0.0930,
         0.0627,  0.0356,  0.0411,  0.0118,  0.0669,  0.0858,  0.0515,  0.0475,
         0.0431,  0.0298,  0.0646,  0.0760,  0.0663,  0.0536,  0.0428,  0.0489,
         0.0467,  0.0440,  0.0405,  0.0674,  0.0365,  0.0495,  0.0182,  0.1022,
         0.0700,  0.0501,  0.0132,  0.0143,  0.0563,  0.0315,  0.0342,  0.0597,
         0.0598,  0.0360,  0.0711,  0.0708,  0.0813,  0.0267,  0.0451,  0.0564,
         0.0236,  0.0710,  0.0900,  0.0181,  0.0795,  0.0066,  0.0764,  0.0701,
         0.0942,  0.0383,  0.0919,  0.0924,  0.0352,  0.0566,  0.0643,  0.0371,
         0.0461,  0.0164,  0.0519,  0.0961,  0.0362,  0.0864,  0.0203,  0.0856,
         0.0448,  0.0392,  0.0221,  0.0360,  0.0454,  0.0589,  0.0964,  0.0415,
         0.0085, -0.0067,  0.0672,  0.0424,  0.0705,  0.0777, -0.0164,  0.0679,
         0.0382,  0.0645,  0.0356,  0.0074,  0.0293,  0.0551,  0.0358,  0.0186,
         0.0376,  0.0671,  0.0481,  0.0368,  0.0709,  0.0305,  0.0405,  0.0835,
         0.1139,  0.1096,  0.1113,  0.1075,  0.1052,  0.1255,  0.1001,  0.0654,
         0.0185,  0.0555,  0.0962,  0.0304,  0.1296,  0.0298,  0.1218,  0.1180,
         0.1187,  0.1240,  0.0959,  0.1021,  0.1171,  0.1085,  0.0179,  0.1193,
         0.1217, -0.0607,  0.0651,  0.0173,  0.0368, -0.0590,  0.0963, -0.0546,
         0.1175, -0.0577,  0.1236,  0.0778,  0.1112,  0.0668, -0.0121,  0.0602,
         0.0430,  0.1031,  0.1334,  0.1085, -0.0598,  0.1181,  0.1286,  0.0114,
         0.0657,  0.1186,  0.0722,  0.0411,  0.1135,  0.0385,  0.1001, -0.0552,
         0.1146,  0.1333, -0.0464,  0.1237,  0.1479, -0.0608, -0.0111,  0.0708,
         0.1173, -0.0417,  0.0889,  0.1270,  0.1242,  0.0516,  0.1094,  0.1160,
         0.1231,  0.1307,  0.1080,  0.1097,  0.1135,  0.1218,  0.0254, -0.0629,
         0.0964,  0.0870,  0.1382,  0.1195,  0.0827,  0.0129,  0.1101,  0.1368,
         0.0866, -0.0126,  0.1204, -0.0577,  0.1193,  0.0648,  0.1352,  0.1208,
         0.1133,  0.1344,  0.1106,  0.1261,  0.1010,  0.1328,  0.1125,  0.0889,
         0.0945,  0.0964,  0.1175,  0.1070,  0.0682,  0.0599,  0.0559,  0.1244,
         0.1064,  0.1005,  0.0944,  0.1149,  0.0402,  0.1107, -0.0507,  0.1002,
         0.0758,  0.0593,  0.0120,  0.1328,  0.1142,  0.1225, -0.0340,  0.1085,
         0.1227,  0.1205,  0.1271,  0.1116,  0.1185,  0.1247,  0.0650,  0.0879,
         0.1284,  0.1210,  0.1251,  0.0975,  0.1081,  0.1047,  0.1040,  0.0956,
         0.1123,  0.1082,  0.1050,  0.1046,  0.1236,  0.1185,  0.1107,  0.1082,
         0.1037,  0.1364,  0.1096,  0.1148,  0.1320,  0.1082,  0.0728,  0.0990,
         0.1111,  0.0802,  0.1147,  0.0953,  0.1295,  0.1006,  0.1014,  0.1152,
         0.1156,  0.0908,  0.1046,  0.1066,  0.1087,  0.1443,  0.1329,  0.1176,
         0.1268,  0.1258,  0.0984,  0.1064,  0.1117,  0.1173,  0.1245,  0.1153,
         0.1250,  0.1224,  0.1138,  0.1032,  0.1074,  0.1352,  0.0837,  0.1057,
         0.1078,  0.1195,  0.1128,  0.1066,  0.1200,  0.1135,  0.1173,  0.0989,
         0.1084,  0.0956,  0.0967,  0.1012,  0.0801,  0.0778,  0.1225,  0.1043,
         0.0874,  0.0916,  0.1171,  0.1016,  0.1040,  0.0962,  0.0861,  0.1189,
         0.1084,  0.1054,  0.1015,  0.1134,  0.1047,  0.0555,  0.1036,  0.1052,
         0.0925,  0.0923,  0.1148,  0.0987,  0.1159,  0.1240,  0.1088,  0.1064,
         0.1158,  0.1201,  0.1396,  0.1152,  0.1190,  0.1157,  0.1030,  0.1155,
         0.1142,  0.0880,  0.0714,  0.1029,  0.1128,  0.1052,  0.1329,  0.1378,
         0.1119,  0.1037,  0.1037,  0.1056,  0.1045,  0.0743,  0.0917,  0.1193],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.4227, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:16,171 :: INFO :: Epoch 35: loss tensor(159.3541, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0647,  0.0733,  0.0090,  0.0932,  0.0799,  0.0283,  0.0859,  0.1006,
         0.0649,  0.0673,  0.0265,  0.0552,  0.0686,  0.1030,  0.0989,  0.0830,
         0.0665,  0.0982,  0.0493,  0.0566,  0.0559,  0.0547,  0.1125,  0.0533,
         0.0745,  0.0754,  0.0524,  0.0692,  0.0723,  0.0583,  0.1033,  0.1046,
         0.0796,  0.0486,  0.0539,  0.0230,  0.0749,  0.0973,  0.0606,  0.0621,
         0.0554,  0.0438,  0.0781,  0.0826,  0.0736,  0.0643,  0.0529,  0.0633,
         0.0645,  0.0520,  0.0524,  0.0765,  0.0474,  0.0550,  0.0343,  0.1171,
         0.0864,  0.0699,  0.0273,  0.0272,  0.0639,  0.0433,  0.0474,  0.0714,
         0.0731,  0.0462,  0.0846,  0.0833,  0.0910,  0.0383,  0.0562,  0.0718,
         0.0296,  0.0891,  0.1029,  0.0285,  0.0938,  0.0195,  0.0847,  0.0857,
         0.1122,  0.0593,  0.1009,  0.1126,  0.0480,  0.0721,  0.0736,  0.0524,
         0.0559,  0.0267,  0.0624,  0.1122,  0.0452,  0.0956,  0.0370,  0.1021,
         0.0517,  0.0515,  0.0347,  0.0492,  0.0609,  0.0642,  0.1073,  0.0577,
         0.0230,  0.0041,  0.0789,  0.0518,  0.0837,  0.0880, -0.0068,  0.0767,
         0.0557,  0.0788,  0.0469,  0.0160,  0.0420,  0.0715,  0.0421,  0.0309,
         0.0485,  0.0793,  0.0646,  0.0496,  0.0803,  0.0422,  0.0545,  0.0980,
         0.1304,  0.1246,  0.1294,  0.1137,  0.1153,  0.1409,  0.1147,  0.0825,
         0.0359,  0.0563,  0.1119,  0.0457,  0.1478,  0.0451,  0.1404,  0.1450,
         0.1385,  0.1393,  0.1022,  0.1258,  0.1362,  0.1298,  0.0329,  0.1416,
         0.1384, -0.0651,  0.0812,  0.0353,  0.0518, -0.0615,  0.1005, -0.0566,
         0.1303, -0.0608,  0.1375,  0.0953,  0.1232,  0.0659, -0.0036,  0.0761,
         0.0604,  0.1187,  0.1536,  0.1198, -0.0627,  0.1366,  0.1481,  0.0267,
         0.0660,  0.1331,  0.0882,  0.0516,  0.1302,  0.0566,  0.1027, -0.0567,
         0.1287,  0.1538, -0.0465,  0.1411,  0.1698, -0.0649,  0.0025,  0.0880,
         0.1299, -0.0391,  0.0924,  0.1451,  0.1476,  0.0492,  0.1263,  0.1368,
         0.1419,  0.1536,  0.1212,  0.1161,  0.1298,  0.1454,  0.0398, -0.0673,
         0.0998,  0.1039,  0.1570,  0.1324,  0.0837,  0.0255,  0.1259,  0.1598,
         0.1037, -0.0041,  0.1350, -0.0599,  0.1402,  0.0806,  0.1544,  0.1421,
         0.1247,  0.1567,  0.1246,  0.1482,  0.1070,  0.1520,  0.1264,  0.0886,
         0.1018,  0.0983,  0.1297,  0.1297,  0.0863,  0.0758,  0.0565,  0.1393,
         0.1230,  0.1200,  0.1165,  0.1299,  0.0567,  0.1292, -0.0504,  0.1173,
         0.0919,  0.0768,  0.0296,  0.1523,  0.1358,  0.1442, -0.0266,  0.1251,
         0.1481,  0.1435,  0.1528,  0.1289,  0.1406,  0.1498,  0.0844,  0.1081,
         0.1546,  0.1461,  0.1483,  0.1165,  0.1245,  0.1184,  0.1222,  0.1165,
         0.1358,  0.1292,  0.1221,  0.1251,  0.1478,  0.1405,  0.1280,  0.1264,
         0.1237,  0.1626,  0.1274,  0.1388,  0.1591,  0.1197,  0.0935,  0.1200,
         0.1260,  0.1020,  0.1376,  0.1160,  0.1568,  0.1173,  0.1176,  0.1341,
         0.1354,  0.1124,  0.1157,  0.1227,  0.1331,  0.1696,  0.1590,  0.1436,
         0.1509,  0.1507,  0.1110,  0.1284,  0.1291,  0.1405,  0.1487,  0.1406,
         0.1490,  0.1455,  0.1364,  0.1193,  0.1276,  0.1611,  0.1026,  0.1297,
         0.1251,  0.1450,  0.1319,  0.1271,  0.1430,  0.1300,  0.1422,  0.1216,
         0.1279,  0.1159,  0.1161,  0.1210,  0.1015,  0.0971,  0.1470,  0.1159,
         0.0977,  0.1110,  0.1415,  0.1135,  0.1231,  0.1156,  0.1048,  0.1410,
         0.1281,  0.1189,  0.1168,  0.1353,  0.1250,  0.0745,  0.1222,  0.1271,
         0.1133,  0.1057,  0.1370,  0.1169,  0.1413,  0.1475,  0.1273,  0.1265,
         0.1392,  0.1462,  0.1648,  0.1419,  0.1418,  0.1324,  0.1247,  0.1389,
         0.1324,  0.1088,  0.0926,  0.1287,  0.1290,  0.1225,  0.1590,  0.1623,
         0.1275,  0.1101,  0.1195,  0.1306,  0.1223,  0.0950,  0.1152,  0.1378],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.7612, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:18,436 :: INFO :: Epoch 40: loss tensor(158.5820, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0761,  0.0777,  0.0210,  0.1063,  0.0871,  0.0413,  0.0997,  0.1104,
         0.0753,  0.0774,  0.0415,  0.0634,  0.0761,  0.1153,  0.1096,  0.0894,
         0.0719,  0.1058,  0.0572,  0.0715,  0.0617,  0.0623,  0.1229,  0.0676,
         0.0867,  0.0799,  0.0568,  0.0810,  0.0913,  0.0671,  0.1132,  0.1124,
         0.0933,  0.0589,  0.0640,  0.0328,  0.0796,  0.1066,  0.0677,  0.0746,
         0.0659,  0.0566,  0.0883,  0.0863,  0.0772,  0.0708,  0.0605,  0.0755,
         0.0807,  0.0578,  0.0621,  0.0820,  0.0574,  0.0575,  0.0497,  0.1290,
         0.0996,  0.0899,  0.0398,  0.0386,  0.0681,  0.0536,  0.0575,  0.0805,
         0.0837,  0.0536,  0.0944,  0.0925,  0.0980,  0.0482,  0.0646,  0.0835,
         0.0342,  0.1064,  0.1123,  0.0375,  0.1049,  0.0319,  0.0887,  0.0981,
         0.1281,  0.0796,  0.1071,  0.1307,  0.0578,  0.0853,  0.0803,  0.0650,
         0.0633,  0.0348,  0.0709,  0.1260,  0.0528,  0.1010,  0.0522,  0.1149,
         0.0565,  0.0615,  0.0460,  0.0609,  0.0729,  0.0659,  0.1148,  0.0724,
         0.0376,  0.0155,  0.0873,  0.0581,  0.0945,  0.0955,  0.0041,  0.0826,
         0.0710,  0.0906,  0.0560,  0.0229,  0.0518,  0.0864,  0.0455,  0.0417,
         0.0573,  0.0887,  0.0790,  0.0599,  0.0861,  0.0534,  0.0670,  0.1099,
         0.1448,  0.1369,  0.1449,  0.1157,  0.1214,  0.1527,  0.1262,  0.0971,
         0.0524,  0.0568,  0.1248,  0.0595,  0.1630,  0.0588,  0.1562,  0.1706,
         0.1559,  0.1518,  0.1047,  0.1472,  0.1523,  0.1483,  0.0468,  0.1609,
         0.1521, -0.0688,  0.0939,  0.0521,  0.0649, -0.0631,  0.1009, -0.0575,
         0.1401, -0.0630,  0.1474,  0.1099,  0.1326,  0.0627,  0.0048,  0.0894,
         0.0757,  0.1316,  0.1716,  0.1278, -0.0648,  0.1527,  0.1655,  0.0411,
         0.0647,  0.1443,  0.1013,  0.0601,  0.1439,  0.0727,  0.1000, -0.0572,
         0.1396,  0.1719, -0.0458,  0.1552,  0.1894, -0.0682,  0.0173,  0.1022,
         0.1392, -0.0359,  0.0922,  0.1606,  0.1681,  0.0460,  0.1407,  0.1552,
         0.1584,  0.1739,  0.1313,  0.1168,  0.1424,  0.1664,  0.0528, -0.0709,
         0.0986,  0.1180,  0.1724,  0.1418,  0.0801,  0.0366,  0.1386,  0.1809,
         0.1175,  0.0045,  0.1458, -0.0613,  0.1589,  0.0937,  0.1706,  0.1617,
         0.1319,  0.1763,  0.1356,  0.1681,  0.1077,  0.1687,  0.1365,  0.0846,
         0.1066,  0.0959,  0.1382,  0.1497,  0.1015,  0.0894,  0.0571,  0.1494,
         0.1368,  0.1370,  0.1366,  0.1428,  0.0714,  0.1455, -0.0493,  0.1310,
         0.1053,  0.0922,  0.0467,  0.1702,  0.1548,  0.1636, -0.0187,  0.1385,
         0.1720,  0.1651,  0.1772,  0.1441,  0.1616,  0.1740,  0.1022,  0.1263,
         0.1798,  0.1700,  0.1708,  0.1339,  0.1387,  0.1286,  0.1387,  0.1356,
         0.1580,  0.1488,  0.1366,  0.1440,  0.1707,  0.1611,  0.1437,  0.1424,
         0.1419,  0.1875,  0.1430,  0.1616,  0.1848,  0.1279,  0.1122,  0.1395,
         0.1397,  0.1222,  0.1597,  0.1350,  0.1830,  0.1326,  0.1312,  0.1520,
         0.1533,  0.1334,  0.1235,  0.1367,  0.1564,  0.1937,  0.1845,  0.1681,
         0.1738,  0.1743,  0.1204,  0.1501,  0.1444,  0.1624,  0.1721,  0.1646,
         0.1720,  0.1676,  0.1580,  0.1331,  0.1471,  0.1859,  0.1196,  0.1524,
         0.1399,  0.1694,  0.1490,  0.1460,  0.1649,  0.1444,  0.1657,  0.1433,
         0.1456,  0.1343,  0.1335,  0.1391,  0.1211,  0.1141,  0.1704,  0.1240,
         0.1052,  0.1283,  0.1646,  0.1216,  0.1403,  0.1329,  0.1214,  0.1619,
         0.1463,  0.1317,  0.1304,  0.1560,  0.1437,  0.0922,  0.1386,  0.1477,
         0.1323,  0.1180,  0.1577,  0.1326,  0.1653,  0.1703,  0.1440,  0.1447,
         0.1615,  0.1713,  0.1883,  0.1675,  0.1640,  0.1470,  0.1449,  0.1614,
         0.1489,  0.1276,  0.1123,  0.1532,  0.1422,  0.1381,  0.1839,  0.1856,
         0.1403,  0.1131,  0.1321,  0.1545,  0.1379,  0.1143,  0.1370,  0.1548],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.9839, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:33:18,436 :: INFO :: ----- frontend -----
2023-04-11 14:33:18,436 :: INFO :: Environment 0
2023-04-11 14:33:32,952 :: INFO :: Epoch 5: loss tensor(894.5388, device='cuda:0'), U.norm 14.65379524230957, V.norm 18.38434410095215, MLP.norm 1.8876240253448486
2023-04-11 14:33:33,061 :: INFO :: Epoch 10: loss tensor(879.9222, device='cuda:0'), U.norm 12.055262565612793, V.norm 17.90987205505371, MLP.norm 2.7109875679016113
2023-04-11 14:33:33,155 :: INFO :: Epoch 15: loss tensor(859.2750, device='cuda:0'), U.norm 10.399970054626465, V.norm 17.695152282714844, MLP.norm 3.7741401195526123
2023-04-11 14:33:33,248 :: INFO :: Epoch 20: loss tensor(834.0741, device='cuda:0'), U.norm 9.215624809265137, V.norm 17.556541442871094, MLP.norm 4.885380268096924
2023-04-11 14:33:33,264 :: INFO :: Environment 1
2023-04-11 14:33:46,655 :: INFO :: Epoch 5: loss tensor(916.7294, device='cuda:0'), U.norm 14.651824951171875, V.norm 18.426198959350586, MLP.norm 1.8875352144241333
2023-04-11 14:33:46,765 :: INFO :: Epoch 10: loss tensor(901.1984, device='cuda:0'), U.norm 12.051736831665039, V.norm 17.970293045043945, MLP.norm 2.718963146209717
2023-04-11 14:33:46,858 :: INFO :: Epoch 15: loss tensor(878.7108, device='cuda:0'), U.norm 10.394953727722168, V.norm 17.76483917236328, MLP.norm 3.801055669784546
2023-04-11 14:33:46,952 :: INFO :: Epoch 20: loss tensor(850.5258, device='cuda:0'), U.norm 9.209013938903809, V.norm 17.63256072998047, MLP.norm 4.945972919464111
2023-04-11 14:33:46,952 :: INFO :: Environment 2
2023-04-11 14:34:01,202 :: INFO :: Epoch 5: loss tensor(777.3432, device='cuda:0'), U.norm 14.644787788391113, V.norm 18.17815399169922, MLP.norm 1.8350152969360352
2023-04-11 14:34:01,296 :: INFO :: Epoch 10: loss tensor(767.1918, device='cuda:0'), U.norm 12.03645133972168, V.norm 17.63092613220215, MLP.norm 2.547416925430298
2023-04-11 14:34:01,390 :: INFO :: Epoch 15: loss tensor(752.0007, device='cuda:0'), U.norm 10.369559288024902, V.norm 17.38736915588379, MLP.norm 3.487640619277954
2023-04-11 14:34:01,467 :: INFO :: Epoch 20: loss tensor(733.6307, device='cuda:0'), U.norm 9.1718168258667, V.norm 17.233762741088867, MLP.norm 4.477759838104248
2023-04-11 14:34:01,467 :: INFO :: Environment 3
2023-04-11 14:34:14,873 :: INFO :: Epoch 5: loss tensor(863.8544, device='cuda:0'), U.norm 14.646376609802246, V.norm 18.32630157470703, MLP.norm 1.8742793798446655
2023-04-11 14:34:14,967 :: INFO :: Epoch 10: loss tensor(850.3865, device='cuda:0'), U.norm 12.039825439453125, V.norm 17.83001708984375, MLP.norm 2.6647045612335205
2023-04-11 14:34:15,061 :: INFO :: Epoch 15: loss tensor(831.0716, device='cuda:0'), U.norm 10.37489128112793, V.norm 17.604646682739258, MLP.norm 3.695297956466675
2023-04-11 14:34:15,155 :: INFO :: Epoch 20: loss tensor(807.3835, device='cuda:0'), U.norm 9.179485321044922, V.norm 17.46078109741211, MLP.norm 4.776248931884766
2023-04-11 14:34:15,155 :: INFO :: Environment 4
2023-04-11 14:34:29,733 :: INFO :: Epoch 5: loss tensor(799.6936, device='cuda:0'), U.norm 14.644957542419434, V.norm 18.218351364135742, MLP.norm 1.8522101640701294
2023-04-11 14:34:29,811 :: INFO :: Epoch 10: loss tensor(789.1824, device='cuda:0'), U.norm 12.036465644836426, V.norm 17.683494567871094, MLP.norm 2.5774953365325928
2023-04-11 14:34:29,905 :: INFO :: Epoch 15: loss tensor(772.9573, device='cuda:0'), U.norm 10.369324684143066, V.norm 17.446033477783203, MLP.norm 3.5387461185455322
2023-04-11 14:34:29,999 :: INFO :: Epoch 20: loss tensor(753.0526, device='cuda:0'), U.norm 9.171339988708496, V.norm 17.29547691345215, MLP.norm 4.57301664352417
2023-04-11 14:34:29,999 :: INFO :: Environment 5
2023-04-11 14:34:45,499 :: INFO :: Epoch 5: loss tensor(880.3274, device='cuda:0'), U.norm 14.646906852722168, V.norm 18.355867385864258, MLP.norm 1.8825514316558838
2023-04-11 14:34:45,592 :: INFO :: Epoch 10: loss tensor(865.9647, device='cuda:0'), U.norm 12.04190731048584, V.norm 17.87177276611328, MLP.norm 2.6855361461639404
2023-04-11 14:34:45,670 :: INFO :: Epoch 15: loss tensor(846.1929, device='cuda:0'), U.norm 10.378859519958496, V.norm 17.652835845947266, MLP.norm 3.7303626537323
2023-04-11 14:34:45,764 :: INFO :: Epoch 20: loss tensor(821.9562, device='cuda:0'), U.norm 9.185568809509277, V.norm 17.513914108276367, MLP.norm 4.82747220993042
2023-04-11 14:34:45,764 :: INFO :: Environment 6
2023-04-11 14:34:59,608 :: INFO :: Epoch 5: loss tensor(825.4675, device='cuda:0'), U.norm 14.6473970413208, V.norm 18.26229476928711, MLP.norm 1.8483976125717163
2023-04-11 14:34:59,717 :: INFO :: Epoch 10: loss tensor(813.6189, device='cuda:0'), U.norm 12.041337013244629, V.norm 17.742870330810547, MLP.norm 2.5956950187683105
2023-04-11 14:34:59,827 :: INFO :: Epoch 15: loss tensor(796.1737, device='cuda:0'), U.norm 10.37690544128418, V.norm 17.50921630859375, MLP.norm 3.5891036987304688
2023-04-11 14:34:59,952 :: INFO :: Epoch 20: loss tensor(774.4465, device='cuda:0'), U.norm 9.182196617126465, V.norm 17.358627319335938, MLP.norm 4.644659996032715
2023-04-11 14:34:59,952 :: INFO :: Environment 7
2023-04-11 14:35:15,030 :: INFO :: Epoch 5: loss tensor(892.9253, device='cuda:0'), U.norm 14.649234771728516, V.norm 18.389667510986328, MLP.norm 1.8880053758621216
2023-04-11 14:35:15,123 :: INFO :: Epoch 10: loss tensor(876.2776, device='cuda:0'), U.norm 12.046866416931152, V.norm 17.917465209960938, MLP.norm 2.722612142562866
2023-04-11 14:35:15,218 :: INFO :: Epoch 15: loss tensor(853.5200, device='cuda:0'), U.norm 10.387066841125488, V.norm 17.70317268371582, MLP.norm 3.796128749847412
2023-04-11 14:35:15,327 :: INFO :: Epoch 20: loss tensor(825.4651, device='cuda:0'), U.norm 9.197308540344238, V.norm 17.564546585083008, MLP.norm 4.926065921783447
2023-04-11 14:35:15,327 :: INFO :: Environment 8
2023-04-11 14:35:28,249 :: INFO :: Epoch 5: loss tensor(801.3393, device='cuda:0'), U.norm 14.647775650024414, V.norm 18.216909408569336, MLP.norm 1.850866675376892
2023-04-11 14:35:28,327 :: INFO :: Epoch 10: loss tensor(789.2790, device='cuda:0'), U.norm 12.042227745056152, V.norm 17.675222396850586, MLP.norm 2.5951123237609863
2023-04-11 14:35:28,405 :: INFO :: Epoch 15: loss tensor(770.8939, device='cuda:0'), U.norm 10.37871265411377, V.norm 17.429746627807617, MLP.norm 3.594111680984497
2023-04-11 14:35:28,483 :: INFO :: Epoch 20: loss tensor(749.2645, device='cuda:0'), U.norm 9.1847505569458, V.norm 17.271150588989258, MLP.norm 4.649590969085693
2023-04-11 14:35:28,483 :: INFO :: Environment 9
2023-04-11 14:35:44,030 :: INFO :: Epoch 5: loss tensor(851.0930, device='cuda:0'), U.norm 14.648991584777832, V.norm 18.313119888305664, MLP.norm 1.8743083477020264
2023-04-11 14:35:44,140 :: INFO :: Epoch 10: loss tensor(836.8433, device='cuda:0'), U.norm 12.045454025268555, V.norm 17.811628341674805, MLP.norm 2.6613845825195312
2023-04-11 14:35:44,233 :: INFO :: Epoch 15: loss tensor(816.3601, device='cuda:0'), U.norm 10.384438514709473, V.norm 17.584238052368164, MLP.norm 3.6935067176818848
2023-04-11 14:35:44,359 :: INFO :: Epoch 20: loss tensor(792.2039, device='cuda:0'), U.norm 9.193598747253418, V.norm 17.43659782409668, MLP.norm 4.78265905380249
2023-04-11 14:35:44,359 :: INFO :: Ite = 1, Delta = 4167
2023-04-11 14:35:44,359 :: INFO :: ----- backend -----
2023-04-11 14:35:46,733 :: INFO :: Epoch 5: loss tensor(186.2415, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0120, -0.0095, -0.0149, -0.0040, -0.0091, -0.0140, -0.0133,  0.0005,
        -0.0124, -0.0074, -0.0145, -0.0104, -0.0074, -0.0009, -0.0018,  0.0051,
        -0.0091,  0.0008,  0.0023, -0.0106, -0.0068, -0.0070, -0.0054, -0.0151,
        -0.0108, -0.0067, -0.0099, -0.0110, -0.0113, -0.0094, -0.0049,  0.0074,
        -0.0058, -0.0076, -0.0139, -0.0152, -0.0011, -0.0061, -0.0109, -0.0057,
        -0.0146, -0.0127, -0.0031, -0.0044, -0.0037, -0.0063, -0.0072, -0.0155,
        -0.0093, -0.0090, -0.0094, -0.0065, -0.0030, -0.0044, -0.0145,  0.0058,
        -0.0048, -0.0036, -0.0132, -0.0158, -0.0043, -0.0009, -0.0144, -0.0082,
        -0.0042, -0.0087, -0.0008, -0.0022,  0.0022, -0.0140, -0.0063, -0.0130,
         0.0091, -0.0069, -0.0021, -0.0142,  0.0137, -0.0125, -0.0038,  0.0154,
        -0.0112, -0.0145, -0.0002, -0.0091, -0.0136, -0.0096,  0.0065, -0.0135,
        -0.0099, -0.0140, -0.0108, -0.0033, -0.0099, -0.0021, -0.0121, -0.0086,
        -0.0060, -0.0070, -0.0145, -0.0047, -0.0047, -0.0035, -0.0077, -0.0133,
        -0.0152, -0.0147, -0.0039, -0.0063,  0.0011,  0.0017, -0.0145,  0.0004,
        -0.0135, -0.0106, -0.0143, -0.0130, -0.0084, -0.0127,  0.0027, -0.0114,
        -0.0096, -0.0087, -0.0021, -0.0042, -0.0055, -0.0087, -0.0148, -0.0045,
         0.0056, -0.0071, -0.0117,  0.0164, -0.0032,  0.0156, -0.0114, -0.0142,
        -0.0148,  0.0122, -0.0145, -0.0162,  0.0090, -0.0154, -0.0042, -0.0077,
        -0.0100,  0.0140,  0.0055, -0.0108, -0.0134, -0.0157, -0.0147, -0.0038,
        -0.0083, -0.0144, -0.0162, -0.0161, -0.0152, -0.0158,  0.0173, -0.0176,
         0.0111, -0.0176,  0.0154, -0.0159,  0.0126,  0.0172, -0.0158, -0.0150,
        -0.0160, -0.0126,  0.0111,  0.0080, -0.0167, -0.0010, -0.0068, -0.0143,
         0.0156,  0.0036, -0.0157, -0.0148, -0.0019, -0.0154,  0.0140, -0.0162,
         0.0059,  0.0121, -0.0150, -0.0045,  0.0177, -0.0158, -0.0174, -0.0145,
         0.0168, -0.0155,  0.0020,  0.0110, -0.0018,  0.0177, -0.0132, -0.0096,
         0.0050,  0.0038, -0.0123,  0.0157, -0.0116, -0.0024, -0.0164, -0.0169,
         0.0116, -0.0135,  0.0184,  0.0146,  0.0172, -0.0153, -0.0146,  0.0108,
        -0.0119, -0.0163,  0.0171, -0.0157, -0.0024, -0.0165,  0.0161, -0.0056,
         0.0053,  0.0073,  0.0004,  0.0034,  0.0030, -0.0087, -0.0081,  0.0128,
         0.0150,  0.0193,  0.0163, -0.0128, -0.0155, -0.0141,  0.0157,  0.0135,
        -0.0106, -0.0132, -0.0119,  0.0088, -0.0145, -0.0084, -0.0157, -0.0152,
        -0.0139, -0.0149, -0.0156,  0.0175, -0.0033, -0.0094, -0.0159, -0.0097,
        -0.0091, -0.0106, -0.0114, -0.0098, -0.0136, -0.0049, -0.0153, -0.0148,
        -0.0131, -0.0123, -0.0124, -0.0140, -0.0114, -0.0124, -0.0141, -0.0135,
        -0.0152, -0.0148, -0.0122, -0.0150, -0.0126, -0.0114, -0.0129, -0.0131,
        -0.0141, -0.0108, -0.0107, -0.0134, -0.0028, -0.0079, -0.0148, -0.0140,
        -0.0090, -0.0145, -0.0142, -0.0136, -0.0154, -0.0122, -0.0147, -0.0118,
        -0.0090, -0.0160,  0.0002, -0.0127, -0.0145, -0.0035, -0.0154, -0.0144,
        -0.0108, -0.0106, -0.0085, -0.0142, -0.0122, -0.0151, -0.0111, -0.0141,
        -0.0124, -0.0140, -0.0155, -0.0134, -0.0111, -0.0084, -0.0149, -0.0145,
        -0.0108, -0.0161, -0.0066, -0.0150, -0.0136, -0.0113, -0.0151, -0.0162,
        -0.0141, -0.0148, -0.0140, -0.0143, -0.0151, -0.0152, -0.0093, -0.0064,
        -0.0136, -0.0135, -0.0153, -0.0088, -0.0130, -0.0150, -0.0145, -0.0086,
        -0.0119, -0.0074, -0.0147, -0.0138, -0.0119, -0.0158, -0.0139, -0.0118,
        -0.0136, -0.0143, -0.0136, -0.0133, -0.0145, -0.0051, -0.0121, -0.0133,
        -0.0131, -0.0123, -0.0136, -0.0170, -0.0061, -0.0090, -0.0128, -0.0138,
        -0.0116, -0.0148, -0.0150, -0.0148, -0.0102, -0.0085, -0.0168,  0.0011,
        -0.0114, -0.0087, -0.0126, -0.0150, -0.0119, -0.0146, -0.0144, -0.0090],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.8935, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:49,280 :: INFO :: Epoch 10: loss tensor(183.7529, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.5890e-02, -1.0987e-02, -2.6234e-02, -1.7009e-03, -5.6542e-03,
        -2.4590e-02, -2.0246e-02,  8.7998e-03, -1.9137e-02, -5.5571e-03,
        -2.5460e-02, -1.3460e-02, -3.4671e-04,  3.7016e-03, -9.5566e-04,
         1.1201e-02, -1.0241e-02,  7.0955e-03,  5.8688e-03, -1.3014e-02,
        -5.6398e-03, -6.9715e-03,  4.1728e-04, -2.6150e-02, -1.5917e-02,
        -1.5544e-04, -1.1698e-02, -1.2797e-02, -1.6760e-02, -1.3829e-02,
         1.5112e-04,  1.5821e-02, -7.8088e-03, -1.0276e-02, -2.4528e-02,
        -2.8372e-02,  2.9984e-03,  4.2146e-04, -1.2670e-02, -1.0580e-02,
        -2.5180e-02, -1.7491e-02, -3.3796e-03, -2.2486e-04,  1.4776e-04,
        -7.2973e-03, -3.0661e-03, -2.6544e-02, -1.4266e-02, -1.0471e-02,
        -1.3195e-02, -6.4221e-03, -2.8535e-03, -1.1912e-03, -2.5948e-02,
         1.7296e-02, -2.2185e-03, -5.4194e-04, -2.3227e-02, -2.8610e-02,
         8.1404e-04,  3.6584e-03, -2.6604e-02, -6.3646e-03, -3.5955e-03,
        -7.0677e-03, -2.0526e-03, -4.6503e-03,  1.0087e-02, -2.4213e-02,
        -5.5508e-03, -2.2045e-02,  1.4702e-02, -1.4092e-03,  5.9031e-03,
        -2.4089e-02,  1.6457e-02, -2.0281e-02,  2.1385e-03,  2.0089e-02,
        -1.1539e-02, -2.7078e-02,  1.1035e-02, -8.6020e-03, -2.2575e-02,
        -1.3826e-02,  1.0453e-02, -2.3465e-02, -1.2202e-02, -2.3563e-02,
        -9.9578e-03,  6.8523e-03, -1.0011e-02,  5.0408e-03, -2.3255e-02,
        -7.7807e-03, -3.6427e-03, -1.2722e-02, -2.4149e-02, -3.7098e-03,
        -1.0803e-02,  6.2992e-03, -3.9842e-03, -2.1051e-02, -2.6998e-02,
        -2.7975e-02, -2.7524e-03, -8.2637e-03,  8.5026e-03,  1.1807e-02,
        -2.5924e-02,  7.1789e-03, -2.3531e-02, -1.4901e-02, -2.4275e-02,
        -2.2529e-02, -1.2085e-02, -1.8928e-02,  7.7962e-03, -1.6545e-02,
        -1.2649e-02, -7.4635e-03,  2.8007e-04, -5.9719e-03,  8.6549e-04,
        -1.2083e-02, -2.5078e-02,  4.2110e-03,  2.5451e-02,  5.9921e-03,
        -6.0708e-03,  3.4188e-02,  1.1212e-02,  3.6705e-02, -3.9567e-03,
        -2.0496e-02, -2.4738e-02,  2.6089e-02, -1.4111e-02, -2.7882e-02,
         2.9076e-02, -2.5718e-02,  1.5246e-02,  7.2502e-03,  1.7560e-03,
         3.2103e-02,  2.4356e-02, -9.4654e-04, -1.4466e-02, -2.4148e-02,
        -2.4352e-02,  1.8171e-02,  5.4127e-03, -2.5066e-02, -2.6924e-02,
        -2.8254e-02, -2.5079e-02, -2.8062e-02,  3.2765e-02, -3.2337e-02,
         2.9347e-02, -3.2176e-02,  3.3532e-02, -2.6136e-02,  2.8353e-02,
         2.9473e-02, -2.7296e-02, -2.3377e-02, -2.7405e-02, -5.4490e-03,
         3.4291e-02,  2.4252e-02, -3.0013e-02,  1.7289e-02,  1.1197e-02,
        -2.2654e-02,  2.9326e-02,  2.1255e-02, -2.5361e-02, -2.2174e-02,
         1.5525e-02, -2.5878e-02,  3.1758e-02, -2.8773e-02,  2.3660e-02,
         3.1857e-02, -2.5905e-02,  1.6495e-02,  3.4636e-02, -2.7677e-02,
        -2.9742e-02, -2.1977e-02,  3.2926e-02, -2.7187e-02,  2.3367e-02,
         2.9455e-02,  2.0094e-02,  3.1645e-02, -5.9593e-03,  3.0476e-03,
         2.4245e-02,  2.6162e-02, -6.1028e-03,  3.5991e-02, -3.3570e-03,
         1.9088e-02, -2.8631e-02, -3.0271e-02,  2.6963e-02, -1.5508e-02,
         3.8830e-02,  3.1199e-02,  2.4306e-02, -2.5718e-02, -1.3013e-02,
         3.1851e-02, -9.7557e-03, -2.8463e-02,  3.4235e-02, -2.7934e-02,
         1.9808e-02, -2.5957e-02,  3.7351e-02,  1.3742e-02,  1.9758e-02,
         3.0133e-02,  1.7509e-02,  2.4085e-02,  2.0780e-02,  6.9199e-03,
         4.5877e-03,  2.8383e-02,  3.0389e-02,  3.4246e-02,  3.2329e-02,
        -9.8902e-03, -2.5135e-02, -2.0779e-02,  2.9986e-02,  3.3068e-02,
        -3.1650e-04, -1.3925e-02, -7.8038e-03,  2.4615e-02, -2.3170e-02,
         2.2839e-03, -2.7844e-02, -2.0107e-02, -1.9540e-02, -2.3596e-02,
        -2.2394e-02,  3.5130e-02,  1.4923e-02,  4.2338e-03, -2.8557e-02,
         1.7324e-03,  9.2360e-03,  3.8517e-03,  2.5261e-03,  5.7511e-03,
        -9.4662e-03,  1.5168e-02, -2.1924e-02, -1.6880e-02, -5.4649e-03,
        -1.8955e-03, -1.3423e-04, -1.0522e-02,  2.5271e-03, -1.8893e-03,
        -1.5995e-02, -6.8758e-03, -1.9119e-02, -1.4378e-02,  3.7676e-04,
        -2.0633e-02,  1.8268e-05, -9.8116e-05, -6.8152e-03, -6.2790e-03,
        -9.7673e-03,  6.8784e-03,  3.7057e-03, -1.8429e-03,  1.8201e-02,
         1.0519e-02, -1.5923e-02, -1.5275e-02,  8.0832e-03, -2.0421e-02,
        -1.3987e-02, -8.6385e-03, -9.1867e-03, -3.3502e-04, -1.7261e-02,
        -8.4419e-04,  8.3046e-03, -1.9162e-02,  2.2376e-02, -2.6363e-03,
        -1.6348e-02,  1.8096e-02, -1.0409e-02, -1.6734e-02,  3.1844e-03,
         4.2704e-03,  9.4536e-03, -1.1507e-02, -2.1808e-04, -2.0502e-02,
         2.6648e-03, -1.1662e-02, -4.3022e-03, -8.0971e-03, -1.8261e-02,
        -6.0209e-03,  1.0408e-03,  9.8194e-03, -1.7157e-02, -1.7914e-02,
         3.4863e-03, -1.9546e-02,  1.3186e-02, -1.7080e-02, -6.7475e-03,
        -2.2928e-03, -1.4774e-02, -2.4109e-02, -1.1720e-02, -1.8607e-02,
        -1.1752e-02, -1.5136e-02, -1.9875e-02, -1.7552e-02,  8.2606e-03,
         1.4181e-02, -1.2557e-02, -1.1771e-02, -1.2749e-02,  9.1978e-03,
        -6.1669e-03, -1.5151e-02, -1.2254e-02,  7.8453e-03, -2.0947e-03,
         1.2489e-02, -1.6374e-02, -1.1253e-02, -3.2435e-03, -2.3956e-02,
        -7.5480e-03,  1.2407e-03, -9.2041e-03, -1.3318e-02, -1.2972e-02,
        -1.0853e-02, -1.5360e-02,  1.5895e-02, -1.6251e-03, -8.2242e-03,
        -3.1325e-03, -8.4781e-04, -3.9283e-03, -2.2820e-02,  1.4466e-02,
         8.5339e-03, -5.9856e-03, -9.7449e-03,  2.2033e-03, -1.7219e-02,
        -1.9065e-02, -2.0571e-02,  5.3490e-03,  9.5759e-03, -1.6092e-02,
         2.3709e-02,  2.1712e-03,  5.8564e-03, -4.0690e-03, -2.0997e-02,
         2.0482e-05, -1.6981e-02, -2.0527e-02,  9.8078e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.5740, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:51,952 :: INFO :: Epoch 15: loss tensor(179.1891, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.8212e-02, -1.5096e-02, -3.5752e-02, -4.9820e-03, -2.5561e-03,
        -3.2927e-02, -2.4348e-02,  9.9295e-03, -2.4833e-02, -8.2185e-03,
        -3.3674e-02, -1.9317e-02,  5.3319e-03,  1.1659e-03, -6.7166e-03,
         7.0306e-03, -1.4946e-02,  5.7403e-03, -5.2583e-04, -1.5891e-02,
        -9.6598e-03, -1.1851e-02,  2.6824e-03, -3.4849e-02, -2.2741e-02,
         3.5448e-03, -1.6303e-02, -1.4693e-02, -1.8960e-02, -2.0879e-02,
         4.9740e-04,  1.4901e-02, -1.2532e-02, -1.6270e-02, -3.4419e-02,
        -4.0558e-02, -1.4852e-03,  1.6931e-03, -1.5009e-02, -1.8826e-02,
        -3.4053e-02, -2.0815e-02, -8.4008e-03, -2.6280e-03, -2.1807e-03,
        -1.2814e-02, -3.4901e-03, -3.5669e-02, -2.0429e-02, -1.5208e-02,
        -1.9393e-02, -1.1935e-02, -1.0212e-02, -4.8773e-03, -3.4645e-02,
         2.2472e-02, -2.7331e-03, -1.3669e-03, -3.2373e-02, -3.8916e-02,
        -6.9957e-04,  7.7977e-04, -3.6965e-02, -7.4247e-03, -9.2345e-03,
        -8.5593e-03, -8.2747e-03, -1.2781e-02,  9.0000e-03, -3.3431e-02,
        -1.0693e-02, -2.9443e-02,  9.8787e-03,  9.6094e-04,  8.2552e-03,
        -3.3483e-02,  9.6536e-03, -2.8049e-02,  1.3821e-03,  1.4619e-02,
        -1.0263e-02, -3.6517e-02,  1.5948e-02, -9.2471e-03, -3.1323e-02,
        -1.8416e-02,  5.2859e-03, -3.2414e-02, -1.6670e-02, -3.2432e-02,
        -9.2078e-03,  1.2997e-02, -1.2424e-02,  5.7982e-03, -3.3856e-02,
        -6.6736e-03, -8.5814e-03, -2.2443e-02, -3.1369e-02, -8.2658e-03,
        -2.1719e-02,  1.1253e-02, -2.5011e-03, -2.6452e-02, -3.6463e-02,
        -3.9960e-02, -7.7571e-03, -1.4942e-02,  8.0384e-03,  1.4819e-02,
        -3.5179e-02,  6.4637e-03, -2.9771e-02, -1.9368e-02, -3.2923e-02,
        -3.2371e-02, -1.9240e-02, -2.2012e-02,  2.7304e-03, -2.2365e-02,
        -1.9059e-02, -9.3373e-03, -5.5017e-03, -1.4611e-02,  3.5367e-03,
        -2.0043e-02, -3.3010e-02,  9.3786e-03,  4.6415e-02,  2.1307e-02,
         9.0278e-03,  5.1483e-02,  3.4341e-02,  5.9653e-02,  1.0152e-02,
        -2.0490e-02, -2.9486e-02,  3.8076e-02, -2.0996e-03, -3.5486e-02,
         4.9975e-02, -3.2078e-02,  3.7726e-02,  2.9289e-02,  2.2125e-02,
         4.9170e-02,  4.2345e-02,  2.1111e-02, -3.7815e-03, -2.1966e-02,
        -2.9895e-02,  4.4236e-02,  2.4151e-02, -3.3013e-02, -3.2285e-02,
        -3.6567e-02, -3.0830e-02, -3.7783e-02,  4.4919e-02, -4.4480e-02,
         4.7528e-02, -4.4223e-02,  5.1359e-02, -3.0898e-02,  4.1366e-02,
         3.4646e-02, -3.5525e-02, -2.6163e-02, -3.4213e-02,  1.0779e-02,
         5.9519e-02,  3.8444e-02, -4.0935e-02,  3.6812e-02,  3.3879e-02,
        -2.5137e-02,  3.9904e-02,  3.8514e-02, -2.9135e-02, -2.4821e-02,
         3.3868e-02, -3.1827e-02,  4.7461e-02, -3.8696e-02,  4.0290e-02,
         5.2595e-02, -3.3934e-02,  4.1011e-02,  5.7896e-02, -3.6919e-02,
        -3.5629e-02, -2.2995e-02,  4.6782e-02, -3.5965e-02,  4.4609e-02,
         4.8248e-02,  4.5446e-02,  3.9427e-02,  1.2263e-02,  2.4198e-02,
         4.4037e-02,  5.0856e-02,  6.5540e-03,  5.6267e-02,  1.3927e-02,
         4.4314e-02, -3.6893e-02, -4.1454e-02,  3.9573e-02, -9.6385e-03,
         6.0346e-02,  4.5928e-02,  3.2164e-02, -3.2308e-02,  2.3994e-03,
         5.4546e-02,  1.1479e-03, -3.7427e-02,  5.0162e-02, -3.7684e-02,
         4.5008e-02, -2.6062e-02,  6.0480e-02,  3.6661e-02,  3.9048e-02,
         5.4863e-02,  3.4841e-02,  4.6448e-02,  3.8018e-02,  2.8566e-02,
         2.1550e-02,  4.0594e-02,  4.5684e-02,  5.4036e-02,  4.6776e-02,
         6.7563e-03, -2.9018e-02, -2.1827e-02,  4.1237e-02,  5.3066e-02,
         1.7606e-02, -4.5647e-03,  8.5536e-03,  3.9350e-02, -2.7031e-02,
         1.8550e-02, -3.7265e-02, -1.4612e-02, -1.8905e-02, -2.7270e-02,
        -1.7156e-02,  5.2134e-02,  3.6486e-02,  2.6724e-02, -3.8231e-02,
         1.9471e-02,  3.4652e-02,  2.8043e-02,  2.6671e-02,  2.9177e-02,
         9.1124e-03,  3.9766e-02, -1.5688e-02, -2.6938e-03,  1.5498e-02,
         2.1160e-02,  2.3351e-02,  9.3773e-03,  2.6175e-02,  2.0528e-02,
        -3.3361e-03,  1.5391e-02, -7.9000e-03,  1.4445e-03,  2.4186e-02,
        -1.3715e-02,  2.3052e-02,  2.3146e-02,  1.3876e-02,  1.5644e-02,
         1.0953e-02,  3.2804e-02,  2.8884e-02,  2.2068e-02,  4.2929e-02,
         3.3726e-02, -2.4463e-04, -1.7378e-03,  3.0517e-02, -1.4661e-02,
         1.0876e-03,  1.2042e-02,  1.3411e-02,  2.2283e-02, -4.2601e-03,
         2.1197e-02,  3.3494e-02, -6.2522e-03,  4.6053e-02,  1.9065e-02,
        -3.1178e-03,  4.3707e-02,  1.0365e-02, -4.1911e-03,  2.6809e-02,
         2.9285e-02,  3.4036e-02,  6.2114e-03,  2.3521e-02, -1.2465e-02,
         2.6277e-02,  6.6621e-03,  1.7507e-02,  1.2993e-02, -5.1264e-03,
         1.5060e-02,  2.2523e-02,  3.4667e-02, -3.2054e-03, -7.7263e-03,
         2.7491e-02, -6.6867e-03,  3.8053e-02, -2.2136e-03,  1.4693e-02,
         1.9958e-02,  1.8300e-03, -2.1664e-02,  6.8992e-03, -8.1854e-03,
         6.2428e-03,  8.5612e-05, -8.6274e-03, -3.4853e-03,  3.2756e-02,
         3.8455e-02,  3.0372e-03,  5.9460e-03,  6.3895e-03,  3.3459e-02,
         1.6167e-02,  1.3565e-03,  6.4435e-03,  3.0779e-02,  2.0202e-02,
         3.6422e-02, -4.0076e-03,  7.8945e-03,  1.8751e-02, -2.0417e-02,
         1.4106e-02,  2.5063e-02,  1.1340e-02,  1.2146e-03,  1.7646e-03,
         7.5352e-03, -8.8165e-04,  4.0586e-02,  2.1435e-02,  1.2414e-02,
         1.9554e-02,  2.2691e-02,  1.9043e-02, -1.3746e-02,  3.9573e-02,
         3.2902e-02,  1.4919e-02,  9.8039e-03,  2.6276e-02, -2.9204e-03,
        -7.4894e-03, -1.4144e-02,  2.9402e-02,  3.3924e-02,  2.3765e-03,
         4.8882e-02,  2.5938e-02,  2.7555e-02,  1.7850e-02, -1.4807e-02,
         2.3617e-02, -3.1700e-03, -1.3808e-02,  3.4894e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(6.1159, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:54,593 :: INFO :: Epoch 20: loss tensor(178.1380, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0121, -0.0101, -0.0368,  0.0009,  0.0051, -0.0335, -0.0179,  0.0158,
        -0.0214, -0.0039, -0.0348, -0.0187,  0.0136,  0.0079, -0.0033,  0.0089,
        -0.0138,  0.0136, -0.0005, -0.0096, -0.0067, -0.0086,  0.0124, -0.0359,
        -0.0218,  0.0108, -0.0142, -0.0081, -0.0085, -0.0199,  0.0088,  0.0213,
        -0.0062, -0.0118, -0.0351, -0.0468,  0.0017,  0.0058, -0.0118, -0.0165,
        -0.0368, -0.0185, -0.0026, -0.0002,  0.0020, -0.0110,  0.0013, -0.0373,
        -0.0174, -0.0131, -0.0172, -0.0085, -0.0126, -0.0020, -0.0336,  0.0334,
         0.0060,  0.0076, -0.0321, -0.0427,  0.0037,  0.0031, -0.0397, -0.0011,
        -0.0046, -0.0042, -0.0021, -0.0112,  0.0137, -0.0356, -0.0074, -0.0257,
         0.0089,  0.0077,  0.0171, -0.0361,  0.0118, -0.0273,  0.0090,  0.0161,
        -0.0009, -0.0368,  0.0240,  0.0008, -0.0324, -0.0134,  0.0078, -0.0320,
        -0.0140, -0.0357, -0.0036,  0.0232, -0.0095,  0.0134, -0.0338,  0.0041,
        -0.0069, -0.0220, -0.0330, -0.0047, -0.0215,  0.0190,  0.0059, -0.0264,
        -0.0381, -0.0466, -0.0038, -0.0130,  0.0132,  0.0231, -0.0387,  0.0108,
        -0.0260, -0.0144, -0.0350, -0.0357, -0.0169, -0.0169,  0.0019, -0.0192,
        -0.0178, -0.0048, -0.0027, -0.0154,  0.0110, -0.0217, -0.0343,  0.0197,
         0.0674,  0.0368,  0.0264,  0.0662,  0.0586,  0.0834,  0.0257, -0.0163,
        -0.0302,  0.0472,  0.0142, -0.0396,  0.0705, -0.0355,  0.0600,  0.0549,
         0.0435,  0.0660,  0.0581,  0.0457,  0.0127, -0.0110, -0.0327,  0.0706,
         0.0428, -0.0398, -0.0332, -0.0419, -0.0336, -0.0460,  0.0575, -0.0544,
         0.0656, -0.0541,  0.0700, -0.0316,  0.0544,  0.0438, -0.0415, -0.0249,
        -0.0376,  0.0288,  0.0843,  0.0522, -0.0501,  0.0563,  0.0561, -0.0233,
         0.0495,  0.0552, -0.0285, -0.0238,  0.0520, -0.0344,  0.0617, -0.0469,
         0.0562,  0.0737, -0.0404,  0.0649,  0.0856, -0.0448, -0.0351, -0.0194,
         0.0604, -0.0429,  0.0629,  0.0672,  0.0713,  0.0462,  0.0321,  0.0465,
         0.0637,  0.0760,  0.0211,  0.0741,  0.0328,  0.0702, -0.0421, -0.0509,
         0.0508,  0.0007,  0.0821,  0.0601,  0.0499, -0.0360,  0.0207,  0.0779,
         0.0158, -0.0440,  0.0665, -0.0459,  0.0698, -0.0190,  0.0841,  0.0599,
         0.0646,  0.0799,  0.0514,  0.0697,  0.0540,  0.0504,  0.0381,  0.0507,
         0.0609,  0.0768,  0.0607,  0.0288, -0.0285, -0.0189,  0.0487,  0.0733,
         0.0363,  0.0100,  0.0290,  0.0547, -0.0276,  0.0361, -0.0451, -0.0029,
        -0.0137, -0.0273, -0.0028,  0.0695,  0.0590,  0.0503, -0.0456,  0.0379,
         0.0610,  0.0533,  0.0529,  0.0516,  0.0312,  0.0657, -0.0008,  0.0178,
         0.0405,  0.0467,  0.0478,  0.0325,  0.0496,  0.0428,  0.0152,  0.0399,
         0.0112,  0.0220,  0.0482,  0.0006,  0.0477,  0.0475,  0.0365,  0.0393,
         0.0348,  0.0602,  0.0541,  0.0478,  0.0691,  0.0552,  0.0220,  0.0178,
         0.0509, -0.0012,  0.0220,  0.0358,  0.0403,  0.0449,  0.0143,  0.0437,
         0.0581,  0.0135,  0.0677,  0.0410,  0.0171,  0.0703,  0.0359,  0.0160,
         0.0521,  0.0556,  0.0579,  0.0281,  0.0476,  0.0037,  0.0512,  0.0312,
         0.0418,  0.0366,  0.0151,  0.0375,  0.0447,  0.0610,  0.0167,  0.0097,
         0.0517,  0.0145,  0.0625,  0.0188,  0.0391,  0.0435,  0.0251, -0.0112,
         0.0297,  0.0091,  0.0283,  0.0209,  0.0104,  0.0173,  0.0581,  0.0609,
         0.0208,  0.0281,  0.0301,  0.0562,  0.0403,  0.0231,  0.0288,  0.0545,
         0.0433,  0.0585,  0.0122,  0.0313,  0.0426, -0.0086,  0.0380,  0.0497,
         0.0352,  0.0184,  0.0220,  0.0295,  0.0205,  0.0652,  0.0460,  0.0359,
         0.0440,  0.0488,  0.0448,  0.0052,  0.0642,  0.0563,  0.0383,  0.0334,
         0.0503,  0.0180,  0.0123,  0.0008,  0.0524,  0.0572,  0.0270,  0.0747,
         0.0496,  0.0471,  0.0406, -0.0007,  0.0472,  0.0180,  0.0015,  0.0593],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.6240, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:57,342 :: INFO :: Epoch 25: loss tensor(175.0796, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.6087e-03, -9.5967e-04, -3.1484e-02,  1.1223e-02,  1.4311e-02,
        -2.8387e-02, -6.2100e-03,  2.4514e-02, -1.3022e-02,  4.0292e-03,
        -3.0107e-02, -1.3249e-02,  2.2131e-02,  1.9177e-02,  4.9250e-03,
         1.4327e-02, -9.7528e-03,  2.5451e-02,  3.7941e-03,  1.4440e-03,
         4.4474e-05, -1.2833e-03,  2.5165e-02, -3.1273e-02, -1.5577e-02,
         1.9037e-02, -8.7064e-03,  2.7445e-03,  7.3772e-03, -1.4297e-02,
         1.9851e-02,  3.1550e-02,  5.8443e-03, -2.0739e-03, -2.9915e-02,
        -4.8118e-02,  8.8157e-03,  1.2119e-02, -5.7746e-03, -8.3309e-03,
        -3.5022e-02, -1.1931e-02,  8.9468e-03,  5.5550e-03,  9.2770e-03,
        -5.1423e-03,  8.9426e-03, -3.3181e-02, -8.5821e-03, -7.3626e-03,
        -1.0740e-02, -5.5873e-04, -1.0306e-02,  3.9261e-03, -2.5893e-02,
         4.6696e-02,  1.9885e-02,  2.1956e-02, -2.5653e-02, -4.1265e-02,
         1.0323e-02,  8.9345e-03, -3.6719e-02,  8.2084e-03,  4.3476e-03,
         3.1969e-03,  1.0097e-02, -3.8586e-03,  2.1727e-02, -3.3268e-02,
         3.4891e-04, -1.5796e-02,  1.1374e-02,  1.7365e-02,  2.8685e-02,
        -3.3889e-02,  1.9645e-02, -2.0550e-02,  2.0044e-02,  2.3215e-02,
         1.1212e-02, -3.0071e-02,  3.2775e-02,  1.5270e-02, -2.8472e-02,
        -3.3174e-03,  1.4176e-02, -2.5216e-02, -6.8657e-03, -3.4938e-02,
         4.5568e-03,  3.4730e-02, -3.4619e-03,  2.4676e-02, -2.6367e-02,
         1.9138e-02, -1.6980e-03, -1.6065e-02, -3.0141e-02,  3.4685e-03,
        -1.4001e-02,  2.7806e-02,  1.6482e-02, -2.2222e-02, -3.4148e-02,
        -4.7880e-02,  5.2127e-03, -6.6298e-03,  2.1790e-02,  3.4276e-02,
        -3.5848e-02,  1.8211e-02, -1.5730e-02, -4.3829e-03, -3.2302e-02,
        -3.4288e-02, -9.6411e-03, -8.0516e-03,  4.2772e-03, -1.0733e-02,
        -1.2189e-02,  2.1183e-03,  4.8089e-03, -1.1409e-02,  2.0523e-02,
        -1.9066e-02, -3.0252e-02,  3.2253e-02,  8.7899e-02,  5.1455e-02,
         4.3730e-02,  7.8088e-02,  7.7983e-02,  1.0556e-01,  4.0729e-02,
        -9.4995e-03, -2.7623e-02,  5.3499e-02,  3.1037e-02, -4.1053e-02,
         9.0231e-02, -3.6640e-02,  8.1384e-02,  8.1427e-02,  6.3907e-02,
         8.2155e-02,  7.1307e-02,  7.0591e-02,  3.0380e-02,  4.4621e-03,
        -3.3069e-02,  9.6251e-02,  5.9890e-02, -4.5687e-02, -3.1091e-02,
        -4.4502e-02, -3.4268e-02, -5.3052e-02,  6.9825e-02, -6.2320e-02,
         8.2981e-02, -6.2521e-02,  8.8209e-02, -2.9198e-02,  6.7174e-02,
         5.3481e-02, -4.5861e-02, -2.0912e-02, -3.8005e-02,  4.6461e-02,
         1.0770e-01,  6.4966e-02, -5.7982e-02,  7.4962e-02,  7.7399e-02,
        -1.8196e-02,  5.7998e-02,  7.0794e-02, -2.4874e-02, -2.0247e-02,
         6.8999e-02, -3.4197e-02,  7.3822e-02, -5.3917e-02,  7.0963e-02,
         9.4765e-02, -4.5717e-02,  8.7453e-02,  1.1275e-01, -5.1968e-02,
        -2.9031e-02, -1.3026e-02,  7.3228e-02, -4.8595e-02,  7.7380e-02,
         8.5932e-02,  9.6727e-02,  5.1756e-02,  5.0854e-02,  6.8581e-02,
         8.2807e-02,  1.0083e-01,  3.5237e-02,  8.8405e-02,  5.0747e-02,
         9.5755e-02, -4.4826e-02, -5.9099e-02,  6.0787e-02,  1.2712e-02,
         1.0342e-01,  7.3437e-02,  6.5413e-02, -3.7918e-02,  3.8172e-02,
         1.0140e-01,  3.1208e-02, -4.8527e-02,  8.2778e-02, -5.2791e-02,
         9.3851e-02, -7.7077e-03,  1.0689e-01,  8.2696e-02,  8.6983e-02,
         1.0454e-01,  6.6841e-02,  9.2890e-02,  6.8096e-02,  7.1279e-02,
         5.3035e-02,  5.8471e-02,  7.5388e-02,  9.3551e-02,  7.3776e-02,
         5.2090e-02, -2.5043e-02, -1.3406e-02,  5.3172e-02,  9.2438e-02,
         5.3894e-02,  2.6265e-02,  5.0362e-02,  7.0046e-02, -2.5742e-02,
         5.3534e-02, -5.1580e-02,  1.0938e-02, -6.2851e-03, -2.4271e-02,
         1.6527e-02,  8.6809e-02,  8.1510e-02,  7.3490e-02, -5.1191e-02,
         5.5127e-02,  8.7224e-02,  7.8291e-02,  7.9454e-02,  7.0947e-02,
         5.3453e-02,  9.1632e-02,  1.6940e-02,  3.8821e-02,  6.6740e-02,
         7.2986e-02,  7.1962e-02,  5.5044e-02,  7.0528e-02,  6.2132e-02,
         3.4485e-02,  6.3501e-02,  3.3312e-02,  4.2907e-02,  7.0001e-02,
         1.7284e-02,  7.3089e-02,  7.0842e-02,  5.7385e-02,  6.1293e-02,
         5.7336e-02,  8.8376e-02,  7.7111e-02,  7.3439e-02,  9.5876e-02,
         7.2992e-02,  4.4656e-02,  3.8296e-02,  6.8046e-02,  1.5131e-02,
         4.4358e-02,  5.8965e-02,  6.8635e-02,  6.5281e-02,  3.3249e-02,
         6.4527e-02,  8.0612e-02,  3.5200e-02,  8.5770e-02,  6.0344e-02,
         3.9667e-02,  9.7165e-02,  6.2778e-02,  4.0029e-02,  7.7485e-02,
         8.1970e-02,  7.8953e-02,  5.0962e-02,  6.9665e-02,  2.3131e-02,
         7.5800e-02,  5.7001e-02,  6.5692e-02,  6.0135e-02,  3.7201e-02,
         5.7905e-02,  6.6130e-02,  8.7687e-02,  3.6846e-02,  2.9926e-02,
         7.3712e-02,  3.8893e-02,  8.5118e-02,  4.0454e-02,  6.3657e-02,
         6.5501e-02,  4.9759e-02,  3.8502e-03,  5.1451e-02,  2.6832e-02,
         4.9651e-02,  4.2022e-02,  3.1347e-02,  3.8743e-02,  8.2942e-02,
         7.9660e-02,  3.6603e-02,  4.9723e-02,  5.4825e-02,  7.5791e-02,
         6.2843e-02,  4.4296e-02,  5.0070e-02,  7.7746e-02,  6.5102e-02,
         7.7570e-02,  2.6808e-02,  5.4910e-02,  6.6077e-02,  6.8559e-03,
         6.0317e-02,  7.3344e-02,  5.8455e-02,  3.4319e-02,  4.3395e-02,
         5.0153e-02,  4.4280e-02,  8.9391e-02,  6.8790e-02,  5.8227e-02,
         6.8120e-02,  7.5657e-02,  7.1614e-02,  2.8847e-02,  8.8221e-02,
         7.6727e-02,  6.1068e-02,  5.7452e-02,  7.2232e-02,  3.9627e-02,
         3.3751e-02,  2.0309e-02,  7.2291e-02,  7.8200e-02,  5.3573e-02,
         1.0066e-01,  7.0283e-02,  6.0343e-02,  6.0854e-02,  1.8101e-02,
         6.8543e-02,  4.0140e-02,  1.9920e-02,  8.1562e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(9.0633, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:35:59,748 :: INFO :: Epoch 30: loss tensor(175.7254, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.3548e-03,  5.7602e-03, -2.5160e-02,  1.8364e-02,  2.0382e-02,
        -2.0329e-02,  5.3447e-03,  3.0131e-02, -5.3963e-03,  9.9065e-03,
        -2.2860e-02, -8.4877e-03,  2.8275e-02,  2.7956e-02,  1.1880e-02,
         1.7808e-02, -6.7377e-03,  3.4731e-02,  6.9765e-03,  1.0797e-02,
         3.8319e-03,  4.2822e-03,  3.5328e-02, -2.5461e-02, -1.0071e-02,
         2.3396e-02, -4.9955e-03,  1.1272e-02,  2.3234e-02, -1.0614e-02,
         2.7328e-02,  3.8543e-02,  1.6719e-02,  5.3563e-03, -2.4224e-02,
        -4.7610e-02,  1.3287e-02,  1.4840e-02, -1.9497e-03, -5.4822e-04,
        -3.2228e-02, -4.5210e-03,  2.0369e-02,  8.2121e-03,  1.3922e-02,
        -1.5474e-04,  1.4519e-02, -2.7730e-02,  1.3798e-04, -3.0326e-03,
        -5.4869e-03,  4.6307e-03, -7.7542e-03,  7.5249e-03, -1.6775e-02,
         5.6314e-02,  3.3414e-02,  3.3956e-02, -1.7702e-02, -3.7355e-02,
         1.4695e-02,  1.3628e-02, -3.1571e-02,  1.5153e-02,  1.1511e-02,
         7.8482e-03,  2.0885e-02,  2.2811e-03,  2.6814e-02, -3.0199e-02,
         6.2028e-03, -5.8410e-03,  1.2873e-02,  2.5293e-02,  3.6442e-02,
        -3.0879e-02,  2.6690e-02, -1.3979e-02,  2.7477e-02,  2.9988e-02,
         2.0525e-02, -1.9934e-02,  3.7954e-02,  2.6568e-02, -2.3761e-02,
         5.3594e-03,  1.9004e-02, -1.8160e-02, -9.1392e-04, -3.3623e-02,
         1.0270e-02,  4.2397e-02,  2.4924e-04,  3.2262e-02, -1.7444e-02,
         3.1601e-02,  7.7014e-04, -1.0425e-02, -2.6502e-02,  9.3928e-03,
        -6.4838e-03,  3.3170e-02,  2.3804e-02, -1.6657e-02, -2.8011e-02,
        -4.6323e-02,  1.2639e-02, -1.3682e-03,  2.8635e-02,  4.2048e-02,
        -3.0521e-02,  2.3238e-02, -4.0595e-03,  4.4166e-03, -2.8869e-02,
        -3.3236e-02, -3.8002e-03,  1.3977e-03,  5.9534e-03, -3.3441e-03,
        -7.8721e-03,  6.4256e-03,  1.1302e-02, -8.0720e-03,  2.6803e-02,
        -1.6726e-02, -2.5637e-02,  4.0895e-02,  1.0706e-01,  6.4600e-02,
         6.0210e-02,  8.6823e-02,  9.1320e-02,  1.2488e-01,  5.4329e-02,
        -1.4509e-03, -2.2426e-02,  5.7790e-02,  4.7002e-02, -4.0341e-02,
         1.0816e-01, -3.5994e-02,  1.0116e-01,  1.0749e-01,  8.3052e-02,
         9.6886e-02,  8.1227e-02,  9.4832e-02,  4.7434e-02,  2.1342e-02,
        -3.1688e-02,  1.2014e-01,  7.4839e-02, -5.0881e-02, -2.6844e-02,
        -4.4615e-02, -3.3152e-02, -5.8960e-02,  7.9678e-02, -6.8643e-02,
         9.8974e-02, -6.9454e-02,  1.0424e-01, -2.4606e-02,  7.8598e-02,
         5.8401e-02, -4.8715e-02, -1.5235e-02, -3.5823e-02,  6.2634e-02,
         1.2947e-01,  7.5951e-02, -6.4597e-02,  9.2338e-02,  9.7228e-02,
        -1.0957e-02,  6.3965e-02,  8.4462e-02, -1.9407e-02, -1.5293e-02,
         8.4014e-02, -3.1692e-02,  8.1600e-02, -5.9693e-02,  8.3672e-02,
         1.1486e-01, -5.0192e-02,  1.0754e-01,  1.3801e-01, -5.8088e-02,
        -1.9749e-02, -4.9092e-03,  8.4238e-02, -5.2876e-02,  8.7672e-02,
         1.0362e-01,  1.2035e-01,  5.3840e-02,  6.7905e-02,  8.9441e-02,
         1.0041e-01,  1.2430e-01,  4.7686e-02,  9.8657e-02,  6.6748e-02,
         1.1984e-01, -4.5450e-02, -6.6175e-02,  6.8375e-02,  2.4710e-02,
         1.2314e-01,  8.4704e-02,  7.4693e-02, -3.8287e-02,  5.3851e-02,
         1.2398e-01,  4.5568e-02, -5.1388e-02,  9.7278e-02, -5.8529e-02,
         1.1603e-01,  4.8153e-03,  1.2740e-01,  1.0426e-01,  1.0331e-01,
         1.2786e-01,  8.0404e-02,  1.1498e-01,  7.9218e-02,  9.0835e-02,
         6.5486e-02,  6.3172e-02,  8.7988e-02,  1.0247e-01,  8.5075e-02,
         7.4663e-02, -1.9381e-02, -6.4106e-03,  5.5849e-02,  1.0896e-01,
         6.9656e-02,  4.2488e-02,  7.1577e-02,  8.4767e-02, -2.1994e-02,
         6.9949e-02, -5.6767e-02,  2.4726e-02,  2.3530e-03, -1.9136e-02,
         3.6012e-02,  1.0332e-01,  1.0247e-01,  9.5708e-02, -5.5099e-02,
         7.0206e-02,  1.1342e-01,  1.0304e-01,  1.0606e-01,  8.7506e-02,
         7.5856e-02,  1.1708e-01,  3.5197e-02,  5.8966e-02,  9.3152e-02,
         9.9274e-02,  9.5418e-02,  7.6134e-02,  8.8997e-02,  7.7820e-02,
         5.3146e-02,  8.5857e-02,  5.6074e-02,  6.2684e-02,  8.9385e-02,
         3.5314e-02,  9.8394e-02,  9.3250e-02,  7.6346e-02,  8.1104e-02,
         7.8064e-02,  1.1638e-01,  9.7958e-02,  9.8528e-02,  1.2251e-01,
         8.7391e-02,  6.6519e-02,  5.8363e-02,  8.3148e-02,  3.3161e-02,
         6.6837e-02,  8.0803e-02,  9.6978e-02,  8.3404e-02,  5.0830e-02,
         8.4642e-02,  1.0085e-01,  5.6950e-02,  1.0012e-01,  7.6816e-02,
         6.3251e-02,  1.2373e-01,  8.9915e-02,  6.5217e-02,  1.0287e-01,
         1.0808e-01,  9.7722e-02,  7.4238e-02,  8.9572e-02,  4.3767e-02,
         1.0005e-01,  8.3023e-02,  8.9450e-02,  8.3196e-02,  6.0281e-02,
         7.5525e-02,  8.6909e-02,  1.1434e-01,  5.5934e-02,  5.1529e-02,
         9.3506e-02,  6.4309e-02,  1.0599e-01,  6.1170e-02,  8.8045e-02,
         8.5213e-02,  7.5016e-02,  2.1815e-02,  7.1547e-02,  4.3477e-02,
         6.9450e-02,  6.2105e-02,  5.2295e-02,  5.9359e-02,  1.0743e-01,
         9.4428e-02,  5.0137e-02,  7.0086e-02,  7.9762e-02,  9.2091e-02,
         8.3736e-02,  6.4011e-02,  6.9907e-02,  1.0064e-01,  8.5562e-02,
         9.4884e-02,  3.9541e-02,  7.8022e-02,  8.8467e-02,  2.3643e-02,
         8.0463e-02,  9.5777e-02,  8.0804e-02,  4.8850e-02,  6.4838e-02,
         6.9067e-02,  6.8918e-02,  1.1303e-01,  8.9653e-02,  7.9068e-02,
         9.1899e-02,  1.0240e-01,  9.8045e-02,  5.4320e-02,  1.1144e-01,
         9.4311e-02,  8.3131e-02,  8.1135e-02,  9.1795e-02,  6.0444e-02,
         5.5150e-02,  4.2583e-02,  8.9461e-02,  9.6805e-02,  8.0538e-02,
         1.2630e-01,  8.8287e-02,  6.8999e-02,  7.8364e-02,  3.9107e-02,
         8.7531e-02,  6.1934e-02,  3.9981e-02,  1.0227e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.4068, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:02,405 :: INFO :: Epoch 35: loss tensor(171.2412, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0158,  0.0113, -0.0175,  0.0244,  0.0258, -0.0108,  0.0164,  0.0358,
         0.0021,  0.0162, -0.0138, -0.0031,  0.0339,  0.0360,  0.0180,  0.0209,
        -0.0035,  0.0423,  0.0106,  0.0199,  0.0076,  0.0096,  0.0446, -0.0189,
        -0.0048,  0.0276, -0.0010,  0.0196,  0.0383, -0.0062,  0.0338,  0.0446,
         0.0267,  0.0125, -0.0182, -0.0455,  0.0176,  0.0178,  0.0013,  0.0073,
        -0.0281,  0.0034,  0.0316,  0.0108,  0.0178,  0.0048,  0.0200, -0.0205,
         0.0084,  0.0017, -0.0010,  0.0081, -0.0039,  0.0105, -0.0065,  0.0642,
         0.0467,  0.0460, -0.0089, -0.0316,  0.0182,  0.0193, -0.0250,  0.0218,
         0.0179,  0.0121,  0.0307,  0.0076,  0.0314, -0.0262,  0.0119,  0.0033,
         0.0151,  0.0344,  0.0428, -0.0272,  0.0341, -0.0070,  0.0337,  0.0367,
         0.0290, -0.0084,  0.0421,  0.0365, -0.0186,  0.0124,  0.0236, -0.0106,
         0.0043, -0.0313,  0.0152,  0.0489,  0.0047,  0.0383, -0.0076,  0.0418,
         0.0031, -0.0054, -0.0216,  0.0161,  0.0013,  0.0379,  0.0300, -0.0096,
        -0.0206, -0.0426,  0.0195,  0.0033,  0.0359,  0.0483, -0.0226,  0.0280,
         0.0077,  0.0129, -0.0248, -0.0311,  0.0017,  0.0113,  0.0083,  0.0039,
        -0.0039,  0.0101,  0.0179, -0.0039,  0.0324, -0.0129, -0.0197,  0.0484,
         0.1247,  0.0766,  0.0749,  0.0926,  0.0991,  0.1408,  0.0664,  0.0071,
        -0.0160,  0.0607,  0.0611, -0.0383,  0.1242, -0.0341,  0.1190,  0.1324,
         0.1003,  0.1105,  0.0882,  0.1176,  0.0626,  0.0372, -0.0294,  0.1423,
         0.0874, -0.0556, -0.0216, -0.0431, -0.0308, -0.0641,  0.0874, -0.0736,
         0.1132, -0.0752,  0.1185, -0.0190,  0.0890,  0.0603, -0.0506, -0.0085,
        -0.0320,  0.0773,  0.1492,  0.0855, -0.0703,  0.1085,  0.1153, -0.0028,
         0.0680,  0.0963, -0.0132, -0.0095,  0.0973, -0.0279,  0.0856, -0.0647,
         0.0948,  0.1339, -0.0539,  0.1248,  0.1609, -0.0635, -0.0083,  0.0035,
         0.0940, -0.0564,  0.0944,  0.1202,  0.1422,  0.0544,  0.0831,  0.1086,
         0.1167,  0.1462,  0.0585,  0.1052,  0.0800,  0.1425, -0.0446, -0.0724,
         0.0740,  0.0363,  0.1411,  0.0946,  0.0788, -0.0378,  0.0666,  0.1456,
         0.0587, -0.0530,  0.1106, -0.0635,  0.1363,  0.0169,  0.1458,  0.1242,
         0.1142,  0.1496,  0.0923,  0.1360,  0.0874,  0.1082,  0.0752,  0.0655,
         0.0988,  0.1051,  0.0950,  0.0958, -0.0127,  0.0012,  0.0566,  0.1232,
         0.0835,  0.0577,  0.0916,  0.0989, -0.0174,  0.0853, -0.0611,  0.0371,
         0.0112, -0.0128,  0.0551,  0.1192,  0.1221,  0.1161, -0.0579,  0.0832,
         0.1388,  0.1265,  0.1321,  0.1003,  0.0979,  0.1417,  0.0520,  0.0766,
         0.1193,  0.1255,  0.1177,  0.0951,  0.1045,  0.0893,  0.0701,  0.1060,
         0.0783,  0.0809,  0.1059,  0.0520,  0.1236,  0.1138,  0.0932,  0.0976,
         0.0961,  0.1439,  0.1155,  0.1225,  0.1489,  0.0977,  0.0865,  0.0765,
         0.0963,  0.0502,  0.0881,  0.1004,  0.1249,  0.0994,  0.0657,  0.1033,
         0.1181,  0.0782,  0.1100,  0.0902,  0.0859,  0.1499,  0.1165,  0.0907,
         0.1280,  0.1335,  0.1128,  0.0974,  0.1065,  0.0636,  0.1235,  0.1083,
         0.1122,  0.1057,  0.0829,  0.0896,  0.1074,  0.1404,  0.0724,  0.0725,
         0.1104,  0.0893,  0.1244,  0.0797,  0.1118,  0.1022,  0.0995,  0.0407,
         0.0889,  0.0578,  0.0866,  0.0801,  0.0715,  0.0774,  0.1309,  0.1048,
         0.0598,  0.0882,  0.1044,  0.1039,  0.1021,  0.0809,  0.0873,  0.1227,
         0.1040,  0.1107,  0.0499,  0.0996,  0.1093,  0.0396,  0.0974,  0.1164,
         0.1012,  0.0623,  0.0852,  0.0845,  0.0935,  0.1364,  0.1076,  0.0976,
         0.1149,  0.1288,  0.1238,  0.0804,  0.1340,  0.1084,  0.1036,  0.1043,
         0.1086,  0.0792,  0.0746,  0.0654,  0.1032,  0.1129,  0.1072,  0.1511,
         0.1028,  0.0725,  0.0920,  0.0606,  0.1036,  0.0817,  0.0592,  0.1209],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.6615, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:05,155 :: INFO :: Epoch 40: loss tensor(171.0488, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0229,  0.0152, -0.0103,  0.0293,  0.0288, -0.0024,  0.0256,  0.0391,
         0.0075,  0.0203, -0.0049,  0.0012,  0.0367,  0.0418,  0.0231,  0.0226,
        -0.0018,  0.0472,  0.0131,  0.0273,  0.0106,  0.0133,  0.0516, -0.0128,
        -0.0004,  0.0298,  0.0007,  0.0265,  0.0515, -0.0030,  0.0381,  0.0484,
         0.0353,  0.0170, -0.0132, -0.0429,  0.0200,  0.0190,  0.0036,  0.0130,
        -0.0230,  0.0098,  0.0400,  0.0122,  0.0201,  0.0073,  0.0236, -0.0136,
         0.0150,  0.0052,  0.0025,  0.0106, -0.0008,  0.0116,  0.0031,  0.0703,
         0.0566,  0.0568, -0.0012, -0.0256,  0.0203,  0.0227, -0.0192,  0.0266,
         0.0228,  0.0152,  0.0377,  0.0114,  0.0344, -0.0232,  0.0165,  0.0111,
         0.0163,  0.0432,  0.0466, -0.0237,  0.0396, -0.0006,  0.0373,  0.0422,
         0.0363,  0.0029,  0.0445,  0.0440, -0.0147,  0.0178,  0.0261, -0.0041,
         0.0083, -0.0292,  0.0185,  0.0539,  0.0073,  0.0426,  0.0011,  0.0488,
         0.0034, -0.0015, -0.0170,  0.0214,  0.0067,  0.0399,  0.0335, -0.0023,
        -0.0128, -0.0381,  0.0252,  0.0062,  0.0406,  0.0518, -0.0140,  0.0302,
         0.0175,  0.0198, -0.0207, -0.0290,  0.0059,  0.0200,  0.0097,  0.0098,
        -0.0013,  0.0123,  0.0226, -0.0009,  0.0355, -0.0095, -0.0147,  0.0548,
         0.1406,  0.0865,  0.0871,  0.0958,  0.1023,  0.1534,  0.0761,  0.0152,
        -0.0086,  0.0634,  0.0732, -0.0354,  0.1378, -0.0317,  0.1346,  0.1565,
         0.1153,  0.1223,  0.0922,  0.1391,  0.0754,  0.0512, -0.0265,  0.1619,
         0.0971, -0.0598, -0.0162, -0.0405, -0.0280, -0.0686,  0.0923, -0.0776,
         0.1254, -0.0801,  0.1299, -0.0132,  0.0978,  0.0586, -0.0517, -0.0018,
        -0.0277,  0.0898,  0.1668,  0.0930, -0.0753,  0.1226,  0.1313,  0.0058,
         0.0694,  0.1058, -0.0073, -0.0033,  0.1081, -0.0232,  0.0862, -0.0688,
         0.1035,  0.1511, -0.0571,  0.1392,  0.1813, -0.0683,  0.0044,  0.0111,
         0.1017, -0.0593,  0.0981,  0.1349,  0.1616,  0.0535,  0.0957,  0.1257,
         0.1308,  0.1661,  0.0672,  0.1079,  0.0904,  0.1632, -0.0430, -0.0778,
         0.0773,  0.0462,  0.1565,  0.1021,  0.0801, -0.0369,  0.0767,  0.1656,
         0.0695, -0.0537,  0.1215, -0.0679,  0.1543,  0.0272,  0.1610,  0.1426,
         0.1210,  0.1691,  0.1023,  0.1551,  0.0923,  0.1231,  0.0816,  0.0654,
         0.1079,  0.1044,  0.1029,  0.1152, -0.0058,  0.0084,  0.0571,  0.1341,
         0.0948,  0.0712,  0.1101,  0.1119, -0.0126,  0.0983, -0.0647,  0.0475,
         0.0192, -0.0060,  0.0730,  0.1336,  0.1397,  0.1347, -0.0597,  0.0936,
         0.1633,  0.1490,  0.1575,  0.1103,  0.1187,  0.1657,  0.0670,  0.0920,
         0.1444,  0.1508,  0.1394,  0.1126,  0.1172,  0.0969,  0.0853,  0.1244,
         0.0997,  0.0982,  0.1196,  0.0669,  0.1479,  0.1329,  0.1086,  0.1114,
         0.1117,  0.1701,  0.1305,  0.1458,  0.1745,  0.1047,  0.1042,  0.0927,
         0.1082,  0.0661,  0.1082,  0.1178,  0.1518,  0.1137,  0.0785,  0.1206,
         0.1328,  0.0988,  0.1163,  0.1005,  0.1086,  0.1750,  0.1421,  0.1155,
         0.1521,  0.1579,  0.1250,  0.1204,  0.1203,  0.0824,  0.1463,  0.1326,
         0.1338,  0.1272,  0.1051,  0.1005,  0.1273,  0.1657,  0.0864,  0.0935,
         0.1245,  0.1134,  0.1406,  0.0970,  0.1345,  0.1173,  0.1236,  0.0593,
         0.1047,  0.0694,  0.1015,  0.0968,  0.0888,  0.0933,  0.1535,  0.1118,
         0.0672,  0.1042,  0.1276,  0.1121,  0.1179,  0.0955,  0.1028,  0.1437,
         0.1207,  0.1256,  0.0589,  0.1201,  0.1288,  0.0543,  0.1119,  0.1359,
         0.1195,  0.0752,  0.1034,  0.0970,  0.1176,  0.1589,  0.1232,  0.1140,
         0.1370,  0.1545,  0.1481,  0.1058,  0.1559,  0.1194,  0.1221,  0.1271,
         0.1233,  0.0957,  0.0922,  0.0883,  0.1135,  0.1268,  0.1327,  0.1752,
         0.1142,  0.0725,  0.1021,  0.0824,  0.1168,  0.0998,  0.0769,  0.1377],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.8263, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:36:05,155 :: INFO :: ----- frontend -----
2023-04-11 14:36:05,171 :: INFO :: Environment 0
2023-04-11 14:36:20,155 :: INFO :: Epoch 5: loss tensor(867.4779, device='cuda:0'), U.norm 14.6512451171875, V.norm 18.335725784301758, MLP.norm 1.881217360496521
2023-04-11 14:36:20,249 :: INFO :: Epoch 10: loss tensor(853.1010, device='cuda:0'), U.norm 12.049667358398438, V.norm 17.845590591430664, MLP.norm 2.6881327629089355
2023-04-11 14:36:20,327 :: INFO :: Epoch 15: loss tensor(832.8370, device='cuda:0'), U.norm 10.39042854309082, V.norm 17.62509536743164, MLP.norm 3.738251209259033
2023-04-11 14:36:20,421 :: INFO :: Epoch 20: loss tensor(807.6191, device='cuda:0'), U.norm 9.201498031616211, V.norm 17.485361099243164, MLP.norm 4.859912872314453
2023-04-11 14:36:20,421 :: INFO :: Environment 1
2023-04-11 14:36:34,624 :: INFO :: Epoch 5: loss tensor(790.1326, device='cuda:0'), U.norm 14.646727561950684, V.norm 18.199363708496094, MLP.norm 1.8353337049484253
2023-04-11 14:36:34,717 :: INFO :: Epoch 10: loss tensor(779.4006, device='cuda:0'), U.norm 12.0400972366333, V.norm 17.654314041137695, MLP.norm 2.548527956008911
2023-04-11 14:36:34,811 :: INFO :: Epoch 15: loss tensor(762.9784, device='cuda:0'), U.norm 10.37524700164795, V.norm 17.408733367919922, MLP.norm 3.50042986869812
2023-04-11 14:36:34,889 :: INFO :: Epoch 20: loss tensor(743.2811, device='cuda:0'), U.norm 9.179858207702637, V.norm 17.254140853881836, MLP.norm 4.513216972351074
2023-04-11 14:36:34,889 :: INFO :: Environment 2
2023-04-11 14:36:49,264 :: INFO :: Epoch 5: loss tensor(867.1854, device='cuda:0'), U.norm 14.649201393127441, V.norm 18.339313507080078, MLP.norm 1.8726075887680054
2023-04-11 14:36:49,342 :: INFO :: Epoch 10: loss tensor(853.4922, device='cuda:0'), U.norm 12.045860290527344, V.norm 17.848207473754883, MLP.norm 2.665830612182617
2023-04-11 14:36:49,436 :: INFO :: Epoch 15: loss tensor(833.8452, device='cuda:0'), U.norm 10.385004997253418, V.norm 17.625732421875, MLP.norm 3.715772867202759
2023-04-11 14:36:49,515 :: INFO :: Epoch 20: loss tensor(809.1098, device='cuda:0'), U.norm 9.194503784179688, V.norm 17.481609344482422, MLP.norm 4.816987991333008
2023-04-11 14:36:49,515 :: INFO :: Environment 3
2023-04-11 14:37:01,811 :: INFO :: Epoch 5: loss tensor(842.1672, device='cuda:0'), U.norm 14.64866828918457, V.norm 18.30278778076172, MLP.norm 1.8715590238571167
2023-04-11 14:37:01,921 :: INFO :: Epoch 10: loss tensor(828.3147, device='cuda:0'), U.norm 12.044986724853516, V.norm 17.798677444458008, MLP.norm 2.6583030223846436
2023-04-11 14:37:02,014 :: INFO :: Epoch 15: loss tensor(808.8510, device='cuda:0'), U.norm 10.383768081665039, V.norm 17.571340560913086, MLP.norm 3.6767709255218506
2023-04-11 14:37:02,125 :: INFO :: Epoch 20: loss tensor(785.2296, device='cuda:0'), U.norm 9.192596435546875, V.norm 17.42459487915039, MLP.norm 4.737098693847656
2023-04-11 14:37:02,125 :: INFO :: Environment 4
2023-04-11 14:37:15,468 :: INFO :: Epoch 5: loss tensor(844.5430, device='cuda:0'), U.norm 14.648178100585938, V.norm 18.296794891357422, MLP.norm 1.8686424493789673
2023-04-11 14:37:15,561 :: INFO :: Epoch 10: loss tensor(831.2016, device='cuda:0'), U.norm 12.04350757598877, V.norm 17.789663314819336, MLP.norm 2.638491153717041
2023-04-11 14:37:15,655 :: INFO :: Epoch 15: loss tensor(811.8282, device='cuda:0'), U.norm 10.381245613098145, V.norm 17.560016632080078, MLP.norm 3.654783010482788
2023-04-11 14:37:15,749 :: INFO :: Epoch 20: loss tensor(788.2235, device='cuda:0'), U.norm 9.188879013061523, V.norm 17.412944793701172, MLP.norm 4.745960235595703
2023-04-11 14:37:15,749 :: INFO :: Environment 5
2023-04-11 14:37:30,063 :: INFO :: Epoch 5: loss tensor(860.7850, device='cuda:0'), U.norm 14.650106430053711, V.norm 18.32836151123047, MLP.norm 1.8761662244796753
2023-04-11 14:37:30,155 :: INFO :: Epoch 10: loss tensor(847.8452, device='cuda:0'), U.norm 12.047295570373535, V.norm 17.834775924682617, MLP.norm 2.6703073978424072
2023-04-11 14:37:30,233 :: INFO :: Epoch 15: loss tensor(828.9939, device='cuda:0'), U.norm 10.387002944946289, V.norm 17.61174774169922, MLP.norm 3.7093913555145264
2023-04-11 14:37:30,327 :: INFO :: Epoch 20: loss tensor(805.5172, device='cuda:0'), U.norm 9.196952819824219, V.norm 17.468788146972656, MLP.norm 4.819164276123047
2023-04-11 14:37:30,327 :: INFO :: Environment 6
2023-04-11 14:37:43,077 :: INFO :: Epoch 5: loss tensor(825.5038, device='cuda:0'), U.norm 14.648138999938965, V.norm 18.257549285888672, MLP.norm 1.8619695901870728
2023-04-11 14:37:43,186 :: INFO :: Epoch 10: loss tensor(813.0729, device='cuda:0'), U.norm 12.04360580444336, V.norm 17.737010955810547, MLP.norm 2.6255075931549072
2023-04-11 14:37:43,296 :: INFO :: Epoch 15: loss tensor(795.1753, device='cuda:0'), U.norm 10.381319046020508, V.norm 17.50526237487793, MLP.norm 3.628584861755371
2023-04-11 14:37:43,421 :: INFO :: Epoch 20: loss tensor(773.3159, device='cuda:0'), U.norm 9.188830375671387, V.norm 17.35702896118164, MLP.norm 4.692885398864746
2023-04-11 14:37:43,436 :: INFO :: Environment 7
2023-04-11 14:37:56,717 :: INFO :: Epoch 5: loss tensor(835.9232, device='cuda:0'), U.norm 14.647627830505371, V.norm 18.287508010864258, MLP.norm 1.869094729423523
2023-04-11 14:37:56,811 :: INFO :: Epoch 10: loss tensor(822.9033, device='cuda:0'), U.norm 12.04224681854248, V.norm 17.778467178344727, MLP.norm 2.6512551307678223
2023-04-11 14:37:56,905 :: INFO :: Epoch 15: loss tensor(803.8011, device='cuda:0'), U.norm 10.378817558288574, V.norm 17.54844093322754, MLP.norm 3.6694705486297607
2023-04-11 14:37:56,999 :: INFO :: Epoch 20: loss tensor(781.3701, device='cuda:0'), U.norm 9.184982299804688, V.norm 17.40264320373535, MLP.norm 4.73938512802124
2023-04-11 14:37:56,999 :: INFO :: Environment 8
2023-04-11 14:38:10,671 :: INFO :: Epoch 5: loss tensor(834.1295, device='cuda:0'), U.norm 14.650970458984375, V.norm 18.276355743408203, MLP.norm 1.860849142074585
2023-04-11 14:38:10,765 :: INFO :: Epoch 10: loss tensor(821.6655, device='cuda:0'), U.norm 12.048829078674316, V.norm 17.760719299316406, MLP.norm 2.6194746494293213
2023-04-11 14:38:10,890 :: INFO :: Epoch 15: loss tensor(803.3708, device='cuda:0'), U.norm 10.38896656036377, V.norm 17.528621673583984, MLP.norm 3.6146528720855713
2023-04-11 14:38:11,030 :: INFO :: Epoch 20: loss tensor(781.8036, device='cuda:0'), U.norm 9.19910717010498, V.norm 17.379695892333984, MLP.norm 4.647181510925293
2023-04-11 14:38:11,030 :: INFO :: Environment 9
2023-04-11 14:38:25,436 :: INFO :: Epoch 5: loss tensor(935.1064, device='cuda:0'), U.norm 14.64990234375, V.norm 18.45781135559082, MLP.norm 1.8955483436584473
2023-04-11 14:38:25,530 :: INFO :: Epoch 10: loss tensor(918.5028, device='cuda:0'), U.norm 12.048521041870117, V.norm 18.008472442626953, MLP.norm 2.7406105995178223
2023-04-11 14:38:25,639 :: INFO :: Epoch 15: loss tensor(894.9691, device='cuda:0'), U.norm 10.390390396118164, V.norm 17.802396774291992, MLP.norm 3.8310720920562744
2023-04-11 14:38:25,719 :: INFO :: Epoch 20: loss tensor(866.3409, device='cuda:0'), U.norm 9.202865600585938, V.norm 17.66798973083496, MLP.norm 4.973869323730469
2023-04-11 14:38:25,735 :: INFO :: Ite = 1, Delta = 4135
2023-04-11 14:38:25,735 :: INFO :: ----- backend -----
2023-04-11 14:38:27,702 :: INFO :: Epoch 5: loss tensor(356.8416, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-5.8756e-03, -3.8951e-03, -6.3913e-03, -2.5317e-03, -9.7716e-03,
        -1.0132e-02, -5.9785e-03,  1.2966e-03, -7.0441e-03, -2.0819e-03,
        -1.0237e-02, -5.4217e-03, -8.0190e-03, -1.4008e-03,  3.4296e-04,
         1.8207e-03, -3.8707e-03, -3.1617e-03,  1.4686e-03, -1.5900e-03,
        -5.6274e-03, -1.3398e-03,  2.7169e-04, -9.9384e-03, -5.3918e-03,
        -7.5852e-03, -1.1184e-03, -3.7681e-03, -2.7951e-03, -3.6520e-03,
        -6.2415e-03,  4.4045e-03, -3.1081e-03, -4.6841e-03, -8.1600e-03,
        -9.4794e-03,  7.6755e-04, -6.8805e-03, -7.9334e-03,  1.8923e-03,
        -9.3151e-03, -8.7034e-03,  3.5510e-03, -3.0720e-03, -6.6342e-03,
         1.3899e-03, -2.5404e-03, -7.2886e-03, -4.7764e-03, -2.3726e-03,
        -7.0446e-03, -9.1618e-04,  1.7008e-03, -4.6281e-03, -7.3401e-03,
         6.0408e-03, -3.2158e-03,  7.2010e-04, -3.9408e-03, -1.0194e-02,
        -2.2378e-03, -1.7130e-03, -8.8539e-03, -7.2610e-03, -2.4614e-03,
        -5.1409e-03, -3.9779e-06,  2.5840e-03,  3.6266e-03, -8.9047e-03,
         5.9838e-04, -8.4381e-03,  1.1573e-02, -6.2046e-03, -2.2437e-03,
        -6.8583e-03,  1.0499e-02, -7.4864e-03, -1.3772e-03,  1.1480e-02,
        -8.2936e-03, -1.0268e-02, -3.0247e-05, -2.7004e-03, -9.4073e-03,
        -6.1282e-03,  4.8927e-03, -5.2748e-03, -6.8429e-03, -9.9026e-03,
        -8.3542e-03,  8.1940e-04, -5.8768e-03,  2.0413e-03, -4.5371e-03,
        -5.8306e-03, -3.9215e-03, -4.9226e-03, -9.9221e-03, -5.8181e-03,
        -2.0786e-03, -4.5561e-03, -4.8100e-03, -1.0577e-02, -9.3409e-03,
        -1.0057e-02,  3.9373e-03, -3.7736e-03, -4.0887e-03, -5.9330e-04,
        -7.6238e-03,  2.3540e-03, -7.4507e-03,  9.8647e-04, -1.0483e-02,
        -9.9833e-03, -4.3401e-03, -9.6729e-03, -2.5448e-03, -6.6956e-03,
        -5.9313e-03, -7.3735e-03, -2.1634e-03, -3.6732e-03, -3.9371e-03,
        -5.9962e-03, -6.3630e-03, -4.4431e-03,  6.5353e-03,  2.9837e-03,
        -8.1991e-03,  1.4244e-02,  5.4451e-03,  9.8165e-03, -5.6147e-03,
        -1.1282e-02, -1.2203e-02,  1.1087e-02, -3.7361e-03, -1.1975e-02,
         1.9589e-03, -1.1683e-02, -4.1805e-03, -8.5737e-03, -7.1978e-03,
         8.4071e-03,  2.1331e-03, -1.1615e-02, -9.2997e-03, -1.1957e-02,
        -1.1925e-02,  6.3501e-04,  1.4510e-03, -1.1864e-02, -1.1343e-02,
        -1.2245e-02, -1.1532e-02, -1.2231e-02,  1.2982e-02, -1.3237e-02,
         1.1319e-02, -1.2775e-02,  1.0484e-02, -1.0711e-02,  1.0651e-02,
         1.2778e-02, -1.2026e-02, -1.1117e-02, -1.1737e-02, -2.5363e-03,
         6.8695e-03,  1.0609e-02, -1.2577e-02, -2.8597e-03,  2.0714e-03,
        -1.1973e-02,  1.2735e-02,  7.0668e-03, -1.0987e-02, -1.0836e-02,
        -3.5081e-03, -1.2179e-02,  9.6777e-03, -1.2254e-02,  3.3172e-03,
         6.1964e-03, -1.1973e-02, -1.2121e-03,  1.2052e-02, -1.2245e-02,
        -1.2943e-02, -1.1264e-02,  1.0890e-02, -1.2236e-02,  2.9657e-03,
         7.9165e-03, -1.0108e-02,  1.2977e-02,  4.6806e-03, -9.9570e-03,
        -3.0985e-03, -8.7028e-03,  8.1255e-03,  1.5120e-02, -7.5630e-03,
        -1.0410e-02, -1.2088e-02, -1.2611e-02,  1.3014e-02, -9.9604e-03,
         4.4671e-03,  9.9760e-03,  1.1940e-02, -1.2076e-02, -2.4736e-03,
         4.3517e-04, -1.0574e-02, -1.2068e-02,  1.3261e-02, -1.2301e-02,
        -2.7714e-03, -1.0260e-02,  1.2835e-02,  3.2902e-03,  4.8642e-03,
        -2.3185e-03,  2.2561e-03, -4.7473e-03,  1.1327e-03,  2.6533e-03,
         1.0911e-04,  1.0554e-02,  1.4171e-02,  1.1864e-02,  1.3531e-02,
        -1.2050e-02, -1.1522e-02, -1.1327e-02,  9.9704e-03,  7.6670e-03,
        -6.8751e-03, -1.0529e-02, -1.1252e-02,  1.3651e-02, -1.1709e-02,
        -8.3465e-03, -1.2155e-02, -9.1069e-03, -1.0565e-02, -1.1171e-02,
        -1.2408e-02,  1.3510e-02, -9.5588e-03, -9.8089e-03, -1.2439e-02,
        -6.6287e-03, -1.7337e-03, -1.1428e-02, -1.0560e-02,  5.5507e-03,
         5.4297e-03,  5.8889e-03, -1.2330e-02, -8.6437e-03, -8.6202e-03,
        -4.0191e-03, -1.0220e-02, -1.2709e-02, -5.8649e-03, -3.3108e-03,
        -1.2494e-02, -7.7197e-03, -9.3088e-03, -1.2515e-02, -3.3506e-03,
        -1.2232e-02, -6.6483e-04, -1.6950e-03, -1.1549e-02, -8.0160e-03,
        -1.0066e-02, -5.8743e-03, -6.1357e-03, -1.1883e-02, -5.9743e-03,
         1.1711e-03, -1.0391e-02, -3.6217e-03,  2.0464e-03, -7.5709e-03,
        -9.3050e-03, -5.8506e-03, -6.6114e-03, -5.3347e-03, -1.3356e-02,
         3.5155e-03, -3.8422e-03, -1.2763e-02, -3.8248e-03,  3.4447e-03,
        -1.3334e-02, -1.2898e-03, -1.0818e-02, -1.2919e-02,  5.7047e-03,
        -1.1508e-02, -6.6689e-03, -1.1485e-02, -8.0846e-03, -1.1838e-02,
        -1.1965e-02, -5.3206e-03, -7.7775e-03, -4.4292e-03, -1.2258e-02,
        -1.2990e-03, -9.5943e-04, -1.2207e-02, -6.1102e-03, -1.2812e-02,
        -1.4169e-03, -1.1671e-02,  1.0048e-03, -1.1166e-02, -2.4804e-03,
        -1.0668e-02, -1.2866e-02, -6.9750e-03, -1.2033e-02, -9.6170e-03,
        -4.8574e-03, -1.2123e-02, -1.1238e-02, -1.2142e-02, -2.3863e-03,
        -6.4786e-03, -1.0154e-02, -4.4573e-03, -5.0909e-03, -8.4442e-03,
        -8.2741e-03, -1.0381e-02, -8.2320e-03,  3.6494e-03, -4.0415e-03,
        -8.7271e-03, -8.6228e-03, -1.3245e-02, -1.1455e-02, -1.1687e-02,
        -1.1992e-02, -1.2408e-02, -8.9203e-03, -1.1929e-02, -5.6839e-03,
        -1.0952e-02, -1.3026e-02,  4.8330e-03, -1.1094e-02, -4.0883e-03,
        -1.1385e-02, -1.2404e-02, -1.0092e-02, -1.0057e-02,  1.7624e-03,
        -1.5619e-03,  4.4380e-04, -1.2058e-02, -1.2642e-02, -9.4844e-03,
        -1.0594e-02, -1.2105e-02, -1.9016e-03, -3.1895e-03, -1.2573e-03,
        -5.4307e-03, -9.4192e-03, -6.0390e-03, -7.9941e-03, -1.3393e-02,
         2.2222e-03, -9.1004e-03, -8.7541e-03, -1.1304e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(2.5031, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:29,811 :: INFO :: Epoch 10: loss tensor(347.8445, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-2.3051e-03,  1.8094e-03, -6.9183e-03,  3.9799e-03, -9.5844e-03,
        -1.5356e-02, -4.0913e-03,  9.9809e-03, -5.8839e-03,  4.2609e-03,
        -1.5265e-02, -2.4893e-03, -4.2104e-03,  3.6228e-03,  6.0237e-03,
         9.8116e-03,  1.4956e-03,  3.1169e-03,  6.7573e-03,  5.3425e-03,
        -1.3944e-03,  5.2632e-03,  6.5312e-03, -1.3605e-02, -3.5696e-03,
        -3.5128e-03,  5.1303e-03,  1.9978e-03,  1.5741e-03,  1.4603e-03,
        -2.0977e-03,  1.2786e-02,  1.3605e-04, -1.3260e-03, -1.0528e-02,
        -1.4951e-02,  7.3922e-03, -1.7077e-03, -5.7412e-03,  6.2258e-03,
        -1.1618e-02, -9.0169e-03,  9.9874e-03,  2.7251e-03, -4.3697e-03,
         7.9928e-03,  3.7431e-03, -8.5677e-03, -1.5163e-03,  3.6913e-03,
        -6.6452e-03,  6.4695e-03,  8.6058e-03,  2.6153e-04, -8.4351e-03,
         1.6579e-02,  1.7947e-03,  8.1750e-03, -3.3375e-03, -1.6799e-02,
         5.1136e-03,  4.2492e-03, -1.1374e-02, -5.8847e-03,  1.7124e-03,
        -7.9723e-04,  5.3353e-03,  7.6396e-03,  1.2743e-02, -1.2045e-02,
         7.9592e-03, -1.0852e-02,  2.0311e-02, -8.2962e-04,  5.4947e-03,
        -7.8845e-03,  1.6757e-02, -8.5035e-03,  5.6323e-03,  1.8890e-02,
        -5.9042e-03, -1.6913e-02,  9.3526e-03,  4.2666e-03, -1.3546e-02,
        -4.1141e-03,  1.2214e-02, -5.3980e-03, -4.2541e-03, -1.3757e-02,
        -5.9900e-03,  1.1060e-02, -8.4085e-04,  1.0932e-02, -5.1681e-03,
        -1.0600e-03,  2.6211e-03, -3.3936e-03, -1.3362e-02, -3.6881e-03,
        -5.6033e-04,  3.0576e-03,  1.2831e-03, -1.4951e-02, -1.2546e-02,
        -1.6234e-02,  1.0324e-02, -7.3253e-04,  7.3126e-04,  7.9032e-03,
        -7.1658e-03,  1.0456e-02, -1.0058e-02,  6.5569e-03, -1.5340e-02,
        -1.4744e-02, -1.1930e-03, -1.2246e-02,  3.0053e-03, -5.3411e-03,
        -4.5478e-03, -4.9439e-03,  2.5715e-03, -1.6711e-03,  3.2801e-03,
        -4.6608e-03, -4.8631e-03,  2.5476e-03,  2.1767e-02,  1.7400e-02,
        -1.8530e-03,  2.9840e-02,  1.7735e-02,  2.5559e-02,  6.3668e-03,
        -1.4706e-02, -2.1231e-02,  2.6065e-02,  1.0423e-02, -1.8928e-02,
         1.7646e-02, -1.8729e-02,  1.0310e-02, -1.0990e-03,  2.6332e-03,
         2.2787e-02,  1.7504e-02, -1.7609e-02, -5.0944e-03, -1.5640e-02,
        -2.0267e-02,  1.6673e-02,  1.6618e-02, -2.0787e-02, -1.4645e-02,
        -2.1516e-02, -1.7984e-02, -2.2015e-02,  2.5236e-02, -2.4326e-02,
         2.6336e-02, -2.3158e-02,  2.3887e-02, -1.2426e-02,  2.3990e-02,
         2.2709e-02, -2.0308e-02, -1.4653e-02, -1.8931e-02,  1.1654e-02,
         2.4246e-02,  2.4549e-02, -2.2690e-02,  1.0835e-02,  1.8341e-02,
        -2.0419e-02,  2.4318e-02,  2.2160e-02, -1.3084e-02, -1.2073e-02,
         9.6133e-03, -2.0718e-02,  2.4417e-02, -2.1952e-02,  1.8002e-02,
         2.1769e-02, -2.0691e-02,  1.5309e-02,  2.5337e-02, -2.1721e-02,
        -2.2918e-02, -1.5693e-02,  2.4848e-02, -2.1729e-02,  1.9962e-02,
         2.2638e-02, -6.0540e-03,  2.3743e-02,  2.0892e-02, -6.4138e-03,
         1.0582e-02,  8.5944e-04,  2.3445e-02,  3.2301e-02,  1.0606e-03,
        -8.1231e-03, -1.9174e-02, -2.2652e-02,  2.6560e-02, -8.1126e-03,
         2.0796e-02,  2.4051e-02,  1.7607e-02, -2.0174e-02,  1.3158e-02,
         1.5823e-02, -1.1306e-02, -1.9983e-02,  2.6930e-02, -2.1990e-02,
         1.3373e-02, -1.6228e-03,  2.7338e-02,  1.9857e-02,  1.5952e-02,
         1.2897e-02,  1.6881e-02,  8.0415e-03,  1.5625e-02,  1.8889e-02,
         1.4783e-02,  2.4191e-02,  2.8677e-02,  2.1074e-02,  2.7485e-02,
        -1.8153e-02, -1.6776e-02, -1.5608e-02,  2.2292e-02,  2.2359e-02,
         3.8857e-03, -1.2016e-02, -1.5811e-02,  2.7464e-02, -1.8826e-02,
        -2.5939e-03, -2.1906e-02, -2.3689e-03, -1.2750e-02, -1.6443e-02,
        -2.1811e-02,  2.7590e-02, -5.7986e-03, -5.8457e-03, -2.2773e-02,
         4.2425e-03,  1.4847e-02, -1.4765e-02, -2.8597e-03,  2.2895e-02,
         2.2385e-02,  2.2370e-02, -1.1036e-02,  3.1943e-03, -2.8458e-03,
         1.1802e-02, -6.7263e-03, -1.4493e-02,  9.5465e-03,  1.2976e-02,
        -1.7081e-02,  6.3282e-03,  1.5412e-04, -1.7777e-02,  1.2910e-02,
        -1.1324e-02,  1.5440e-02,  1.4281e-02, -5.8895e-03,  5.1571e-03,
        -3.1461e-05,  8.7496e-03,  9.1427e-03, -1.6197e-02,  7.7452e-03,
         1.8281e-02, -2.7361e-03,  1.2039e-02,  1.9037e-02,  5.2397e-03,
        -5.4347e-03,  9.1229e-03,  7.5434e-03,  1.0140e-02, -1.9436e-02,
         2.0711e-02,  1.2506e-02, -1.5435e-02,  1.2250e-02,  2.0619e-02,
        -2.0020e-02,  1.4357e-02, -6.3890e-03, -1.6417e-02,  2.2234e-02,
        -9.5116e-03,  7.3180e-03, -6.7540e-03,  5.5324e-03, -1.5327e-02,
        -1.6302e-02,  9.5924e-03,  4.3639e-03,  1.1503e-02, -1.1288e-02,
         1.5467e-02,  1.5280e-02, -1.2891e-02,  7.9158e-03, -1.7345e-02,
         1.5104e-02, -1.2632e-02,  1.7971e-02, -1.1628e-02,  1.3776e-02,
        -4.2801e-03, -1.4421e-02,  6.6580e-03, -1.3090e-02, -6.6774e-04,
         1.0567e-02, -1.7327e-02, -6.1547e-03, -8.4381e-03,  1.3003e-02,
         8.5671e-03, -1.8560e-03,  1.1138e-02,  1.0064e-02,  5.4090e-03,
         3.9202e-03, -7.3010e-04,  6.6680e-04,  2.0373e-02,  1.2038e-02,
         4.0739e-03,  3.1256e-03, -1.6525e-02, -1.2469e-02, -7.1605e-03,
        -8.9111e-03, -1.8243e-02,  2.6658e-03, -6.5497e-03,  9.1813e-03,
        -4.8367e-03, -1.9649e-02,  2.1975e-02, -7.7905e-03,  1.2019e-02,
        -6.9024e-03, -1.6329e-02, -4.4289e-03,  5.1760e-04,  1.8794e-02,
         1.4931e-02,  1.6887e-02, -1.0695e-02, -1.4008e-02,  2.6057e-03,
        -3.1800e-03, -1.1794e-02,  1.4761e-02,  1.2952e-02,  1.5151e-02,
         9.0955e-03,  1.5248e-03,  8.0676e-03,  5.2713e-03, -2.0313e-02,
         1.9368e-02,  1.8591e-03,  1.7322e-03, -8.9439e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.8912, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:31,593 :: INFO :: Epoch 15: loss tensor(343.4918, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-1.8846e-04,  3.9155e-03, -9.3151e-03,  7.0725e-03, -7.3712e-03,
        -2.0020e-02, -4.7072e-03,  1.4815e-02, -5.8850e-03,  6.7040e-03,
        -1.9173e-02, -2.7121e-03,  1.4106e-03,  3.8528e-03,  6.8234e-03,
         1.2827e-02,  3.7721e-03,  5.7602e-03,  6.2310e-03,  9.4216e-03,
         1.1878e-03,  7.2917e-03,  8.3277e-03, -1.6421e-02, -4.8637e-03,
         3.3438e-04,  7.3689e-03,  4.3916e-03,  3.0774e-03,  2.5798e-03,
         6.4145e-04,  1.5862e-02, -1.3117e-03, -1.1838e-03, -1.5775e-02,
        -2.1419e-02,  7.9426e-03,  3.7272e-03, -3.4905e-03,  4.2002e-03,
        -1.4278e-02, -8.5640e-03,  1.0239e-02,  5.2906e-03, -4.3308e-03,
         8.8929e-03,  6.1488e-03, -1.2201e-02, -2.5531e-03,  5.2111e-03,
        -8.4578e-03,  9.0189e-03,  1.0189e-02,  2.7265e-03, -1.1141e-02,
         2.3487e-02,  2.5068e-03,  1.2361e-02, -8.1357e-03, -2.4068e-02,
         8.9119e-03,  5.6925e-03, -1.5493e-02, -6.3166e-03,  8.4347e-04,
         3.4721e-04,  4.9000e-03,  6.1004e-03,  1.7156e-02, -1.6228e-02,
         9.9272e-03, -1.4911e-02,  2.3546e-02,  4.0889e-03,  9.8642e-03,
        -1.1689e-02,  1.6768e-02, -1.2387e-02,  7.8385e-03,  1.9795e-02,
        -1.2818e-03, -2.3484e-02,  1.5755e-02,  9.2192e-03, -1.9232e-02,
        -3.8245e-03,  1.3883e-02, -9.3792e-03, -3.5301e-03, -1.7176e-02,
        -3.2025e-03,  1.8883e-02,  3.5434e-03,  1.5432e-02, -1.1627e-02,
         1.5382e-03,  7.1014e-03, -6.3864e-03, -1.6319e-02, -4.8191e-03,
        -5.3898e-03,  8.6975e-03,  5.1027e-03, -1.7461e-02, -1.5425e-02,
        -2.3266e-02,  1.0958e-02, -2.6813e-03,  2.4408e-03,  1.3031e-02,
        -8.1225e-03,  1.4328e-02, -1.4603e-02,  6.8071e-03, -1.9999e-02,
        -2.0168e-02, -2.1451e-03, -1.4268e-02,  4.0144e-03, -5.8956e-03,
        -5.8085e-03, -2.5763e-03,  2.0216e-03, -4.8801e-03,  8.4687e-03,
        -6.2397e-03, -5.6786e-03,  7.2360e-03,  3.7872e-02,  3.1854e-02,
         1.1527e-02,  4.6096e-02,  3.5601e-02,  4.3494e-02,  2.1559e-02,
        -1.1007e-02, -2.7311e-02,  3.9543e-02,  2.7450e-02, -1.9875e-02,
         3.4891e-02, -2.1466e-02,  2.8345e-02,  1.4390e-02,  1.8841e-02,
         3.6832e-02,  3.3813e-02, -1.6388e-02,  8.4889e-03, -6.6316e-03,
        -2.5466e-02,  3.5433e-02,  3.3156e-02, -2.7292e-02, -8.9958e-03,
        -2.7104e-02, -1.9943e-02, -2.9754e-02,  3.4940e-02, -3.3188e-02,
         4.1481e-02, -3.1469e-02,  3.6422e-02, -5.2678e-03,  3.5592e-02,
         2.6604e-02, -2.5318e-02, -1.1147e-02, -2.0443e-02,  2.8091e-02,
         4.3816e-02,  3.7663e-02, -3.0733e-02,  2.6557e-02,  3.6630e-02,
        -2.5843e-02,  3.3236e-02,  3.7376e-02, -6.0992e-03, -6.9503e-03,
         2.5044e-02, -2.5424e-02,  3.8332e-02, -2.9549e-02,  3.2590e-02,
         3.8336e-02, -2.6960e-02,  3.4591e-02,  4.3055e-02, -2.8845e-02,
        -2.9072e-02, -1.3416e-02,  3.7490e-02, -2.8781e-02,  3.8610e-02,
         3.7712e-02,  8.4328e-03,  2.9867e-02,  3.8664e-02,  6.7287e-03,
         2.6499e-02,  1.8288e-02,  3.8921e-02,  5.1280e-02,  1.6263e-02,
         5.1873e-03, -2.0183e-02, -3.0653e-02,  3.8659e-02,  1.2569e-03,
         3.8580e-02,  3.7014e-02,  1.9629e-02, -2.4668e-02,  3.1868e-02,
         3.3413e-02, -4.4607e-03, -2.3647e-02,  3.9679e-02, -2.9526e-02,
         3.2530e-02,  1.6609e-02,  4.3228e-02,  3.8166e-02,  3.0176e-02,
         3.1085e-02,  3.2194e-02,  2.4485e-02,  3.0871e-02,  3.7013e-02,
         3.0652e-02,  3.6330e-02,  4.3051e-02,  3.2581e-02,  4.0635e-02,
        -1.6376e-02, -1.5481e-02, -1.3653e-02,  3.1522e-02,  3.7109e-02,
         1.9637e-02, -5.6134e-03, -1.3504e-02,  4.0527e-02, -2.1965e-02,
         9.5763e-03, -2.9538e-02,  1.3223e-02, -8.4730e-03, -1.6085e-02,
        -2.7968e-02,  4.1272e-02,  6.0844e-03,  8.5434e-03, -3.0993e-02,
         1.9836e-02,  3.4583e-02, -8.4279e-03,  1.4383e-02,  4.2158e-02,
         4.1247e-02,  4.0667e-02,  4.4347e-03,  2.2353e-02,  1.1906e-02,
         3.0926e-02,  8.4030e-03, -2.6781e-03,  2.8862e-02,  3.2208e-02,
        -1.0656e-02,  2.5856e-02,  1.8509e-02, -1.3274e-02,  3.2380e-02,
         4.0845e-03,  3.4089e-02,  3.3719e-02,  1.1303e-02,  2.4686e-02,
         1.8935e-02,  2.7713e-02,  2.8694e-02, -1.0524e-02,  2.5811e-02,
         3.7525e-02,  1.5628e-02,  3.1478e-02,  3.7780e-02,  2.4402e-02,
         9.7429e-03,  2.8638e-02,  2.6574e-02,  2.9380e-02, -1.4962e-02,
         3.9887e-02,  3.2220e-02, -4.4252e-03,  3.1351e-02,  3.9765e-02,
        -1.8194e-02,  3.2886e-02,  8.8528e-03, -9.5491e-03,  4.0648e-02,
         3.8260e-03,  2.6404e-02,  9.6566e-03,  2.4866e-02, -5.6089e-03,
        -1.0162e-02,  2.8873e-02,  2.3183e-02,  3.1058e-02,  3.8971e-03,
         3.4778e-02,  3.3991e-02, -2.0194e-03,  2.7305e-02, -1.1676e-02,
         3.4429e-02, -1.1866e-03,  3.7303e-02,  3.9764e-04,  3.3208e-02,
         1.2740e-02, -3.6267e-03,  2.5703e-02,  3.5912e-05,  1.7925e-02,
         3.0065e-02, -9.9662e-03,  1.1289e-02,  8.5614e-03,  3.1932e-02,
         2.7849e-02,  1.6697e-02,  3.0572e-02,  2.9302e-02,  2.4604e-02,
         2.3234e-02,  1.8091e-02,  1.9117e-02,  3.8918e-02,  3.1389e-02,
         2.2541e-02,  2.1829e-02, -7.2851e-03, -2.1316e-03,  1.0112e-02,
         7.6836e-03, -1.5858e-02,  2.1849e-02,  1.0357e-02,  2.8390e-02,
         1.3092e-02, -1.9016e-02,  4.0976e-02,  7.9167e-03,  3.1491e-02,
         9.1568e-03, -1.0383e-02,  1.1355e-02,  1.9254e-02,  3.8303e-02,
         3.4385e-02,  3.6004e-02,  3.2779e-03, -2.1499e-03,  2.1934e-02,
         1.5274e-02,  6.1875e-04,  3.4328e-02,  3.2248e-02,  3.4577e-02,
         2.7423e-02,  2.0258e-02,  2.6394e-02,  2.4457e-02, -1.9580e-02,
         3.8686e-02,  2.0937e-02,  2.0485e-02,  6.3826e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(5.1756, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:33,514 :: INFO :: Epoch 20: loss tensor(333.4193, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0049,  0.0076, -0.0084,  0.0120, -0.0034, -0.0217, -0.0009,  0.0195,
        -0.0036,  0.0095, -0.0189, -0.0028,  0.0078,  0.0056,  0.0086,  0.0154,
         0.0058,  0.0095,  0.0058,  0.0153,  0.0043,  0.0092,  0.0119, -0.0139,
        -0.0054,  0.0045,  0.0092,  0.0085,  0.0106,  0.0043,  0.0050,  0.0191,
        -0.0006,  0.0010, -0.0185, -0.0256,  0.0086,  0.0091, -0.0006,  0.0034,
        -0.0149, -0.0062,  0.0117,  0.0079, -0.0034,  0.0098,  0.0082, -0.0135,
        -0.0018,  0.0066, -0.0075,  0.0117,  0.0107,  0.0052, -0.0083,  0.0320,
         0.0045,  0.0192, -0.0098, -0.0276,  0.0124,  0.0071, -0.0171, -0.0053,
         0.0021,  0.0019,  0.0063,  0.0055,  0.0208, -0.0183,  0.0113, -0.0152,
         0.0253,  0.0094,  0.0154, -0.0137,  0.0171, -0.0135,  0.0105,  0.0201,
         0.0061, -0.0247,  0.0218,  0.0168, -0.0226, -0.0007,  0.0156, -0.0108,
        -0.0019, -0.0190,  0.0006,  0.0272,  0.0080,  0.0207, -0.0142,  0.0071,
         0.0114, -0.0077, -0.0166, -0.0042, -0.0083,  0.0139,  0.0105, -0.0174,
        -0.0127, -0.0280,  0.0116, -0.0034,  0.0051,  0.0180, -0.0062,  0.0179,
        -0.0127,  0.0078, -0.0220, -0.0237, -0.0018, -0.0131,  0.0041, -0.0033,
        -0.0057,  0.0012,  0.0019, -0.0075,  0.0135, -0.0080, -0.0037,  0.0134,
         0.0540,  0.0460,  0.0271,  0.0617,  0.0570,  0.0626,  0.0370, -0.0023,
        -0.0305,  0.0513,  0.0446, -0.0154,  0.0529, -0.0204,  0.0472,  0.0333,
         0.0369,  0.0504,  0.0498, -0.0085,  0.0257,  0.0100, -0.0279,  0.0554,
         0.0500, -0.0327,  0.0026, -0.0287, -0.0183, -0.0360,  0.0434, -0.0401,
         0.0560, -0.0382,  0.0493,  0.0075,  0.0464,  0.0298, -0.0277, -0.0029,
        -0.0162,  0.0445,  0.0642,  0.0502, -0.0373,  0.0427,  0.0555, -0.0287,
         0.0405,  0.0524,  0.0064,  0.0014,  0.0407, -0.0264,  0.0515, -0.0357,
         0.0467,  0.0555, -0.0320,  0.0547,  0.0642, -0.0347, -0.0314, -0.0059,
         0.0493, -0.0340,  0.0571,  0.0531,  0.0272,  0.0330,  0.0565,  0.0239,
         0.0430,  0.0381,  0.0540,  0.0704,  0.0334,  0.0234, -0.0156, -0.0375,
         0.0492,  0.0139,  0.0574,  0.0493,  0.0273, -0.0263,  0.0509,  0.0521,
         0.0065, -0.0238,  0.0525, -0.0356,  0.0522,  0.0363,  0.0606,  0.0567,
         0.0503,  0.0503,  0.0471,  0.0423,  0.0465,  0.0559,  0.0465,  0.0469,
         0.0562,  0.0506,  0.0533, -0.0072, -0.0086, -0.0076,  0.0364,  0.0521,
         0.0362,  0.0060, -0.0054,  0.0533, -0.0217,  0.0238, -0.0355,  0.0312,
         0.0003, -0.0110, -0.0307,  0.0550,  0.0218,  0.0268, -0.0372,  0.0362,
         0.0555,  0.0040,  0.0340,  0.0616,  0.0606,  0.0599,  0.0240,  0.0427,
         0.0302,  0.0508,  0.0275,  0.0147,  0.0487,  0.0517,  0.0027,  0.0462,
         0.0392, -0.0020,  0.0523,  0.0230,  0.0536,  0.0538,  0.0308,  0.0451,
         0.0393,  0.0481,  0.0489,  0.0021,  0.0452,  0.0568,  0.0360,  0.0516,
         0.0563,  0.0447,  0.0284,  0.0488,  0.0472,  0.0488, -0.0030,  0.0592,
         0.0525,  0.0132,  0.0501,  0.0590, -0.0097,  0.0525,  0.0281,  0.0040,
         0.0597,  0.0218,  0.0460,  0.0289,  0.0447,  0.0119,  0.0031,  0.0493,
         0.0432,  0.0514,  0.0235,  0.0544,  0.0529,  0.0145,  0.0473,  0.0009,
         0.0541,  0.0166,  0.0570,  0.0181,  0.0534,  0.0320,  0.0135,  0.0459,
         0.0184,  0.0380,  0.0502,  0.0059,  0.0315,  0.0285,  0.0517,  0.0474,
         0.0366,  0.0506,  0.0498,  0.0442,  0.0435,  0.0383,  0.0396,  0.0579,
         0.0512,  0.0407,  0.0415,  0.0087,  0.0137,  0.0301,  0.0274, -0.0070,
         0.0421,  0.0290,  0.0483,  0.0331, -0.0124,  0.0604,  0.0270,  0.0516,
         0.0284,  0.0023,  0.0303,  0.0399,  0.0585,  0.0542,  0.0557,  0.0217,
         0.0146,  0.0424,  0.0358,  0.0185,  0.0542,  0.0517,  0.0552,  0.0469,
         0.0401,  0.0442,  0.0442, -0.0129,  0.0583,  0.0413,  0.0409,  0.0252],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.4178, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:35,655 :: INFO :: Epoch 25: loss tensor(330.2258, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0140,  0.0145, -0.0022,  0.0204,  0.0023, -0.0194,  0.0084,  0.0258,
         0.0018,  0.0143, -0.0135, -0.0008,  0.0146,  0.0112,  0.0132,  0.0197,
         0.0086,  0.0160,  0.0075,  0.0246,  0.0092,  0.0135,  0.0196, -0.0058,
        -0.0035,  0.0099,  0.0123,  0.0160,  0.0246,  0.0081,  0.0121,  0.0247,
         0.0053,  0.0068, -0.0168, -0.0265,  0.0118,  0.0149,  0.0033,  0.0061,
        -0.0126, -0.0016,  0.0172,  0.0115,  0.0004,  0.0125,  0.0119, -0.0103,
         0.0022,  0.0095, -0.0025,  0.0163,  0.0119,  0.0084,  0.0008,  0.0437,
         0.0097,  0.0301, -0.0049, -0.0265,  0.0167,  0.0101, -0.0154, -0.0011,
         0.0074,  0.0056,  0.0114,  0.0084,  0.0258, -0.0178,  0.0154, -0.0093,
         0.0279,  0.0157,  0.0236, -0.0127,  0.0206, -0.0100,  0.0159,  0.0224,
         0.0167, -0.0198,  0.0288,  0.0277, -0.0226,  0.0060,  0.0197, -0.0070,
         0.0017, -0.0188,  0.0063,  0.0379,  0.0134,  0.0283, -0.0101,  0.0170,
         0.0161, -0.0055, -0.0135, -0.0006, -0.0072,  0.0196,  0.0187, -0.0148,
        -0.0042, -0.0295,  0.0153, -0.0015,  0.0098,  0.0255,  0.0003,  0.0228,
        -0.0025,  0.0118, -0.0208, -0.0242,  0.0020, -0.0086,  0.0055,  0.0036,
        -0.0024,  0.0074,  0.0037, -0.0074,  0.0195, -0.0080,  0.0025,  0.0225,
         0.0698,  0.0595,  0.0426,  0.0757,  0.0779,  0.0820,  0.0519,  0.0088,
        -0.0315,  0.0597,  0.0612, -0.0074,  0.0706, -0.0167,  0.0659,  0.0534,
         0.0548,  0.0636,  0.0647,  0.0036,  0.0433,  0.0285, -0.0284,  0.0755,
         0.0662, -0.0374,  0.0161, -0.0275, -0.0144, -0.0413,  0.0519, -0.0454,
         0.0699, -0.0438,  0.0625,  0.0218,  0.0567,  0.0351, -0.0282,  0.0076,
        -0.0086,  0.0603,  0.0844,  0.0622, -0.0428,  0.0584,  0.0741, -0.0297,
         0.0473,  0.0669,  0.0206,  0.0111,  0.0559, -0.0248,  0.0635, -0.0407,
         0.0601,  0.0726, -0.0360,  0.0745,  0.0863, -0.0397, -0.0301,  0.0044,
         0.0605, -0.0381,  0.0737,  0.0683,  0.0467,  0.0352,  0.0738,  0.0418,
         0.0591,  0.0581,  0.0683,  0.0882,  0.0503,  0.0430, -0.0076, -0.0433,
         0.0584,  0.0276,  0.0761,  0.0610,  0.0403, -0.0262,  0.0693,  0.0709,
         0.0191, -0.0214,  0.0652, -0.0407,  0.0717,  0.0556,  0.0782,  0.0750,
         0.0717,  0.0696,  0.0611,  0.0606,  0.0617,  0.0745,  0.0616,  0.0555,
         0.0678,  0.0703,  0.0652,  0.0060,  0.0014,  0.0007,  0.0400,  0.0670,
         0.0523,  0.0193,  0.0058,  0.0657, -0.0193,  0.0384, -0.0404,  0.0489,
         0.0110, -0.0033, -0.0305,  0.0685,  0.0388,  0.0458, -0.0418,  0.0520,
         0.0766,  0.0195,  0.0543,  0.0803,  0.0796,  0.0797,  0.0440,  0.0627,
         0.0498,  0.0708,  0.0471,  0.0333,  0.0677,  0.0702,  0.0185,  0.0663,
         0.0600,  0.0126,  0.0715,  0.0423,  0.0729,  0.0736,  0.0500,  0.0649,
         0.0593,  0.0688,  0.0685,  0.0185,  0.0651,  0.0751,  0.0563,  0.0715,
         0.0732,  0.0649,  0.0476,  0.0686,  0.0685,  0.0673,  0.0116,  0.0777,
         0.0723,  0.0326,  0.0674,  0.0774,  0.0028,  0.0724,  0.0485,  0.0207,
         0.0790,  0.0410,  0.0645,  0.0483,  0.0639,  0.0315,  0.0193,  0.0699,
         0.0634,  0.0714,  0.0439,  0.0733,  0.0713,  0.0328,  0.0670,  0.0167,
         0.0730,  0.0364,  0.0762,  0.0373,  0.0736,  0.0509,  0.0326,  0.0661,
         0.0378,  0.0578,  0.0699,  0.0244,  0.0517,  0.0485,  0.0716,  0.0657,
         0.0553,  0.0703,  0.0700,  0.0625,  0.0633,  0.0580,  0.0601,  0.0770,
         0.0706,  0.0567,  0.0602,  0.0267,  0.0312,  0.0502,  0.0471,  0.0055,
         0.0620,  0.0463,  0.0684,  0.0528, -0.0020,  0.0795,  0.0459,  0.0712,
         0.0479,  0.0182,  0.0500,  0.0610,  0.0783,  0.0734,  0.0751,  0.0408,
         0.0319,  0.0625,  0.0564,  0.0381,  0.0732,  0.0703,  0.0762,  0.0667,
         0.0594,  0.0598,  0.0631, -0.0022,  0.0771,  0.0616,  0.0614,  0.0439],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.6196, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:37,498 :: INFO :: Epoch 30: loss tensor(329.2366, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 2.5405e-02,  2.3563e-02,  7.0788e-03,  3.0519e-02,  9.0039e-03,
        -1.3711e-02,  2.0736e-02,  3.3430e-02,  9.7672e-03,  2.1165e-02,
        -4.3821e-03,  3.2629e-03,  2.1916e-02,  1.9478e-02,  1.9684e-02,
         2.5330e-02,  1.2728e-02,  2.4343e-02,  1.1074e-02,  3.5684e-02,
         1.5173e-02,  1.9786e-02,  2.9857e-02,  5.3912e-03,  1.1426e-03,
         1.5790e-02,  1.6787e-02,  2.5484e-02,  4.1264e-02,  1.3514e-02,
         2.0215e-02,  3.1975e-02,  1.4581e-02,  1.5422e-02, -1.2000e-02,
        -2.4201e-02,  1.7219e-02,  2.1257e-02,  8.3309e-03,  1.2127e-02,
        -8.3692e-03,  5.2504e-03,  2.5915e-02,  1.6589e-02,  5.7406e-03,
         1.7316e-02,  1.7352e-02, -4.1644e-03,  9.0891e-03,  1.4152e-02,
         4.9765e-03,  2.2576e-02,  1.4568e-02,  1.2548e-02,  1.3014e-02,
         5.6812e-02,  1.7568e-02,  4.3415e-02,  4.6650e-03, -2.1503e-02,
         2.2007e-02,  1.4991e-02, -1.0480e-02,  5.0454e-03,  1.5436e-02,
         1.1701e-02,  1.9642e-02,  1.4713e-02,  3.2213e-02, -1.4866e-02,
         2.1678e-02,  2.5696e-04,  3.2007e-02,  2.3480e-02,  3.3755e-02,
        -9.1441e-03,  2.6842e-02, -3.0177e-03,  2.3629e-02,  2.7244e-02,
         2.8860e-02, -1.0799e-02,  3.6590e-02,  4.0517e-02, -1.9457e-02,
         1.5449e-02,  2.5914e-02,  5.9464e-04,  6.6892e-03, -1.6380e-02,
         1.2972e-02,  4.9610e-02,  1.9454e-02,  3.7447e-02, -1.1714e-03,
         2.8943e-02,  2.1308e-02, -3.0709e-05, -7.9352e-03,  4.9792e-03,
        -2.2982e-03,  2.5894e-02,  2.8384e-02, -1.0050e-02,  7.5542e-03,
        -2.8126e-02,  2.1385e-02,  2.6062e-03,  1.5986e-02,  3.4562e-02,
         9.8538e-03,  2.9300e-02,  1.1424e-02,  1.8055e-02, -1.6877e-02,
        -2.2132e-02,  8.5267e-03, -2.0299e-03,  8.0828e-03,  1.2670e-02,
         3.0999e-03,  1.5068e-02,  8.0655e-03, -4.5474e-03,  2.6658e-02,
        -6.1526e-03,  1.1688e-02,  3.2929e-02,  8.5189e-02,  7.2531e-02,
         5.8004e-02,  8.7922e-02,  9.6037e-02,  1.0066e-01,  6.6152e-02,
         2.1082e-02, -3.0603e-02,  6.6464e-02,  7.6862e-02,  2.7436e-03,
         8.7729e-02, -1.1171e-02,  8.4287e-02,  7.4099e-02,  7.2327e-02,
         7.6472e-02,  7.8145e-02,  1.8202e-02,  6.0770e-02,  4.6820e-02,
        -2.7394e-02,  9.5181e-02,  8.1831e-02, -4.1713e-02,  2.9713e-02,
        -2.3689e-02, -8.7850e-03, -4.5870e-02,  6.0028e-02, -4.8762e-02,
         8.2779e-02, -4.8037e-02,  7.5615e-02,  3.6579e-02,  6.6598e-02,
         4.1510e-02, -2.7260e-02,  1.9231e-02,  1.4410e-03,  7.5347e-02,
         1.0412e-01,  7.3721e-02, -4.7369e-02,  7.3822e-02,  9.2206e-02,
        -2.9069e-02,  5.2691e-02,  8.0887e-02,  3.4951e-02,  2.1777e-02,
         7.0706e-02, -2.1193e-02,  7.4242e-02, -4.4913e-02,  7.2974e-02,
         8.9753e-02, -3.9234e-02,  9.3644e-02,  1.0799e-01, -4.4054e-02,
        -2.5490e-02,  1.6184e-02,  7.1251e-02, -4.1339e-02,  8.8031e-02,
         8.3482e-02,  6.6051e-02,  3.7248e-02,  9.0329e-02,  5.9867e-02,
         7.5019e-02,  7.7950e-02,  8.1888e-02,  1.0400e-01,  6.6881e-02,
         6.2885e-02,  2.3811e-03, -4.8298e-02,  6.6294e-02,  4.1456e-02,
         9.4544e-02,  7.2202e-02,  5.3216e-02, -2.4701e-02,  8.6545e-02,
         8.9908e-02,  3.2586e-02, -1.7117e-02,  7.8089e-02, -4.5172e-02,
         9.0573e-02,  7.3706e-02,  9.5453e-02,  9.2885e-02,  9.1489e-02,
         8.8632e-02,  7.4467e-02,  7.8996e-02,  7.6126e-02,  9.2484e-02,
         7.6049e-02,  6.2959e-02,  7.8346e-02,  8.7696e-02,  7.6588e-02,
         2.1388e-02,  1.3210e-02,  1.0405e-02,  4.4609e-02,  8.1371e-02,
         6.7712e-02,  3.3785e-02,  1.9057e-02,  7.7809e-02, -1.4973e-02,
         5.2721e-02, -4.4431e-02,  6.6139e-02,  2.2613e-02,  6.2486e-03,
        -2.7516e-02,  8.2129e-02,  5.6141e-02,  6.4625e-02, -4.5268e-02,
         6.7385e-02,  9.7804e-02,  3.6446e-02,  7.5002e-02,  9.7722e-02,
         9.8280e-02,  9.9600e-02,  6.2980e-02,  8.1879e-02,  6.9941e-02,
         9.0965e-02,  6.6644e-02,  5.1497e-02,  8.5234e-02,  8.6903e-02,
         3.4366e-02,  8.5657e-02,  8.0639e-02,  2.8017e-02,  8.9455e-02,
         6.1063e-02,  9.2506e-02,  9.2695e-02,  6.7807e-02,  8.3474e-02,
         7.8099e-02,  8.9834e-02,  8.6857e-02,  3.6318e-02,  8.5517e-02,
         9.1544e-02,  7.5663e-02,  9.0760e-02,  8.8660e-02,  8.4665e-02,
         6.6486e-02,  8.7692e-02,  9.0049e-02,  8.4295e-02,  2.5920e-02,
         9.5096e-02,  9.0913e-02,  5.1747e-02,  8.2612e-02,  9.4476e-02,
         1.7604e-02,  9.2634e-02,  6.9220e-02,  3.8850e-02,  9.8464e-02,
         6.0927e-02,  8.0880e-02,  6.7160e-02,  8.1454e-02,  5.1329e-02,
         3.6485e-02,  9.0643e-02,  8.3298e-02,  9.0793e-02,  6.3737e-02,
         9.0773e-02,  8.9284e-02,  5.2040e-02,  8.5540e-02,  3.3858e-02,
         9.0603e-02,  5.6822e-02,  9.4097e-02,  5.6233e-02,  9.3598e-02,
         6.8703e-02,  5.2564e-02,  8.6008e-02,  5.6136e-02,  7.6402e-02,
         8.8534e-02,  4.2307e-02,  7.1290e-02,  6.7391e-02,  9.1242e-02,
         8.1620e-02,  7.1511e-02,  8.9081e-02,  9.0044e-02,  7.8642e-02,
         8.1854e-02,  7.6632e-02,  7.9671e-02,  9.5847e-02,  8.9226e-02,
         6.9655e-02,  7.6989e-02,  4.5286e-02,  4.8944e-02,  6.9328e-02,
         6.5166e-02,  1.9036e-02,  8.1017e-02,  6.1518e-02,  8.7991e-02,
         7.1102e-02,  1.0993e-02,  9.8043e-02,  6.3184e-02,  8.9808e-02,
         6.7076e-02,  3.6034e-02,  6.9906e-02,  8.2142e-02,  9.7726e-02,
         9.0951e-02,  9.4185e-02,  5.9995e-02,  4.7764e-02,  8.1809e-02,
         7.5856e-02,  5.8592e-02,  9.0586e-02,  8.7901e-02,  9.7230e-02,
         8.6670e-02,  7.6666e-02,  7.1177e-02,  8.0301e-02,  1.1104e-02,
         9.4860e-02,  8.1058e-02,  8.1455e-02,  6.1212e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(8.7739, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:39,718 :: INFO :: Epoch 35: loss tensor(326.1851, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0369,  0.0326,  0.0162,  0.0407,  0.0153, -0.0061,  0.0329,  0.0409,
         0.0192,  0.0278,  0.0063,  0.0075,  0.0292,  0.0277,  0.0262,  0.0313,
         0.0172,  0.0329,  0.0149,  0.0465,  0.0208,  0.0266,  0.0403,  0.0172,
         0.0063,  0.0215,  0.0216,  0.0342,  0.0574,  0.0192,  0.0278,  0.0389,
         0.0247,  0.0241, -0.0061, -0.0206,  0.0231,  0.0269,  0.0132,  0.0198,
        -0.0032,  0.0135,  0.0356,  0.0215,  0.0106,  0.0231,  0.0229,  0.0037,
         0.0172,  0.0190,  0.0120,  0.0291,  0.0180,  0.0168,  0.0256,  0.0692,
         0.0273,  0.0558,  0.0151, -0.0150,  0.0273,  0.0202, -0.0035,  0.0113,
         0.0235,  0.0177,  0.0291,  0.0219,  0.0390, -0.0111,  0.0279,  0.0105,
         0.0361,  0.0317,  0.0436, -0.0045,  0.0342,  0.0052,  0.0317,  0.0326,
         0.0403,  0.0003,  0.0439,  0.0533, -0.0151,  0.0251,  0.0321,  0.0089,
         0.0121, -0.0130,  0.0195,  0.0607,  0.0254,  0.0466,  0.0092,  0.0406,
         0.0262,  0.0059, -0.0024,  0.0106,  0.0041,  0.0319,  0.0378, -0.0040,
         0.0185, -0.0248,  0.0281,  0.0075,  0.0225,  0.0429,  0.0199,  0.0362,
         0.0251,  0.0250, -0.0121, -0.0200,  0.0149,  0.0058,  0.0109,  0.0212,
         0.0089,  0.0226,  0.0135, -0.0008,  0.0337, -0.0033,  0.0213,  0.0428,
         0.1000,  0.0846,  0.0725,  0.0982,  0.1106,  0.1183,  0.0793,  0.0332,
        -0.0284,  0.0695,  0.0916,  0.0133,  0.1042, -0.0049,  0.1018,  0.0950,
         0.0889,  0.0887,  0.0896,  0.0337,  0.0767,  0.0639, -0.0255,  0.1144,
         0.0964, -0.0457,  0.0421, -0.0185, -0.0025, -0.0500,  0.0672, -0.0511,
         0.0947, -0.0516,  0.0883,  0.0498,  0.0759,  0.0476, -0.0255,  0.0308,
         0.0124,  0.0893,  0.1229,  0.0844, -0.0514,  0.0887,  0.1095, -0.0274,
         0.0576,  0.0942,  0.0479,  0.0320,  0.0847, -0.0165,  0.0837, -0.0486,
         0.0850,  0.1066, -0.0420,  0.1116,  0.1287, -0.0480, -0.0184,  0.0279,
         0.0812, -0.0439,  0.0998,  0.0982,  0.0851,  0.0382,  0.1056,  0.0774,
         0.0904,  0.0974,  0.0943,  0.1173,  0.0822,  0.0825,  0.0123, -0.0528,
         0.0729,  0.0547,  0.1123,  0.0827,  0.0629, -0.0224,  0.1025,  0.1087,
         0.0458, -0.0119,  0.0905, -0.0491,  0.1089,  0.0901,  0.1122,  0.1103,
         0.1083,  0.1073,  0.0868,  0.0973,  0.0893,  0.1096,  0.0891,  0.0685,
         0.0875,  0.1005,  0.0872,  0.0374,  0.0252,  0.0205,  0.0460,  0.0949,
         0.0821,  0.0481,  0.0331,  0.0892, -0.0097,  0.0664, -0.0479,  0.0817,
         0.0339,  0.0163, -0.0222,  0.0957,  0.0735,  0.0828, -0.0479,  0.0818,
         0.1189,  0.0537,  0.0958,  0.1139,  0.1164,  0.1192,  0.0806,  0.1000,
         0.0901,  0.1109,  0.0859,  0.0690,  0.1013,  0.1016,  0.0496,  0.1041,
         0.1008,  0.0435,  0.1060,  0.0787,  0.1118,  0.1110,  0.0841,  0.1008,
         0.0956,  0.1110,  0.1040,  0.0549,  0.1061,  0.1064,  0.0940,  0.1093,
         0.1023,  0.1036,  0.0853,  0.1057,  0.1115,  0.0994,  0.0394,  0.1115,
         0.1084,  0.0707,  0.0948,  0.1101,  0.0336,  0.1129,  0.0900,  0.0578,
         0.1179,  0.0810,  0.0947,  0.0856,  0.0974,  0.0710,  0.0541,  0.1111,
         0.1028,  0.1099,  0.0826,  0.1066,  0.1066,  0.0717,  0.1029,  0.0516,
         0.1068,  0.0774,  0.1106,  0.0748,  0.1130,  0.0852,  0.0727,  0.1053,
         0.0732,  0.0939,  0.1061,  0.0593,  0.0898,  0.0849,  0.1106,  0.0953,
         0.0852,  0.1068,  0.1096,  0.0927,  0.0992,  0.0939,  0.0984,  0.1144,
         0.1069,  0.0801,  0.0918,  0.0636,  0.0661,  0.0873,  0.0818,  0.0330,
         0.0991,  0.0753,  0.1069,  0.0879,  0.0261,  0.1158,  0.0788,  0.1075,
         0.0858,  0.0547,  0.0899,  0.1031,  0.1166,  0.1071,  0.1125,  0.0790,
         0.0618,  0.1000,  0.0943,  0.0791,  0.1066,  0.1040,  0.1181,  0.1064,
         0.0922,  0.0791,  0.0957,  0.0262,  0.1112,  0.0995,  0.1009,  0.0772],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.8694, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:41,967 :: INFO :: Epoch 40: loss tensor(320.3560, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 4.7258e-02,  4.1120e-02,  2.5329e-02,  5.0574e-02,  2.2088e-02,
         3.9123e-03,  4.4739e-02,  4.8161e-02,  2.9368e-02,  3.4555e-02,
         1.7399e-02,  1.2403e-02,  3.6697e-02,  3.6214e-02,  3.2645e-02,
         3.6928e-02,  2.2284e-02,  4.0989e-02,  1.9256e-02,  5.7206e-02,
         2.6903e-02,  3.3538e-02,  5.0404e-02,  2.8454e-02,  1.1706e-02,
         2.7165e-02,  2.7124e-02,  4.2237e-02,  7.2621e-02,  2.4914e-02,
         3.5068e-02,  4.5285e-02,  3.4887e-02,  3.2251e-02,  6.1154e-04,
        -1.6188e-02,  2.8765e-02,  3.2694e-02,  1.8377e-02,  2.8149e-02,
         2.7299e-03,  2.2521e-02,  4.5997e-02,  2.6071e-02,  1.5595e-02,
         2.9775e-02,  2.9080e-02,  1.2299e-02,  2.6374e-02,  2.4228e-02,
         1.8445e-02,  3.5410e-02,  2.2523e-02,  2.1503e-02,  3.7659e-02,
         8.0532e-02,  3.8285e-02,  6.8080e-02,  2.5410e-02, -7.7456e-03,
         3.2727e-02,  2.6236e-02,  4.4903e-03,  1.7431e-02,  3.1460e-02,
         2.3883e-02,  3.9316e-02,  2.9222e-02,  4.5596e-02, -6.3878e-03,
         3.4202e-02,  2.0584e-02,  4.0390e-02,  4.0885e-02,  5.2276e-02,
         9.6497e-05,  4.2261e-02,  1.3490e-02,  3.9213e-02,  3.8468e-02,
         5.0497e-02,  1.2802e-02,  5.1355e-02,  6.5572e-02, -9.8646e-03,
         3.3952e-02,  3.8145e-02,  1.7535e-02,  1.7904e-02, -8.7649e-03,
         2.5258e-02,  7.1176e-02,  3.1171e-02,  5.5002e-02,  2.0084e-02,
         5.1589e-02,  3.0820e-02,  1.1928e-02,  3.9142e-03,  1.6142e-02,
         1.1716e-02,  3.7662e-02,  4.5906e-02,  3.4066e-03,  2.9080e-02,
        -1.9600e-02,  3.5016e-02,  1.2792e-02,  2.9232e-02,  5.0933e-02,
         3.1136e-02,  4.2817e-02,  3.7399e-02,  3.2458e-02, -6.9801e-03,
        -1.6799e-02,  2.1869e-02,  1.4777e-02,  1.5116e-02,  2.9315e-02,
         1.5406e-02,  2.9740e-02,  1.9703e-02,  4.2642e-03,  4.0543e-02,
         9.8623e-04,  3.0840e-02,  5.1850e-02,  1.1350e-01,  9.5166e-02,
         8.5492e-02,  1.0600e-01,  1.2086e-01,  1.3342e-01,  9.0629e-02,
         4.4178e-02, -2.5255e-02,  7.2158e-02,  1.0475e-01,  2.2752e-02,
         1.1934e-01,  1.2512e-03,  1.1822e-01,  1.1525e-01,  1.0428e-01,
         1.0007e-01,  9.8787e-02,  4.9547e-02,  9.0698e-02,  7.9254e-02,
        -2.3081e-02,  1.3265e-01,  1.0939e-01, -4.9625e-02,  5.2524e-02,
        -1.2760e-02,  3.3558e-03, -5.4053e-02,  7.3042e-02, -5.2331e-02,
         1.0480e-01, -5.4317e-02,  1.0029e-01,  6.1056e-02,  8.4209e-02,
         5.2673e-02, -2.3754e-02,  4.0923e-02,  2.2572e-02,  1.0164e-01,
         1.4065e-01,  9.3644e-02, -5.5284e-02,  1.0257e-01,  1.2580e-01,
        -2.4972e-02,  6.0235e-02,  1.0615e-01,  5.8499e-02,  4.0188e-02,
         9.7608e-02, -1.1168e-02,  9.0561e-02, -5.2206e-02,  9.5814e-02,
         1.2286e-01, -4.4825e-02,  1.2806e-01,  1.4805e-01, -5.1842e-02,
        -9.2811e-03,  3.8371e-02,  9.0221e-02, -4.6195e-02,  1.0806e-01,
         1.1240e-01,  1.0328e-01,  3.8719e-02,  1.1937e-01,  9.3886e-02,
         1.0487e-01,  1.1594e-01,  1.0443e-01,  1.2775e-01,  9.5274e-02,
         1.0156e-01,  2.0862e-02, -5.7023e-02,  7.7335e-02,  6.6777e-02,
         1.2899e-01,  9.2243e-02,  6.8735e-02, -1.9944e-02,  1.1642e-01,
         1.2706e-01,  5.7914e-02, -7.0582e-03,  1.0235e-01, -5.3022e-02,
         1.2622e-01,  1.0449e-01,  1.2790e-01,  1.2653e-01,  1.2198e-01,
         1.2492e-01,  9.7686e-02,  1.1517e-01,  1.0059e-01,  1.2552e-01,
         9.9994e-02,  7.1771e-02,  9.4472e-02,  1.0867e-01,  9.6721e-02,
         5.3089e-02,  3.6300e-02,  2.9525e-02,  4.5148e-02,  1.0728e-01,
         9.5024e-02,  6.1435e-02,  4.7232e-02,  9.9356e-02, -4.3075e-03,
         7.9238e-02, -5.1224e-02,  9.4587e-02,  4.3736e-02,  2.5727e-02,
        -1.4825e-02,  1.0895e-01,  9.0338e-02,  9.9905e-02, -4.9857e-02,
         9.4801e-02,  1.3964e-01,  7.1371e-02,  1.1646e-01,  1.2847e-01,
         1.3401e-01,  1.3866e-01,  9.6683e-02,  1.1660e-01,  1.1033e-01,
         1.3068e-01,  1.0447e-01,  8.5312e-02,  1.1540e-01,  1.1408e-01,
         6.3939e-02,  1.2119e-01,  1.2017e-01,  5.8804e-02,  1.2077e-01,
         9.5484e-02,  1.3088e-01,  1.2863e-01,  9.9051e-02,  1.1635e-01,
         1.1174e-01,  1.3215e-01,  1.1952e-01,  7.3718e-02,  1.2676e-01,
         1.1858e-01,  1.1119e-01,  1.2683e-01,  1.1401e-01,  1.2180e-01,
         1.0343e-01,  1.2258e-01,  1.3290e-01,  1.1267e-01,  5.1383e-02,
         1.2659e-01,  1.2421e-01,  8.8852e-02,  1.0319e-01,  1.2401e-01,
         5.0273e-02,  1.3312e-01,  1.1061e-01,  7.7043e-02,  1.3712e-01,
         1.0107e-01,  1.0570e-01,  1.0362e-01,  1.1145e-01,  8.9704e-02,
         7.2079e-02,  1.3145e-01,  1.2182e-01,  1.2827e-01,  1.0109e-01,
         1.2045e-01,  1.2333e-01,  9.1452e-02,  1.1878e-01,  6.9180e-02,
         1.2122e-01,  9.7827e-02,  1.2538e-01,  9.2495e-02,  1.3197e-01,
         1.0012e-01,  9.2918e-02,  1.2423e-01,  8.8772e-02,  1.0981e-01,
         1.2224e-01,  7.5292e-02,  1.0718e-01,  1.0083e-01,  1.2960e-01,
         1.0548e-01,  9.5758e-02,  1.2337e-01,  1.2876e-01,  1.0343e-01,
         1.1489e-01,  1.0941e-01,  1.1592e-01,  1.3256e-01,  1.2334e-01,
         8.8108e-02,  1.0484e-01,  8.1482e-02,  8.3022e-02,  1.0402e-01,
         9.6272e-02,  4.7704e-02,  1.1581e-01,  8.7069e-02,  1.2547e-01,
         1.0257e-01,  4.2400e-02,  1.3311e-01,  9.2654e-02,  1.2378e-01,
         1.0398e-01,  7.4004e-02,  1.0952e-01,  1.2407e-01,  1.3468e-01,
         1.2134e-01,  1.3013e-01,  9.7721e-02,  7.4059e-02,  1.1684e-01,
         1.1164e-01,  9.9603e-02,  1.2026e-01,  1.1863e-01,  1.3862e-01,
         1.2594e-01,  1.0556e-01,  8.2703e-02,  1.0875e-01,  4.2083e-02,
         1.2578e-01,  1.1675e-01,  1.1971e-01,  9.1472e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.8959, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:38:41,967 :: INFO :: ----- frontend -----
2023-04-11 14:38:41,967 :: INFO :: Environment 0
2023-04-11 14:38:56,859 :: INFO :: Epoch 5: loss tensor(871.9537, device='cuda:0'), U.norm 14.65007209777832, V.norm 18.348159790039062, MLP.norm 1.8738207817077637
2023-04-11 14:38:56,952 :: INFO :: Epoch 10: loss tensor(858.0969, device='cuda:0'), U.norm 12.047375679016113, V.norm 17.86322784423828, MLP.norm 2.6654584407806396
2023-04-11 14:38:57,061 :: INFO :: Epoch 15: loss tensor(837.8220, device='cuda:0'), U.norm 10.387129783630371, V.norm 17.64420509338379, MLP.norm 3.7001216411590576
2023-04-11 14:38:57,171 :: INFO :: Epoch 20: loss tensor(812.7770, device='cuda:0'), U.norm 9.197229385375977, V.norm 17.503385543823242, MLP.norm 4.786757946014404
2023-04-11 14:38:57,171 :: INFO :: Environment 1
2023-04-11 14:39:10,796 :: INFO :: Epoch 5: loss tensor(892.0544, device='cuda:0'), U.norm 14.649955749511719, V.norm 18.37648582458496, MLP.norm 1.8853497505187988
2023-04-11 14:39:10,890 :: INFO :: Epoch 10: loss tensor(877.4917, device='cuda:0'), U.norm 12.047818183898926, V.norm 17.900484085083008, MLP.norm 2.7056422233581543
2023-04-11 14:39:10,967 :: INFO :: Epoch 15: loss tensor(856.6741, device='cuda:0'), U.norm 10.388382911682129, V.norm 17.683725357055664, MLP.norm 3.7759764194488525
2023-04-11 14:39:11,062 :: INFO :: Epoch 20: loss tensor(831.0859, device='cuda:0'), U.norm 9.19904899597168, V.norm 17.54302215576172, MLP.norm 4.892043590545654
2023-04-11 14:39:11,062 :: INFO :: Environment 2
2023-04-11 14:39:24,717 :: INFO :: Epoch 5: loss tensor(808.9810, device='cuda:0'), U.norm 14.64728832244873, V.norm 18.234901428222656, MLP.norm 1.865416169166565
2023-04-11 14:39:24,796 :: INFO :: Epoch 10: loss tensor(796.8972, device='cuda:0'), U.norm 12.041679382324219, V.norm 17.705604553222656, MLP.norm 2.6223936080932617
2023-04-11 14:39:24,905 :: INFO :: Epoch 15: loss tensor(779.1282, device='cuda:0'), U.norm 10.377819061279297, V.norm 17.46865463256836, MLP.norm 3.618049383163452
2023-04-11 14:39:24,999 :: INFO :: Epoch 20: loss tensor(757.0125, device='cuda:0'), U.norm 9.183448791503906, V.norm 17.318092346191406, MLP.norm 4.669224739074707
2023-04-11 14:39:24,999 :: INFO :: Environment 3
2023-04-11 14:39:39,624 :: INFO :: Epoch 5: loss tensor(854.7393, device='cuda:0'), U.norm 14.647916793823242, V.norm 18.317607879638672, MLP.norm 1.8776838779449463
2023-04-11 14:39:39,718 :: INFO :: Epoch 10: loss tensor(841.0697, device='cuda:0'), U.norm 12.043427467346191, V.norm 17.82198143005371, MLP.norm 2.6753368377685547
2023-04-11 14:39:39,796 :: INFO :: Epoch 15: loss tensor(821.4424, device='cuda:0'), U.norm 10.381027221679688, V.norm 17.597923278808594, MLP.norm 3.7081551551818848
2023-04-11 14:39:39,890 :: INFO :: Epoch 20: loss tensor(797.9224, device='cuda:0'), U.norm 9.188231468200684, V.norm 17.45454216003418, MLP.norm 4.793680191040039
2023-04-11 14:39:39,890 :: INFO :: Environment 4
2023-04-11 14:39:53,796 :: INFO :: Epoch 5: loss tensor(808.0692, device='cuda:0'), U.norm 14.64615535736084, V.norm 18.2281551361084, MLP.norm 1.8581500053405762
2023-04-11 14:39:53,890 :: INFO :: Epoch 10: loss tensor(796.5109, device='cuda:0'), U.norm 12.03886890411377, V.norm 17.692949295043945, MLP.norm 2.6056885719299316
2023-04-11 14:39:53,983 :: INFO :: Epoch 15: loss tensor(779.1539, device='cuda:0'), U.norm 10.37314224243164, V.norm 17.451335906982422, MLP.norm 3.5874011516571045
2023-04-11 14:39:54,061 :: INFO :: Epoch 20: loss tensor(758.2098, device='cuda:0'), U.norm 9.176689147949219, V.norm 17.295629501342773, MLP.norm 4.62274694442749
2023-04-11 14:39:54,077 :: INFO :: Environment 5
2023-04-11 14:40:07,311 :: INFO :: Epoch 5: loss tensor(922.3793, device='cuda:0'), U.norm 14.648239135742188, V.norm 18.440532684326172, MLP.norm 1.9003721475601196
2023-04-11 14:40:07,436 :: INFO :: Epoch 10: loss tensor(904.9771, device='cuda:0'), U.norm 12.045121192932129, V.norm 17.98641586303711, MLP.norm 2.7632336616516113
2023-04-11 14:40:07,530 :: INFO :: Epoch 15: loss tensor(880.5332, device='cuda:0'), U.norm 10.384700775146484, V.norm 17.779932022094727, MLP.norm 3.8631250858306885
2023-04-11 14:40:07,655 :: INFO :: Epoch 20: loss tensor(850.8412, device='cuda:0'), U.norm 9.194611549377441, V.norm 17.646038055419922, MLP.norm 5.004215717315674
2023-04-11 14:40:07,655 :: INFO :: Environment 6
2023-04-11 14:40:21,733 :: INFO :: Epoch 5: loss tensor(887.7643, device='cuda:0'), U.norm 14.651345252990723, V.norm 18.382646560668945, MLP.norm 1.8801169395446777
2023-04-11 14:40:21,811 :: INFO :: Epoch 10: loss tensor(873.0331, device='cuda:0'), U.norm 12.05070686340332, V.norm 17.910247802734375, MLP.norm 2.6903843879699707
2023-04-11 14:40:21,890 :: INFO :: Epoch 15: loss tensor(851.6454, device='cuda:0'), U.norm 10.393327713012695, V.norm 17.698421478271484, MLP.norm 3.736555337905884
2023-04-11 14:40:21,983 :: INFO :: Epoch 20: loss tensor(825.2469, device='cuda:0'), U.norm 9.206603050231934, V.norm 17.56308937072754, MLP.norm 4.839842319488525
2023-04-11 14:40:21,983 :: INFO :: Environment 7
2023-04-11 14:40:35,547 :: INFO :: Epoch 5: loss tensor(824.7815, device='cuda:0'), U.norm 14.649168968200684, V.norm 18.260757446289062, MLP.norm 1.8575187921524048
2023-04-11 14:40:35,640 :: INFO :: Epoch 10: loss tensor(812.7148, device='cuda:0'), U.norm 12.045165061950684, V.norm 17.74173927307129, MLP.norm 2.6187593936920166
2023-04-11 14:40:35,717 :: INFO :: Epoch 15: loss tensor(795.2211, device='cuda:0'), U.norm 10.383101463317871, V.norm 17.507944107055664, MLP.norm 3.617598533630371
2023-04-11 14:40:35,811 :: INFO :: Epoch 20: loss tensor(774.4431, device='cuda:0'), U.norm 9.190771102905273, V.norm 17.35738754272461, MLP.norm 4.670670032501221
2023-04-11 14:40:35,811 :: INFO :: Environment 8
2023-04-11 14:40:49,265 :: INFO :: Epoch 5: loss tensor(800.9776, device='cuda:0'), U.norm 14.649097442626953, V.norm 18.210405349731445, MLP.norm 1.8461579084396362
2023-04-11 14:40:49,358 :: INFO :: Epoch 10: loss tensor(789.8535, device='cuda:0'), U.norm 12.044869422912598, V.norm 17.666568756103516, MLP.norm 2.583378314971924
2023-04-11 14:40:49,467 :: INFO :: Epoch 15: loss tensor(772.7527, device='cuda:0'), U.norm 10.382999420166016, V.norm 17.42137908935547, MLP.norm 3.5688788890838623
2023-04-11 14:40:49,577 :: INFO :: Epoch 20: loss tensor(752.1366, device='cuda:0'), U.norm 9.190940856933594, V.norm 17.2650146484375, MLP.norm 4.620466709136963
2023-04-11 14:40:49,577 :: INFO :: Environment 9
2023-04-11 14:41:01,655 :: INFO :: Epoch 5: loss tensor(830.2570, device='cuda:0'), U.norm 14.646376609802246, V.norm 18.276906967163086, MLP.norm 1.875023365020752
2023-04-11 14:41:01,733 :: INFO :: Epoch 10: loss tensor(818.0978, device='cuda:0'), U.norm 12.039774894714355, V.norm 17.763010025024414, MLP.norm 2.6412851810455322
2023-04-11 14:41:01,811 :: INFO :: Epoch 15: loss tensor(800.8668, device='cuda:0'), U.norm 10.374897003173828, V.norm 17.53166389465332, MLP.norm 3.630338668823242
2023-04-11 14:41:01,889 :: INFO :: Epoch 20: loss tensor(780.1610, device='cuda:0'), U.norm 9.179505348205566, V.norm 17.383066177368164, MLP.norm 4.676366806030273
2023-04-11 14:41:01,889 :: INFO :: Ite = 1, Delta = 4195
2023-04-11 14:41:01,905 :: INFO :: ----- backend -----
2023-04-11 14:41:04,155 :: INFO :: Epoch 5: loss tensor(133.1255, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-0.0097, -0.0110, -0.0122, -0.0054, -0.0115, -0.0142, -0.0113, -0.0054,
        -0.0133, -0.0101, -0.0133, -0.0123, -0.0105, -0.0083, -0.0063, -0.0077,
        -0.0106, -0.0067, -0.0052, -0.0101, -0.0102, -0.0067, -0.0086, -0.0136,
        -0.0103, -0.0111, -0.0070, -0.0101, -0.0095, -0.0081, -0.0102,  0.0013,
        -0.0083, -0.0089, -0.0132, -0.0139, -0.0045, -0.0095, -0.0130, -0.0055,
        -0.0140, -0.0128, -0.0088, -0.0107, -0.0114, -0.0096, -0.0084, -0.0134,
        -0.0101, -0.0074, -0.0121, -0.0099, -0.0094, -0.0074, -0.0132,  0.0006,
        -0.0107, -0.0049, -0.0123, -0.0138, -0.0063, -0.0021, -0.0136, -0.0115,
        -0.0084, -0.0085, -0.0002, -0.0108, -0.0032, -0.0135,  0.0012, -0.0122,
         0.0104, -0.0099, -0.0059, -0.0133,  0.0050, -0.0124, -0.0061,  0.0126,
        -0.0106, -0.0130, -0.0055, -0.0079, -0.0137, -0.0073, -0.0057, -0.0120,
        -0.0108, -0.0132, -0.0125, -0.0062, -0.0077, -0.0064, -0.0128, -0.0092,
        -0.0074, -0.0121, -0.0134, -0.0116, -0.0104, -0.0061, -0.0093, -0.0127,
        -0.0121, -0.0147, -0.0071, -0.0111, -0.0078, -0.0058, -0.0111, -0.0071,
        -0.0116, -0.0073, -0.0134, -0.0134, -0.0095, -0.0136, -0.0057, -0.0108,
        -0.0115, -0.0110, -0.0083, -0.0106, -0.0113, -0.0097, -0.0130, -0.0068,
         0.0060, -0.0007, -0.0015,  0.0182,  0.0012,  0.0140, -0.0081, -0.0124,
        -0.0139,  0.0142, -0.0111, -0.0139,  0.0150, -0.0137,  0.0028, -0.0117,
         0.0010,  0.0161,  0.0073, -0.0077, -0.0004, -0.0114, -0.0136, -0.0079,
         0.0112, -0.0151, -0.0133, -0.0140, -0.0138, -0.0148,  0.0180, -0.0152,
         0.0154, -0.0150,  0.0145, -0.0124,  0.0154,  0.0166, -0.0142, -0.0132,
        -0.0134, -0.0079,  0.0132,  0.0124, -0.0149,  0.0046,  0.0102, -0.0138,
         0.0174,  0.0091, -0.0125, -0.0130,  0.0047, -0.0135,  0.0151, -0.0149,
         0.0105,  0.0133, -0.0146,  0.0066,  0.0174, -0.0150, -0.0152, -0.0119,
         0.0169, -0.0145,  0.0038,  0.0136, -0.0108,  0.0172, -0.0046, -0.0015,
         0.0094,  0.0005, -0.0037,  0.0172,  0.0010, -0.0067, -0.0142, -0.0149,
         0.0180, -0.0116,  0.0187,  0.0186,  0.0142, -0.0137, -0.0047,  0.0095,
        -0.0093, -0.0144,  0.0158, -0.0147, -0.0061, -0.0137,  0.0096, -0.0045,
         0.0065,  0.0037,  0.0048,  0.0035,  0.0064,  0.0160,  0.0055,  0.0155,
         0.0188,  0.0121,  0.0161, -0.0073, -0.0129, -0.0127,  0.0114,  0.0129,
        -0.0058, -0.0070, -0.0074,  0.0114, -0.0132,  0.0009, -0.0148, -0.0090,
        -0.0118, -0.0130, -0.0148,  0.0185, -0.0036,  0.0041, -0.0143, -0.0030,
        -0.0118, -0.0132, -0.0135, -0.0014, -0.0092,  0.0042, -0.0151, -0.0135,
        -0.0133, -0.0101, -0.0122, -0.0137, -0.0118, -0.0062, -0.0137, -0.0126,
        -0.0143, -0.0132, -0.0097, -0.0144, -0.0084, -0.0070, -0.0125, -0.0119,
        -0.0116, -0.0122, -0.0122, -0.0134, -0.0105, -0.0078, -0.0145, -0.0103,
        -0.0097, -0.0141, -0.0107, -0.0110, -0.0135, -0.0105, -0.0133, -0.0108,
        -0.0109, -0.0142, -0.0011, -0.0075, -0.0145, -0.0131, -0.0143, -0.0146,
        -0.0024, -0.0136, -0.0083, -0.0137, -0.0061, -0.0150, -0.0136, -0.0090,
        -0.0088, -0.0090, -0.0132, -0.0094, -0.0098, -0.0137, -0.0123, -0.0139,
        -0.0084, -0.0143, -0.0020, -0.0135, -0.0084, -0.0129, -0.0139, -0.0131,
        -0.0127, -0.0129, -0.0118, -0.0139, -0.0145, -0.0141, -0.0062, -0.0079,
        -0.0123, -0.0123, -0.0131, -0.0112, -0.0113, -0.0137, -0.0140,  0.0034,
        -0.0082, -0.0086, -0.0135, -0.0134, -0.0133, -0.0146, -0.0126, -0.0139,
        -0.0121, -0.0137, -0.0099, -0.0137, -0.0142, -0.0083, -0.0122, -0.0100,
        -0.0133, -0.0145, -0.0135, -0.0127, -0.0018, -0.0080, -0.0079, -0.0140,
        -0.0130, -0.0124, -0.0141, -0.0137, -0.0083, -0.0060, -0.0098, -0.0097,
        -0.0100, -0.0016, -0.0103, -0.0145, -0.0092, -0.0138, -0.0140, -0.0125],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.9699, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:06,733 :: INFO :: Epoch 10: loss tensor(126.8229, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([-7.8603e-03, -1.1950e-02, -1.9523e-02, -1.1704e-03, -1.1541e-02,
        -2.3199e-02, -1.4555e-02,  2.7721e-04, -1.9520e-02, -9.2529e-03,
        -2.1336e-02, -1.9940e-02, -7.3071e-03, -8.8456e-03, -6.2939e-03,
        -8.0667e-03, -1.2539e-02, -4.0172e-03, -7.0902e-03, -1.0153e-02,
        -1.2878e-02, -6.5108e-03, -7.0592e-03, -2.2077e-02, -1.5939e-02,
        -1.0703e-02, -4.1142e-03, -9.5204e-03, -8.9588e-03, -9.1380e-03,
        -1.1018e-02,  6.8014e-03, -1.0468e-02, -9.6374e-03, -2.2840e-02,
        -2.5077e-02,  4.4148e-05, -6.7940e-03, -1.8256e-02, -7.1211e-03,
        -2.2804e-02, -1.5232e-02, -9.6454e-03, -1.3818e-02, -1.4001e-02,
        -1.2686e-02, -4.7197e-03, -2.2460e-02, -1.3777e-02, -6.5449e-03,
        -1.7843e-02, -1.2486e-02, -1.2486e-02, -6.2246e-03, -2.1148e-02,
         8.9698e-03, -1.2231e-02,  1.0443e-03, -1.9605e-02, -2.3814e-02,
        -2.5267e-03,  1.8406e-03, -2.2654e-02, -1.4375e-02, -9.6008e-03,
        -6.8487e-03,  4.2589e-03, -1.7162e-02,  2.4753e-03, -2.2127e-02,
         7.8225e-03, -1.8264e-02,  1.7751e-02, -5.6837e-03,  1.0809e-03,
        -2.1820e-02,  5.3932e-03, -2.0071e-02, -9.7715e-04,  1.4923e-02,
        -9.6709e-03, -2.2292e-02,  2.1226e-03, -2.8356e-03, -2.2975e-02,
        -5.2804e-03, -5.8571e-03, -1.8491e-02, -1.1426e-02, -2.0798e-02,
        -1.5574e-02,  2.4741e-03, -2.7223e-03, -1.4978e-03, -2.1795e-02,
        -7.1502e-03, -2.7326e-03, -1.9762e-02, -1.9519e-02, -1.8264e-02,
        -1.6889e-02,  9.2772e-04, -8.5795e-03, -1.7350e-02, -1.8352e-02,
        -2.6053e-02, -8.1280e-03, -1.6419e-02, -7.0457e-03,  2.5211e-05,
        -1.6406e-02, -3.3598e-03, -1.7355e-02, -6.7206e-03, -2.2322e-02,
        -2.2687e-02, -1.1589e-02, -2.0833e-02, -4.3510e-03, -1.4080e-02,
        -1.6806e-02, -1.2572e-02, -1.1203e-02, -1.5765e-02, -1.3211e-02,
        -1.3666e-02, -2.0077e-02, -1.1679e-04,  2.6797e-02,  1.8890e-02,
         1.8423e-02,  3.7601e-02,  2.0987e-02,  3.0230e-02,  6.9892e-03,
        -1.0389e-02, -1.9298e-02,  2.6216e-02, -3.4471e-03, -2.1745e-02,
         3.6016e-02, -2.0560e-02,  2.4332e-02, -1.1386e-02,  2.2648e-02,
         3.5816e-02,  2.6754e-02,  9.5821e-03,  2.0980e-02, -1.4091e-03,
        -1.9788e-02,  7.9723e-03,  3.2932e-02, -2.6012e-02, -1.8283e-02,
        -2.1948e-02, -1.8753e-02, -2.6054e-02,  3.3411e-02, -2.7763e-02,
         3.5868e-02, -2.7299e-02,  3.0962e-02, -1.4865e-02,  3.3784e-02,
         2.4410e-02, -2.3486e-02, -1.5375e-02, -1.8685e-02,  7.6169e-03,
         3.4337e-02,  3.1731e-02, -2.6611e-02,  2.5579e-02,  3.2043e-02,
        -1.8636e-02,  3.0968e-02,  3.0090e-02, -1.3287e-02, -1.9019e-02,
         2.5048e-02, -1.8012e-02,  3.2252e-02, -2.6160e-02,  3.0561e-02,
         3.3824e-02, -2.4906e-02,  2.7840e-02,  3.6978e-02, -2.6148e-02,
        -2.6698e-02, -9.6908e-03,  3.5896e-02, -2.4861e-02,  2.4556e-02,
         3.3041e-02,  4.2462e-04,  2.7518e-02,  1.3937e-02,  1.9464e-02,
         3.0801e-02,  2.1831e-02,  1.3439e-02,  3.6056e-02,  2.2332e-02,
         1.1820e-02, -2.2181e-02, -2.6444e-02,  3.5765e-02, -6.3346e-03,
         3.8942e-02,  3.6877e-02,  2.0371e-02, -2.0712e-02,  1.5082e-02,
         3.0644e-02,  3.3806e-03, -2.4135e-02,  3.4672e-02, -2.5951e-02,
         1.3426e-02, -1.5132e-02,  2.8671e-02,  1.3903e-02,  2.3211e-02,
         2.4985e-02,  2.5265e-02,  2.3980e-02,  2.5148e-02,  3.7877e-02,
         2.6908e-02,  3.3399e-02,  3.5346e-02,  2.4899e-02,  3.5810e-02,
         1.0421e-02, -1.4463e-02, -1.4748e-02,  2.0264e-02,  3.1072e-02,
         1.2615e-02,  9.5860e-03,  9.0921e-03,  3.1501e-02, -1.5757e-02,
         2.1123e-02, -2.5907e-02,  2.6636e-03, -9.2813e-03, -1.6532e-02,
        -2.3709e-02,  3.7194e-02,  1.5422e-02,  2.6147e-02, -2.4595e-02,
         1.7047e-02,  3.9232e-03, -1.5602e-02, -1.0065e-02,  2.0809e-02,
         7.2001e-03,  2.6459e-02, -1.4221e-02, -4.3244e-03, -1.7791e-02,
         4.9253e-03, -1.0290e-02, -1.8492e-02,  7.7518e-04,  1.3209e-02,
        -1.9718e-02, -9.3234e-04, -1.7715e-02, -1.8650e-02,  7.5021e-03,
        -1.3838e-02,  1.0180e-02,  1.2856e-02, -1.2178e-02, -4.1152e-03,
        -1.4653e-03, -4.8891e-03, -4.5946e-03, -1.5549e-02,  3.6291e-03,
         1.0641e-02, -8.2290e-03,  6.7386e-03,  4.2264e-03, -6.5063e-03,
         2.6325e-04,  6.8696e-03, -1.2188e-02,  2.2396e-03, -1.8911e-02,
         4.0589e-04,  2.9331e-03, -2.2108e-02,  2.0864e-02,  1.1380e-02,
        -2.1060e-02, -1.0292e-02, -1.7025e-02, -2.2968e-02,  1.9085e-02,
        -1.6801e-02,  8.6669e-03, -1.8569e-02,  1.4395e-02, -1.3570e-02,
        -1.8734e-02,  8.5022e-03,  1.0450e-02,  8.4097e-03, -1.2738e-02,
         8.0475e-03,  6.0301e-03, -1.6712e-02, -1.6461e-03, -1.9247e-02,
         1.0195e-02, -2.1409e-02,  2.0325e-02, -1.8042e-02,  1.0195e-02,
        -1.4708e-02, -1.5312e-02, -6.9679e-03, -1.0054e-02, -9.9093e-03,
         1.7725e-03, -1.8450e-02, -1.0511e-02, -1.1177e-02,  1.4181e-02,
         1.0735e-02, -1.1471e-02,  1.4288e-03, -1.2630e-02,  1.5957e-03,
         3.0679e-03, -4.7211e-03, -1.1988e-02,  2.6111e-02,  1.1339e-02,
         7.4835e-03, -1.6509e-02, -1.6694e-02, -1.8603e-02, -1.1077e-02,
        -6.8756e-03, -1.9660e-02,  1.2339e-03, -1.6508e-02,  5.7231e-03,
        -1.0869e-02, -2.1148e-02,  8.8813e-03, -1.0807e-02,  6.7727e-03,
        -1.3256e-02, -2.0539e-02, -1.5072e-02, -3.5191e-03,  2.0320e-02,
         8.8128e-03,  1.1767e-02, -2.0448e-02, -1.3652e-02,  1.1315e-03,
        -1.0472e-02, -9.4537e-03,  9.7634e-03,  1.5038e-02,  7.1121e-03,
         5.2037e-03,  4.2921e-03,  2.0173e-02,  5.9264e-03, -2.2559e-02,
         8.2000e-03, -3.4820e-03, -4.4051e-03, -6.6738e-03], device='cuda:0',
       requires_grad=True) MLP.norm tensor(4.7531, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:09,202 :: INFO :: Epoch 15: loss tensor(125.2907, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0016, -0.0063, -0.0202,  0.0075, -0.0052, -0.0268, -0.0075,  0.0093,
        -0.0183, -0.0033, -0.0207, -0.0236,  0.0015, -0.0021, -0.0010, -0.0038,
        -0.0100,  0.0054, -0.0061, -0.0033, -0.0098, -0.0021,  0.0016, -0.0232,
        -0.0171, -0.0052,  0.0010, -0.0017,  0.0043, -0.0076, -0.0036,  0.0145,
        -0.0029, -0.0033, -0.0275, -0.0326,  0.0067,  0.0010, -0.0189, -0.0015,
        -0.0278, -0.0122, -0.0041, -0.0124, -0.0108, -0.0099,  0.0030, -0.0259,
        -0.0111, -0.0028, -0.0167, -0.0093, -0.0134, -0.0019, -0.0204,  0.0195,
        -0.0066,  0.0124, -0.0186, -0.0277,  0.0030,  0.0061, -0.0263, -0.0110,
        -0.0036, -0.0026,  0.0149, -0.0168,  0.0089, -0.0262,  0.0140, -0.0149,
         0.0196,  0.0035,  0.0118, -0.0248,  0.0107, -0.0210,  0.0085,  0.0168,
         0.0002, -0.0221,  0.0109,  0.0097, -0.0273,  0.0036, -0.0018, -0.0183,
        -0.0069, -0.0240, -0.0127,  0.0156,  0.0042,  0.0087, -0.0226,  0.0041,
         0.0044, -0.0218, -0.0201, -0.0183, -0.0159,  0.0097, -0.0009, -0.0167,
        -0.0157, -0.0338, -0.0044, -0.0173, -0.0017,  0.0101, -0.0150,  0.0042,
        -0.0127, -0.0011, -0.0264, -0.0282, -0.0077, -0.0227, -0.0026, -0.0098,
        -0.0171, -0.0086, -0.0099, -0.0156, -0.0087, -0.0148, -0.0204,  0.0116,
         0.0467,  0.0391,  0.0403,  0.0555,  0.0424,  0.0470,  0.0269,  0.0015,
        -0.0167,  0.0318,  0.0136, -0.0240,  0.0570, -0.0221,  0.0466,  0.0006,
         0.0453,  0.0548,  0.0438,  0.0333,  0.0441,  0.0211, -0.0200,  0.0313,
         0.0542, -0.0344, -0.0158, -0.0240, -0.0167, -0.0351,  0.0453, -0.0385,
         0.0551, -0.0377,  0.0469, -0.0089,  0.0496,  0.0288, -0.0292, -0.0092,
        -0.0164,  0.0283,  0.0567,  0.0493, -0.0362,  0.0468,  0.0540, -0.0158,
         0.0399,  0.0500, -0.0051, -0.0204,  0.0451, -0.0141,  0.0466, -0.0351,
         0.0496,  0.0549, -0.0324,  0.0491,  0.0599, -0.0352, -0.0348,  0.0023,
         0.0532, -0.0320,  0.0434,  0.0524,  0.0222,  0.0315,  0.0362,  0.0423,
         0.0520,  0.0450,  0.0328,  0.0540,  0.0445,  0.0353, -0.0250, -0.0362,
         0.0504,  0.0083,  0.0595,  0.0536,  0.0338, -0.0222,  0.0377,  0.0528,
         0.0228, -0.0304,  0.0527, -0.0349,  0.0375, -0.0046,  0.0491,  0.0358,
         0.0420,  0.0473,  0.0451,  0.0457,  0.0414,  0.0596,  0.0476,  0.0485,
         0.0470,  0.0413,  0.0539,  0.0339, -0.0065, -0.0088,  0.0263,  0.0486,
         0.0344,  0.0312,  0.0313,  0.0500, -0.0103,  0.0422, -0.0342,  0.0229,
         0.0026, -0.0123, -0.0267,  0.0555,  0.0371,  0.0495, -0.0315,  0.0387,
         0.0292, -0.0065,  0.0087,  0.0455,  0.0313,  0.0510,  0.0035,  0.0189,
        -0.0120,  0.0286,  0.0053, -0.0129,  0.0239,  0.0378, -0.0172,  0.0230,
        -0.0075, -0.0144,  0.0323,  0.0021,  0.0343,  0.0385,  0.0013,  0.0172,
         0.0217,  0.0171,  0.0166, -0.0048,  0.0275,  0.0344,  0.0136,  0.0317,
         0.0259,  0.0158,  0.0226,  0.0323,  0.0054,  0.0248, -0.0159,  0.0225,
         0.0269, -0.0233,  0.0445,  0.0358, -0.0198,  0.0073, -0.0050, -0.0250,
         0.0438, -0.0084,  0.0318, -0.0137,  0.0392,  0.0043, -0.0132,  0.0333,
         0.0362,  0.0333,  0.0022,  0.0326,  0.0291, -0.0079,  0.0217, -0.0155,
         0.0353, -0.0190,  0.0454, -0.0117,  0.0352, -0.0048, -0.0035,  0.0140,
         0.0070,  0.0078,  0.0263, -0.0110,  0.0104,  0.0081,  0.0395,  0.0345,
         0.0022,  0.0259,  0.0023,  0.0241,  0.0275,  0.0180,  0.0065,  0.0509,
         0.0361,  0.0297, -0.0081, -0.0086, -0.0149,  0.0091,  0.0136, -0.0167,
         0.0257, -0.0094,  0.0304,  0.0074, -0.0214,  0.0324,  0.0045,  0.0316,
         0.0004, -0.0177, -0.0032,  0.0203,  0.0451,  0.0331,  0.0368, -0.0178,
        -0.0018,  0.0259,  0.0098,  0.0094,  0.0347,  0.0394,  0.0323,  0.0294,
         0.0276,  0.0437,  0.0304, -0.0245,  0.0327,  0.0204,  0.0190,  0.0128],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.4100, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:11,686 :: INFO :: Epoch 20: loss tensor(124.0249, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.6717e-02,  5.2482e-03, -1.2695e-02,  2.2053e-02,  5.4758e-03,
        -2.3156e-02,  7.5015e-03,  2.2037e-02, -1.0846e-02,  6.6743e-03,
        -1.0871e-02, -2.2322e-02,  1.2800e-02,  1.1839e-02,  1.0637e-02,
         3.9535e-03, -2.3179e-03,  1.9903e-02,  2.1894e-03,  1.0755e-02,
        -1.0043e-03,  9.1566e-03,  1.6548e-02, -1.5543e-02, -1.1816e-02,
         3.7729e-03,  1.0042e-02,  1.1962e-02,  2.4971e-02, -4.1950e-05,
         8.5283e-03,  2.6309e-02,  1.3755e-02,  9.7533e-03, -2.5399e-02,
        -3.4999e-02,  1.7893e-02,  1.1403e-02, -1.5288e-02,  1.2311e-02,
        -2.8254e-02, -3.5132e-03,  8.3163e-03, -6.4011e-03, -2.2732e-03,
        -1.4129e-03,  1.4221e-02, -2.1996e-02, -2.5949e-04,  5.5988e-03,
        -8.0230e-03, -9.4392e-04, -1.0134e-02,  6.8525e-03, -8.5819e-03,
         3.4616e-02,  5.8139e-03,  2.8794e-02, -7.8461e-03, -2.5235e-02,
         1.2277e-02,  1.4754e-02, -2.2910e-02, -1.4609e-03,  9.7662e-03,
         6.1091e-03,  3.1854e-02, -7.7252e-03,  1.9742e-02, -2.4646e-02,
         2.5434e-02, -2.6649e-03,  2.5861e-02,  1.6702e-02,  2.6038e-02,
        -2.0023e-02,  2.4414e-02, -1.3394e-02,  2.1835e-02,  2.6143e-02,
         1.4320e-02, -1.0544e-02,  2.2603e-02,  2.7863e-02, -2.5173e-02,
         1.8163e-02,  8.5795e-03, -8.9566e-03,  2.0464e-03, -2.0665e-02,
        -5.2537e-03,  3.1117e-02,  1.4787e-02,  2.2549e-02, -1.2057e-02,
         2.0439e-02,  1.4161e-02, -1.4485e-02, -1.5161e-02, -1.1406e-02,
        -5.3664e-03,  2.0400e-02,  1.0774e-02, -9.7705e-03, -3.1374e-03,
        -3.5985e-02,  6.8088e-03, -1.2925e-02,  8.6343e-03,  2.2726e-02,
        -4.5678e-03,  1.5280e-02,  2.4554e-04,  1.0806e-02, -2.5581e-02,
        -2.7585e-02,  3.3609e-03, -1.9404e-02,  3.5539e-03,  1.4618e-03,
        -9.8510e-03,  1.4091e-03, -3.2911e-04, -8.0921e-03,  1.2621e-03,
        -1.0664e-02, -1.2470e-02,  2.6526e-02,  6.4975e-02,  5.8525e-02,
         6.1899e-02,  7.0024e-02,  5.9718e-02,  6.3850e-02,  4.6310e-02,
         1.7856e-02, -7.1602e-03,  3.5244e-02,  3.2005e-02, -2.1604e-02,
         7.7211e-02, -1.9133e-02,  6.7933e-02,  2.2134e-02,  6.7390e-02,
         7.2952e-02,  5.6936e-02,  5.7853e-02,  6.6674e-02,  4.4622e-02,
        -1.5447e-02,  5.5176e-02,  7.4327e-02, -4.1249e-02, -7.9217e-03,
        -2.0563e-02, -9.5019e-03, -4.2279e-02,  5.5996e-02, -4.7387e-02,
         7.2665e-02, -4.6243e-02,  6.3039e-02,  1.7085e-03,  6.3897e-02,
         3.5660e-02, -3.1949e-02,  2.6493e-03, -8.4842e-03,  4.8280e-02,
         7.8057e-02,  6.4928e-02, -4.3932e-02,  6.7307e-02,  7.4795e-02,
        -6.7569e-03,  4.6593e-02,  6.8437e-02,  7.6871e-03, -1.7821e-02,
         6.4020e-02, -3.7371e-03,  5.8184e-02, -4.2042e-02,  6.7243e-02,
         7.5712e-02, -3.7509e-02,  6.8660e-02,  8.2992e-02, -4.2561e-02,
        -3.8627e-02,  1.8905e-02,  6.9295e-02, -3.6433e-02,  5.7679e-02,
         7.1343e-02,  4.6620e-02,  3.4123e-02,  5.7283e-02,  6.4936e-02,
         7.2608e-02,  6.8286e-02,  5.1653e-02,  6.9100e-02,  6.5721e-02,
         5.9945e-02, -2.3753e-02, -4.4244e-02,  6.1848e-02,  2.6091e-02,
         7.9746e-02,  6.8761e-02,  4.6381e-02, -1.9138e-02,  5.8304e-02,
         7.5282e-02,  4.3345e-02, -3.3648e-02,  7.0280e-02, -4.1933e-02,
         6.1849e-02,  1.0835e-02,  6.9629e-02,  5.8376e-02,  5.8511e-02,
         6.9851e-02,  6.2891e-02,  6.7921e-02,  5.4812e-02,  8.0422e-02,
         6.6294e-02,  6.0440e-02,  5.6352e-02,  5.4224e-02,  7.0229e-02,
         5.8220e-02,  7.0700e-03,  2.1733e-03,  3.0887e-02,  6.5314e-02,
         5.5031e-02,  5.3155e-02,  5.4407e-02,  6.7352e-02,  1.0871e-03,
         6.2885e-02, -4.0450e-02,  4.4232e-02,  1.8595e-02, -2.4521e-03,
        -2.3071e-02,  7.3613e-02,  5.9109e-02,  7.2447e-02, -3.4808e-02,
         5.9164e-02,  5.5860e-02,  9.8352e-03,  3.3097e-02,  6.8980e-02,
         5.5169e-02,  7.5983e-02,  2.6527e-02,  4.3898e-02,  2.1429e-03,
         5.3143e-02,  2.6868e-02, -1.1637e-03,  4.6933e-02,  6.1235e-02,
        -8.9169e-03,  4.7940e-02,  1.0426e-02, -3.8358e-03,  5.6509e-02,
         2.3035e-02,  5.8809e-02,  6.4045e-02,  1.9888e-02,  4.0230e-02,
         4.6061e-02,  4.3712e-02,  3.9245e-02,  1.3452e-02,  5.3623e-02,
         5.6086e-02,  3.8165e-02,  5.6918e-02,  4.5872e-02,  4.1110e-02,
         4.6294e-02,  5.7484e-02,  3.0542e-02,  4.7546e-02, -7.3013e-03,
         4.4767e-02,  5.0822e-02, -1.9059e-02,  6.5699e-02,  5.9284e-02,
        -1.2451e-02,  3.1156e-02,  1.6315e-02, -2.1420e-02,  6.8258e-02,
         8.3942e-03,  5.3528e-02, -2.3448e-03,  6.2944e-02,  2.7139e-02,
         6.9774e-04,  5.9116e-02,  6.2247e-02,  5.8604e-02,  2.3207e-02,
         5.6434e-02,  5.1422e-02,  9.5678e-03,  4.6123e-02, -4.9670e-03,
         5.9663e-02, -7.7103e-03,  6.9447e-02,  1.3296e-03,  6.0398e-02,
         1.0698e-02,  1.6159e-02,  3.8599e-02,  2.7919e-02,  2.9794e-02,
         5.0933e-02,  3.7283e-03,  3.5002e-02,  3.0959e-02,  6.5475e-02,
         5.6294e-02,  1.9252e-02,  5.0811e-02,  2.4147e-02,  4.5200e-02,
         5.1663e-02,  4.1831e-02,  2.9028e-02,  7.5498e-02,  5.9779e-02,
         4.9676e-02,  5.0069e-03,  6.3355e-03, -4.9807e-03,  3.2818e-02,
         3.6433e-02, -7.6839e-03,  5.0822e-02,  1.4833e-03,  5.5739e-02,
         2.8772e-02, -1.5693e-02,  5.5696e-02,  2.3362e-02,  5.6503e-02,
         2.0435e-02, -6.3641e-03,  1.7108e-02,  4.7181e-02,  6.9222e-02,
         5.7024e-02,  6.1700e-02, -8.4223e-03,  1.5095e-02,  5.1346e-02,
         3.4104e-02,  3.3829e-02,  5.8683e-02,  6.2365e-02,  5.8762e-02,
         5.5363e-02,  5.0166e-02,  6.3848e-02,  5.3949e-02, -2.1559e-02,
         5.6243e-02,  4.5583e-02,  4.4680e-02,  3.4441e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.9793, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:14,530 :: INFO :: Epoch 25: loss tensor(122.1127, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0299,  0.0147, -0.0030,  0.0347,  0.0153, -0.0156,  0.0219,  0.0322,
        -0.0014,  0.0142,  0.0012, -0.0200,  0.0224,  0.0247,  0.0205,  0.0091,
         0.0056,  0.0325,  0.0094,  0.0234,  0.0072,  0.0188,  0.0295, -0.0045,
        -0.0065,  0.0112,  0.0173,  0.0238,  0.0446,  0.0060,  0.0187,  0.0354,
         0.0290,  0.0219, -0.0206, -0.0346,  0.0257,  0.0206, -0.0108,  0.0249,
        -0.0261,  0.0055,  0.0195,  0.0005,  0.0053,  0.0061,  0.0231, -0.0149,
         0.0110,  0.0124,  0.0004,  0.0063, -0.0057,  0.0136,  0.0055,  0.0470,
         0.0186,  0.0435,  0.0041, -0.0197,  0.0185,  0.0224, -0.0165,  0.0077,
         0.0219,  0.0130,  0.0464,  0.0019,  0.0278, -0.0213,  0.0346,  0.0106,
         0.0311,  0.0299,  0.0367, -0.0135,  0.0372, -0.0045,  0.0324,  0.0364,
         0.0270,  0.0045,  0.0310,  0.0442, -0.0202,  0.0310,  0.0163,  0.0021,
         0.0099, -0.0162,  0.0021,  0.0431,  0.0221,  0.0338,  0.0008,  0.0347,
         0.0205, -0.0060, -0.0090, -0.0041,  0.0066,  0.0273,  0.0194, -0.0011,
         0.0106, -0.0353,  0.0166, -0.0077,  0.0178,  0.0332,  0.0064,  0.0254,
         0.0129,  0.0222, -0.0222, -0.0254,  0.0130, -0.0131,  0.0088,  0.0118,
        -0.0023,  0.0099,  0.0091, -0.0001,  0.0115, -0.0061, -0.0033,  0.0392,
         0.0814,  0.0759,  0.0819,  0.0805,  0.0714,  0.0788,  0.0639,  0.0352,
         0.0067,  0.0387,  0.0492, -0.0157,  0.0958, -0.0129,  0.0876,  0.0464,
         0.0878,  0.0895,  0.0664,  0.0815,  0.0877,  0.0665, -0.0072,  0.0772,
         0.0927, -0.0471,  0.0023, -0.0125,  0.0010, -0.0478,  0.0642, -0.0544,
         0.0881, -0.0532,  0.0773,  0.0155,  0.0762,  0.0399, -0.0322,  0.0170,
         0.0031,  0.0665,  0.0976,  0.0783, -0.0499,  0.0865,  0.0940,  0.0058,
         0.0518,  0.0848,  0.0220, -0.0130,  0.0812,  0.0103,  0.0659, -0.0470,
         0.0828,  0.0955, -0.0409,  0.0860,  0.1043, -0.0487, -0.0390,  0.0366,
         0.0834, -0.0385,  0.0670,  0.0891,  0.0699,  0.0347,  0.0764,  0.0863,
         0.0919,  0.0905,  0.0681,  0.0804,  0.0852,  0.0835, -0.0192, -0.0508,
         0.0697,  0.0437,  0.0986,  0.0818,  0.0547, -0.0132,  0.0759,  0.0969,
         0.0627, -0.0343,  0.0864, -0.0471,  0.0846,  0.0270,  0.0881,  0.0799,
         0.0706,  0.0911,  0.0782,  0.0893,  0.0652,  0.0996,  0.0826,  0.0686,
         0.0644,  0.0618,  0.0843,  0.0814,  0.0230,  0.0158,  0.0329,  0.0797,
         0.0736,  0.0742,  0.0766,  0.0832,  0.0152,  0.0822, -0.0447,  0.0644,
         0.0355,  0.0106, -0.0149,  0.0908,  0.0801,  0.0941, -0.0352,  0.0779,
         0.0825,  0.0295,  0.0593,  0.0900,  0.0773,  0.1007,  0.0492,  0.0678,
         0.0212,  0.0769,  0.0496,  0.0135,  0.0679,  0.0823,  0.0024,  0.0718,
         0.0311,  0.0101,  0.0786,  0.0445,  0.0824,  0.0886,  0.0390,  0.0617,
         0.0690,  0.0711,  0.0604,  0.0351,  0.0798,  0.0747,  0.0620,  0.0809,
         0.0625,  0.0660,  0.0688,  0.0814,  0.0573,  0.0679,  0.0039,  0.0648,
         0.0730, -0.0108,  0.0839,  0.0802, -0.0003,  0.0564,  0.0404, -0.0131,
         0.0918,  0.0291,  0.0719,  0.0131,  0.0848,  0.0502,  0.0184,  0.0845,
         0.0876,  0.0829,  0.0452,  0.0778,  0.0718,  0.0317,  0.0691,  0.0101,
         0.0820,  0.0090,  0.0917,  0.0177,  0.0849,  0.0275,  0.0391,  0.0631,
         0.0482,  0.0514,  0.0742,  0.0210,  0.0593,  0.0530,  0.0909,  0.0744,
         0.0353,  0.0742,  0.0476,  0.0633,  0.0744,  0.0642,  0.0515,  0.0993,
         0.0818,  0.0667,  0.0183,  0.0243,  0.0088,  0.0560,  0.0582,  0.0050,
         0.0746,  0.0120,  0.0803,  0.0491, -0.0052,  0.0780,  0.0413,  0.0800,
         0.0427,  0.0106,  0.0407,  0.0742,  0.0921,  0.0787,  0.0852,  0.0054,
         0.0324,  0.0755,  0.0581,  0.0598,  0.0805,  0.0827,  0.0851,  0.0815,
         0.0704,  0.0800,  0.0753, -0.0144,  0.0777,  0.0698,  0.0698,  0.0553],
       device='cuda:0', requires_grad=True) MLP.norm tensor(9.4430, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:17,140 :: INFO :: Epoch 30: loss tensor(121.2446, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 4.1244e-02,  2.0634e-02,  7.4022e-03,  4.4896e-02,  2.3132e-02,
        -7.3479e-03,  3.4677e-02,  4.0244e-02,  7.2681e-03,  1.9573e-02,
         1.3944e-02, -1.6738e-02,  2.9726e-02,  3.5362e-02,  2.8986e-02,
         1.3262e-02,  1.1479e-02,  4.1869e-02,  1.4311e-02,  3.4592e-02,
         1.3601e-02,  2.6165e-02,  4.0834e-02,  7.0816e-03, -9.5078e-04,
         1.7581e-02,  2.1206e-02,  3.3125e-02,  6.2073e-02,  1.0528e-02,
         2.6908e-02,  4.1831e-02,  4.1477e-02,  3.1874e-02, -1.4742e-02,
        -3.2579e-02,  3.0039e-02,  2.7753e-02, -6.3286e-03,  3.4732e-02,
        -2.2440e-02,  1.4011e-02,  2.8841e-02,  5.7410e-03,  1.0580e-02,
         1.1793e-02,  2.9095e-02, -6.4873e-03,  2.0713e-02,  1.7581e-02,
         7.4813e-03,  1.2399e-02, -1.6547e-03,  1.7338e-02,  1.9437e-02,
         5.6346e-02,  2.9405e-02,  5.6330e-02,  1.5204e-02, -1.2056e-02,
         2.2811e-02,  2.7757e-02, -9.3112e-03,  1.5102e-02,  3.1491e-02,
         1.7353e-02,  5.7618e-02,  1.0830e-02,  3.3305e-02, -1.7166e-02,
         4.0846e-02,  2.2969e-02,  3.4381e-02,  4.2015e-02,  4.5468e-02,
        -6.5291e-03,  4.8342e-02,  4.6555e-03,  3.9817e-02,  4.5612e-02,
         3.7262e-02,  1.9997e-02,  3.6402e-02,  5.9047e-02, -1.4642e-02,
         4.1792e-02,  2.2193e-02,  1.2858e-02,  1.5779e-02, -1.1634e-02,
         8.8341e-03,  5.2509e-02,  2.6126e-02,  4.1644e-02,  1.3385e-02,
         4.5544e-02,  2.4913e-02,  2.2387e-03, -3.1442e-03,  2.5580e-03,
         1.7716e-02,  3.1097e-02,  2.6411e-02,  6.7318e-03,  2.3859e-02,
        -3.2605e-02,  2.4271e-02, -3.3275e-03,  2.4752e-02,  4.1158e-02,
         1.8386e-02,  3.2761e-02,  2.5194e-02,  3.2047e-02, -1.7691e-02,
        -2.3151e-02,  2.0918e-02, -5.1978e-03,  1.1571e-02,  2.1361e-02,
         3.9129e-03,  1.6319e-02,  1.5827e-02,  5.7289e-03,  1.9112e-02,
        -1.7529e-03,  5.4856e-03,  4.8939e-02,  9.6261e-02,  9.0546e-02,
         1.0004e-01,  8.7575e-02,  7.8950e-02,  9.1553e-02,  7.8388e-02,
         5.1775e-02,  2.2797e-02,  4.1354e-02,  6.4193e-02, -7.9521e-03,
         1.1231e-01, -4.9406e-03,  1.0517e-01,  7.1611e-02,  1.0647e-01,
         1.0393e-01,  7.2364e-02,  1.0438e-01,  1.0652e-01,  8.6139e-02,
         2.6921e-03,  9.7754e-02,  1.0858e-01, -5.1985e-02,  1.3108e-02,
        -1.3919e-03,  1.2528e-02, -5.2010e-02,  6.9566e-02, -5.9744e-02,
         1.0129e-01, -5.8961e-02,  8.9144e-02,  2.9727e-02,  8.5993e-02,
         4.0461e-02, -3.0895e-02,  3.1459e-02,  1.6425e-02,  8.2086e-02,
         1.1501e-01,  8.8465e-02, -5.4742e-02,  1.0396e-01,  1.1136e-01,
         2.0216e-02,  5.5059e-02,  9.8244e-02,  3.5840e-02, -7.4393e-03,
         9.6026e-02,  2.5335e-02,  6.9837e-02, -5.0779e-02,  9.5590e-02,
         1.1380e-01, -4.3145e-02,  1.0102e-01,  1.2376e-01, -5.3641e-02,
        -3.6049e-02,  5.3428e-02,  9.4882e-02, -3.9170e-02,  7.1909e-02,
         1.0518e-01,  9.2089e-02,  3.3101e-02,  9.3297e-02,  1.0615e-01,
         1.0948e-01,  1.1167e-01,  8.1100e-02,  8.8105e-02,  1.0210e-01,
         1.0601e-01, -1.2500e-02, -5.6353e-02,  7.3550e-02,  5.9607e-02,
         1.1567e-01,  9.1927e-02,  5.9287e-02, -5.8855e-03,  9.0719e-02,
         1.1738e-01,  7.9537e-02, -3.3171e-02,  1.0010e-01, -5.1157e-02,
         1.0649e-01,  4.1726e-02,  1.0456e-01,  1.0018e-01,  7.9118e-02,
         1.1080e-01,  9.0655e-02,  1.0943e-01,  7.2325e-02,  1.1693e-01,
         9.5899e-02,  7.2289e-02,  7.1210e-02,  6.5031e-02,  9.5589e-02,
         1.0331e-01,  3.8983e-02,  2.9434e-02,  3.4122e-02,  9.1570e-02,
         8.9490e-02,  9.3405e-02,  9.7487e-02,  9.7381e-02,  3.0017e-02,
         9.9561e-02, -4.7337e-02,  8.2345e-02,  5.1322e-02,  2.4786e-02,
        -3.1648e-03,  1.0658e-01,  9.9702e-02,  1.1430e-01, -3.3231e-02,
         9.3968e-02,  1.0822e-01,  5.0423e-02,  8.5322e-02,  1.0804e-01,
         9.8505e-02,  1.2460e-01,  7.0287e-02,  8.9629e-02,  4.2879e-02,
         1.0044e-01,  7.2363e-02,  2.8917e-02,  8.6216e-02,  1.0054e-01,
         1.5418e-02,  9.3849e-02,  5.2327e-02,  2.5562e-02,  9.7615e-02,
         6.4715e-02,  1.0569e-01,  1.1206e-01,  5.7551e-02,  8.0968e-02,
         9.0159e-02,  9.8106e-02,  7.9265e-02,  5.7890e-02,  1.0550e-01,
         8.9869e-02,  8.4130e-02,  1.0280e-01,  7.5924e-02,  8.9089e-02,
         9.0677e-02,  1.0310e-01,  8.4251e-02,  8.4874e-02,  1.5814e-02,
         8.2823e-02,  9.2487e-02, -1.4241e-05,  9.8460e-02,  9.8048e-02,
         1.4602e-02,  8.1287e-02,  6.5743e-02, -1.1025e-03,  1.1519e-01,
         5.1260e-02,  8.6947e-02,  3.0427e-02,  1.0446e-01,  7.2716e-02,
         3.8300e-02,  1.0904e-01,  1.1222e-01,  1.0610e-01,  6.6774e-02,
         9.5773e-02,  9.0710e-02,  5.5223e-02,  8.9722e-02,  2.7464e-02,
         1.0195e-01,  2.8880e-02,  1.1172e-01,  3.5118e-02,  1.0835e-01,
         4.3834e-02,  6.2804e-02,  8.6968e-02,  6.7138e-02,  7.0908e-02,
         9.5362e-02,  3.8456e-02,  8.1805e-02,  7.2915e-02,  1.1537e-01,
         8.8009e-02,  4.9177e-02,  9.5243e-02,  7.1067e-02,  7.7300e-02,
         9.4525e-02,  8.4237e-02,  7.2102e-02,  1.2233e-01,  1.0139e-01,
         8.1555e-02,  3.0501e-02,  4.3116e-02,  2.4837e-02,  7.7343e-02,
         7.8073e-02,  1.9557e-02,  9.6195e-02,  2.1830e-02,  1.0363e-01,
         6.7252e-02,  8.5835e-03,  9.9557e-02,  5.6712e-02,  1.0132e-01,
         6.4990e-02,  3.0494e-02,  6.5011e-02,  1.0088e-01,  1.1402e-01,
         9.7670e-02,  1.0694e-01,  2.1616e-02,  4.8476e-02,  9.7755e-02,
         8.0399e-02,  8.5554e-02,  9.9659e-02,  9.9696e-02,  1.1078e-01,
         1.0721e-01,  8.7937e-02,  9.2356e-02,  9.3395e-02, -4.0825e-03,
         9.6379e-02,  9.2291e-02,  9.3712e-02,  7.4366e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(10.7999, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:19,624 :: INFO :: Epoch 35: loss tensor(119.3798, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0476,  0.0234,  0.0150,  0.0520,  0.0286, -0.0003,  0.0432,  0.0458,
         0.0136,  0.0228,  0.0239, -0.0134,  0.0341,  0.0425,  0.0342,  0.0146,
         0.0141,  0.0477,  0.0160,  0.0420,  0.0177,  0.0289,  0.0481,  0.0163,
         0.0022,  0.0206,  0.0223,  0.0395,  0.0757,  0.0124,  0.0321,  0.0441,
         0.0502,  0.0380, -0.0093, -0.0305,  0.0319,  0.0333, -0.0028,  0.0405,
        -0.0185,  0.0193,  0.0340,  0.0090,  0.0128,  0.0155,  0.0316,  0.0018,
         0.0279,  0.0193,  0.0118,  0.0144,  0.0006,  0.0175,  0.0309,  0.0623,
         0.0364,  0.0675,  0.0233, -0.0054,  0.0237,  0.0310, -0.0033,  0.0189,
         0.0374,  0.0188,  0.0642,  0.0160,  0.0357, -0.0142,  0.0442,  0.0319,
         0.0345,  0.0523,  0.0502, -0.0026,  0.0557,  0.0117,  0.0432,  0.0520,
         0.0438,  0.0334,  0.0388,  0.0697, -0.0105,  0.0478,  0.0254,  0.0203,
         0.0194, -0.0088,  0.0134,  0.0590,  0.0277,  0.0449,  0.0222,  0.0528,
         0.0255,  0.0074,  0.0013,  0.0068,  0.0251,  0.0321,  0.0296,  0.0141,
         0.0343, -0.0294,  0.0279, -0.0014,  0.0291,  0.0461,  0.0275,  0.0356,
         0.0340,  0.0381, -0.0137, -0.0223,  0.0259,  0.0028,  0.0119,  0.0285,
         0.0079,  0.0195,  0.0200,  0.0085,  0.0242,  0.0009,  0.0122,  0.0548,
         0.1087,  0.1022,  0.1160,  0.0910,  0.0822,  0.1018,  0.0895,  0.0667,
         0.0392,  0.0431,  0.0753,  0.0004,  0.1266,  0.0038,  0.1204,  0.0957,
         0.1233,  0.1159,  0.0752,  0.1258,  0.1231,  0.1027,  0.0134,  0.1150,
         0.1220, -0.0561,  0.0229,  0.0105,  0.0243, -0.0553,  0.0720, -0.0641,
         0.1112, -0.0639,  0.0985,  0.0425,  0.0927,  0.0387, -0.0287,  0.0446,
         0.0292,  0.0941,  0.1303,  0.0955, -0.0586,  0.1196,  0.1267,  0.0352,
         0.0565,  0.1086,  0.0478, -0.0019,  0.1083,  0.0401,  0.0709, -0.0537,
         0.1055,  0.1303, -0.0445,  0.1135,  0.1409, -0.0578, -0.0312,  0.0686,
         0.1033, -0.0387,  0.0732,  0.1194,  0.1117,  0.0307,  0.1070,  0.1241,
         0.1252,  0.1307,  0.0906,  0.0924,  0.1162,  0.1264, -0.0049, -0.0611,
         0.0734,  0.0730,  0.1304,  0.0991,  0.0612,  0.0018,  0.1016,  0.1363,
         0.0942, -0.0309,  0.1114, -0.0543,  0.1252,  0.0535,  0.1179,  0.1177,
         0.0850,  0.1280,  0.1001,  0.1279,  0.0761,  0.1320,  0.1056,  0.0723,
         0.0764,  0.0648,  0.1038,  0.1236,  0.0538,  0.0424,  0.0344,  0.1006,
         0.1021,  0.1107,  0.1169,  0.1093,  0.0445,  0.1146, -0.0489,  0.0971,
         0.0652,  0.0386,  0.0098,  0.1207,  0.1175,  0.1328, -0.0297,  0.1073,
         0.1332,  0.0717,  0.1108,  0.1229,  0.1180,  0.1475,  0.0883,  0.1086,
         0.0652,  0.1230,  0.0948,  0.0441,  0.1016,  0.1151,  0.0287,  0.1136,
         0.0735,  0.0414,  0.1135,  0.0824,  0.1280,  0.1333,  0.0746,  0.0977,
         0.1084,  0.1239,  0.0953,  0.0810,  0.1309,  0.1012,  0.1036,  0.1226,
         0.0869,  0.1093,  0.1108,  0.1223,  0.1107,  0.0989,  0.0272,  0.0982,
         0.1087,  0.0130,  0.1091,  0.1118,  0.0313,  0.1056,  0.0909,  0.0134,
         0.1373,  0.0740,  0.0978,  0.0483,  0.1211,  0.0944,  0.0584,  0.1326,
         0.1351,  0.1285,  0.0878,  0.1100,  0.1073,  0.0790,  0.1073,  0.0461,
         0.1190,  0.0502,  0.1289,  0.0526,  0.1311,  0.0592,  0.0868,  0.1093,
         0.0838,  0.0870,  0.1138,  0.0552,  0.1018,  0.0899,  0.1382,  0.0975,
         0.0596,  0.1134,  0.0941,  0.0865,  0.1115,  0.1014,  0.0907,  0.1441,
         0.1187,  0.0949,  0.0410,  0.0623,  0.0415,  0.0958,  0.0949,  0.0347,
         0.1150,  0.0310,  0.1251,  0.0821,  0.0245,  0.1200,  0.0690,  0.1201,
         0.0871,  0.0517,  0.0889,  0.1269,  0.1351,  0.1135,  0.1262,  0.0394,
         0.0626,  0.1174,  0.0998,  0.1104,  0.1152,  0.1133,  0.1358,  0.1320,
         0.1022,  0.1007,  0.1073,  0.0085,  0.1114,  0.1124,  0.1151,  0.0921],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.0509, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:22,218 :: INFO :: Epoch 40: loss tensor(119.4157, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 5.2258e-02,  2.5886e-02,  2.2224e-02,  5.7203e-02,  3.2748e-02,
         6.9685e-03,  4.9967e-02,  4.9221e-02,  1.9263e-02,  2.5301e-02,
         3.3163e-02, -1.0389e-02,  3.8085e-02,  4.8632e-02,  3.8352e-02,
         1.5544e-02,  1.6866e-02,  5.1985e-02,  1.8198e-02,  4.8314e-02,
         2.1030e-02,  3.1056e-02,  5.3619e-02,  2.4622e-02,  5.2534e-03,
         2.3075e-02,  2.2405e-02,  4.5241e-02,  8.7794e-02,  1.3209e-02,
         3.7086e-02,  4.4925e-02,  5.7398e-02,  4.2469e-02, -4.2985e-03,
        -2.8465e-02,  3.3516e-02,  3.7921e-02,  6.3776e-04,  4.5186e-02,
        -1.3957e-02,  2.3986e-02,  3.8371e-02,  1.1372e-02,  1.4742e-02,
         1.7864e-02,  3.3741e-02,  1.0897e-02,  3.3932e-02,  2.0826e-02,
         1.5713e-02,  1.5578e-02,  2.9769e-03,  1.7135e-02,  4.1249e-02,
         6.6695e-02,  4.1758e-02,  7.7936e-02,  3.0112e-02,  1.8292e-03,
         2.3741e-02,  3.3124e-02,  1.8093e-03,  2.2106e-02,  4.1212e-02,
         1.8589e-02,  6.9679e-02,  2.0009e-02,  3.7360e-02, -1.1407e-02,
         4.6212e-02,  3.8531e-02,  3.3422e-02,  6.3201e-02,  5.3482e-02,
         7.3102e-05,  6.1031e-02,  1.7640e-02,  4.4384e-02,  5.7272e-02,
         4.9090e-02,  4.5690e-02,  3.9892e-02,  7.9456e-02, -6.9695e-03,
         5.2154e-02,  2.6807e-02,  2.5574e-02,  2.2370e-02, -6.6883e-03,
         1.7013e-02,  6.4714e-02,  2.8232e-02,  4.7089e-02,  2.9936e-02,
         5.7640e-02,  2.5992e-02,  1.1580e-02,  5.2646e-03,  1.0459e-02,
         3.0951e-02,  3.1649e-02,  3.1302e-02,  2.1125e-02,  4.3271e-02,
        -2.5856e-02,  3.0164e-02, -6.2550e-04,  3.2791e-02,  4.9045e-02,
         3.5592e-02,  3.8221e-02,  4.1610e-02,  4.2650e-02, -1.0123e-02,
        -2.1203e-02,  2.9157e-02,  1.1118e-02,  1.1856e-02,  3.4088e-02,
         1.1024e-02,  2.1998e-02,  2.3775e-02,  1.1360e-02,  2.6781e-02,
         3.4698e-03,  1.7821e-02,  5.9766e-02,  1.1877e-01,  1.1116e-01,
         1.2968e-01,  9.1428e-02,  8.2231e-02,  1.0978e-01,  9.7195e-02,
         7.9287e-02,  5.4647e-02,  4.3893e-02,  8.3553e-02,  8.7152e-03,
         1.3838e-01,  1.2438e-02,  1.3314e-01,  1.1907e-01,  1.3789e-01,
         1.2529e-01,  7.5510e-02,  1.4537e-01,  1.3671e-01,  1.1629e-01,
         2.3774e-02,  1.2903e-01,  1.3231e-01, -5.9502e-02,  3.1189e-02,
         2.2430e-02,  3.5566e-02, -5.7786e-02,  7.2434e-02, -6.7579e-02,
         1.1814e-01, -6.7908e-02,  1.0536e-01,  5.3080e-02,  9.7059e-02,
         3.5503e-02, -2.5711e-02,  5.5950e-02,  4.0789e-02,  1.0326e-01,
         1.4340e-01,  9.9243e-02, -6.1744e-02,  1.3307e-01,  1.3969e-01,
         4.9475e-02,  5.6594e-02,  1.1592e-01,  5.7334e-02,  3.5654e-03,
         1.1794e-01,  5.3866e-02,  6.9609e-02, -5.5703e-02,  1.1221e-01,
         1.4468e-01, -4.5016e-02,  1.2335e-01,  1.5600e-01, -6.1240e-02,
        -2.4651e-02,  8.1024e-02,  1.0893e-01, -3.7628e-02,  7.1810e-02,
         1.3144e-01,  1.2825e-01,  2.7716e-02,  1.1809e-01,  1.3994e-01,
         1.3891e-01,  1.4742e-01,  9.7444e-02,  9.4077e-02,  1.2700e-01,
         1.4466e-01,  2.7511e-03, -6.5072e-02,  7.0484e-02,  8.3913e-02,
         1.4255e-01,  1.0309e-01,  6.2003e-02,  9.6015e-03,  1.0880e-01,
         1.5360e-01,  1.0604e-01, -2.7785e-02,  1.1980e-01, -5.6758e-02,
         1.4105e-01,  6.2214e-02,  1.2829e-01,  1.3315e-01,  8.8248e-02,
         1.4282e-01,  1.0667e-01,  1.4446e-01,  7.6963e-02,  1.4470e-01,
         1.1169e-01,  6.8993e-02,  8.0989e-02,  6.2216e-02,  1.0892e-01,
         1.4159e-01,  6.6398e-02,  5.4316e-02,  3.3420e-02,  1.0652e-01,
         1.1149e-01,  1.2560e-01,  1.3439e-01,  1.1946e-01,  5.7905e-02,
         1.2752e-01, -4.9687e-02,  1.0839e-01,  7.7159e-02,  5.1484e-02,
         2.2645e-02,  1.3298e-01,  1.3314e-01,  1.4933e-01, -2.5166e-02,
         1.1747e-01,  1.5721e-01,  9.2773e-02,  1.3549e-01,  1.3489e-01,
         1.3527e-01,  1.6946e-01,  1.0369e-01,  1.2500e-01,  8.7634e-02,
         1.4457e-01,  1.1699e-01,  5.9392e-02,  1.1462e-01,  1.2676e-01,
         4.2148e-02,  1.3139e-01,  9.4120e-02,  5.7682e-02,  1.2636e-01,
         9.7455e-02,  1.4885e-01,  1.5268e-01,  9.1014e-02,  1.1111e-01,
         1.2451e-01,  1.4887e-01,  1.0854e-01,  1.0411e-01,  1.5528e-01,
         1.0852e-01,  1.2038e-01,  1.4029e-01,  9.5040e-02,  1.2811e-01,
         1.3031e-01,  1.3949e-01,  1.3594e-01,  1.1084e-01,  3.8117e-02,
         1.1149e-01,  1.2213e-01,  2.6947e-02,  1.1616e-01,  1.2237e-01,
         4.8958e-02,  1.2880e-01,  1.1600e-01,  2.9396e-02,  1.5829e-01,
         9.6582e-02,  1.0491e-01,  6.6284e-02,  1.3513e-01,  1.1507e-01,
         7.8476e-02,  1.5513e-01,  1.5714e-01,  1.4949e-01,  1.0817e-01,
         1.2117e-01,  1.2204e-01,  1.0250e-01,  1.2193e-01,  6.5230e-02,
         1.3324e-01,  7.1787e-02,  1.4343e-01,  6.9852e-02,  1.5285e-01,
         7.3404e-02,  1.1022e-01,  1.3012e-01,  9.8077e-02,  1.0019e-01,
         1.2963e-01,  7.0787e-02,  1.1935e-01,  1.0390e-01,  1.5966e-01,
         1.0344e-01,  6.7218e-02,  1.2904e-01,  1.1618e-01,  9.1276e-02,
         1.2544e-01,  1.1551e-01,  1.0707e-01,  1.6433e-01,  1.3382e-01,
         1.0728e-01,  5.0479e-02,  8.1299e-02,  5.8011e-02,  1.1226e-01,
         1.0937e-01,  4.9990e-02,  1.3125e-01,  3.9608e-02,  1.4480e-01,
         9.3685e-02,  4.1762e-02,  1.3943e-01,  7.9659e-02,  1.3681e-01,
         1.0875e-01,  7.3475e-02,  1.1184e-01,  1.5201e-01,  1.5519e-01,
         1.2706e-01,  1.4358e-01,  5.7753e-02,  7.5511e-02,  1.3471e-01,
         1.1698e-01,  1.3449e-01,  1.2665e-01,  1.2422e-01,  1.5988e-01,
         1.5583e-01,  1.1372e-01,  1.0504e-01,  1.1744e-01,  2.2693e-02,
         1.2341e-01,  1.3022e-01,  1.3455e-01,  1.0826e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(13.1994, device='cuda:0', grad_fn=<CopyBackwards>)
2023-04-11 14:41:22,249 :: INFO :: mask tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], device='cuda:0')
2023-04-11 14:41:44,405 :: INFO :: Epoch 5: loss tensor(20.5272, device='cuda:0', grad_fn=<AddBackward0>), U.norm 12.451095581054688, V.norm 16.635616302490234, MLP.norm 6.552561283111572
2023-04-11 14:42:16,264 :: INFO :: ----- val -----
2023-04-11 14:42:16,264 :: INFO :: Precision [0.009426864350520026, 0.005744633768532913, 0.0048329276388580835]
2023-04-11 14:42:16,264 :: INFO :: Recall [0.0027266490566328658, 0.009389824011991555, 0.016412702456379897]
2023-04-11 14:42:16,264 :: INFO :: ndcg [0.009426864350520026, 0.01804877382597124, 0.023854171709903293]
2023-04-11 14:42:36,843 :: INFO :: ----- test -----
2023-04-11 14:42:36,843 :: INFO :: Precision [0.005595283585348545, 0.0034281347235111423, 0.0031142529614061353]
2023-04-11 14:42:36,843 :: INFO :: Recall [0.002122106873555476, 0.00789561392346932, 0.015154842209854408]
2023-04-11 14:42:36,843 :: INFO :: ndcg [0.005595283585348545, 0.010882521903631525, 0.015257342939821214]
2023-04-11 14:42:41,623 :: INFO :: Epoch 10: loss tensor(30.1840, device='cuda:0', grad_fn=<AddBackward0>), U.norm 20.899818420410156, V.norm 20.37004280090332, MLP.norm 12.201753616333008
2023-04-11 14:43:03,280 :: INFO :: ----- val -----
2023-04-11 14:43:03,280 :: INFO :: Precision [0.010223500774507635, 0.004275282142066866, 0.005067492808143308]
2023-04-11 14:43:03,280 :: INFO :: Recall [0.0027537201021814502, 0.007056563676442436, 0.017333767603987523]
2023-04-11 14:43:03,280 :: INFO :: ndcg [0.010223500774507635, 0.015546402968639558, 0.02448778829895055]
2023-04-11 14:43:24,717 :: INFO :: ----- test -----
2023-04-11 14:43:24,717 :: INFO :: Precision [0.0059228123805884604, 0.0027949123860472984, 0.0034581581964080277]
2023-04-11 14:43:24,717 :: INFO :: Recall [0.002352384311240523, 0.0070003287672525455, 0.0174043846505559]
2023-04-11 14:43:24,717 :: INFO :: ndcg [0.0059228123805884604, 0.009755928400204832, 0.0161831181696122]
2023-04-11 14:43:30,577 :: INFO :: Epoch 15: loss tensor(36.5848, device='cuda:0', grad_fn=<AddBackward0>), U.norm 28.136281967163086, V.norm 29.10418128967285, MLP.norm 14.205952644348145
2023-04-11 14:43:52,639 :: INFO :: ----- val -----
2023-04-11 14:43:52,639 :: INFO :: Precision [0.010931622040274397, 0.0051604337242753305, 0.004930294312901005]
2023-04-11 14:43:52,639 :: INFO :: Recall [0.0029201163058646647, 0.008552236004498037, 0.016435412916368432]
2023-04-11 14:43:52,639 :: INFO :: ndcg [0.010931622040274397, 0.017445488921638636, 0.024438368773926144]
2023-04-11 14:44:13,811 :: INFO :: ----- test -----
2023-04-11 14:44:13,811 :: INFO :: Precision [0.005977400513128446, 0.003340793711447164, 0.0032288880397400997]
2023-04-11 14:44:13,811 :: INFO :: Recall [0.002153181936018715, 0.007950462889912462, 0.015818125346110695]
2023-04-11 14:44:13,811 :: INFO :: ndcg [0.005977400513128446, 0.01068450297328572, 0.015454248789522217]
2023-04-11 14:44:20,202 :: INFO :: Epoch 20: loss tensor(37.0261, device='cuda:0', grad_fn=<AddBackward0>), U.norm 34.83544158935547, V.norm 46.73408889770508, MLP.norm 14.36330509185791
2023-04-11 14:44:42,155 :: INFO :: ----- val -----
2023-04-11 14:44:42,155 :: INFO :: Precision [0.011108652356716087, 0.006319982296968375, 0.006373091391900708]
2023-04-11 14:44:42,155 :: INFO :: Recall [0.002989084366645073, 0.010154574750012243, 0.021926026327784873]
2023-04-11 14:44:42,155 :: INFO :: ndcg [0.011108652356716087, 0.020342184648813565, 0.02957306666955446]
2023-04-11 14:45:03,077 :: INFO :: ----- test -----
2023-04-11 14:45:03,077 :: INFO :: Precision [0.0066324581036082755, 0.004143239259784923, 0.004181450952562797]
2023-04-11 14:45:03,077 :: INFO :: Recall [0.0025716909521586047, 0.009592924184797527, 0.02006861355519425]
2023-04-11 14:45:03,077 :: INFO :: ndcg [0.0066324581036082755, 0.012895974663466406, 0.01930151028078666]
2023-04-11 14:45:08,608 :: INFO :: Epoch 25: loss tensor(35.2542, device='cuda:0', grad_fn=<AddBackward0>), U.norm 42.134132385253906, V.norm 74.08811950683594, MLP.norm 14.143243789672852
2023-04-11 14:45:30,374 :: INFO :: ----- val -----
2023-04-11 14:45:30,374 :: INFO :: Precision [0.011241425094047356, 0.009524231024562804, 0.007731799070590608]
2023-04-11 14:45:30,374 :: INFO :: Recall [0.0030572157702780435, 0.01539080025936558, 0.02647992217661038]
2023-04-11 14:45:30,374 :: INFO :: ndcg [0.011241425094047356, 0.027274186974225012, 0.036015232338668074]
2023-04-11 14:45:50,733 :: INFO :: ----- test -----
2023-04-11 14:45:50,733 :: INFO :: Precision [0.007014575031388176, 0.005971941699874351, 0.0050739669195915185]
2023-04-11 14:45:50,733 :: INFO :: Recall [0.002697840067196068, 0.01373663567269854, 0.024409650503749804]
2023-04-11 14:45:50,733 :: INFO :: ndcg [0.007014575031388176, 0.017174939468364365, 0.023591769240013317]
2023-04-11 14:45:56,249 :: INFO :: Epoch 30: loss tensor(33.7505, device='cuda:0', grad_fn=<AddBackward0>), U.norm 50.409847259521484, V.norm 108.38743591308594, MLP.norm 13.657752990722656
2023-04-11 14:46:17,936 :: INFO :: ----- val -----
2023-04-11 14:46:17,936 :: INFO :: Precision [0.012126576676255808, 0.011666297853507158, 0.008816109758795907]
2023-04-11 14:46:17,936 :: INFO :: Recall [0.0031335180825596983, 0.02036949815161827, 0.03010272261793727]
2023-04-11 14:46:17,936 :: INFO :: ndcg [0.012126576676255808, 0.03303117155720429, 0.04150471530424993]
2023-04-11 14:46:39,061 :: INFO :: ----- test -----
2023-04-11 14:46:39,061 :: INFO :: Precision [0.00742398602543807, 0.007533162290517861, 0.005709918663682319]
2023-04-11 14:46:39,061 :: INFO :: Recall [0.0029850558832732964, 0.017797657611506083, 0.026970783842767434]
2023-04-11 14:46:39,061 :: INFO :: ndcg [0.00742398602543807, 0.021129973751593492, 0.02698387661686974]
2023-04-11 14:46:44,967 :: INFO :: Epoch 35: loss tensor(34.9929, device='cuda:0', grad_fn=<AddBackward0>), U.norm 59.454586029052734, V.norm 145.64039611816406, MLP.norm 13.176545143127441
2023-04-11 14:47:07,124 :: INFO :: ----- val -----
2023-04-11 14:47:07,124 :: INFO :: Precision [0.014427970789997788, 0.01321531312237186, 0.01011285682673122]
2023-04-11 14:47:07,124 :: INFO :: Recall [0.003904226040867172, 0.023522660871181406, 0.03460105174265675]
2023-04-11 14:47:07,124 :: INFO :: ndcg [0.014427970789997788, 0.03874598771091059, 0.04828248092062977]
2023-04-11 14:47:28,373 :: INFO :: ----- test -----
2023-04-11 14:47:28,373 :: INFO :: Precision [0.00892515967028768, 0.008870571537747438, 0.006722528522299005]
2023-04-11 14:47:28,373 :: INFO :: Recall [0.003528396951622332, 0.021545631162565238, 0.031793399897824655]
2023-04-11 14:47:28,373 :: INFO :: ndcg [0.00892515967028768, 0.025826465594379663, 0.03224597293242081]
2023-04-11 14:47:34,140 :: INFO :: Epoch 40: loss tensor(35.0245, device='cuda:0', grad_fn=<AddBackward0>), U.norm 68.70018768310547, V.norm 182.2181854248047, MLP.norm 12.828021049499512
2023-04-11 14:47:56,483 :: INFO :: ----- val -----
2023-04-11 14:47:56,483 :: INFO :: Precision [0.01531312237220624, 0.014188979862801111, 0.011391900863022391]
2023-04-11 14:47:56,483 :: INFO :: Recall [0.00459965194554772, 0.025610548991999812, 0.03807728053121988]
2023-04-11 14:47:56,483 :: INFO :: ndcg [0.01531312237220624, 0.04123892938427431, 0.052832160832616014]
2023-04-11 14:48:17,686 :: INFO :: ----- test -----
2023-04-11 14:48:17,686 :: INFO :: Precision [0.010289862983787324, 0.00988045198973712, 0.007486762377858885]
2023-04-11 14:48:17,686 :: INFO :: Recall [0.004499376705098081, 0.024145673200859294, 0.0345864687256831]
2023-04-11 14:48:17,686 :: INFO :: ndcg [0.010289862983787324, 0.029024808607862784, 0.036027314327961246]
2023-04-11 14:48:23,890 :: INFO :: Epoch 45: loss tensor(37.9771, device='cuda:0', grad_fn=<AddBackward0>), U.norm 77.82238006591797, V.norm 216.4127960205078, MLP.norm 12.258986473083496
2023-04-11 14:48:46,312 :: INFO :: ----- val -----
2023-04-11 14:48:46,312 :: INFO :: Precision [0.01597698605886258, 0.015074131445009513, 0.012259349413586825]
2023-04-11 14:48:46,312 :: INFO :: Recall [0.005234345136861629, 0.027218380564027433, 0.0411436155546918]
2023-04-11 14:48:46,312 :: INFO :: ndcg [0.01597698605886258, 0.04362761722219858, 0.056052513193374495]
2023-04-11 14:49:06,233 :: INFO :: ----- test -----
2023-04-11 14:49:06,233 :: INFO :: Precision [0.01119056717069709, 0.010568262459740905, 0.008040831923139858]
2023-04-11 14:49:06,233 :: INFO :: Recall [0.005083288006401595, 0.025941714506890583, 0.037349580694985195]
2023-04-11 14:49:06,233 :: INFO :: ndcg [0.01119056717069709, 0.030955727295022314, 0.038683666853379074]
2023-04-11 14:49:11,468 :: INFO :: Epoch 50: loss tensor(39.2168, device='cuda:0', grad_fn=<AddBackward0>), U.norm 86.4638671875, V.norm 246.8034210205078, MLP.norm 11.89857006072998
2023-04-11 14:49:31,952 :: INFO :: ----- val -----
2023-04-11 14:49:31,952 :: INFO :: Precision [0.017348971011285682, 0.015950431511395836, 0.013020579774286233]
2023-04-11 14:49:31,952 :: INFO :: Recall [0.006097745681961666, 0.029272314744996128, 0.044361457244435444]
2023-04-11 14:49:31,952 :: INFO :: ndcg [0.017348971011285682, 0.045952319470624316, 0.05916319507247214]
2023-04-11 14:49:52,593 :: INFO :: ----- test -----
2023-04-11 14:49:52,609 :: INFO :: Precision [0.012637152683006714, 0.011212402423712698, 0.008633113161198828]
2023-04-11 14:49:52,609 :: INFO :: Recall [0.005868611685678997, 0.02819480567053259, 0.04116016609432302]
2023-04-11 14:49:52,609 :: INFO :: ndcg [0.012637152683006714, 0.03316566984698877, 0.04165139962386801]
2023-04-11 14:49:57,859 :: INFO :: Epoch 55: loss tensor(40.3019, device='cuda:0', grad_fn=<AddBackward0>), U.norm 94.45633697509766, V.norm 272.6502380371094, MLP.norm 11.594002723693848
2023-04-11 14:50:18,155 :: INFO :: ----- val -----
2023-04-11 14:50:18,155 :: INFO :: Precision [0.01712768311573357, 0.016100907280371262, 0.01341447222836908]
2023-04-11 14:50:18,155 :: INFO :: Recall [0.00614402484806916, 0.029692553805563682, 0.046068248524515104]
2023-04-11 14:50:18,155 :: INFO :: ndcg [0.01712768311573357, 0.046218678942762846, 0.06047135124809206]
2023-04-11 14:50:38,233 :: INFO :: ----- test -----
2023-04-11 14:50:38,233 :: INFO :: Precision [0.012882799279436651, 0.011452590206888623, 0.00886784213112081]
2023-04-11 14:50:38,233 :: INFO :: Recall [0.006111414414321132, 0.02878901239626683, 0.042256728606771154]
2023-04-11 14:50:38,233 :: INFO :: ndcg [0.012882799279436651, 0.03370611254654156, 0.042557091692899084]
2023-04-11 14:50:43,484 :: INFO :: Epoch 60: loss tensor(41.9557, device='cuda:0', grad_fn=<AddBackward0>), U.norm 101.7737808227539, V.norm 294.065673828125, MLP.norm 11.308511734008789
2023-04-11 14:51:03,421 :: INFO :: ----- val -----
2023-04-11 14:51:03,421 :: INFO :: Precision [0.018366895330825403, 0.016269086080990862, 0.01380836468245189]
2023-04-11 14:51:03,421 :: INFO :: Recall [0.0067645048469838695, 0.030125487671012148, 0.048195524997415244]
2023-04-11 14:51:03,421 :: INFO :: ndcg [0.018366895330825403, 0.04731801871175586, 0.06229340115672276]
2023-04-11 14:51:23,577 :: INFO :: ----- test -----
2023-04-11 14:51:23,577 :: INFO :: Precision [0.013865385665156394, 0.011769201375620523, 0.009138053387193798]
2023-04-11 14:51:23,577 :: INFO :: Recall [0.00659959041127838, 0.02992150520708868, 0.04388309715342678]
2023-04-11 14:51:23,577 :: INFO :: ndcg [0.013865385665156394, 0.03494429803331103, 0.04405595561880293]
2023-04-11 14:51:28,468 :: INFO :: Epoch 65: loss tensor(44.2964, device='cuda:0', grad_fn=<AddBackward0>), U.norm 108.51793670654297, V.norm 311.9453125, MLP.norm 11.079216003417969
2023-04-11 14:51:48,906 :: INFO :: ----- val -----
2023-04-11 14:51:48,906 :: INFO :: Precision [0.019207789333923433, 0.01637530427085587, 0.013905731356494823]
2023-04-11 14:51:48,906 :: INFO :: Recall [0.007344992460358769, 0.030680667701796848, 0.04964430932229283]
2023-04-11 14:51:48,906 :: INFO :: ndcg [0.019207789333923433, 0.04814999790744107, 0.06323675438050981]
2023-04-11 14:52:09,109 :: INFO :: ----- test -----
2023-04-11 14:52:09,109 :: INFO :: Precision [0.014793383918336154, 0.012063977291336427, 0.009468311589060781]
2023-04-11 14:52:09,109 :: INFO :: Recall [0.007494774107752659, 0.0312797347035379, 0.046569535231859804]
2023-04-11 14:52:09,109 :: INFO :: ndcg [0.014793383918336154, 0.036068942541621664, 0.04571882054859854]
2023-04-11 14:52:14,202 :: INFO :: Epoch 70: loss tensor(43.1571, device='cuda:0', grad_fn=<AddBackward0>), U.norm 114.66687774658203, V.norm 326.2845153808594, MLP.norm 10.879875183105469
2023-04-11 14:52:34,687 :: INFO :: ----- val -----
2023-04-11 14:52:34,687 :: INFO :: Precision [0.01880947112192963, 0.016516928524009215, 0.014007523788448824]
2023-04-11 14:52:34,687 :: INFO :: Recall [0.00726323982905101, 0.031004268501112858, 0.05077432149753089]
2023-04-11 14:52:34,687 :: INFO :: ndcg [0.01880947112192963, 0.04829600615307544, 0.06371712468917368]
2023-04-11 14:52:55,468 :: INFO :: ----- test -----
2023-04-11 14:52:55,468 :: INFO :: Precision [0.015311971177466018, 0.012184071182924389, 0.009716687592117766]
2023-04-11 14:52:55,468 :: INFO :: Recall [0.007784778079605754, 0.031844744533056926, 0.04865125843793864]
2023-04-11 14:52:55,468 :: INFO :: ndcg [0.015311971177466018, 0.036834187174716405, 0.047085929043108106]
2023-04-11 14:53:01,515 :: INFO :: Epoch 75: loss tensor(43.0374, device='cuda:0', grad_fn=<AddBackward0>), U.norm 120.33415985107422, V.norm 338.60260009765625, MLP.norm 10.635605812072754
2023-04-11 14:53:21,905 :: INFO :: ----- val -----
2023-04-11 14:53:21,905 :: INFO :: Precision [0.01996016817880062, 0.016463819429076713, 0.014087187430847598]
2023-04-11 14:53:21,905 :: INFO :: Recall [0.007724121004435652, 0.031136529788346023, 0.05194604344313953]
2023-04-11 14:53:21,905 :: INFO :: ndcg [0.01996016817880062, 0.04852327592285259, 0.06426816477138006]
2023-04-11 14:53:42,421 :: INFO :: ----- test -----
2023-04-11 14:53:42,421 :: INFO :: Precision [0.01561220590643594, 0.012347835580544338, 0.009861346143348748]
2023-04-11 14:53:42,421 :: INFO :: Recall [0.008338540824383615, 0.032501431793949966, 0.049747270189299926]
2023-04-11 14:53:42,421 :: INFO :: ndcg [0.01561220590643594, 0.03738487618456219, 0.047806608224545785]
2023-04-11 14:53:47,843 :: INFO :: Epoch 80: loss tensor(46.9132, device='cuda:0', grad_fn=<AddBackward0>), U.norm 125.48605346679688, V.norm 348.5749206542969, MLP.norm 10.392645835876465
2023-04-11 14:54:08,327 :: INFO :: ----- val -----
2023-04-11 14:54:08,327 :: INFO :: Precision [0.019517592387696393, 0.016401858818322124, 0.0139322859039611]
2023-04-11 14:54:08,327 :: INFO :: Recall [0.007833635976198915, 0.03153825484571379, 0.05154096572383927]
2023-04-11 14:54:08,327 :: INFO :: ndcg [0.019517592387696393, 0.04824568165456126, 0.06373075668514083]
2023-04-11 14:54:28,843 :: INFO :: ----- test -----
2023-04-11 14:54:28,843 :: INFO :: Precision [0.015694088105245918, 0.012495223538402286, 0.009940498935531747]
2023-04-11 14:54:28,843 :: INFO :: Recall [0.008653805943641426, 0.03325126472817624, 0.050740250600422795]
2023-04-11 14:54:28,843 :: INFO :: ndcg [0.015694088105245918, 0.03800610480479287, 0.048356259343949846]
2023-04-11 14:54:33,874 :: INFO :: Epoch 85: loss tensor(48.1768, device='cuda:0', grad_fn=<AddBackward0>), U.norm 130.2052764892578, V.norm 356.3014831542969, MLP.norm 10.340903282165527
2023-04-11 14:54:54,249 :: INFO :: ----- val -----
2023-04-11 14:54:54,249 :: INFO :: Precision [0.020048683337021464, 0.016481522460720887, 0.014060632883381347]
2023-04-11 14:54:54,249 :: INFO :: Recall [0.007822563183338115, 0.03159642462627701, 0.052454661941832674]
2023-04-11 14:54:54,249 :: INFO :: ndcg [0.020048683337021464, 0.04876497915145394, 0.06460147502406687]
2023-04-11 14:55:14,859 :: INFO :: ----- test -----
2023-04-11 14:55:14,859 :: INFO :: Precision [0.015475735575085976, 0.012511599978164282, 0.009948687155412745]
2023-04-11 14:55:14,859 :: INFO :: Recall [0.008435028839117282, 0.03354393432150361, 0.0514004319171631]
2023-04-11 14:55:14,859 :: INFO :: ndcg [0.015475735575085976, 0.03808248010314654, 0.04857223077365428]
2023-04-11 14:55:20,952 :: INFO :: Epoch 90: loss tensor(47.9197, device='cuda:0', grad_fn=<AddBackward0>), U.norm 134.5854034423828, V.norm 362.6773986816406, MLP.norm 10.129890441894531
2023-04-11 14:55:41,030 :: INFO :: ----- val -----
2023-04-11 14:55:41,030 :: INFO :: Precision [0.020181456074352733, 0.016481522460720884, 0.014144722283691156]
2023-04-11 14:55:41,030 :: INFO :: Recall [0.007891974524800794, 0.03176613050450694, 0.05245872095027936]
2023-04-11 14:55:41,030 :: INFO :: ndcg [0.020181456074352733, 0.04907403755657224, 0.06486104552050911]
2023-04-11 14:56:01,093 :: INFO :: ----- test -----
2023-04-11 14:56:01,093 :: INFO :: Precision [0.015694088105245918, 0.012533435231180275, 0.010016922321087746]
2023-04-11 14:56:01,093 :: INFO :: Recall [0.008635606291370697, 0.033786416605855005, 0.051885513111580926]
2023-04-11 14:56:01,093 :: INFO :: ndcg [0.015694088105245918, 0.0382649268897233, 0.04894499458215072]
2023-04-11 14:56:06,171 :: INFO :: Epoch 95: loss tensor(44.8375, device='cuda:0', grad_fn=<AddBackward0>), U.norm 138.6709442138672, V.norm 368.07611083984375, MLP.norm 9.985416412353516
2023-04-11 14:56:26,437 :: INFO :: ----- val -----
2023-04-11 14:56:26,437 :: INFO :: Precision [0.01965036512502766, 0.01649037397654297, 0.014109316220402818]
2023-04-11 14:56:26,437 :: INFO :: Recall [0.0077563481414647415, 0.03210398366786978, 0.052873320847548186]
2023-04-11 14:56:26,437 :: INFO :: ndcg [0.01965036512502766, 0.04896165525898405, 0.06473119383544071]
2023-04-11 14:56:46,499 :: INFO :: ----- test -----
2023-04-11 14:56:46,499 :: INFO :: Precision [0.015803264370325892, 0.012467929472132296, 0.01006605164037374]
2023-04-11 14:56:46,499 :: INFO :: Recall [0.008975675458373005, 0.033781962006103144, 0.052364317430254756]
2023-04-11 14:56:46,499 :: INFO :: ndcg [0.015803264370325892, 0.03831460665467213, 0.049204416921215625]
2023-04-11 14:56:51,234 :: INFO :: Epoch 100: loss tensor(50.0672, device='cuda:0', grad_fn=<AddBackward0>), U.norm 142.61447143554688, V.norm 372.25592041015625, MLP.norm 10.004242897033691
2023-04-11 14:57:11,405 :: INFO :: ----- val -----
2023-04-11 14:57:11,405 :: INFO :: Precision [0.020535516707236114, 0.016658552777162564, 0.014078335915025508]
2023-04-11 14:57:11,405 :: INFO :: Recall [0.008129902862419333, 0.03223376562488129, 0.05259627367043654]
2023-04-11 14:57:11,405 :: INFO :: ndcg [0.020535516707236114, 0.04960101092920426, 0.06496775812234615]
2023-04-11 14:57:31,343 :: INFO :: ----- test -----
2023-04-11 14:57:31,343 :: INFO :: Precision [0.016212675364375786, 0.01255527048419627, 0.010090616300016737]
2023-04-11 14:57:31,343 :: INFO :: Recall [0.009158287857367943, 0.03418187723757809, 0.05346251291391485]
2023-04-11 14:57:31,343 :: INFO :: ndcg [0.016212675364375786, 0.0386695984450583, 0.04954369213906383]
2023-04-11 14:57:36,468 :: INFO :: Epoch 105: loss tensor(47.7438, device='cuda:0', grad_fn=<AddBackward0>), U.norm 146.2319793701172, V.norm 375.4200134277344, MLP.norm 10.00153636932373
2023-04-11 14:57:56,358 :: INFO :: ----- val -----
2023-04-11 14:57:56,358 :: INFO :: Precision [0.01996016817880062, 0.016570037618941727, 0.013865899535295444]
2023-04-11 14:57:56,358 :: INFO :: Recall [0.007989934967217976, 0.03220798961410077, 0.05249626955590878]
2023-04-11 14:57:56,358 :: INFO :: ndcg [0.01996016817880062, 0.049130776283937555, 0.06416077285638232]
2023-04-11 14:58:16,344 :: INFO :: ----- test -----
2023-04-11 14:58:16,344 :: INFO :: Precision [0.015803264370325892, 0.012478847098640295, 0.010085157486762739]
2023-04-11 14:58:16,344 :: INFO :: Recall [0.00906646390723873, 0.03381348271928959, 0.0535570048837557]
2023-04-11 14:58:16,344 :: INFO :: ndcg [0.015803264370325892, 0.0383244626119424, 0.04940440481595701]
2023-04-11 14:58:21,390 :: INFO :: Epoch 110: loss tensor(49.6100, device='cuda:0', grad_fn=<AddBackward0>), U.norm 149.69161987304688, V.norm 377.9937438964844, MLP.norm 9.862857818603516
2023-04-11 14:58:40,937 :: INFO :: ----- val -----
2023-04-11 14:58:40,937 :: INFO :: Precision [0.02026997123257358, 0.016587740650585887, 0.01391015711440588]
2023-04-11 14:58:40,937 :: INFO :: Recall [0.007842068278081094, 0.0324259512532078, 0.05264912998175385]
2023-04-11 14:58:40,937 :: INFO :: ndcg [0.02026997123257358, 0.049416343432677055, 0.06455607590421525]
2023-04-11 14:59:00,687 :: INFO :: ----- test -----
2023-04-11 14:59:00,687 :: INFO :: Precision [0.01607620503302582, 0.012664446749276236, 0.010167039685572738]
2023-04-11 14:59:00,687 :: INFO :: Recall [0.009388380286254296, 0.03487252145040133, 0.054339914335517284]
2023-04-11 14:59:00,687 :: INFO :: ndcg [0.01607620503302582, 0.03898051237417681, 0.05002294877884972]
2023-04-11 14:59:05,749 :: INFO :: Epoch 115: loss tensor(47.1706, device='cuda:0', grad_fn=<AddBackward0>), U.norm 153.02284240722656, V.norm 380.3173522949219, MLP.norm 9.578078269958496
2023-04-11 14:59:25,187 :: INFO :: ----- val -----
2023-04-11 14:59:25,187 :: INFO :: Precision [0.019827395441469352, 0.016561186103119633, 0.013848196503651282]
2023-04-11 14:59:25,187 :: INFO :: Recall [0.00796817034521475, 0.032415118364525826, 0.0529490307084878]
2023-04-11 14:59:25,187 :: INFO :: ndcg [0.019827395441469352, 0.04908999358620891, 0.06419993241748494]
2023-04-11 14:59:45,484 :: INFO :: ----- test -----
2023-04-11 14:59:45,484 :: INFO :: Precision [0.015830558436595882, 0.012582564550466264, 0.010210710191604739]
2023-04-11 14:59:45,484 :: INFO :: Recall [0.009395545268834765, 0.03457017657130291, 0.05467715509870187]
2023-04-11 14:59:45,484 :: INFO :: ndcg [0.015830558436595882, 0.03855138628644079, 0.04990170760238748]
2023-04-11 14:59:50,405 :: INFO :: Epoch 120: loss tensor(51.4396, device='cuda:0', grad_fn=<AddBackward0>), U.norm 156.2386474609375, V.norm 382.19427490234375, MLP.norm 9.52497386932373
2023-04-11 15:00:10,187 :: INFO :: ----- val -----
2023-04-11 15:00:10,187 :: INFO :: Precision [0.020225713653463154, 0.016623146713874225, 0.013963266209338404]
2023-04-11 15:00:10,187 :: INFO :: Recall [0.007914911235827406, 0.0328466905154467, 0.053378957808843566]
2023-04-11 15:00:10,187 :: INFO :: ndcg [0.020225713653463154, 0.049565030997492994, 0.06474795888002402]
2023-04-11 15:00:30,312 :: INFO :: ----- test -----
2023-04-11 15:00:30,312 :: INFO :: Precision [0.01607620503302582, 0.01259348217697426, 0.010216169004858736]
2023-04-11 15:00:30,312 :: INFO :: Recall [0.009242525202063945, 0.03466121170732128, 0.054774013588998666]
2023-04-11 15:00:30,312 :: INFO :: ndcg [0.01607620503302582, 0.03871913316784146, 0.05004548618599855]
2023-04-11 15:00:35,483 :: INFO :: Epoch 125: loss tensor(50.3174, device='cuda:0', grad_fn=<AddBackward0>), U.norm 159.2787628173828, V.norm 383.67626953125, MLP.norm 9.450550079345703
2023-04-11 15:00:55,327 :: INFO :: ----- val -----
2023-04-11 15:00:55,327 :: INFO :: Precision [0.02009294091613189, 0.016623146713874225, 0.013857048019473373]
2023-04-11 15:00:55,327 :: INFO :: Recall [0.008021900394992908, 0.03279684862474301, 0.05338362534345375]
2023-04-11 15:00:55,327 :: INFO :: ndcg [0.02009294091613189, 0.0494413744072963, 0.06454617080684515]
2023-04-11 15:01:15,342 :: INFO :: ----- test -----
2023-04-11 15:01:15,342 :: INFO :: Precision [0.015912440635405863, 0.012697199628800226, 0.010251651291009727]
2023-04-11 15:01:15,342 :: INFO :: Recall [0.009308519780281238, 0.03511696039270859, 0.055291453793732945]
2023-04-11 15:01:15,342 :: INFO :: ndcg [0.015912440635405863, 0.03902652033052808, 0.050211188424118196]
2023-04-11 15:01:20,483 :: INFO :: Epoch 130: loss tensor(50.9750, device='cuda:0', grad_fn=<AddBackward0>), U.norm 162.25509643554688, V.norm 384.57415771484375, MLP.norm 9.488091468811035
2023-04-11 15:01:40,874 :: INFO :: ----- val -----
2023-04-11 15:01:40,874 :: INFO :: Precision [0.020314228811684, 0.01645496791325463, 0.013830493472007101]
2023-04-11 15:01:40,874 :: INFO :: Recall [0.008043947911831955, 0.03249591685337372, 0.053508346410497384]
2023-04-11 15:01:40,874 :: INFO :: ndcg [0.020314228811684, 0.0491069940551393, 0.06445079792522397]
2023-04-11 15:02:00,733 :: INFO :: ----- test -----
2023-04-11 15:02:00,733 :: INFO :: Precision [0.016021616900485834, 0.012828211146896185, 0.01031442764343072]
2023-04-11 15:02:00,733 :: INFO :: Recall [0.009451888019920673, 0.03553199840706468, 0.055679133252563205]
2023-04-11 15:02:00,733 :: INFO :: ndcg [0.016021616900485834, 0.03926677116822631, 0.05044036130735607]
2023-04-11 15:02:06,077 :: INFO :: Epoch 135: loss tensor(50.4606, device='cuda:0', grad_fn=<AddBackward0>), U.norm 165.07130432128906, V.norm 385.20953369140625, MLP.norm 9.393136024475098
2023-04-11 15:02:26,249 :: INFO :: ----- val -----
2023-04-11 15:02:26,249 :: INFO :: Precision [0.01947333480858597, 0.016401858818322124, 0.0137375525558752]
2023-04-11 15:02:26,249 :: INFO :: Recall [0.007903242866833922, 0.03247778802738697, 0.05364398404780182]
2023-04-11 15:02:26,249 :: INFO :: ndcg [0.01947333480858597, 0.04870308533492207, 0.06390415348307514]
2023-04-11 15:02:46,452 :: INFO :: ----- test -----
2023-04-11 15:02:46,452 :: INFO :: Precision [0.015912440635405863, 0.012910093345706159, 0.010251651291009727]
2023-04-11 15:02:46,452 :: INFO :: Recall [0.009324717747000088, 0.035844171965971575, 0.05561471781325716]
2023-04-11 15:02:46,452 :: INFO :: ndcg [0.015912440635405863, 0.03931011590834003, 0.05021674726308336]
2023-04-11 15:02:51,796 :: INFO :: Epoch 140: loss tensor(53.0610, device='cuda:0', grad_fn=<AddBackward0>), U.norm 167.84657287597656, V.norm 385.6908874511719, MLP.norm 9.32092571258545
2023-04-11 15:03:12,031 :: INFO :: ----- val -----
2023-04-11 15:03:12,031 :: INFO :: Precision [0.01876521354281921, 0.01641071033414421, 0.013746404071697292]
2023-04-11 15:03:12,031 :: INFO :: Recall [0.008128250701774243, 0.03273557721680303, 0.053973993710724075]
2023-04-11 15:03:12,031 :: INFO :: ndcg [0.01876521354281921, 0.04843844423442129, 0.06383154968687715]
2023-04-11 15:03:32,124 :: INFO :: ----- test -----
2023-04-11 15:03:32,124 :: INFO :: Precision [0.01550302964135597, 0.01281729352038819, 0.010254380697636721]
2023-04-11 15:03:32,124 :: INFO :: Recall [0.009508140092655397, 0.03553430937911757, 0.05594778920255676]
2023-04-11 15:03:32,124 :: INFO :: ndcg [0.01550302964135597, 0.03908854242279604, 0.0501164350660469]
2023-04-11 15:03:36,718 :: INFO :: Epoch 145: loss tensor(51.9074, device='cuda:0', grad_fn=<AddBackward0>), U.norm 170.47872924804688, V.norm 385.9593200683594, MLP.norm 9.408304214477539
2023-04-11 15:03:56,671 :: INFO :: ----- val -----
2023-04-11 15:03:56,671 :: INFO :: Precision [0.018189865014383712, 0.016331046691745454, 0.013799513166629812]
2023-04-11 15:03:56,671 :: INFO :: Recall [0.007923026136071694, 0.032588964937791585, 0.05396078761423936]
2023-04-11 15:03:56,671 :: INFO :: ndcg [0.018189865014383712, 0.04812999276351581, 0.06367707458816009]
2023-04-11 15:04:15,562 :: INFO :: ----- test -----
2023-04-11 15:04:15,562 :: INFO :: Precision [0.014929854249686118, 0.012784540640864199, 0.010287133577160722]
2023-04-11 15:04:15,562 :: INFO :: Recall [0.00924236412235134, 0.03555783270097628, 0.05630858337995888]
2023-04-11 15:04:15,562 :: INFO :: ndcg [0.014929854249686118, 0.03888964266934764, 0.05001930079294082]
2023-04-11 15:04:20,312 :: INFO :: Epoch 150: loss tensor(50.9957, device='cuda:0', grad_fn=<AddBackward0>), U.norm 173.01145935058594, V.norm 386.2641296386719, MLP.norm 9.530111312866211
2023-04-11 15:04:39,374 :: INFO :: ----- val -----
2023-04-11 15:04:39,374 :: INFO :: Precision [0.017526001327727372, 0.016251383049346698, 0.01379508740871877]
2023-04-11 15:04:39,390 :: INFO :: Recall [0.007710193331502036, 0.03233836993108188, 0.054081542420190865]
2023-04-11 15:04:39,390 :: INFO :: ndcg [0.017526001327727372, 0.04767618059733761, 0.06340336431771419]
2023-04-11 15:04:59,687 :: INFO :: ----- test -----
2023-04-11 15:04:59,687 :: INFO :: Precision [0.014684207653256181, 0.012773623014356202, 0.010330804083192717]
2023-04-11 15:04:59,687 :: INFO :: Recall [0.009198763425704198, 0.03568940058934543, 0.05643318876236897]
2023-04-11 15:04:59,687 :: INFO :: ndcg [0.014684207653256181, 0.03885680678608399, 0.050108596154888205]
2023-04-11 15:05:05,296 :: INFO :: Epoch 155: loss tensor(48.9829, device='cuda:0', grad_fn=<AddBackward0>), U.norm 175.53123474121094, V.norm 386.3345031738281, MLP.norm 9.397186279296875
2023-04-11 15:05:26,390 :: INFO :: ----- val -----
2023-04-11 15:05:26,390 :: INFO :: Precision [0.01743748616950653, 0.016375304270855876, 0.013812790440362939]
2023-04-11 15:05:26,390 :: INFO :: Recall [0.0077597950931197306, 0.032671338826497934, 0.05430647330703477]
2023-04-11 15:05:26,390 :: INFO :: ndcg [0.01743748616950653, 0.048013009356440525, 0.06345637419157744]
2023-04-11 15:05:47,686 :: INFO :: ----- test -----
2023-04-11 15:05:47,686 :: INFO :: Precision [0.014165620394126317, 0.012789999454118198, 0.010281674763906713]
2023-04-11 15:05:47,686 :: INFO :: Recall [0.008896309330739594, 0.0357332243953833, 0.05629686297460226]
2023-04-11 15:05:47,686 :: INFO :: ndcg [0.014165620394126317, 0.03873622182644314, 0.04991501874189487]
2023-04-11 15:05:53,280 :: INFO :: Epoch 160: loss tensor(54.1189, device='cuda:0', grad_fn=<AddBackward0>), U.norm 177.9518280029297, V.norm 386.19171142578125, MLP.norm 9.372292518615723
2023-04-11 15:06:13,407 :: INFO :: ----- val -----
2023-04-11 15:06:13,407 :: INFO :: Precision [0.017570258906837798, 0.01628678911263503, 0.013843770745740246]
2023-04-11 15:06:13,407 :: INFO :: Recall [0.00795497548745923, 0.032757510036604626, 0.05421489915208673]
2023-04-11 15:06:13,407 :: INFO :: ndcg [0.017570258906837798, 0.04789846332793798, 0.06380339126685997]
2023-04-11 15:06:34,267 :: INFO :: ----- test -----
2023-04-11 15:06:34,267 :: INFO :: Precision [0.014220208526666302, 0.012708117255308224, 0.010352639336208717]
2023-04-11 15:06:34,267 :: INFO :: Recall [0.009021179683924812, 0.03531231166220179, 0.056342628091104574]
2023-04-11 15:06:34,267 :: INFO :: ndcg [0.014220208526666302, 0.03860804697732012, 0.050098037832011694]
2023-04-11 15:06:40,593 :: INFO :: Epoch 165: loss tensor(48.3447, device='cuda:0', grad_fn=<AddBackward0>), U.norm 180.30213928222656, V.norm 385.99951171875, MLP.norm 9.379304885864258
2023-04-11 15:07:01,609 :: INFO :: ----- val -----
2023-04-11 15:07:01,609 :: INFO :: Precision [0.01717194069484399, 0.016339898207567535, 0.013768532861252503]
2023-04-11 15:07:01,609 :: INFO :: Recall [0.0076237980346840805, 0.032907991555903175, 0.053868467607939625]
2023-04-11 15:07:01,609 :: INFO :: ndcg [0.01717194069484399, 0.04776840272164941, 0.06322226387635832]
2023-04-11 15:07:22,499 :: INFO :: ----- test -----
2023-04-11 15:07:22,499 :: INFO :: Precision [0.014438561056826246, 0.012768164201102203, 0.01040176865549471]
2023-04-11 15:07:22,499 :: INFO :: Recall [0.009139563844712715, 0.03557892418192341, 0.056960977828538296]
2023-04-11 15:07:22,499 :: INFO :: ndcg [0.014438561056826246, 0.03879417980409194, 0.05031358625620914]
2023-04-11 15:07:28,249 :: INFO :: Epoch 170: loss tensor(48.5050, device='cuda:0', grad_fn=<AddBackward0>), U.norm 182.62088012695312, V.norm 385.6714172363281, MLP.norm 9.30756950378418
2023-04-11 15:07:49,124 :: INFO :: ----- val -----
2023-04-11 15:07:49,124 :: INFO :: Precision [0.017570258906837798, 0.016295640628457116, 0.013733126797964165]
2023-04-11 15:07:49,124 :: INFO :: Recall [0.007600731696187174, 0.032843867086600566, 0.05377695095884877]
2023-04-11 15:07:49,124 :: INFO :: ndcg [0.017570258906837798, 0.04776177433144647, 0.06321666550714954]
2023-04-11 15:08:09,874 :: INFO :: ----- test -----
2023-04-11 15:08:09,874 :: INFO :: Precision [0.014957148315956112, 0.012789999454118198, 0.010352639336208716]
2023-04-11 15:08:09,874 :: INFO :: Recall [0.009472756257167094, 0.035888677674847394, 0.05663528186164292]
2023-04-11 15:08:09,874 :: INFO :: ndcg [0.014957148315956112, 0.03906736848342941, 0.05037789741119012]
2023-04-11 15:08:14,686 :: INFO :: Epoch 175: loss tensor(50.6579, device='cuda:0', grad_fn=<AddBackward0>), U.norm 184.92498779296875, V.norm 385.56268310546875, MLP.norm 9.409402847290039
2023-04-11 15:08:34,358 :: INFO :: ----- val -----
2023-04-11 15:08:34,358 :: INFO :: Precision [0.01810134985616287, 0.016224828501880444, 0.01371984952423104]
2023-04-11 15:08:34,358 :: INFO :: Recall [0.007724927203113117, 0.03297137135154108, 0.05370354138866472]
2023-04-11 15:08:34,358 :: INFO :: ndcg [0.01810134985616287, 0.04786696668373854, 0.06344385759235115]
2023-04-11 15:08:54,296 :: INFO :: ----- test -----
2023-04-11 15:08:54,296 :: INFO :: Precision [0.015202794912386047, 0.012773623014356202, 0.010322615863311714]
2023-04-11 15:08:54,296 :: INFO :: Recall [0.009620819194703457, 0.03574463747403554, 0.05691030775564622]
2023-04-11 15:08:54,296 :: INFO :: ndcg [0.015202794912386047, 0.03902112416890802, 0.05029226221241469]
2023-04-11 15:10:26,436 :: INFO :: Epoch 180: loss tensor(51.1983, device='cuda:0', grad_fn=<AddBackward0>), U.norm 187.1385955810547, V.norm 385.2073059082031, MLP.norm 9.367634773254395
2023-04-11 15:10:49,014 :: INFO :: ----- val -----
2023-04-11 15:10:49,014 :: INFO :: Precision [0.01810134985616287, 0.01631334366010128, 0.013613631334365996]
2023-04-11 15:10:49,014 :: INFO :: Recall [0.007474067135524645, 0.03296997354268496, 0.053453033306641716]
2023-04-11 15:10:49,014 :: INFO :: ndcg [0.01810134985616287, 0.04802187819638341, 0.06300779717905444]
2023-04-11 15:11:11,483 :: INFO :: ----- test -----
2023-04-11 15:11:11,483 :: INFO :: Precision [0.015311971177466018, 0.012877340466182169, 0.010363556962716711]
2023-04-11 15:11:11,483 :: INFO :: Recall [0.009499604080737328, 0.036146965008029656, 0.0573636856757228]
2023-04-11 15:11:11,483 :: INFO :: ndcg [0.015311971177466018, 0.03929329546746559, 0.050565906620103115]
2023-04-11 15:11:17,671 :: INFO :: Epoch 185: loss tensor(49.0611, device='cuda:0', grad_fn=<AddBackward0>), U.norm 189.3159942626953, V.norm 384.8448486328125, MLP.norm 9.454532623291016
2023-04-11 15:11:40,030 :: INFO :: ----- val -----
2023-04-11 15:11:40,030 :: INFO :: Precision [0.01765877406505864, 0.016277937596812946, 0.013423323744191158]
2023-04-11 15:11:40,030 :: INFO :: Recall [0.007280686878979709, 0.03282295439730526, 0.052778828776579745]
2023-04-11 15:11:40,030 :: INFO :: ndcg [0.01765877406505864, 0.047772575734158954, 0.06248239206948535]
2023-04-11 15:12:02,061 :: INFO :: ----- test -----
2023-04-11 15:12:02,061 :: INFO :: Precision [0.015584911840165947, 0.012773623014356202, 0.010232545444620725]
2023-04-11 15:12:02,061 :: INFO :: Recall [0.00946762936537378, 0.03581347917451654, 0.05683649850468885]
2023-04-11 15:12:02,061 :: INFO :: ndcg [0.015584911840165947, 0.039259035812486194, 0.050264792846912695]
2023-04-11 15:12:08,296 :: INFO :: Epoch 190: loss tensor(53.2147, device='cuda:0', grad_fn=<AddBackward0>), U.norm 191.49917602539062, V.norm 384.470458984375, MLP.norm 9.516301155090332
2023-04-11 15:12:29,733 :: INFO :: ----- val -----
2023-04-11 15:12:29,733 :: INFO :: Precision [0.018057092277052444, 0.016269086080990862, 0.01331267979641508]
2023-04-11 15:12:29,733 :: INFO :: Recall [0.007445331312267649, 0.03285348076988392, 0.052234681717404335]
2023-04-11 15:12:29,733 :: INFO :: ndcg [0.018057092277052444, 0.04789376736098715, 0.06233166167495102]
2023-04-11 15:12:51,499 :: INFO :: ----- test -----
2023-04-11 15:12:51,499 :: INFO :: Precision [0.015748676237785905, 0.012888258092690166, 0.010262568917517723]
2023-04-11 15:12:51,499 :: INFO :: Recall [0.009902905193768663, 0.036296383533214224, 0.05727302063882705]
2023-04-11 15:12:51,499 :: INFO :: ndcg [0.015748676237785905, 0.039572864321804936, 0.05049894242653938]
2023-04-11 15:12:58,311 :: INFO :: Epoch 195: loss tensor(51.2598, device='cuda:0', grad_fn=<AddBackward0>), U.norm 193.6625518798828, V.norm 384.0536193847656, MLP.norm 9.377906799316406
2023-04-11 15:13:20,593 :: INFO :: ----- val -----
2023-04-11 15:13:20,593 :: INFO :: Precision [0.019340562071254702, 0.016348749723389622, 0.013436601017924285]
2023-04-11 15:13:20,593 :: INFO :: Recall [0.007872117959062066, 0.03275229806160269, 0.05271533088766888]
2023-04-11 15:13:20,593 :: INFO :: ndcg [0.019340562071254702, 0.04847201483788064, 0.06317933318432284]
2023-04-11 15:13:43,015 :: INFO :: ----- test -----
2023-04-11 15:13:43,015 :: INFO :: Precision [0.016376439761995744, 0.012964681478246143, 0.010295321797041727]
2023-04-11 15:13:43,015 :: INFO :: Recall [0.009946683781584167, 0.036439640985887055, 0.05670000235798056]
2023-04-11 15:13:43,015 :: INFO :: ndcg [0.016376439761995744, 0.039913016964958005, 0.050913252944205976]
2023-04-11 15:13:49,155 :: INFO :: Epoch 200: loss tensor(48.7703, device='cuda:0', grad_fn=<AddBackward0>), U.norm 195.77000427246094, V.norm 383.4802551269531, MLP.norm 9.521916389465332
2023-04-11 15:14:11,077 :: INFO :: ----- val -----
2023-04-11 15:14:11,077 :: INFO :: Precision [0.01942907722947555, 0.016233680017702527, 0.013626908608099134]
2023-04-11 15:14:11,077 :: INFO :: Recall [0.007879610773591833, 0.032573887045143565, 0.05315562365474958]
2023-04-11 15:14:11,077 :: INFO :: ndcg [0.01942907722947555, 0.04827395001704033, 0.06367286374646199]
2023-04-11 15:14:33,217 :: INFO :: ----- test -----
2023-04-11 15:14:33,217 :: INFO :: Precision [0.01588514656913587, 0.012866422839674171, 0.010388121622359723]
2023-04-11 15:14:33,217 :: INFO :: Recall [0.009703075075450212, 0.036132967066333704, 0.05724886351254896]
2023-04-11 15:14:33,217 :: INFO :: ndcg [0.01588514656913587, 0.03952898525330097, 0.050922603338694555]
2023-04-11 15:14:40,765 :: INFO :: Epoch 205: loss tensor(53.6538, device='cuda:0', grad_fn=<AddBackward0>), U.norm 197.87020874023438, V.norm 383.04522705078125, MLP.norm 9.55610179901123
2023-04-11 15:15:03,546 :: INFO :: ----- val -----
2023-04-11 15:15:03,546 :: INFO :: Precision [0.01898650143837132, 0.016242531533524614, 0.013671166187209564]
2023-04-11 15:15:03,546 :: INFO :: Recall [0.007891612901106243, 0.032733892541070474, 0.05333034285066062]
2023-04-11 15:15:03,546 :: INFO :: ndcg [0.01898650143837132, 0.048012340379611815, 0.06361069763951437]
2023-04-11 15:15:25,874 :: INFO :: ----- test -----
2023-04-11 15:15:25,874 :: INFO :: Precision [0.01607620503302582, 0.013052022490310114, 0.010579180086249702]
2023-04-11 15:15:25,874 :: INFO :: Recall [0.009831974378944841, 0.03670620201279254, 0.058849145774933984]
2023-04-11 15:15:25,874 :: INFO :: ndcg [0.01607620503302582, 0.03992294654114846, 0.051480989358469724]
2023-04-11 15:15:32,889 :: INFO :: Epoch 210: loss tensor(50.2458, device='cuda:0', grad_fn=<AddBackward0>), U.norm 199.90997314453125, V.norm 382.63470458984375, MLP.norm 9.502634048461914
2023-04-11 15:15:54,155 :: INFO :: ----- val -----
2023-04-11 15:15:54,155 :: INFO :: Precision [0.019340562071254702, 0.016286789112635036, 0.013702146492586864]
2023-04-11 15:15:54,155 :: INFO :: Recall [0.007995670508541403, 0.032855374456457025, 0.0536847404770471]
2023-04-11 15:15:54,155 :: INFO :: ndcg [0.019340562071254702, 0.04831831704269498, 0.06396295872918102]
2023-04-11 15:16:16,124 :: INFO :: ----- test -----
2023-04-11 15:16:16,124 :: INFO :: Precision [0.016294557563185763, 0.012953763851738145, 0.010551886019979712]
2023-04-11 15:16:16,124 :: INFO :: Recall [0.010074328357856495, 0.03640511496911583, 0.058321744182040246]
2023-04-11 15:16:16,124 :: INFO :: ndcg [0.016294557563185763, 0.039920257414752976, 0.05158850495031286]
2023-04-11 15:16:23,171 :: INFO :: Epoch 215: loss tensor(49.2480, device='cuda:0', grad_fn=<AddBackward0>), U.norm 201.97779846191406, V.norm 382.279296875, MLP.norm 9.66431713104248
2023-04-11 15:16:45,265 :: INFO :: ----- val -----
2023-04-11 15:16:45,265 :: INFO :: Precision [0.01845541048904625, 0.016233680017702527, 0.013759681345430421]
2023-04-11 15:16:45,265 :: INFO :: Recall [0.00780211000401317, 0.03282147995335375, 0.05358505539223644]
2023-04-11 15:16:45,265 :: INFO :: ndcg [0.01845541048904625, 0.047926722905869704, 0.0637787682315814]
2023-04-11 15:17:06,702 :: INFO :: ----- test -----
2023-04-11 15:17:06,702 :: INFO :: Precision [0.015939734701675857, 0.013008351984278129, 0.010521862547082709]
2023-04-11 15:17:06,702 :: INFO :: Recall [0.010039364292377659, 0.03679808433704049, 0.058253822448670736]
2023-04-11 15:17:06,702 :: INFO :: ndcg [0.015939734701675857, 0.03980625199979614, 0.051312077410166454]
2023-04-11 15:17:12,983 :: INFO :: Epoch 220: loss tensor(51.4988, device='cuda:0', grad_fn=<AddBackward0>), U.norm 203.99037170410156, V.norm 381.9180908203125, MLP.norm 9.524710655212402
2023-04-11 15:17:33,623 :: INFO :: ----- val -----
2023-04-11 15:17:33,623 :: INFO :: Precision [0.01898650143837132, 0.016357601239211702, 0.013777384377074594]
2023-04-11 15:17:33,623 :: INFO :: Recall [0.007687222514706562, 0.032981842594605466, 0.05367526267243179]
2023-04-11 15:17:33,623 :: INFO :: ndcg [0.01898650143837132, 0.048436544790262465, 0.0640571087681439]
2023-04-11 15:17:55,565 :: INFO :: ----- test -----
2023-04-11 15:17:55,565 :: INFO :: Precision [0.016485616027075715, 0.013079316556580105, 0.010611932965773702]
2023-04-11 15:17:55,565 :: INFO :: Recall [0.010221077287972417, 0.03697334878224063, 0.05844389502676562]
2023-04-11 15:17:55,565 :: INFO :: ndcg [0.016485616027075715, 0.04023174053282782, 0.05189592391457146]
2023-04-11 15:18:02,155 :: INFO :: Epoch 225: loss tensor(48.5984, device='cuda:0', grad_fn=<AddBackward0>), U.norm 205.93017578125, V.norm 381.35516357421875, MLP.norm 9.500960350036621
2023-04-11 15:18:24,843 :: INFO :: ----- val -----
2023-04-11 15:18:24,843 :: INFO :: Precision [0.01911927417570259, 0.01643726488161046, 0.01364461163974331]
2023-04-11 15:18:24,843 :: INFO :: Recall [0.00768991755269702, 0.032995043330239913, 0.053430822442659974]
2023-04-11 15:18:24,843 :: INFO :: ndcg [0.01911927417570259, 0.048672337423308704, 0.064009164423855]
2023-04-11 15:18:46,608 :: INFO :: ----- test -----
2023-04-11 15:18:46,608 :: INFO :: Precision [0.016485616027075715, 0.013101151809596097, 0.010652874065178697]
2023-04-11 15:18:46,608 :: INFO :: Recall [0.010112411063524765, 0.03708783750483252, 0.05909206671392188]
2023-04-11 15:18:46,608 :: INFO :: ndcg [0.016485616027075715, 0.04022532563966734, 0.05198364347186739]
2023-04-11 15:18:52,780 :: INFO :: Epoch 230: loss tensor(51.7355, device='cuda:0', grad_fn=<AddBackward0>), U.norm 207.8987274169922, V.norm 380.8987731933594, MLP.norm 9.641303062438965
2023-04-11 15:19:15,092 :: INFO :: ----- val -----
2023-04-11 15:19:15,092 :: INFO :: Precision [0.019384819650365124, 0.01642841336578837, 0.013587076786899747]
2023-04-11 15:19:15,092 :: INFO :: Recall [0.007797449260250856, 0.03291745621921232, 0.05319026491132367]
2023-04-11 15:19:15,092 :: INFO :: ndcg [0.019384819650365124, 0.04865649440919274, 0.06374362096448263]
2023-04-11 15:19:36,717 :: INFO :: ----- test -----
2023-04-11 15:19:36,717 :: INFO :: Precision [0.01626726349691577, 0.013030187237294122, 0.010581909492876707]
2023-04-11 15:19:36,717 :: INFO :: Recall [0.009954393925222562, 0.0368261688660528, 0.058696190022723735]
2023-04-11 15:19:36,717 :: INFO :: ndcg [0.01626726349691577, 0.04001652878408702, 0.05166803945513375]
2023-04-11 15:19:44,171 :: INFO :: Epoch 235: loss tensor(52.5690, device='cuda:0', grad_fn=<AddBackward0>), U.norm 209.8702392578125, V.norm 380.3409729003906, MLP.norm 9.682829856872559
2023-04-11 15:20:05,252 :: INFO :: ----- val -----
2023-04-11 15:20:05,252 :: INFO :: Precision [0.019075016596592165, 0.016552334587297546, 0.013383491922991765]
2023-04-11 15:20:05,252 :: INFO :: Recall [0.007661961910172481, 0.033251194767041546, 0.05247060573224188]
2023-04-11 15:20:05,252 :: INFO :: ndcg [0.019075016596592165, 0.048817139086617715, 0.06322779730212907]
2023-04-11 15:20:26,108 :: INFO :: ----- test -----
2023-04-11 15:20:26,108 :: INFO :: Precision [0.016485616027075715, 0.013101151809596095, 0.010587368306130709]
2023-04-11 15:20:26,108 :: INFO :: Recall [0.010089378443224708, 0.036826792076370116, 0.0586771924490309]
2023-04-11 15:20:26,108 :: INFO :: ndcg [0.016485616027075715, 0.04030501778156977, 0.051889249795701205]
2023-04-11 15:20:32,530 :: INFO :: Epoch 240: loss tensor(50.0902, device='cuda:0', grad_fn=<AddBackward0>), U.norm 211.82200622558594, V.norm 379.8705749511719, MLP.norm 9.590601921081543
2023-04-11 15:20:54,296 :: INFO :: ----- val -----
2023-04-11 15:20:54,296 :: INFO :: Precision [0.01880947112192963, 0.01639300730250004, 0.013795087408718779]
2023-04-11 15:20:54,296 :: INFO :: Recall [0.007758267647659896, 0.032946104996877675, 0.05365230672706229]
2023-04-11 15:20:54,296 :: INFO :: ndcg [0.01880947112192963, 0.04843291649129653, 0.06417284651240944]
2023-04-11 15:21:16,202 :: INFO :: ----- test -----
2023-04-11 15:21:16,202 :: INFO :: Precision [0.01662208635842568, 0.013232163327692056, 0.0106610622850597]
2023-04-11 15:21:16,202 :: INFO :: Recall [0.010199768762551107, 0.037151102253155856, 0.05913750557988335]
2023-04-11 15:21:16,202 :: INFO :: ndcg [0.01662208635842568, 0.04064485594370693, 0.0521621825558877]
2023-04-11 15:21:22,436 :: INFO :: Epoch 245: loss tensor(52.7369, device='cuda:0', grad_fn=<AddBackward0>), U.norm 213.7471923828125, V.norm 379.4072265625, MLP.norm 9.570721626281738
2023-04-11 15:21:44,217 :: INFO :: ----- val -----
2023-04-11 15:21:44,217 :: INFO :: Precision [0.01916353175481301, 0.01633989820756753, 0.013834919229918165]
2023-04-11 15:21:44,217 :: INFO :: Recall [0.007995888167364249, 0.03282861499082863, 0.05398761234217045]
2023-04-11 15:21:44,217 :: INFO :: ndcg [0.01916353175481301, 0.04858510266907286, 0.06441389222372937]
2023-04-11 15:22:05,874 :: INFO :: ----- test -----
2023-04-11 15:22:05,874 :: INFO :: Precision [0.016185381298105792, 0.013139363502374083, 0.010669250504940693]
2023-04-11 15:22:05,874 :: INFO :: Recall [0.009892675940804615, 0.03671292379985767, 0.05917584655441105]
2023-04-11 15:22:05,874 :: INFO :: ndcg [0.016185381298105792, 0.04045740703866048, 0.05207039512873832]
2023-04-11 15:22:13,092 :: INFO :: Epoch 250: loss tensor(49.2547, device='cuda:0', grad_fn=<AddBackward0>), U.norm 215.7296142578125, V.norm 378.9424133300781, MLP.norm 9.796409606933594
2023-04-11 15:22:35,717 :: INFO :: ----- val -----
2023-04-11 15:22:35,717 :: INFO :: Precision [0.018411152909935828, 0.016631998229696306, 0.013932285903961113]
2023-04-11 15:22:35,717 :: INFO :: Recall [0.007878549926885118, 0.033186106906054974, 0.054348560363177915]
2023-04-11 15:22:35,717 :: INFO :: ndcg [0.018411152909935828, 0.04876666947500299, 0.06448286943928261]
2023-04-11 15:22:58,264 :: INFO :: ----- test -----
2023-04-11 15:22:58,264 :: INFO :: Precision [0.015748676237785905, 0.013144822315628081, 0.01074021507724269]
2023-04-11 15:22:58,264 :: INFO :: Recall [0.009738386378467582, 0.03676452104354193, 0.059033045154054546]
2023-04-11 15:22:58,264 :: INFO :: ndcg [0.015748676237785905, 0.04024318204682274, 0.05213334770169748]
2023-04-11 15:23:04,155 :: INFO :: Epoch 255: loss tensor(50.7064, device='cuda:0', grad_fn=<AddBackward0>), U.norm 217.64466857910156, V.norm 378.44097900390625, MLP.norm 9.758299827575684
2023-04-11 15:23:25,968 :: INFO :: ----- val -----
2023-04-11 15:23:25,968 :: INFO :: Precision [0.01880947112192963, 0.016658552777162553, 0.014056207125470328]
2023-04-11 15:23:25,968 :: INFO :: Recall [0.007881823385558429, 0.03329771070131901, 0.05452511393394909]
2023-04-11 15:23:25,968 :: INFO :: ndcg [0.01880947112192963, 0.04887084049167303, 0.06476414416439122]
2023-04-11 15:23:48,499 :: INFO :: ----- test -----
2023-04-11 15:23:48,499 :: INFO :: Precision [0.015939734701675857, 0.013166657568644075, 0.010781156176647686]
2023-04-11 15:23:48,499 :: INFO :: Recall [0.009939245211757224, 0.0367689562459956, 0.05917249224901058]
2023-04-11 15:23:48,499 :: INFO :: ndcg [0.015939734701675857, 0.040302415053930646, 0.052216709015730736]
2023-04-11 15:23:54,796 :: INFO :: Epoch 260: loss tensor(54.1365, device='cuda:0', grad_fn=<AddBackward0>), U.norm 219.55540466308594, V.norm 377.9219970703125, MLP.norm 9.662453651428223
2023-04-11 15:24:17,342 :: INFO :: ----- val -----
2023-04-11 15:24:17,342 :: INFO :: Precision [0.018720955963708784, 0.01670281035627298, 0.014034078335915112]
2023-04-11 15:24:17,342 :: INFO :: Recall [0.007721171001626558, 0.03343313938537898, 0.05434180480909843]
2023-04-11 15:24:17,342 :: INFO :: ndcg [0.018720955963708784, 0.04884378578861664, 0.06457814274205448]
2023-04-11 15:24:39,265 :: INFO :: ----- test -----
2023-04-11 15:24:39,265 :: INFO :: Precision [0.015530323707625962, 0.013155739942136079, 0.010786614989901685]
2023-04-11 15:24:39,265 :: INFO :: Recall [0.009569848586765317, 0.03676067762910022, 0.05909641373664498]
2023-04-11 15:24:39,265 :: INFO :: ndcg [0.015530323707625962, 0.0400500968632879, 0.05201768125542941]
2023-04-11 15:24:45,983 :: INFO :: Epoch 265: loss tensor(51.0338, device='cuda:0', grad_fn=<AddBackward0>), U.norm 221.49624633789062, V.norm 377.4306945800781, MLP.norm 9.56605339050293
2023-04-11 15:25:08,171 :: INFO :: ----- val -----
2023-04-11 15:25:08,171 :: INFO :: Precision [0.018012834697942022, 0.01684443460942632, 0.013963266209338422]
2023-04-11 15:25:08,171 :: INFO :: Recall [0.007717069522925584, 0.03364415928974626, 0.05366286879120763]
2023-04-11 15:25:08,171 :: INFO :: ndcg [0.018012834697942022, 0.04907515183561933, 0.06438128524191633]
2023-04-11 15:25:29,655 :: INFO :: ----- test -----
2023-04-11 15:25:29,655 :: INFO :: Precision [0.014902560183416125, 0.013253998580708049, 0.01076205033025869]
2023-04-11 15:25:29,655 :: INFO :: Recall [0.009504670105052094, 0.03700542437213868, 0.059313478579732806]
2023-04-11 15:25:29,655 :: INFO :: ndcg [0.014902560183416125, 0.04016313903460478, 0.051990075266959423]
2023-04-11 15:25:36,640 :: INFO :: Epoch 270: loss tensor(50.0576, device='cuda:0', grad_fn=<AddBackward0>), U.norm 223.3822784423828, V.norm 377.00897216796875, MLP.norm 9.741865158081055
2023-04-11 15:25:58,296 :: INFO :: ----- val -----
2023-04-11 15:25:58,296 :: INFO :: Precision [0.01761451648594822, 0.016782473998671735, 0.014233237441912051]
2023-04-11 15:25:58,296 :: INFO :: Recall [0.007480796026080779, 0.033218512293544195, 0.05468297991550666]
2023-04-11 15:25:58,296 :: INFO :: ndcg [0.01761451648594822, 0.04876609443838512, 0.06489312714505968]
2023-04-11 15:26:20,124 :: INFO :: ----- test -----
2023-04-11 15:26:20,124 :: INFO :: Precision [0.014902560183416125, 0.01334679840602602, 0.010887603035100677]
2023-04-11 15:26:20,124 :: INFO :: Recall [0.009349757165478102, 0.03734463967174147, 0.05950363799500986]
2023-04-11 15:26:20,124 :: INFO :: ndcg [0.014902560183416125, 0.04033111163361198, 0.05232770944691181]
2023-04-11 15:26:26,108 :: INFO :: Epoch 275: loss tensor(49.9569, device='cuda:0', grad_fn=<AddBackward0>), U.norm 225.2993621826172, V.norm 376.6065673828125, MLP.norm 9.731372833251953
2023-04-11 15:26:47,545 :: INFO :: ----- val -----
2023-04-11 15:26:47,545 :: INFO :: Precision [0.018411152909935828, 0.016738216419561316, 0.014109316220402845]
2023-04-11 15:26:47,545 :: INFO :: Recall [0.007547759127760045, 0.033420759202690026, 0.05438440331659633]
2023-04-11 15:26:47,545 :: INFO :: ndcg [0.018411152909935828, 0.04888017012752878, 0.06484157837757358]
2023-04-11 15:27:09,405 :: INFO :: ----- test -----
2023-04-11 15:27:09,405 :: INFO :: Precision [0.01542114744254599, 0.013204869261422064, 0.010827556089306693]
2023-04-11 15:27:09,405 :: INFO :: Recall [0.009537476355326228, 0.03690425207867087, 0.05930003535033006]
2023-04-11 15:27:09,405 :: INFO :: ndcg [0.01542114744254599, 0.04009511193991386, 0.05227568574717713]
2023-04-11 15:27:15,358 :: INFO :: Epoch 280: loss tensor(50.9535, device='cuda:0', grad_fn=<AddBackward0>), U.norm 227.1473388671875, V.norm 376.05224609375, MLP.norm 9.737614631652832
2023-04-11 15:27:36,936 :: INFO :: ----- val -----
2023-04-11 15:27:36,952 :: INFO :: Precision [0.018189865014383712, 0.016782473998671735, 0.014184554104890564]
2023-04-11 15:27:36,952 :: INFO :: Recall [0.007712407509185955, 0.03331489802997387, 0.05456725790570296]
2023-04-11 15:27:36,952 :: INFO :: ndcg [0.018189865014383712, 0.04908355706648773, 0.06502437803546063]
2023-04-11 15:27:58,265 :: INFO :: ----- test -----
2023-04-11 15:27:58,265 :: INFO :: Precision [0.015530323707625962, 0.013204869261422064, 0.01080026202303669]
2023-04-11 15:27:58,265 :: INFO :: Recall [0.009657906007253234, 0.037032684669012154, 0.05895528158689691]
2023-04-11 15:27:58,265 :: INFO :: ndcg [0.015530323707625962, 0.0401452851387536, 0.052178767214652644]
2023-04-11 15:28:04,296 :: INFO :: Epoch 285: loss tensor(52.6119, device='cuda:0', grad_fn=<AddBackward0>), U.norm 229.00733947753906, V.norm 375.5523376464844, MLP.norm 9.656249046325684
2023-04-11 15:28:25,577 :: INFO :: ----- val -----
2023-04-11 15:28:25,577 :: INFO :: Precision [0.01965036512502766, 0.016817880061960076, 0.014091613188758664]
2023-04-11 15:28:25,577 :: INFO :: Recall [0.008425683437169373, 0.03338431185228036, 0.05467686466703646]
2023-04-11 15:28:25,577 :: INFO :: ndcg [0.01965036512502766, 0.04936817417632622, 0.06517588495327163]
2023-04-11 15:28:46,046 :: INFO :: ----- test -----
2023-04-11 15:28:46,046 :: INFO :: Precision [0.015530323707625962, 0.01318303400840607, 0.010830285495933685]
2023-04-11 15:28:46,046 :: INFO :: Recall [0.009399551745662215, 0.036842388993837405, 0.05888970842068273]
2023-04-11 15:28:46,046 :: INFO :: ndcg [0.015530323707625962, 0.03977405932610216, 0.05183672726653711]
2023-04-11 15:28:52,186 :: INFO :: Epoch 290: loss tensor(50.5160, device='cuda:0', grad_fn=<AddBackward0>), U.norm 230.86026000976562, V.norm 375.01190185546875, MLP.norm 9.856731414794922
2023-04-11 15:29:13,139 :: INFO :: ----- val -----
2023-04-11 15:29:13,139 :: INFO :: Precision [0.019517592387696393, 0.01690639522018091, 0.014122593494135968]
2023-04-11 15:29:13,139 :: INFO :: Recall [0.0081515650932347, 0.03359181348914595, 0.05435334613302413]
2023-04-11 15:29:13,139 :: INFO :: ndcg [0.019517592387696393, 0.04981129499573095, 0.06558534487844503]
2023-04-11 15:29:34,217 :: INFO :: ----- test -----
2023-04-11 15:29:34,217 :: INFO :: Precision [0.015694088105245918, 0.013128445875866089, 0.010936732354386679]
2023-04-11 15:29:34,217 :: INFO :: Recall [0.009686570935784847, 0.037059846601352114, 0.0596915859199138]
2023-04-11 15:29:34,217 :: INFO :: ndcg [0.015694088105245918, 0.04001383322097927, 0.05258652253859035]
2023-04-11 15:29:40,905 :: INFO :: Epoch 295: loss tensor(51.1765, device='cuda:0', grad_fn=<AddBackward0>), U.norm 232.67779541015625, V.norm 374.43536376953125, MLP.norm 9.657573699951172
2023-04-11 15:30:02,186 :: INFO :: ----- val -----
2023-04-11 15:30:02,186 :: INFO :: Precision [0.01965036512502766, 0.016809028546137986, 0.014122593494135974]
2023-04-11 15:30:02,186 :: INFO :: Recall [0.007828928540543005, 0.03369318842081428, 0.054198671279997924]
2023-04-11 15:30:02,186 :: INFO :: ndcg [0.01965036512502766, 0.049790574133526985, 0.06564955381908397]
2023-04-11 15:30:23,514 :: INFO :: ----- test -----
2023-04-11 15:30:23,514 :: INFO :: Precision [0.015694088105245918, 0.013226704514438059, 0.010994049893553676]
2023-04-11 15:30:23,514 :: INFO :: Recall [0.009375618953412481, 0.037285593459813964, 0.06018590455024484]
2023-04-11 15:30:23,514 :: INFO :: ndcg [0.015694088105245918, 0.0401933136521725, 0.05272344917808113]
2023-04-11 15:30:29,015 :: INFO :: Epoch 300: loss tensor(51.0712, device='cuda:0', grad_fn=<AddBackward0>), U.norm 234.51368713378906, V.norm 374.0047302246094, MLP.norm 9.793054580688477
2023-04-11 15:30:49,827 :: INFO :: ----- val -----
2023-04-11 15:30:49,827 :: INFO :: Precision [0.020181456074352733, 0.01685328612524841, 0.014188979862801609]
2023-04-11 15:30:49,827 :: INFO :: Recall [0.008158993530767008, 0.0335846564790553, 0.0544128921308532]
2023-04-11 15:30:49,827 :: INFO :: ndcg [0.020181456074352733, 0.05018326748067875, 0.0660149190087149]
2023-04-11 15:31:10,889 :: INFO :: ----- test -----
2023-04-11 15:31:10,889 :: INFO :: Precision [0.016594792292155686, 0.013188492821660068, 0.010887603035100689]
2023-04-11 15:31:10,889 :: INFO :: Recall [0.01025832273542845, 0.03698001558228411, 0.05922531800817805]
2023-04-11 15:31:10,889 :: INFO :: ndcg [0.016594792292155686, 0.040692363380798714, 0.0530056838812997]
2023-04-11 15:31:17,328 :: INFO :: Epoch 305: loss tensor(53.6595, device='cuda:0', grad_fn=<AddBackward0>), U.norm 236.2530059814453, V.norm 373.4435729980469, MLP.norm 9.742780685424805
2023-04-11 15:31:37,858 :: INFO :: ----- val -----
2023-04-11 15:31:37,858 :: INFO :: Precision [0.020845319761009073, 0.016941801283469247, 0.014414693516264817]
2023-04-11 15:31:37,858 :: INFO :: Recall [0.008497773849032622, 0.03361074245252592, 0.054864950313996824]
2023-04-11 15:31:37,858 :: INFO :: ndcg [0.020845319761009073, 0.05053617782492143, 0.06690480704682344]
2023-04-11 15:31:57,640 :: INFO :: ----- test -----
2023-04-11 15:31:57,640 :: INFO :: Precision [0.017249849882635514, 0.013292210273486035, 0.010863038375457683]
2023-04-11 15:31:57,640 :: INFO :: Recall [0.010274171524750134, 0.037118237423980585, 0.0592728442845062]
2023-04-11 15:31:57,640 :: INFO :: ndcg [0.017249849882635514, 0.04094251874817901, 0.053004891891040476]
2023-04-11 15:32:02,452 :: INFO :: Epoch 310: loss tensor(51.8880, device='cuda:0', grad_fn=<AddBackward0>), U.norm 238.05641174316406, V.norm 373.0608825683594, MLP.norm 9.66321849822998
2023-04-11 15:32:22,827 :: INFO :: ----- val -----
2023-04-11 15:32:22,827 :: INFO :: Precision [0.020491259128125692, 0.016888692188536742, 0.014374861695065431]
2023-04-11 15:32:22,842 :: INFO :: Recall [0.008371325219596607, 0.03360746487036459, 0.054517155293019516]
2023-04-11 15:32:22,842 :: INFO :: ndcg [0.020491259128125692, 0.05068078749406393, 0.06681790658139782]
2023-04-11 15:32:43,202 :: INFO :: ----- test -----
2023-04-11 15:32:43,202 :: INFO :: Precision [0.018041377804465308, 0.013303127899994036, 0.010833014902560695]
2023-04-11 15:32:43,202 :: INFO :: Recall [0.010697730451811317, 0.03699361048520517, 0.05902236277878386]
2023-04-11 15:32:43,202 :: INFO :: ndcg [0.018041377804465308, 0.04152971672165373, 0.05367699744351686]
2023-04-11 15:32:48,046 :: INFO :: Epoch 315: loss tensor(52.6158, device='cuda:0', grad_fn=<AddBackward0>), U.norm 239.84242248535156, V.norm 372.7072448730469, MLP.norm 9.71044921875
2023-04-11 15:33:09,561 :: INFO :: ----- val -----
2023-04-11 15:33:09,561 :: INFO :: Precision [0.020668289444567382, 0.01710112856826676, 0.014582872316884432]
2023-04-11 15:33:09,561 :: INFO :: Recall [0.008413649070311517, 0.03382704445786303, 0.054876426246424886]
2023-04-11 15:33:09,561 :: INFO :: ndcg [0.020668289444567382, 0.05121397986870879, 0.06759181355973808]
2023-04-11 15:33:31,171 :: INFO :: ----- test -----
2023-04-11 15:33:31,171 :: INFO :: Precision [0.01785031934057536, 0.013406845351820003, 0.011004967520061675]
2023-04-11 15:33:31,171 :: INFO :: Recall [0.0105593528835795, 0.03735484386018408, 0.05952505670709058]
2023-04-11 15:33:31,171 :: INFO :: ndcg [0.01785031934057536, 0.04163459333190621, 0.053995758908929783]
2023-04-11 15:33:36,592 :: INFO :: Epoch 320: loss tensor(52.2757, device='cuda:0', grad_fn=<AddBackward0>), U.norm 241.63351440429688, V.norm 372.3243408203125, MLP.norm 9.730937957763672
2023-04-11 15:33:57,936 :: INFO :: ----- val -----
2023-04-11 15:33:57,936 :: INFO :: Precision [0.021420668289444566, 0.01687098915689257, 0.014728922327948873]
2023-04-11 15:33:57,936 :: INFO :: Recall [0.008407175631773465, 0.03336308689065177, 0.05560569511026816]
2023-04-11 15:33:57,936 :: INFO :: ndcg [0.021420668289444566, 0.05118938793947766, 0.06833463652363925]
2023-04-11 15:34:19,108 :: INFO :: ----- test -----
2023-04-11 15:34:19,108 :: INFO :: Precision [0.0180686718707353, 0.013516021616899965, 0.010966755827283679]
2023-04-11 15:34:19,108 :: INFO :: Recall [0.010645752585132867, 0.03783477332253856, 0.05949432346706906]
2023-04-11 15:34:19,108 :: INFO :: ndcg [0.0180686718707353, 0.0420745314550519, 0.05405610960971341]
2023-04-11 15:34:24,467 :: INFO :: Epoch 325: loss tensor(47.7335, device='cuda:0', grad_fn=<AddBackward0>), U.norm 243.43502807617188, V.norm 371.80078125, MLP.norm 9.780468940734863
2023-04-11 15:34:44,280 :: INFO :: ----- val -----
2023-04-11 15:34:44,280 :: INFO :: Precision [0.0212878955521133, 0.016932949767647164, 0.014821863244080786]
2023-04-11 15:34:44,280 :: INFO :: Recall [0.008551416149437305, 0.03343258455964437, 0.05598617720276388]
2023-04-11 15:34:44,280 :: INFO :: ndcg [0.0212878955521133, 0.05110792153177079, 0.06847946988162287]
2023-04-11 15:35:04,530 :: INFO :: ----- test -----
2023-04-11 15:35:04,530 :: INFO :: Precision [0.018205142202085266, 0.013603362628963937, 0.010994049893553673]
2023-04-11 15:35:04,530 :: INFO :: Recall [0.010687052773138658, 0.03805892747099467, 0.059555461782926374]
2023-04-11 15:35:04,530 :: INFO :: ndcg [0.018205142202085266, 0.04229937449547201, 0.05421757213537469]
2023-04-11 15:35:10,390 :: INFO :: Epoch 330: loss tensor(51.1397, device='cuda:0', grad_fn=<AddBackward0>), U.norm 245.16139221191406, V.norm 371.2503662109375, MLP.norm 9.649040222167969
2023-04-11 15:35:32,717 :: INFO :: ----- val -----
2023-04-11 15:35:32,717 :: INFO :: Precision [0.021022350077450763, 0.01683558309360424, 0.014675813233016356]
2023-04-11 15:35:32,717 :: INFO :: Recall [0.008326578431917416, 0.03294592028147234, 0.05511987445137398]
2023-04-11 15:35:32,717 :: INFO :: ndcg [0.021022350077450763, 0.05082839563637003, 0.06792331522443366]
2023-04-11 15:35:54,389 :: INFO :: ----- test -----
2023-04-11 15:35:54,389 :: INFO :: Precision [0.017631966810415414, 0.013423221791581997, 0.010931273541132682]
2023-04-11 15:35:54,389 :: INFO :: Recall [0.010513761823402728, 0.037531708989807636, 0.059400705943973515]
2023-04-11 15:35:54,389 :: INFO :: ndcg [0.017631966810415414, 0.04164373248154443, 0.053767539971721255]
2023-04-11 15:35:59,936 :: INFO :: Epoch 335: loss tensor(50.1785, device='cuda:0', grad_fn=<AddBackward0>), U.norm 246.92471313476562, V.norm 370.9394836425781, MLP.norm 9.612980842590332
2023-04-11 15:36:22,186 :: INFO :: ----- val -----
2023-04-11 15:36:22,186 :: INFO :: Precision [0.02111086523567161, 0.016950652799291335, 0.01472007081212678]
2023-04-11 15:36:22,186 :: INFO :: Recall [0.008178710564562746, 0.033259660469494634, 0.055472122693131266]
2023-04-11 15:36:22,186 :: INFO :: ndcg [0.02111086523567161, 0.05094422832620898, 0.06804683314299269]
2023-04-11 15:36:43,765 :: INFO :: ----- test -----
2023-04-11 15:36:43,765 :: INFO :: Precision [0.018041377804465308, 0.013450515857851987, 0.010999508706807676]
2023-04-11 15:36:43,765 :: INFO :: Recall [0.010736339419881741, 0.037550105244865066, 0.05955012884732865]
2023-04-11 15:36:43,765 :: INFO :: ndcg [0.018041377804465308, 0.04175943912191337, 0.053957864249785124]
2023-04-11 15:36:50,296 :: INFO :: Epoch 340: loss tensor(48.0446, device='cuda:0', grad_fn=<AddBackward0>), U.norm 248.65249633789062, V.norm 370.5161437988281, MLP.norm 9.685633659362793
2023-04-11 15:37:11,952 :: INFO :: ----- val -----
2023-04-11 15:37:11,952 :: INFO :: Precision [0.02026997123257358, 0.01679132551449382, 0.014799734454525551]
2023-04-11 15:37:11,952 :: INFO :: Recall [0.007967024007805162, 0.03326235080606972, 0.05570896298621643]
2023-04-11 15:37:11,952 :: INFO :: ndcg [0.02026997123257358, 0.049936727194298765, 0.06743709793328063]
2023-04-11 15:37:33,436 :: INFO :: ----- test -----
2023-04-11 15:37:33,436 :: INFO :: Precision [0.0176865549429554, 0.013597903815709942, 0.01098586167367267]
2023-04-11 15:37:33,436 :: INFO :: Recall [0.010657273275528187, 0.038312553111107255, 0.05968230861029281]
2023-04-11 15:37:33,436 :: INFO :: ndcg [0.0176865549429554, 0.04167338654016571, 0.05363291535925022]
2023-04-11 15:37:39,811 :: INFO :: Epoch 345: loss tensor(51.9630, device='cuda:0', grad_fn=<AddBackward0>), U.norm 250.43194580078125, V.norm 370.14666748046875, MLP.norm 9.541475296020508
2023-04-11 15:38:02,577 :: INFO :: ----- val -----
2023-04-11 15:38:02,577 :: INFO :: Precision [0.020402743969904845, 0.01695950431511342, 0.014795308696614507]
2023-04-11 15:38:02,577 :: INFO :: Recall [0.008138140049689071, 0.033356121287098514, 0.05566173409863063]
2023-04-11 15:38:02,577 :: INFO :: ndcg [0.020402743969904845, 0.05047927837875038, 0.06776956652225379]
2023-04-11 15:38:24,561 :: INFO :: ----- test -----
2023-04-11 15:38:24,561 :: INFO :: Precision [0.017604672744145424, 0.013450515857851989, 0.010934002947759678]
2023-04-11 15:38:24,561 :: INFO :: Recall [0.01048292131798368, 0.037831439061362285, 0.05917214902612033]
2023-04-11 15:38:24,561 :: INFO :: ndcg [0.017604672744145424, 0.041399599970694534, 0.053381460404276874]
2023-04-11 15:38:31,140 :: INFO :: Epoch 350: loss tensor(48.2953, device='cuda:0', grad_fn=<AddBackward0>), U.norm 252.129638671875, V.norm 369.84716796875, MLP.norm 9.652373313903809
2023-04-11 15:38:52,546 :: INFO :: ----- val -----
2023-04-11 15:38:52,546 :: INFO :: Precision [0.021243637973002875, 0.017163089179021356, 0.014795308696614518]
2023-04-11 15:38:52,546 :: INFO :: Recall [0.008489929159499868, 0.03338891829523863, 0.05568190168397963]
2023-04-11 15:38:52,546 :: INFO :: ndcg [0.021243637973002875, 0.051172681608995046, 0.06822372269647939]
2023-04-11 15:39:14,296 :: INFO :: ----- test -----
2023-04-11 15:39:14,296 :: INFO :: Precision [0.01858725912986517, 0.013657950761503923, 0.011007696926688678]
2023-04-11 15:39:14,296 :: INFO :: Recall [0.01091798846684166, 0.03850112180938759, 0.059800821203552734]
2023-04-11 15:39:14,296 :: INFO :: ndcg [0.01858725912986517, 0.04221303679250242, 0.0541688756119803]
2023-04-11 15:39:20,592 :: INFO :: Epoch 355: loss tensor(49.5667, device='cuda:0', grad_fn=<AddBackward0>), U.norm 253.87484741210938, V.norm 369.5245666503906, MLP.norm 9.571731567382812
2023-04-11 15:39:42,217 :: INFO :: ----- val -----
2023-04-11 15:39:42,217 :: INFO :: Precision [0.021907501659659216, 0.01704801947333426, 0.014755476875415126]
2023-04-11 15:39:42,217 :: INFO :: Recall [0.008800363243602888, 0.03301517602702916, 0.055554489053462074]
2023-04-11 15:39:42,217 :: INFO :: ndcg [0.021907501659659216, 0.051528578731727136, 0.06837804665272451]
2023-04-11 15:40:03,233 :: INFO :: ----- test -----
2023-04-11 15:40:03,233 :: INFO :: Precision [0.01858725912986517, 0.01351056280364597, 0.010983132267045677]
2023-04-11 15:40:03,233 :: INFO :: Recall [0.010973056253876685, 0.037703134936712936, 0.05923918380669527]
2023-04-11 15:40:03,233 :: INFO :: ndcg [0.01858725912986517, 0.041931983179656636, 0.053998996107516475]
2023-04-11 15:40:08,092 :: INFO :: Epoch 360: loss tensor(49.2831, device='cuda:0', grad_fn=<AddBackward0>), U.norm 255.590576171875, V.norm 369.2062072753906, MLP.norm 9.567127227783203
2023-04-11 15:40:28,436 :: INFO :: ----- val -----
2023-04-11 15:40:28,436 :: INFO :: Precision [0.02195175923876964, 0.017145386147377182, 0.014808585970347649]
2023-04-11 15:40:28,436 :: INFO :: Recall [0.008497603882295538, 0.03343223117908141, 0.05551144159709245]
2023-04-11 15:40:28,436 :: INFO :: ndcg [0.02195175923876964, 0.05175521025280357, 0.06892872218553973]
2023-04-11 15:40:48,061 :: INFO :: ----- test -----
2023-04-11 15:40:48,061 :: INFO :: Precision [0.018832905726295104, 0.013466892297613987, 0.011056826245974672]
2023-04-11 15:40:48,061 :: INFO :: Recall [0.010966096643707528, 0.037388510570898544, 0.05955360807640795]
2023-04-11 15:40:48,061 :: INFO :: ndcg [0.018832905726295104, 0.04179130494980673, 0.05416911699070038]
2023-04-11 15:40:53,561 :: INFO :: Epoch 365: loss tensor(51.0200, device='cuda:0', grad_fn=<AddBackward0>), U.norm 257.3426208496094, V.norm 368.7695617675781, MLP.norm 9.605810165405273
2023-04-11 15:41:14,327 :: INFO :: ----- val -----
2023-04-11 15:41:14,327 :: INFO :: Precision [0.021996016817880062, 0.0171896437264876, 0.014866120823191203]
2023-04-11 15:41:14,327 :: INFO :: Recall [0.008416830956284383, 0.033644606324924956, 0.05579396746346382]
2023-04-11 15:41:14,327 :: INFO :: ndcg [0.021996016817880062, 0.05170260457291403, 0.06897612285990207]
2023-04-11 15:41:35,342 :: INFO :: ----- test -----
2023-04-11 15:41:35,342 :: INFO :: Precision [0.018259730334625253, 0.013636115508487931, 0.01105409683934768]
2023-04-11 15:41:35,342 :: INFO :: Recall [0.010684178276740857, 0.03796803677288493, 0.05944221130843672]
2023-04-11 15:41:35,342 :: INFO :: ndcg [0.018259730334625253, 0.04183007752136211, 0.05400432779707603]
2023-04-11 15:41:41,827 :: INFO :: Epoch 370: loss tensor(53.2314, device='cuda:0', grad_fn=<AddBackward0>), U.norm 258.988037109375, V.norm 368.36199951171875, MLP.norm 9.530654907226562
2023-04-11 15:42:03,030 :: INFO :: ----- val -----
2023-04-11 15:42:03,030 :: INFO :: Precision [0.022748395662757246, 0.017225049789775935, 0.0148661208231912]
2023-04-11 15:42:03,030 :: INFO :: Recall [0.008593351959315673, 0.033606799869114974, 0.05595268775716163]
2023-04-11 15:42:03,030 :: INFO :: ndcg [0.022748395662757246, 0.05229366104674439, 0.06923458677626713]
2023-04-11 15:42:24,499 :: INFO :: ----- test -----
2023-04-11 15:42:24,499 :: INFO :: Precision [0.018287024400895247, 0.013603362628963946, 0.011070473279109675]
2023-04-11 15:42:24,499 :: INFO :: Recall [0.010866783161772184, 0.0377806598571113, 0.05972441908193498]
2023-04-11 15:42:24,499 :: INFO :: ndcg [0.018287024400895247, 0.041894412045132466, 0.05419874994484483]
2023-04-11 15:42:31,046 :: INFO :: Epoch 375: loss tensor(54.4489, device='cuda:0', grad_fn=<AddBackward0>), U.norm 260.63525390625, V.norm 368.1255187988281, MLP.norm 9.591556549072266
2023-04-11 15:42:51,889 :: INFO :: ----- val -----
2023-04-11 15:42:51,889 :: INFO :: Precision [0.02212878955521133, 0.017348971011285113, 0.014879398096924337]
2023-04-11 15:42:51,889 :: INFO :: Recall [0.008473240294556265, 0.03385816909151615, 0.0559390241311928]
2023-04-11 15:42:51,889 :: INFO :: ndcg [0.02212878955521133, 0.05221365165593868, 0.0690348511630466]
2023-04-11 15:43:12,780 :: INFO :: ----- test -----
2023-04-11 15:43:12,780 :: INFO :: Precision [0.018778317593755117, 0.013761668213329893, 0.011051367432720687]
2023-04-11 15:43:12,795 :: INFO :: Recall [0.011128188832179435, 0.0385327962284788, 0.05996827677228993]
2023-04-11 15:43:12,795 :: INFO :: ndcg [0.018778317593755117, 0.042485078011035925, 0.054472798425230035]
2023-04-11 15:43:18,467 :: INFO :: Epoch 380: loss tensor(50.2936, device='cuda:0', grad_fn=<AddBackward0>), U.norm 262.3133239746094, V.norm 367.7578430175781, MLP.norm 9.54625129699707
2023-04-11 15:43:39,045 :: INFO :: ----- val -----
2023-04-11 15:43:39,045 :: INFO :: Precision [0.022527107767205134, 0.0174640407169722, 0.014901526886479553]
2023-04-11 15:43:39,045 :: INFO :: Recall [0.008669421753185165, 0.03420602624354506, 0.05605661356549927]
2023-04-11 15:43:39,045 :: INFO :: ndcg [0.022527107767205134, 0.05273704969003284, 0.06924653976775422]
2023-04-11 15:44:00,624 :: INFO :: ----- test -----
2023-04-11 15:44:00,624 :: INFO :: Precision [0.018396200665975217, 0.013816256345869875, 0.011015885146569685]
2023-04-11 15:44:00,624 :: INFO :: Recall [0.010796480704298367, 0.03856063645033063, 0.05950407952572273]
2023-04-11 15:44:00,624 :: INFO :: ndcg [0.018396200665975217, 0.04240349626323993, 0.054235203964671415]
2023-04-11 15:44:06,983 :: INFO :: Epoch 385: loss tensor(50.7848, device='cuda:0', grad_fn=<AddBackward0>), U.norm 263.96661376953125, V.norm 367.4115295410156, MLP.norm 9.554244995117188
2023-04-11 15:44:27,390 :: INFO :: ----- val -----
2023-04-11 15:44:27,390 :: INFO :: Precision [0.0212878955521133, 0.017623368001769713, 0.015038725381721891]
2023-04-11 15:44:27,390 :: INFO :: Recall [0.008341706022725465, 0.034386851371650255, 0.05629029399070192]
2023-04-11 15:44:27,390 :: INFO :: ndcg [0.0212878955521133, 0.05247925379156766, 0.06922351792081712]
2023-04-11 15:44:48,467 :: INFO :: ----- test -----
2023-04-11 15:44:48,467 :: INFO :: Precision [0.018177848135815272, 0.01377258583983789, 0.011114143785141677]
2023-04-11 15:44:48,467 :: INFO :: Recall [0.010914171103254119, 0.038643044956571435, 0.06033872391794872]
2023-04-11 15:44:48,467 :: INFO :: ndcg [0.018177848135815272, 0.04226604911376149, 0.05445628042464704]
2023-04-11 15:44:53,968 :: INFO :: Epoch 390: loss tensor(51.0799, device='cuda:0', grad_fn=<AddBackward0>), U.norm 265.6565246582031, V.norm 367.1252746582031, MLP.norm 9.608142852783203
2023-04-11 15:45:14,952 :: INFO :: ----- val -----
2023-04-11 15:45:14,952 :: INFO :: Precision [0.02080106218189865, 0.01740208010621761, 0.015087408718743376]
2023-04-11 15:45:14,952 :: INFO :: Recall [0.008267784261541735, 0.033940349498700235, 0.056530282555314885]
2023-04-11 15:45:14,952 :: INFO :: ndcg [0.02080106218189865, 0.051803524595012564, 0.06913979739412682]
2023-04-11 15:45:35,561 :: INFO :: ----- test -----
2023-04-11 15:45:35,561 :: INFO :: Precision [0.01850537693105519, 0.013554233309677961, 0.01110868497188767]
2023-04-11 15:45:35,561 :: INFO :: Recall [0.010984444375310013, 0.03784481309516453, 0.060228592506054154]
2023-04-11 15:45:35,561 :: INFO :: ndcg [0.01850537693105519, 0.0418771375855061, 0.05452741300829974]
2023-04-11 15:45:41,921 :: INFO :: Epoch 395: loss tensor(54.4475, device='cuda:0', grad_fn=<AddBackward0>), U.norm 267.2991638183594, V.norm 366.6736755371094, MLP.norm 9.47018051147461
2023-04-11 15:46:03,749 :: INFO :: ----- val -----
2023-04-11 15:46:03,749 :: INFO :: Precision [0.022261562292542596, 0.017534852843548873, 0.015109537508298567]
2023-04-11 15:46:03,749 :: INFO :: Recall [0.008724756276371497, 0.034229363806251537, 0.05632585295561473]
2023-04-11 15:46:03,749 :: INFO :: ndcg [0.022261562292542596, 0.052774324145470536, 0.06988934818860199]
2023-04-11 15:46:25,952 :: INFO :: ----- test -----
2023-04-11 15:46:25,952 :: INFO :: Precision [0.01842349473224521, 0.01355423330967796, 0.01118783776407066]
2023-04-11 15:46:25,952 :: INFO :: Recall [0.010814434493353087, 0.037932474875776036, 0.06067802975506599]
2023-04-11 15:46:25,952 :: INFO :: ndcg [0.01842349473224521, 0.04191071244486753, 0.05461980397942682]
2023-04-11 15:46:32,655 :: INFO :: Epoch 400: loss tensor(54.1930, device='cuda:0', grad_fn=<AddBackward0>), U.norm 268.9515075683594, V.norm 366.3333740234375, MLP.norm 9.522387504577637
2023-04-11 15:46:55,327 :: INFO :: ----- val -----
2023-04-11 15:46:55,327 :: INFO :: Precision [0.02212878955521133, 0.017340119495463022, 0.015131666297853785]
2023-04-11 15:46:55,327 :: INFO :: Recall [0.008645613698830344, 0.033915316142625884, 0.05594597579814338]
2023-04-11 15:46:55,327 :: INFO :: ndcg [0.02212878955521133, 0.052327110155576596, 0.06970421041204226]
2023-04-11 15:47:16,561 :: INFO :: ----- test -----
2023-04-11 15:47:16,561 :: INFO :: Precision [0.01861455319613516, 0.013630656695233933, 0.011174190730935658]
2023-04-11 15:47:16,561 :: INFO :: Recall [0.011174644100451174, 0.038339619821960325, 0.060065949389801615]
2023-04-11 15:47:16,561 :: INFO :: ndcg [0.01861455319613516, 0.04224261704414067, 0.05473233816025544]
2023-04-11 15:47:22,108 :: INFO :: Epoch 405: loss tensor(53.4431, device='cuda:0', grad_fn=<AddBackward0>), U.norm 270.54339599609375, V.norm 366.1209716796875, MLP.norm 9.44996166229248
2023-04-11 15:47:43,171 :: INFO :: ----- val -----
2023-04-11 15:47:43,171 :: INFO :: Precision [0.02164195618499668, 0.01729586191635261, 0.015074131445010226]
2023-04-11 15:47:43,171 :: INFO :: Recall [0.00864962905841365, 0.033737584829161234, 0.0560231615577868]
2023-04-11 15:47:43,171 :: INFO :: ndcg [0.02164195618499668, 0.051957103878207386, 0.06943238399697142]
2023-04-11 15:48:03,827 :: INFO :: ----- test -----
2023-04-11 15:48:03,827 :: INFO :: Precision [0.01896937605764507, 0.013690703641027912, 0.011125061411649678]
2023-04-11 15:48:03,827 :: INFO :: Recall [0.011476365324438895, 0.03849271078998388, 0.06046485623741248]
2023-04-11 15:48:03,827 :: INFO :: ndcg [0.01896937605764507, 0.042710192804758094, 0.05499220637905378]
2023-04-11 15:48:09,905 :: INFO :: Epoch 410: loss tensor(50.1055, device='cuda:0', grad_fn=<AddBackward0>), U.norm 272.1696472167969, V.norm 365.95391845703125, MLP.norm 9.45373821258545
2023-04-11 15:48:31,905 :: INFO :: ----- val -----
2023-04-11 15:48:31,905 :: INFO :: Precision [0.020668289444567382, 0.017287010400530524, 0.015021022350077717]
2023-04-11 15:48:31,905 :: INFO :: Recall [0.008311783253870003, 0.0339730153973564, 0.05616817846033328]
2023-04-11 15:48:31,905 :: INFO :: ndcg [0.020668289444567382, 0.051624667518682714, 0.06898566594679471]
2023-04-11 15:48:53,702 :: INFO :: ----- test -----
2023-04-11 15:48:53,702 :: INFO :: Precision [0.018259730334625253, 0.013625197881979934, 0.01112233200502266]
2023-04-11 15:48:53,702 :: INFO :: Recall [0.011002309392970325, 0.037974208853214036, 0.06030619946778201]
2023-04-11 15:48:53,702 :: INFO :: ndcg [0.018259730334625253, 0.04213651935259213, 0.05447618828483242]
2023-04-11 15:49:00,514 :: INFO :: Epoch 415: loss tensor(49.9097, device='cuda:0', grad_fn=<AddBackward0>), U.norm 273.81085205078125, V.norm 365.6802062988281, MLP.norm 9.495132446289062
2023-04-11 15:49:22,765 :: INFO :: ----- val -----
2023-04-11 15:49:22,765 :: INFO :: Precision [0.0212878955521133, 0.017295861916352608, 0.014883823854835375]
2023-04-11 15:49:22,765 :: INFO :: Recall [0.00893812423365695, 0.03394412851228595, 0.05586847193139972]
2023-04-11 15:49:22,765 :: INFO :: ndcg [0.0212878955521133, 0.05176513477819997, 0.06871715858075518]
2023-04-11 15:49:44,749 :: INFO :: ----- test -----
2023-04-11 15:49:44,749 :: INFO :: Precision [0.017140673617555544, 0.013510562803645971, 0.011081390905617675]
2023-04-11 15:49:44,749 :: INFO :: Recall [0.010498779759435469, 0.03803733407108414, 0.05995460989350498]
2023-04-11 15:49:44,749 :: INFO :: ndcg [0.017140673617555544, 0.041393350360740495, 0.0539121403658746]
2023-04-11 15:49:51,124 :: INFO :: Epoch 420: loss tensor(52.0961, device='cuda:0', grad_fn=<AddBackward0>), U.norm 275.4289855957031, V.norm 365.3628234863281, MLP.norm 9.522089004516602
2023-04-11 15:50:13,202 :: INFO :: ----- val -----
2023-04-11 15:50:13,202 :: INFO :: Precision [0.019738880283248505, 0.01734897101128511, 0.014945784465589971]
2023-04-11 15:50:13,202 :: INFO :: Recall [0.008199652843345881, 0.034257911531449865, 0.05575903202911524]
2023-04-11 15:50:13,202 :: INFO :: ndcg [0.019738880283248505, 0.051398655707917934, 0.06844590862388801]
2023-04-11 15:50:34,905 :: INFO :: ----- test -----
2023-04-11 15:50:34,905 :: INFO :: Precision [0.01700420328620558, 0.013810797532615875, 0.011141437851411666]
2023-04-11 15:50:34,905 :: INFO :: Recall [0.01023080297689468, 0.03849708703269572, 0.06062340438873615]
2023-04-11 15:50:34,905 :: INFO :: ndcg [0.01700420328620558, 0.04212744966437043, 0.05429222839950849]
2023-04-11 15:50:41,218 :: INFO :: Epoch 425: loss tensor(47.3486, device='cuda:0', grad_fn=<AddBackward0>), U.norm 277.0647277832031, V.norm 364.99859619140625, MLP.norm 9.523869514465332
2023-04-11 15:51:03,092 :: INFO :: ----- val -----
2023-04-11 15:51:03,092 :: INFO :: Precision [0.01978313786235893, 0.01752600132772679, 0.014892675370657435]
2023-04-11 15:51:03,092 :: INFO :: Recall [0.008261210962204547, 0.03461607200683602, 0.055893822590811874]
2023-04-11 15:51:03,092 :: INFO :: ndcg [0.01978313786235893, 0.05189370063473633, 0.06834662399206477]
2023-04-11 15:51:25,061 :: INFO :: ----- test -----
2023-04-11 15:51:25,061 :: INFO :: Precision [0.016976909219935585, 0.013696162454281914, 0.011291555215896653]
2023-04-11 15:51:25,061 :: INFO :: Recall [0.010206162500400935, 0.038389012832253384, 0.0615901423670116]
2023-04-11 15:51:25,061 :: INFO :: ndcg [0.016976909219935585, 0.041755934297387574, 0.054588190388368416]
2023-04-11 15:51:31,327 :: INFO :: Epoch 430: loss tensor(49.3568, device='cuda:0', grad_fn=<AddBackward0>), U.norm 278.6853332519531, V.norm 364.7541198730469, MLP.norm 9.486503601074219
2023-04-11 15:51:53,139 :: INFO :: ----- val -----
2023-04-11 15:51:53,139 :: INFO :: Precision [0.020668289444567382, 0.017410931622039702, 0.014888249612746403]
2023-04-11 15:51:53,139 :: INFO :: Recall [0.008596656908405088, 0.03406417739055612, 0.05533341379724031]
2023-04-11 15:51:53,139 :: INFO :: ndcg [0.020668289444567382, 0.05186115402485727, 0.06859194440370045]
2023-04-11 15:52:14,999 :: INFO :: ----- test -----
2023-04-11 15:52:14,999 :: INFO :: Precision [0.017550084611605437, 0.013723456520551904, 0.011187837764070657]
2023-04-11 15:52:14,999 :: INFO :: Recall [0.010460655431213531, 0.03829199599757228, 0.06074144366958027]
2023-04-11 15:52:14,999 :: INFO :: ndcg [0.017550084611605437, 0.042182169873249534, 0.05463619763831691]
2023-04-11 15:52:21,202 :: INFO :: Epoch 435: loss tensor(48.2207, device='cuda:0', grad_fn=<AddBackward0>), U.norm 280.2718811035156, V.norm 364.6578369140625, MLP.norm 9.42332935333252
2023-04-11 15:52:43,124 :: INFO :: ----- val -----
2023-04-11 15:52:43,124 :: INFO :: Precision [0.02062403186545696, 0.017543704359370957, 0.014998893560522485]
2023-04-11 15:52:43,124 :: INFO :: Recall [0.008382328258028636, 0.03480763435057419, 0.05543178119312401]
2023-04-11 15:52:43,124 :: INFO :: ndcg [0.02062403186545696, 0.052192066625960314, 0.0688215021777281]
2023-04-11 15:53:04,655 :: INFO :: ----- test -----
2023-04-11 15:53:04,655 :: INFO :: Precision [0.017140673617555544, 0.013919973797695839, 0.011215131830340666]
2023-04-11 15:53:04,655 :: INFO :: Recall [0.010091041012660857, 0.038663922883537136, 0.0610532753020011]
2023-04-11 15:53:04,655 :: INFO :: ndcg [0.017140673617555544, 0.04234724674040202, 0.054649135026532215]
2023-04-11 15:53:10,686 :: INFO :: Epoch 440: loss tensor(49.9722, device='cuda:0', grad_fn=<AddBackward0>), U.norm 281.8433837890625, V.norm 364.51239013671875, MLP.norm 9.509734153747559
2023-04-11 15:53:32,889 :: INFO :: ----- val -----
2023-04-11 15:53:32,889 :: INFO :: Precision [0.02093383491922992, 0.017649922549235968, 0.01498561628678935]
2023-04-11 15:53:32,889 :: INFO :: Recall [0.008657550646550428, 0.03430788025918051, 0.05567691349248957]
2023-04-11 15:53:32,889 :: INFO :: ndcg [0.02093383491922992, 0.05215229276371497, 0.06874725400633604]
2023-04-11 15:53:54,452 :: INFO :: ----- test -----
2023-04-11 15:53:54,452 :: INFO :: Precision [0.017904907473115343, 0.013745291773567899, 0.011185108357443662]
2023-04-11 15:53:54,452 :: INFO :: Recall [0.010486416724647734, 0.038119389306580626, 0.06013484042446878]
2023-04-11 15:53:54,452 :: INFO :: ndcg [0.017904907473115343, 0.04227152655538332, 0.054764016767180845]
2023-04-11 15:53:59,921 :: INFO :: Epoch 445: loss tensor(50.6044, device='cuda:0', grad_fn=<AddBackward0>), U.norm 283.45989990234375, V.norm 364.31231689453125, MLP.norm 9.536885261535645
2023-04-11 15:54:20,363 :: INFO :: ----- val -----
2023-04-11 15:54:20,363 :: INFO :: Precision [0.021509183447665413, 0.01760566497012555, 0.015065279929188134]
2023-04-11 15:54:20,363 :: INFO :: Recall [0.008744861252583025, 0.034128354025276024, 0.05604752633081538]
2023-04-11 15:54:20,363 :: INFO :: ndcg [0.021509183447665413, 0.05263035212367239, 0.06952655795413619]
2023-04-11 15:54:40,722 :: INFO :: ----- test -----
2023-04-11 15:54:40,722 :: INFO :: Precision [0.018751023527485124, 0.013788962279599883, 0.01130520224903165]
2023-04-11 15:54:40,722 :: INFO :: Recall [0.01095275602430217, 0.038500849656464144, 0.06108530125640519]
2023-04-11 15:54:40,722 :: INFO :: ndcg [0.018751023527485124, 0.04279263184787828, 0.05552148326945005]
2023-04-11 15:54:45,941 :: INFO :: Epoch 450: loss tensor(51.5865, device='cuda:0', grad_fn=<AddBackward0>), U.norm 284.995361328125, V.norm 364.1639404296875, MLP.norm 9.288921356201172
2023-04-11 15:55:05,722 :: INFO :: ----- val -----
2023-04-11 15:55:05,722 :: INFO :: Precision [0.0212878955521133, 0.017711883159990553, 0.01517149811905317]
2023-04-11 15:55:05,722 :: INFO :: Recall [0.00853524187036812, 0.03451745015603977, 0.05682437438253616]
2023-04-11 15:55:05,722 :: INFO :: ndcg [0.0212878955521133, 0.052487245081530944, 0.06971647626730747]
2023-04-11 15:55:25,691 :: INFO :: ----- test -----
2023-04-11 15:55:25,691 :: INFO :: Precision [0.01823243626835526, 0.01385992685190186, 0.011228778863475654]
2023-04-11 15:55:25,691 :: INFO :: Recall [0.010458114037938901, 0.03868135647385124, 0.06088441082153278]
2023-04-11 15:55:25,691 :: INFO :: ndcg [0.01823243626835526, 0.04271748127935257, 0.05514614117404551]
2023-04-11 15:55:30,628 :: INFO :: Epoch 455: loss tensor(48.5644, device='cuda:0', grad_fn=<AddBackward0>), U.norm 286.5619812011719, V.norm 363.925048828125, MLP.norm 9.360671997070312
2023-04-11 15:55:51,159 :: INFO :: ----- val -----
2023-04-11 15:55:51,159 :: INFO :: Precision [0.02093383491922992, 0.017782695286567222, 0.014976764770967281]
2023-04-11 15:55:51,159 :: INFO :: Recall [0.008503297222357969, 0.034857704284405946, 0.05596385559719252]
2023-04-11 15:55:51,159 :: INFO :: ndcg [0.02093383491922992, 0.052444622371567926, 0.0689495734014875]
2023-04-11 15:56:11,300 :: INFO :: ----- test -----
2023-04-11 15:56:11,300 :: INFO :: Precision [0.017631966810415414, 0.013745291773567897, 0.01123696708335665]
2023-04-11 15:56:11,300 :: INFO :: Recall [0.010084922924870298, 0.038467220916850016, 0.061015738067640164]
2023-04-11 15:56:11,300 :: INFO :: ndcg [0.017631966810415414, 0.042248874166884755, 0.05483477546298459]
2023-04-11 15:56:16,550 :: INFO :: Epoch 460: loss tensor(51.6028, device='cuda:0', grad_fn=<AddBackward0>), U.norm 288.1144104003906, V.norm 363.78033447265625, MLP.norm 9.323652267456055
2023-04-11 15:56:36,363 :: INFO :: ----- val -----
2023-04-11 15:56:36,363 :: INFO :: Precision [0.02062403186545696, 0.017605664970125542, 0.014901526886479532]
2023-04-11 15:56:36,363 :: INFO :: Recall [0.008200237359465767, 0.0345896031297395, 0.05559783150355333]
2023-04-11 15:56:36,363 :: INFO :: ndcg [0.02062403186545696, 0.05214678588325848, 0.06866510047229615]
2023-04-11 15:56:56,707 :: INFO :: ----- test -----
2023-04-11 15:56:56,707 :: INFO :: Precision [0.017058791418745566, 0.013838091598885867, 0.01123150827010265]
2023-04-11 15:56:56,707 :: INFO :: Recall [0.009979375037647651, 0.0386330426247865, 0.06096259266052665]
2023-04-11 15:56:56,707 :: INFO :: ndcg [0.017058791418745566, 0.04222051677147744, 0.05469771024098185]
2023-04-11 15:57:01,987 :: INFO :: Epoch 465: loss tensor(51.2604, device='cuda:0', grad_fn=<AddBackward0>), U.norm 289.69207763671875, V.norm 363.5775451660156, MLP.norm 9.284123420715332
2023-04-11 15:57:22,597 :: INFO :: ----- val -----
2023-04-11 15:57:22,597 :: INFO :: Precision [0.02097809249834034, 0.017800398318211397, 0.015060854171277067]
2023-04-11 15:57:22,597 :: INFO :: Recall [0.008563418450189047, 0.03495706804228894, 0.0555771702339452]
2023-04-11 15:57:22,597 :: INFO :: ndcg [0.02097809249834034, 0.05262735440024395, 0.06898155885127984]
2023-04-11 15:57:43,300 :: INFO :: ----- test -----
2023-04-11 15:57:43,300 :: INFO :: Precision [0.01757737867787543, 0.013930891424203838, 0.011144167258038653]
2023-04-11 15:57:43,300 :: INFO :: Recall [0.010424188183754535, 0.039102594332511544, 0.05991779990734631]
2023-04-11 15:57:43,300 :: INFO :: ndcg [0.01757737867787543, 0.04272789632803832, 0.054790517351960354]
2023-04-11 15:57:48,362 :: INFO :: Epoch 470: loss tensor(50.7749, device='cuda:0', grad_fn=<AddBackward0>), U.norm 291.2275390625, V.norm 363.4023742675781, MLP.norm 9.382354736328125
2023-04-11 15:58:08,847 :: INFO :: ----- val -----
2023-04-11 15:58:08,847 :: INFO :: Precision [0.020402743969904845, 0.01769418012834639, 0.015175923876964193]
2023-04-11 15:58:08,847 :: INFO :: Recall [0.008309251326417745, 0.03479560646226746, 0.05635160106940329]
2023-04-11 15:58:08,847 :: INFO :: ndcg [0.020402743969904845, 0.052062610318211924, 0.06907759575601947]
2023-04-11 15:58:29,769 :: INFO :: ----- test -----
2023-04-11 15:58:29,769 :: INFO :: Precision [0.01700420328620558, 0.01400731480975982, 0.011258802336372643]
2023-04-11 15:58:29,769 :: INFO :: Recall [0.010022005769726498, 0.039728638232858736, 0.060690141690832734]
2023-04-11 15:58:29,769 :: INFO :: ndcg [0.01700420328620558, 0.04267003686466728, 0.054888928566393186]
2023-04-11 15:58:35,097 :: INFO :: Epoch 475: loss tensor(49.0042, device='cuda:0', grad_fn=<AddBackward0>), U.norm 292.7118225097656, V.norm 363.2809143066406, MLP.norm 9.319555282592773
2023-04-11 15:58:55,722 :: INFO :: ----- val -----
2023-04-11 15:58:55,722 :: INFO :: Precision [0.021155122814782032, 0.01745518920115012, 0.015224607213985667]
2023-04-11 15:58:55,722 :: INFO :: Recall [0.008666229725961866, 0.0341642456031431, 0.05661911352592131]
2023-04-11 15:58:55,722 :: INFO :: ndcg [0.021155122814782032, 0.05199203265132973, 0.0694556091314053]
2023-04-11 15:59:16,550 :: INFO :: ----- test -----
2023-04-11 15:59:16,550 :: INFO :: Precision [0.017440908346525466, 0.013876303291663854, 0.011313390468912646]
2023-04-11 15:59:16,550 :: INFO :: Recall [0.010209196589593773, 0.03923726703307493, 0.061065488542996225]
2023-04-11 15:59:16,550 :: INFO :: ndcg [0.017440908346525466, 0.0425218259857848, 0.05501994780721307]
2023-04-11 15:59:21,722 :: INFO :: Epoch 480: loss tensor(50.8570, device='cuda:0', grad_fn=<AddBackward0>), U.norm 294.1730651855469, V.norm 363.1572570800781, MLP.norm 9.327128410339355
2023-04-11 15:59:42,034 :: INFO :: ----- val -----
2023-04-11 15:59:42,034 :: INFO :: Precision [0.020668289444567382, 0.017384377074573448, 0.015206904182341491]
2023-04-11 15:59:42,034 :: INFO :: Recall [0.008340430092240125, 0.03414285651851921, 0.0562436932353633]
2023-04-11 15:59:42,034 :: INFO :: ndcg [0.020668289444567382, 0.05161658538173563, 0.06901222165568942]
2023-04-11 16:00:02,269 :: INFO :: ----- test -----
2023-04-11 16:00:02,269 :: INFO :: Precision [0.01746820241279546, 0.013881762104917855, 0.01136251978819864]
2023-04-11 16:00:02,269 :: INFO :: Recall [0.010299861727477643, 0.03912226972881677, 0.06104410522997134]
2023-04-11 16:00:02,269 :: INFO :: ndcg [0.01746820241279546, 0.04255102690109648, 0.055264998979113175]
2023-04-11 16:00:07,378 :: INFO :: Epoch 485: loss tensor(50.8426, device='cuda:0', grad_fn=<AddBackward0>), U.norm 295.6375427246094, V.norm 363.0789794921875, MLP.norm 9.281221389770508
2023-04-11 16:00:27,567 :: INFO :: ----- val -----
2023-04-11 16:00:27,567 :: INFO :: Precision [0.020535516707236114, 0.017534852843548877, 0.015215755698163585]
2023-04-11 16:00:27,567 :: INFO :: Recall [0.0080233785655214, 0.033888720108402146, 0.056560582531701785]
2023-04-11 16:00:27,567 :: INFO :: ndcg [0.020535516707236114, 0.05193826179251067, 0.06929803131311203]
2023-04-11 16:00:48,300 :: INFO :: ----- test -----
2023-04-11 16:00:48,300 :: INFO :: Precision [0.017277143948905508, 0.013936350237457837, 0.011337955128555639]
2023-04-11 16:00:48,300 :: INFO :: Recall [0.010316660575408105, 0.039211390994978376, 0.06139052294096483]
2023-04-11 16:00:48,300 :: INFO :: ndcg [0.017277143948905508, 0.042687394537949196, 0.05521150399804096]
2023-04-11 16:00:53,737 :: INFO :: Epoch 490: loss tensor(49.6209, device='cuda:0', grad_fn=<AddBackward0>), U.norm 297.1181945800781, V.norm 362.99310302734375, MLP.norm 9.330217361450195
2023-04-11 16:01:13,628 :: INFO :: ----- val -----
2023-04-11 16:01:13,628 :: INFO :: Precision [0.02177472892232795, 0.017526001327726793, 0.015127240539942723]
2023-04-11 16:01:13,628 :: INFO :: Recall [0.008648495253702057, 0.03405725255367724, 0.05586981265187065]
2023-04-11 16:01:13,628 :: INFO :: ndcg [0.02177472892232795, 0.05232663846297481, 0.06919999051147979]
2023-04-11 16:01:33,565 :: INFO :: ----- test -----
2023-04-11 16:01:33,565 :: INFO :: Precision [0.01795949560565533, 0.013936350237457837, 0.011297014029150643]
2023-04-11 16:01:33,565 :: INFO :: Recall [0.01033455764433764, 0.039054843199146566, 0.06079917548724851]
2023-04-11 16:01:33,565 :: INFO :: ndcg [0.01795949560565533, 0.04279093777779296, 0.05521871376880081]
2023-04-11 16:01:38,425 :: INFO :: Epoch 495: loss tensor(52.1188, device='cuda:0', grad_fn=<AddBackward0>), U.norm 298.6814270019531, V.norm 362.8705139160156, MLP.norm 9.334085464477539
2023-04-11 16:01:59,128 :: INFO :: ----- val -----
2023-04-11 16:01:59,128 :: INFO :: Precision [0.02177472892232795, 0.0176322195175918, 0.015361805709227988]
2023-04-11 16:01:59,128 :: INFO :: Recall [0.008750787528541977, 0.03428672411830063, 0.056499456007047213]
2023-04-11 16:01:59,128 :: INFO :: ndcg [0.02177472892232795, 0.052827319377832714, 0.06991039175618605]
2023-04-11 16:02:20,253 :: INFO :: ----- test -----
2023-04-11 16:02:20,253 :: INFO :: Precision [0.017522790545335443, 0.013925432610949839, 0.011307931655658646]
2023-04-11 16:02:20,253 :: INFO :: Recall [0.010045583142832864, 0.03873661302311907, 0.06081531118256472]
2023-04-11 16:02:20,253 :: INFO :: ndcg [0.017522790545335443, 0.04276366239633595, 0.055208869216186396]
2023-04-11 16:02:24,316 :: INFO :: Epoch 500:
2023-04-11 16:02:44,378 :: INFO :: ----- val -----
2023-04-11 16:02:44,378 :: INFO :: Precision [0.021730471343217525, 0.01751714981190471, 0.015242310245629847]
2023-04-11 16:02:44,378 :: INFO :: Recall [0.00869414532814075, 0.034275888709278605, 0.056739175826377525]
2023-04-11 16:02:44,378 :: INFO :: ndcg [0.021730471343217525, 0.05249024931373722, 0.06962181632351477]
2023-04-11 16:03:04,691 :: INFO :: ----- test -----
2023-04-11 16:03:04,691 :: INFO :: Precision [0.018259730334625253, 0.013838091598885865, 0.01139254326109564]
2023-04-11 16:03:04,691 :: INFO :: Recall [0.010757864784032135, 0.038952670509135964, 0.06139301730280571]
2023-04-11 16:03:04,691 :: INFO :: ndcg [0.018259730334625253, 0.04278108367545381, 0.05556828820914314]
2023-04-11 16:03:04,691 :: INFO :: final:
2023-04-11 16:03:04,691 :: INFO :: ----- test -----
2023-04-11 16:03:04,691 :: INFO :: Precision [0.017522790545335443, 0.013925432610949839, 0.011307931655658646]
2023-04-11 16:03:04,691 :: INFO :: Recall [0.010045583142832864, 0.03873661302311907, 0.06081531118256472]
2023-04-11 16:03:04,691 :: INFO :: ndcg [0.017522790545335443, 0.04276366239633595, 0.055208869216186396]
2023-04-11 16:03:04,691 :: INFO :: max_epoch 495:
2023-04-23 16:09:55,071 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:09:55,071 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:09:56,896 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:27:31,567 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:27:31,568 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:27:33,692 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:28:42,541 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:28:42,541 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:28:44,390 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:28:52,539 :: INFO :: ERM mask: from pre-train <numpy.lib.npyio.NpzFile object at 0x7fdc9fc068e0>
2023-04-23 16:35:34,335 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:35:34,336 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:35:36,614 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:35:46,765 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 16:36:30,139 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:36:30,140 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:36:32,400 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:36:41,359 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 16:53:42,839 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 16:53:42,840 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 16:53:44,856 :: INFO :: torch.Size([76085, 384])
2023-04-23 16:53:52,938 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-23 17:00:40,830 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-23 17:00:40,831 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=1, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-04-23 17:00:43,015 :: INFO :: torch.Size([76085, 384])
2023-04-23 17:00:51,618 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-26 10:11:52,912 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-26 10:11:52,914 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-26 10:11:54,660 :: INFO :: torch.Size([76085, 384])
2023-04-26 10:12:02,911 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 09:54:38,933 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 09:54:38,934 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 09:54:40,730 :: INFO :: torch.Size([76085, 384])
2023-04-27 09:54:49,169 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 09:56:58,487 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 09:56:58,488 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 09:57:00,340 :: INFO :: torch.Size([76085, 384])
2023-04-27 09:57:09,309 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 09:57:29,310 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 09:57:29,326 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 09:57:30,826 :: INFO :: torch.Size([76085, 384])
2023-04-27 09:57:38,737 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:02:07,321 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:02:07,322 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:02:09,094 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:02:17,821 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:04:11,632 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:04:11,632 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:04:13,522 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:04:21,786 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:07:03,941 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:07:04,273 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:07:06,653 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:08:15,383 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:12:59,998 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:12:59,998 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:13:01,635 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:13:09,668 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:18:14,234 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:18:14,234 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:18:16,008 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:18:24,090 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:26:28,979 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:26:28,980 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:26:31,000 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:26:40,498 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 10:30:28,764 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 10:30:28,765 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 10:30:30,863 :: INFO :: torch.Size([76085, 384])
2023-04-27 10:30:39,369 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 14:41:02,095 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 14:41:02,096 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 14:41:04,449 :: INFO :: torch.Size([76085, 384])
2023-04-27 14:41:13,348 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 14:42:17,794 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 14:42:17,795 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 14:42:19,703 :: INFO :: torch.Size([76085, 384])
2023-04-27 14:42:27,525 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-27 14:43:32,211 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-27 14:43:32,211 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-27 14:43:33,823 :: INFO :: torch.Size([76085, 384])
2023-04-27 14:43:42,393 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-28 10:17:13,878 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-28 10:17:13,879 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-28 10:17:15,838 :: INFO :: torch.Size([76085, 384])
2023-04-28 10:17:27,261 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-28 10:27:13,675 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-28 10:27:13,677 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-28 10:27:16,400 :: INFO :: torch.Size([76085, 384])
2023-04-28 10:27:26,281 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-28 11:02:31,687 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-28 11:02:31,688 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-28 11:02:33,603 :: INFO :: torch.Size([76085, 384])
2023-04-28 11:02:41,783 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-28 11:03:22,689 :: INFO :: Epoch 5: loss tensor(1.9275, grad_fn=<AddBackward0>), U.norm 0.33812186121940613, V.norm 0.3449653685092926, MLP.norm 0.024449404329061508
2023-04-29 15:43:21,131 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 15:43:21,137 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 15:43:22,942 :: INFO :: torch.Size([76085, 384])
2023-04-29 15:54:05,307 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 15:54:05,308 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 15:54:07,280 :: INFO :: torch.Size([76085, 384])
2023-04-29 15:54:16,472 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-29 15:55:02,671 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 15:55:02,671 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 15:55:04,585 :: INFO :: torch.Size([76085, 384])
2023-04-29 15:55:12,718 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-29 15:57:16,270 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 15:57:16,270 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 15:57:18,265 :: INFO :: torch.Size([76085, 384])
2023-04-29 15:57:26,632 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-29 16:00:40,695 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:00:40,696 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:00:42,687 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:01:23,475 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:01:23,475 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:01:25,579 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:02:12,512 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:02:12,513 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:02:14,463 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:02:34,404 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:02:34,404 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:02:36,261 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:02:43,966 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-29 16:11:12,031 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:11:12,031 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:11:13,925 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:13:28,769 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:13:28,770 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:13:30,719 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:24:41,137 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:24:41,138 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:24:43,094 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:25:00,637 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:25:00,637 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:25:02,664 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:27:01,424 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:27:01,424 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:27:03,399 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:27:38,709 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:27:38,709 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:27:40,736 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:28:17,296 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:28:17,296 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:28:19,396 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:28:32,177 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:28:32,178 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:28:34,270 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:29:40,292 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:29:40,292 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:29:42,187 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:30:00,407 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:30:00,408 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:30:02,400 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:30:41,441 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:30:41,442 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:30:43,257 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:31:16,589 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:31:16,589 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:31:18,523 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:33:16,046 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:33:16,046 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:33:17,864 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:36:12,512 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:36:12,512 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:36:14,500 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:41:58,045 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:41:58,045 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:41:59,896 :: INFO :: torch.Size([76085, 384])
2023-04-29 16:53:31,376 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 16:53:31,377 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 16:53:33,211 :: INFO :: torch.Size([76085, 384])
2023-04-29 17:08:40,534 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 17:08:40,534 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 17:08:42,318 :: INFO :: torch.Size([76085, 384])
2023-04-29 17:18:11,928 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 17:18:11,929 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 17:18:14,343 :: INFO :: torch.Size([76085, 384])
2023-04-29 17:18:37,651 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 17:18:37,651 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 17:18:39,536 :: INFO :: torch.Size([76085, 384])
2023-04-29 17:21:41,383 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-29 17:21:41,383 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-29 17:21:43,052 :: INFO :: torch.Size([76085, 384])
2023-04-29 17:25:44,541 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-04-29 17:26:26,242 :: INFO :: Epoch 5: loss tensor(1.9275, grad_fn=<AddBackward0>), U.norm 0.33812186121940613, V.norm 0.3449653685092926, MLP.norm 0.024449404329061508
2023-04-30 22:54:10,139 :: INFO :: log info to logs/tiktok_InvRL.log
2023-04-30 22:54:10,146 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cpu', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-04-30 22:54:12,468 :: INFO :: torch.Size([76085, 384])
2023-04-30 22:54:27,284 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-14 11:34:52,229 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:34:52,229 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:34:53,667 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:35:21,421 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:35:21,421 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:35:22,796 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:35:48,410 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:35:48,410 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:35:49,801 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:36:41,371 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:36:41,371 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:36:42,746 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:36:53,373 :: INFO :: ----- frontend -----
2023-05-14 11:36:53,373 :: INFO :: Environment 0
2023-05-14 11:44:05,189 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:44:05,189 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:44:06,611 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:44:17,457 :: INFO :: ----- frontend -----
2023-05-14 11:44:17,472 :: INFO :: Environment 0
2023-05-14 11:44:32,053 :: INFO :: Epoch 5: loss tensor(824.9978, device='cuda:0'), U.norm 11.396390914916992, V.norm 16.889446258544922, MLP.norm 1.5020573139190674
2023-05-14 11:44:32,271 :: INFO :: Epoch 10: loss tensor(812.4426, device='cuda:0'), U.norm 8.957918167114258, V.norm 16.191795349121094, MLP.norm 2.5475401878356934
2023-05-14 11:44:32,490 :: INFO :: Epoch 15: loss tensor(793.9803, device='cuda:0'), U.norm 7.480686187744141, V.norm 15.862152099609375, MLP.norm 3.8448050022125244
2023-05-14 11:44:32,725 :: INFO :: Epoch 20: loss tensor(771.1577, device='cuda:0'), U.norm 6.473093509674072, V.norm 15.647988319396973, MLP.norm 5.155084133148193
2023-05-14 11:44:32,740 :: INFO :: Environment 1
2023-05-14 11:44:42,069 :: INFO :: Epoch 5: loss tensor(827.7271, device='cuda:0'), U.norm 11.399611473083496, V.norm 16.888978958129883, MLP.norm 1.4918551445007324
2023-05-14 11:44:42,288 :: INFO :: Epoch 10: loss tensor(815.2791, device='cuda:0'), U.norm 8.963950157165527, V.norm 16.193113327026367, MLP.norm 2.534506320953369
2023-05-14 11:44:42,491 :: INFO :: Epoch 15: loss tensor(796.5935, device='cuda:0'), U.norm 7.489792346954346, V.norm 15.860926628112793, MLP.norm 3.838718891143799
2023-05-14 11:44:42,710 :: INFO :: Epoch 20: loss tensor(773.2960, device='cuda:0'), U.norm 6.485748291015625, V.norm 15.644908905029297, MLP.norm 5.169428825378418
2023-05-14 11:44:42,726 :: INFO :: Environment 2
2023-05-14 11:44:52,149 :: INFO :: Epoch 5: loss tensor(826.2293, device='cuda:0'), U.norm 11.396538734436035, V.norm 16.87787628173828, MLP.norm 1.4901721477508545
2023-05-14 11:44:52,383 :: INFO :: Epoch 10: loss tensor(813.5641, device='cuda:0'), U.norm 8.958023071289062, V.norm 16.17283058166504, MLP.norm 2.5262489318847656
2023-05-14 11:44:52,602 :: INFO :: Epoch 15: loss tensor(794.1400, device='cuda:0'), U.norm 7.480574607849121, V.norm 15.838315963745117, MLP.norm 3.8269855976104736
2023-05-14 11:44:52,836 :: INFO :: Epoch 20: loss tensor(770.7892, device='cuda:0'), U.norm 6.472248554229736, V.norm 15.62040901184082, MLP.norm 5.164258003234863
2023-05-14 11:44:52,852 :: INFO :: Environment 3
2023-05-14 11:45:02,400 :: INFO :: Epoch 5: loss tensor(873.0391, device='cuda:0'), U.norm 11.400629043579102, V.norm 16.974842071533203, MLP.norm 1.4955497980117798
2023-05-14 11:45:02,634 :: INFO :: Epoch 10: loss tensor(858.4743, device='cuda:0'), U.norm 8.967375755310059, V.norm 16.316152572631836, MLP.norm 2.5955915451049805
2023-05-14 11:45:02,869 :: INFO :: Epoch 15: loss tensor(836.9518, device='cuda:0'), U.norm 7.4964680671691895, V.norm 16.002052307128906, MLP.norm 3.9596104621887207
2023-05-14 11:45:03,103 :: INFO :: Epoch 20: loss tensor(810.8986, device='cuda:0'), U.norm 6.495984077453613, V.norm 15.797063827514648, MLP.norm 5.337513446807861
2023-05-14 11:45:03,119 :: INFO :: Environment 4
2023-05-14 11:45:12,463 :: INFO :: Epoch 5: loss tensor(891.6451, device='cuda:0'), U.norm 11.3975830078125, V.norm 17.001510620117188, MLP.norm 1.5166687965393066
2023-05-14 11:45:12,698 :: INFO :: Epoch 10: loss tensor(877.4676, device='cuda:0'), U.norm 8.96176528930664, V.norm 16.355712890625, MLP.norm 2.6305034160614014
2023-05-14 11:45:12,933 :: INFO :: Epoch 15: loss tensor(856.0299, device='cuda:0'), U.norm 7.488039493560791, V.norm 16.04902458190918, MLP.norm 4.019180774688721
2023-05-14 11:45:13,151 :: INFO :: Epoch 20: loss tensor(829.2972, device='cuda:0'), U.norm 6.484580993652344, V.norm 15.84933090209961, MLP.norm 5.448318958282471
2023-05-14 11:45:13,167 :: INFO :: Environment 5
2023-05-14 11:45:22,496 :: INFO :: Epoch 5: loss tensor(860.1409, device='cuda:0'), U.norm 11.397856712341309, V.norm 16.950876235961914, MLP.norm 1.488229751586914
2023-05-14 11:45:22,715 :: INFO :: Epoch 10: loss tensor(846.4243, device='cuda:0'), U.norm 8.962238311767578, V.norm 16.283954620361328, MLP.norm 2.5746467113494873
2023-05-14 11:45:22,933 :: INFO :: Epoch 15: loss tensor(826.3510, device='cuda:0'), U.norm 7.488336086273193, V.norm 15.967418670654297, MLP.norm 3.9210331439971924
2023-05-14 11:45:23,168 :: INFO :: Epoch 20: loss tensor(802.4388, device='cuda:0'), U.norm 6.48423957824707, V.norm 15.76108169555664, MLP.norm 5.2909369468688965
2023-05-14 11:45:23,183 :: INFO :: Environment 6
2023-05-14 11:45:32,575 :: INFO :: Epoch 5: loss tensor(844.7010, device='cuda:0'), U.norm 11.397683143615723, V.norm 16.922414779663086, MLP.norm 1.493280291557312
2023-05-14 11:45:32,810 :: INFO :: Epoch 10: loss tensor(830.2448, device='cuda:0'), U.norm 8.96130657196045, V.norm 16.242544174194336, MLP.norm 2.592477560043335
2023-05-14 11:45:33,044 :: INFO :: Epoch 15: loss tensor(809.1879, device='cuda:0'), U.norm 7.4866943359375, V.norm 15.921647071838379, MLP.norm 3.9386818408966064
2023-05-14 11:45:33,263 :: INFO :: Epoch 20: loss tensor(783.8627, device='cuda:0'), U.norm 6.481894493103027, V.norm 15.713565826416016, MLP.norm 5.301594257354736
2023-05-14 11:45:33,278 :: INFO :: Environment 7
2023-05-14 11:45:42,623 :: INFO :: Epoch 5: loss tensor(842.6804, device='cuda:0'), U.norm 11.399133682250977, V.norm 16.91058349609375, MLP.norm 1.5103626251220703
2023-05-14 11:45:42,842 :: INFO :: Epoch 10: loss tensor(828.0740, device='cuda:0'), U.norm 8.964159965515137, V.norm 16.22324562072754, MLP.norm 2.6304752826690674
2023-05-14 11:45:43,076 :: INFO :: Epoch 15: loss tensor(807.2388, device='cuda:0'), U.norm 7.490903377532959, V.norm 15.895208358764648, MLP.norm 4.001372337341309
2023-05-14 11:45:43,295 :: INFO :: Epoch 20: loss tensor(782.4644, device='cuda:0'), U.norm 6.48774528503418, V.norm 15.680121421813965, MLP.norm 5.3690505027771
2023-05-14 11:45:43,311 :: INFO :: Environment 8
2023-05-14 11:45:52,671 :: INFO :: Epoch 5: loss tensor(864.6268, device='cuda:0'), U.norm 11.399107933044434, V.norm 16.950281143188477, MLP.norm 1.5010572671890259
2023-05-14 11:45:52,890 :: INFO :: Epoch 10: loss tensor(850.2191, device='cuda:0'), U.norm 8.965441703796387, V.norm 16.27685546875, MLP.norm 2.603813648223877
2023-05-14 11:45:53,124 :: INFO :: Epoch 15: loss tensor(829.3772, device='cuda:0'), U.norm 7.494278430938721, V.norm 15.954851150512695, MLP.norm 3.9469072818756104
2023-05-14 11:45:53,343 :: INFO :: Epoch 20: loss tensor(803.7076, device='cuda:0'), U.norm 6.493730545043945, V.norm 15.745182991027832, MLP.norm 5.312988758087158
2023-05-14 11:45:53,358 :: INFO :: Environment 9
2023-05-14 11:46:02,735 :: INFO :: Epoch 5: loss tensor(863.2863, device='cuda:0'), U.norm 11.397598266601562, V.norm 16.953548431396484, MLP.norm 1.480002760887146
2023-05-14 11:46:02,969 :: INFO :: Epoch 10: loss tensor(849.2161, device='cuda:0'), U.norm 8.96107006072998, V.norm 16.284433364868164, MLP.norm 2.566394090652466
2023-05-14 11:46:03,188 :: INFO :: Epoch 15: loss tensor(828.5081, device='cuda:0'), U.norm 7.485942840576172, V.norm 15.966057777404785, MLP.norm 3.9342875480651855
2023-05-14 11:46:03,422 :: INFO :: Epoch 20: loss tensor(802.4434, device='cuda:0'), U.norm 6.4804368019104, V.norm 15.75888442993164, MLP.norm 5.324034690856934
2023-05-14 11:46:03,438 :: INFO :: Ite = 1, Delta = 4122
2023-05-14 11:46:03,438 :: INFO :: ----- backend -----
2023-05-14 11:48:27,313 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 11:48:27,313 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 11:48:28,735 :: INFO :: torch.Size([76083, 384])
2023-05-14 11:48:39,643 :: INFO :: ----- frontend -----
2023-05-14 11:48:39,658 :: INFO :: Environment 0
2023-05-14 11:48:50,300 :: INFO :: Epoch 5: loss tensor(820.6893, device='cuda:0'), U.norm 13.154335975646973, V.norm 16.90321159362793, MLP.norm 2.1432113647460938
2023-05-14 11:48:50,534 :: INFO :: Epoch 10: loss tensor(796.5191, device='cuda:0'), U.norm 10.340478897094727, V.norm 16.213722229003906, MLP.norm 3.6101739406585693
2023-05-14 11:48:50,753 :: INFO :: Epoch 15: loss tensor(763.9460, device='cuda:0'), U.norm 8.636116027832031, V.norm 15.882392883300781, MLP.norm 5.333689212799072
2023-05-14 11:48:50,987 :: INFO :: Epoch 20: loss tensor(726.9431, device='cuda:0'), U.norm 7.472327709197998, V.norm 15.661770820617676, MLP.norm 7.121977806091309
2023-05-14 11:48:51,003 :: INFO :: Environment 1
2023-05-14 11:49:00,176 :: INFO :: Epoch 5: loss tensor(822.5140, device='cuda:0'), U.norm 13.158339500427246, V.norm 16.898292541503906, MLP.norm 2.165095806121826
2023-05-14 11:49:00,410 :: INFO :: Epoch 10: loss tensor(797.6489, device='cuda:0'), U.norm 10.348400115966797, V.norm 16.202116012573242, MLP.norm 3.6142029762268066
2023-05-14 11:49:00,629 :: INFO :: Epoch 15: loss tensor(764.4840, device='cuda:0'), U.norm 8.648263931274414, V.norm 15.864622116088867, MLP.norm 5.285494804382324
2023-05-14 11:49:00,864 :: INFO :: Epoch 20: loss tensor(726.3179, device='cuda:0'), U.norm 7.489304542541504, V.norm 15.638911247253418, MLP.norm 7.050005912780762
2023-05-14 11:49:00,879 :: INFO :: Environment 2
2023-05-14 11:49:10,224 :: INFO :: Epoch 5: loss tensor(821.6847, device='cuda:0'), U.norm 13.155658721923828, V.norm 16.889244079589844, MLP.norm 2.139667272567749
2023-05-14 11:49:10,459 :: INFO :: Epoch 10: loss tensor(797.0227, device='cuda:0'), U.norm 10.342985153198242, V.norm 16.193347930908203, MLP.norm 3.6063661575317383
2023-05-14 11:49:10,709 :: INFO :: Epoch 15: loss tensor(762.7552, device='cuda:0'), U.norm 8.63952350616455, V.norm 15.85777759552002, MLP.norm 5.345634937286377
2023-05-14 11:49:10,958 :: INFO :: Epoch 20: loss tensor(725.2324, device='cuda:0'), U.norm 7.475948810577393, V.norm 15.633336067199707, MLP.norm 7.148841381072998
2023-05-14 11:49:10,958 :: INFO :: Environment 3
2023-05-14 11:49:20,304 :: INFO :: Epoch 5: loss tensor(868.2234, device='cuda:0'), U.norm 13.159645080566406, V.norm 16.99323844909668, MLP.norm 2.149503469467163
2023-05-14 11:49:20,538 :: INFO :: Epoch 10: loss tensor(840.9801, device='cuda:0'), U.norm 10.352229118347168, V.norm 16.342863082885742, MLP.norm 3.6609532833099365
2023-05-14 11:49:20,772 :: INFO :: Epoch 15: loss tensor(803.5703, device='cuda:0'), U.norm 8.655778884887695, V.norm 16.02696418762207, MLP.norm 5.43019962310791
2023-05-14 11:49:21,007 :: INFO :: Epoch 20: loss tensor(761.7867, device='cuda:0'), U.norm 7.500767707824707, V.norm 15.81550121307373, MLP.norm 7.268904685974121
2023-05-14 11:49:21,007 :: INFO :: Environment 4
2023-05-14 11:49:30,367 :: INFO :: Epoch 5: loss tensor(885.9465, device='cuda:0'), U.norm 13.156479835510254, V.norm 17.014446258544922, MLP.norm 2.192281484603882
2023-05-14 11:49:30,586 :: INFO :: Epoch 10: loss tensor(857.0052, device='cuda:0'), U.norm 10.346564292907715, V.norm 16.368600845336914, MLP.norm 3.774351119995117
2023-05-14 11:49:30,820 :: INFO :: Epoch 15: loss tensor(817.7902, device='cuda:0'), U.norm 8.647082328796387, V.norm 16.055212020874023, MLP.norm 5.612770080566406
2023-05-14 11:49:31,055 :: INFO :: Epoch 20: loss tensor(775.2299, device='cuda:0'), U.norm 7.488407611846924, V.norm 15.845390319824219, MLP.norm 7.528124809265137
2023-05-14 11:49:31,070 :: INFO :: Environment 5
2023-05-14 11:49:40,196 :: INFO :: Epoch 5: loss tensor(854.8801, device='cuda:0'), U.norm 13.157281875610352, V.norm 16.957685470581055, MLP.norm 2.184299945831299
2023-05-14 11:49:40,430 :: INFO :: Epoch 10: loss tensor(828.5184, device='cuda:0'), U.norm 10.347418785095215, V.norm 16.29047203063965, MLP.norm 3.7023673057556152
2023-05-14 11:49:40,665 :: INFO :: Epoch 15: loss tensor(793.2852, device='cuda:0'), U.norm 8.647418975830078, V.norm 15.966443061828613, MLP.norm 5.464942455291748
2023-05-14 11:49:40,900 :: INFO :: Epoch 20: loss tensor(754.2217, device='cuda:0'), U.norm 7.488337993621826, V.norm 15.748204231262207, MLP.norm 7.318854331970215
2023-05-14 11:49:40,900 :: INFO :: Environment 6
2023-05-14 11:49:50,041 :: INFO :: Epoch 5: loss tensor(839.5480, device='cuda:0'), U.norm 13.157156944274902, V.norm 16.92407989501953, MLP.norm 2.175391435623169
2023-05-14 11:49:50,275 :: INFO :: Epoch 10: loss tensor(811.9303, device='cuda:0'), U.norm 10.347271919250488, V.norm 16.244548797607422, MLP.norm 3.7023205757141113
2023-05-14 11:49:50,494 :: INFO :: Epoch 15: loss tensor(775.5492, device='cuda:0'), U.norm 8.647202491760254, V.norm 15.915831565856934, MLP.norm 5.460270881652832
2023-05-14 11:49:50,729 :: INFO :: Epoch 20: loss tensor(736.0673, device='cuda:0'), U.norm 7.4876251220703125, V.norm 15.698244094848633, MLP.norm 7.273549556732178
2023-05-14 11:49:50,744 :: INFO :: Environment 7
2023-05-14 11:49:59,902 :: INFO :: Epoch 5: loss tensor(838.4651, device='cuda:0'), U.norm 13.157768249511719, V.norm 16.924800872802734, MLP.norm 2.144545078277588
2023-05-14 11:50:00,136 :: INFO :: Epoch 10: loss tensor(811.9263, device='cuda:0'), U.norm 10.34786605834961, V.norm 16.242048263549805, MLP.norm 3.6650185585021973
2023-05-14 11:50:00,370 :: INFO :: Epoch 15: loss tensor(776.1516, device='cuda:0'), U.norm 8.648435592651367, V.norm 15.909919738769531, MLP.norm 5.453350067138672
2023-05-14 11:50:00,605 :: INFO :: Epoch 20: loss tensor(737.4321, device='cuda:0'), U.norm 7.489965915679932, V.norm 15.687167167663574, MLP.norm 7.308760643005371
2023-05-14 11:50:00,620 :: INFO :: Environment 8
2023-05-14 11:50:09,746 :: INFO :: Epoch 5: loss tensor(859.8460, device='cuda:0'), U.norm 13.158031463623047, V.norm 16.97214698791504, MLP.norm 2.1555449962615967
2023-05-14 11:50:09,965 :: INFO :: Epoch 10: loss tensor(832.8674, device='cuda:0'), U.norm 10.34985065460205, V.norm 16.311452865600586, MLP.norm 3.6625757217407227
2023-05-14 11:50:10,199 :: INFO :: Epoch 15: loss tensor(796.1583, device='cuda:0'), U.norm 8.652542114257812, V.norm 15.99136734008789, MLP.norm 5.4376678466796875
2023-05-14 11:50:10,434 :: INFO :: Epoch 20: loss tensor(755.9276, device='cuda:0'), U.norm 7.496525287628174, V.norm 15.777645111083984, MLP.norm 7.29607629776001
2023-05-14 11:50:10,449 :: INFO :: Environment 9
2023-05-14 11:50:19,607 :: INFO :: Epoch 5: loss tensor(858.1609, device='cuda:0'), U.norm 13.15622329711914, V.norm 16.966672897338867, MLP.norm 2.160280466079712
2023-05-14 11:50:19,857 :: INFO :: Epoch 10: loss tensor(830.6180, device='cuda:0'), U.norm 10.345008850097656, V.norm 16.301469802856445, MLP.norm 3.6787986755371094
2023-05-14 11:50:20,091 :: INFO :: Epoch 15: loss tensor(793.7725, device='cuda:0'), U.norm 8.643503189086914, V.norm 15.978910446166992, MLP.norm 5.461211204528809
2023-05-14 11:50:20,326 :: INFO :: Epoch 20: loss tensor(753.0798, device='cuda:0'), U.norm 7.482150077819824, V.norm 15.762491226196289, MLP.norm 7.30648946762085
2023-05-14 11:50:20,326 :: INFO :: Ite = 1, Delta = 4067
2023-05-14 11:50:20,341 :: INFO :: ----- backend -----
2023-05-14 11:50:24,951 :: INFO :: Epoch 5: loss tensor(100.2005, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0151,  0.0159,  0.0220,  0.0177,  0.0118,  0.0128,  0.0129,  0.0189,
         0.0071,  0.0217,  0.0147,  0.0193,  0.0100,  0.0201,  0.0249,  0.0225,
         0.0218,  0.0220,  0.0223,  0.0191,  0.0157,  0.0215,  0.0223,  0.0082,
         0.0234,  0.0121,  0.0183,  0.0137,  0.0255,  0.0199,  0.0244,  0.0247,
         0.0134,  0.0239,  0.0053,  0.0045,  0.0235,  0.0201,  0.0108,  0.0255,
         0.0123,  0.0074,  0.0255,  0.0197,  0.0152,  0.0244,  0.0182,  0.0076,
         0.0221,  0.0180,  0.0242,  0.0145,  0.0226,  0.0152,  0.0215,  0.0221,
         0.0168,  0.0244,  0.0115,  0.0099,  0.0198,  0.0235,  0.0067,  0.0173,
         0.0181,  0.0089,  0.0255,  0.0167,  0.0247,  0.0127,  0.0243,  0.0108,
         0.0227,  0.0093,  0.0202,  0.0117,  0.0234,  0.0107,  0.0163,  0.0200,
         0.0211,  0.0147,  0.0220,  0.0150,  0.0056,  0.0230,  0.0241,  0.0168,
         0.0116,  0.0010,  0.0123,  0.0200,  0.0219,  0.0217,  0.0154,  0.0204,
         0.0224,  0.0232,  0.0169,  0.0191,  0.0082,  0.0151,  0.0192,  0.0018,
         0.0202,  0.0166,  0.0212,  0.0205,  0.0197,  0.0201,  0.0223,  0.0172,
         0.0247,  0.0232,  0.0110,  0.0131,  0.0183,  0.0164,  0.0156,  0.0161,
         0.0217,  0.0182,  0.0147,  0.0147,  0.0076,  0.0212,  0.0132,  0.0186,
         0.0174,  0.0202,  0.0253,  0.0241,  0.0234,  0.0198,  0.0098,  0.0043,
        -0.0055,  0.0135,  0.0109,  0.0025,  0.0225,  0.0083,  0.0110, -0.0010,
         0.0212,  0.0254,  0.0141,  0.0037,  0.0227,  0.0125,  0.0057,  0.0155,
         0.0252, -0.0208,  0.0090,  0.0008,  0.0077, -0.0186,  0.0244, -0.0182,
         0.0251, -0.0193,  0.0228,  0.0214,  0.0246,  0.0160, -0.0010,  0.0053,
         0.0079,  0.0127,  0.0205,  0.0197, -0.0186,  0.0205,  0.0255,  0.0027,
         0.0202,  0.0220,  0.0117,  0.0071,  0.0142,  0.0069,  0.0192, -0.0175,
         0.0219,  0.0220, -0.0135,  0.0168,  0.0201, -0.0194, -0.0159,  0.0119,
         0.0250, -0.0107,  0.0151,  0.0211,  0.0082,  0.0178,  0.0248,  0.0122,
         0.0250,  0.0094,  0.0247,  0.0228,  0.0206,  0.0036,  0.0083, -0.0198,
         0.0212,  0.0055,  0.0229,  0.0247,  0.0197,  0.0036,  0.0208,  0.0178,
         0.0078, -0.0050,  0.0247, -0.0178,  0.0130,  0.0102,  0.0222,  0.0219,
         0.0198,  0.0136,  0.0139,  0.0160,  0.0156,  0.0254,  0.0220,  0.0206,
         0.0243,  0.0182,  0.0249, -0.0001,  0.0072,  0.0121,  0.0146,  0.0189,
         0.0099,  0.0142,  0.0095,  0.0246,  0.0096,  0.0250, -0.0159,  0.0167,
         0.0150,  0.0128, -0.0150,  0.0251,  0.0090,  0.0166, -0.0161,  0.0134,
         0.0169,  0.0208,  0.0136,  0.0254,  0.0222,  0.0154,  0.0095,  0.0125,
         0.0254,  0.0109,  0.0252,  0.0254,  0.0169,  0.0249,  0.0124,  0.0129,
         0.0187,  0.0186,  0.0175,  0.0172,  0.0133,  0.0250,  0.0254,  0.0162,
         0.0157,  0.0239,  0.0252,  0.0134,  0.0193,  0.0212,  0.0100,  0.0249,
         0.0253,  0.0138,  0.0137,  0.0137,  0.0135,  0.0206,  0.0095,  0.0254,
         0.0222,  0.0211,  0.0214,  0.0252,  0.0099,  0.0259,  0.0163,  0.0032,
         0.0178,  0.0129,  0.0197,  0.0115,  0.0167,  0.0118,  0.0140,  0.0154,
         0.0212,  0.0200,  0.0102,  0.0211,  0.0145,  0.0186,  0.0253,  0.0142,
         0.0256,  0.0157,  0.0255,  0.0192,  0.0172,  0.0257,  0.0030,  0.0137,
         0.0136,  0.0176,  0.0253,  0.0139,  0.0128,  0.0088,  0.0201,  0.0187,
         0.0140,  0.0188,  0.0247,  0.0163,  0.0193,  0.0148,  0.0155,  0.0167,
         0.0118,  0.0185,  0.0250,  0.0245,  0.0127,  0.0243,  0.0128,  0.0169,
         0.0154,  0.0082,  0.0151,  0.0182,  0.0054,  0.0218,  0.0166,  0.0211,
         0.0160,  0.0095,  0.0208,  0.0068,  0.0254,  0.0255,  0.0206,  0.0149,
         0.0221,  0.0165,  0.0169, -0.0026,  0.0185,  0.0159,  0.0207,  0.0257,
         0.0210,  0.0159,  0.0193,  0.0057,  0.0255,  0.0125,  0.0185,  0.0245],
       device='cuda:0', requires_grad=True) MLP.norm tensor(3.4773, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:29,452 :: INFO :: Epoch 10: loss tensor(96.8955, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0439,  0.0424,  0.0511,  0.0460,  0.0377,  0.0416,  0.0414,  0.0453,
         0.0341,  0.0479,  0.0439,  0.0449,  0.0356,  0.0496,  0.0528,  0.0466,
         0.0442,  0.0492,  0.0461,  0.0473,  0.0414,  0.0463,  0.0498,  0.0381,
         0.0497,  0.0371,  0.0421,  0.0414,  0.0550,  0.0450,  0.0512,  0.0516,
         0.0421,  0.0509,  0.0338,  0.0322,  0.0476,  0.0469,  0.0370,  0.0540,
         0.0392,  0.0341,  0.0532,  0.0451,  0.0397,  0.0502,  0.0426,  0.0356,
         0.0498,  0.0421,  0.0514,  0.0407,  0.0441,  0.0378,  0.0503,  0.0500,
         0.0444,  0.0539,  0.0414,  0.0401,  0.0436,  0.0479,  0.0353,  0.0444,
         0.0465,  0.0346,  0.0528,  0.0460,  0.0500,  0.0409,  0.0484,  0.0390,
         0.0439,  0.0364,  0.0479,  0.0389,  0.0497,  0.0381,  0.0435,  0.0451,
         0.0503,  0.0446,  0.0486,  0.0442,  0.0343,  0.0518,  0.0491,  0.0456,
         0.0381,  0.0264,  0.0395,  0.0482,  0.0482,  0.0479,  0.0446,  0.0481,
         0.0476,  0.0507,  0.0443,  0.0457,  0.0371,  0.0374,  0.0468,  0.0286,
         0.0492,  0.0461,  0.0481,  0.0456,  0.0452,  0.0478,  0.0515,  0.0424,
         0.0534,  0.0497,  0.0386,  0.0390,  0.0448,  0.0446,  0.0361,  0.0440,
         0.0488,  0.0456,  0.0421,  0.0393,  0.0332,  0.0442,  0.0413,  0.0458,
         0.0430,  0.0477,  0.0537,  0.0447,  0.0455,  0.0447,  0.0380,  0.0335,
         0.0234,  0.0168,  0.0388,  0.0309,  0.0511,  0.0378,  0.0404,  0.0272,
         0.0505,  0.0521,  0.0351,  0.0346,  0.0517,  0.0431,  0.0356,  0.0448,
         0.0532, -0.0338,  0.0371,  0.0307,  0.0368, -0.0216,  0.0468, -0.0260,
         0.0511, -0.0327,  0.0464,  0.0500,  0.0500,  0.0289,  0.0260,  0.0341,
         0.0373,  0.0403,  0.0494,  0.0460, -0.0250,  0.0490,  0.0543,  0.0331,
         0.0289,  0.0492,  0.0403,  0.0341,  0.0423,  0.0373,  0.0367, -0.0162,
         0.0487,  0.0515,  0.0013,  0.0458,  0.0499, -0.0295, -0.0134,  0.0412,
         0.0503,  0.0111,  0.0349,  0.0493,  0.0384,  0.0238,  0.0516,  0.0424,
         0.0534,  0.0402,  0.0510,  0.0420,  0.0487,  0.0342,  0.0370, -0.0332,
         0.0453,  0.0341,  0.0527,  0.0497,  0.0331,  0.0325,  0.0479,  0.0486,
         0.0369,  0.0209,  0.0518, -0.0177,  0.0421,  0.0386,  0.0500,  0.0496,
         0.0433,  0.0435,  0.0407,  0.0469,  0.0409,  0.0543,  0.0487,  0.0403,
         0.0430,  0.0360,  0.0497,  0.0300,  0.0367,  0.0409,  0.0202,  0.0427,
         0.0382,  0.0439,  0.0401,  0.0499,  0.0394,  0.0532, -0.0049,  0.0450,
         0.0438,  0.0421, -0.0092,  0.0528,  0.0397,  0.0469, -0.0032,  0.0417,
         0.0469,  0.0511,  0.0446,  0.0550,  0.0519,  0.0460,  0.0401,  0.0431,
         0.0553,  0.0404,  0.0547,  0.0555,  0.0470,  0.0546,  0.0425,  0.0435,
         0.0482,  0.0487,  0.0477,  0.0472,  0.0434,  0.0549,  0.0549,  0.0462,
         0.0461,  0.0545,  0.0551,  0.0446,  0.0500,  0.0503,  0.0406,  0.0550,
         0.0545,  0.0444,  0.0428,  0.0443,  0.0442,  0.0497,  0.0390,  0.0542,
         0.0523,  0.0504,  0.0510,  0.0548,  0.0408,  0.0559,  0.0459,  0.0340,
         0.0475,  0.0435,  0.0496,  0.0407,  0.0468,  0.0416,  0.0444,  0.0459,
         0.0515,  0.0493,  0.0398,  0.0513,  0.0442,  0.0492,  0.0553,  0.0451,
         0.0555,  0.0453,  0.0553,  0.0491,  0.0472,  0.0549,  0.0339,  0.0441,
         0.0438,  0.0481,  0.0553,  0.0438,  0.0434,  0.0394,  0.0505,  0.0477,
         0.0437,  0.0490,  0.0534,  0.0459,  0.0497,  0.0453,  0.0457,  0.0465,
         0.0416,  0.0467,  0.0540,  0.0544,  0.0428,  0.0543,  0.0432,  0.0470,
         0.0458,  0.0367,  0.0448,  0.0484,  0.0364,  0.0511,  0.0464,  0.0512,
         0.0463,  0.0407,  0.0508,  0.0374,  0.0543,  0.0550,  0.0508,  0.0453,
         0.0519,  0.0468,  0.0473,  0.0274,  0.0488,  0.0462,  0.0508,  0.0561,
         0.0507,  0.0441,  0.0493,  0.0361,  0.0550,  0.0431,  0.0489,  0.0544],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.9713, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:33,952 :: INFO :: Epoch 15: loss tensor(95.9389, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0689,  0.0590,  0.0787,  0.0694,  0.0596,  0.0691,  0.0641,  0.0680,
         0.0539,  0.0680,  0.0705,  0.0670,  0.0554,  0.0755,  0.0769,  0.0645,
         0.0568,  0.0686,  0.0619,  0.0720,  0.0637,  0.0622,  0.0734,  0.0665,
         0.0716,  0.0528,  0.0564,  0.0659,  0.0832,  0.0607,  0.0741,  0.0716,
         0.0663,  0.0717,  0.0582,  0.0539,  0.0631,  0.0727,  0.0576,  0.0789,
         0.0634,  0.0570,  0.0769,  0.0650,  0.0563,  0.0685,  0.0623,  0.0616,
         0.0742,  0.0535,  0.0730,  0.0590,  0.0592,  0.0516,  0.0767,  0.0731,
         0.0681,  0.0817,  0.0675,  0.0682,  0.0564,  0.0677,  0.0598,  0.0650,
         0.0686,  0.0528,  0.0758,  0.0703,  0.0705,  0.0639,  0.0622,  0.0623,
         0.0568,  0.0636,  0.0679,  0.0588,  0.0700,  0.0618,  0.0616,  0.0631,
         0.0773,  0.0744,  0.0704,  0.0709,  0.0572,  0.0769,  0.0686,  0.0701,
         0.0572,  0.0425,  0.0617,  0.0746,  0.0696,  0.0665,  0.0712,  0.0700,
         0.0666,  0.0724,  0.0686,  0.0650,  0.0613,  0.0505,  0.0691,  0.0543,
         0.0761,  0.0729,  0.0708,  0.0618,  0.0636,  0.0716,  0.0791,  0.0604,
         0.0791,  0.0703,  0.0620,  0.0602,  0.0640,  0.0708,  0.0467,  0.0679,
         0.0717,  0.0684,  0.0653,  0.0523,  0.0529,  0.0643,  0.0675,  0.0698,
         0.0630,  0.0723,  0.0815,  0.0549,  0.0540,  0.0643,  0.0629,  0.0623,
         0.0562,  0.0186,  0.0632,  0.0591,  0.0784,  0.0671,  0.0693,  0.0576,
         0.0794,  0.0767,  0.0450,  0.0672,  0.0806,  0.0741,  0.0663,  0.0720,
         0.0795, -0.0416,  0.0631,  0.0628,  0.0658, -0.0119,  0.0589, -0.0249,
         0.0712, -0.0411,  0.0629,  0.0774,  0.0684,  0.0282,  0.0539,  0.0623,
         0.0665,  0.0648,  0.0773,  0.0669, -0.0215,  0.0770,  0.0824,  0.0652,
         0.0320,  0.0727,  0.0671,  0.0590,  0.0683,  0.0683,  0.0447, -0.0012,
         0.0720,  0.0804,  0.0257,  0.0725,  0.0785, -0.0326,  0.0035,  0.0700,
         0.0713,  0.0396,  0.0405,  0.0765,  0.0685,  0.0250,  0.0756,  0.0729,
         0.0815,  0.0710,  0.0736,  0.0529,  0.0760,  0.0656,  0.0649, -0.0416,
         0.0578,  0.0611,  0.0821,  0.0693,  0.0367,  0.0613,  0.0715,  0.0797,
         0.0652,  0.0487,  0.0780, -0.0045,  0.0690,  0.0636,  0.0738,  0.0752,
         0.0571,  0.0720,  0.0629,  0.0781,  0.0595,  0.0829,  0.0718,  0.0487,
         0.0500,  0.0431,  0.0690,  0.0623,  0.0661,  0.0687,  0.0240,  0.0591,
         0.0639,  0.0735,  0.0717,  0.0706,  0.0694,  0.0806,  0.0201,  0.0723,
         0.0714,  0.0711,  0.0107,  0.0801,  0.0712,  0.0780,  0.0253,  0.0676,
         0.0790,  0.0827,  0.0782,  0.0840,  0.0820,  0.0780,  0.0718,  0.0746,
         0.0864,  0.0709,  0.0851,  0.0860,  0.0768,  0.0839,  0.0714,  0.0752,
         0.0785,  0.0794,  0.0778,  0.0778,  0.0746,  0.0854,  0.0840,  0.0764,
         0.0770,  0.0879,  0.0852,  0.0775,  0.0830,  0.0782,  0.0723,  0.0861,
         0.0819,  0.0763,  0.0727,  0.0759,  0.0774,  0.0780,  0.0679,  0.0820,
         0.0831,  0.0794,  0.0791,  0.0841,  0.0735,  0.0879,  0.0771,  0.0676,
         0.0784,  0.0767,  0.0777,  0.0697,  0.0775,  0.0722,  0.0749,  0.0779,
         0.0835,  0.0797,  0.0692,  0.0814,  0.0734,  0.0823,  0.0862,  0.0776,
         0.0860,  0.0761,  0.0854,  0.0793,  0.0785,  0.0834,  0.0678,  0.0758,
         0.0738,  0.0790,  0.0859,  0.0743,  0.0753,  0.0711,  0.0824,  0.0750,
         0.0712,  0.0800,  0.0833,  0.0745,  0.0806,  0.0763,  0.0764,  0.0777,
         0.0720,  0.0718,  0.0812,  0.0854,  0.0732,  0.0856,  0.0737,  0.0772,
         0.0771,  0.0617,  0.0755,  0.0785,  0.0697,  0.0804,  0.0753,  0.0820,
         0.0778,  0.0746,  0.0828,  0.0707,  0.0836,  0.0840,  0.0817,  0.0763,
         0.0812,  0.0781,  0.0787,  0.0613,  0.0788,  0.0761,  0.0833,  0.0884,
         0.0798,  0.0693,  0.0792,  0.0686,  0.0846,  0.0750,  0.0805,  0.0837],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.2049, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:38,453 :: INFO :: Epoch 20: loss tensor(93.7275, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0867,  0.0651,  0.1039,  0.0860,  0.0750,  0.0935,  0.0794,  0.0843,
         0.0642,  0.0787,  0.0925,  0.0828,  0.0678,  0.0952,  0.0933,  0.0724,
         0.0594,  0.0790,  0.0688,  0.0920,  0.0797,  0.0684,  0.0909,  0.0906,
         0.0882,  0.0608,  0.0621,  0.0856,  0.1079,  0.0668,  0.0902,  0.0826,
         0.0835,  0.0844,  0.0734,  0.0652,  0.0679,  0.0937,  0.0714,  0.0968,
         0.0791,  0.0737,  0.0931,  0.0771,  0.0627,  0.0779,  0.0743,  0.0808,
         0.0937,  0.0557,  0.0862,  0.0665,  0.0674,  0.0566,  0.0992,  0.0892,
         0.0873,  0.1082,  0.0881,  0.0909,  0.0597,  0.0829,  0.0774,  0.0755,
         0.0802,  0.0640,  0.0915,  0.0876,  0.0835,  0.0804,  0.0668,  0.0775,
         0.0611,  0.0878,  0.0801,  0.0687,  0.0804,  0.0793,  0.0690,  0.0725,
         0.0997,  0.1023,  0.0849,  0.0922,  0.0706,  0.0970,  0.0792,  0.0876,
         0.0677,  0.0460,  0.0773,  0.0968,  0.0844,  0.0748,  0.0930,  0.0841,
         0.0755,  0.0855,  0.0889,  0.0766,  0.0776,  0.0571,  0.0839,  0.0743,
         0.0994,  0.0954,  0.0843,  0.0690,  0.0741,  0.0895,  0.1046,  0.0681,
         0.0988,  0.0842,  0.0793,  0.0745,  0.0757,  0.0924,  0.0502,  0.0865,
         0.0891,  0.0822,  0.0815,  0.0557,  0.0640,  0.0795,  0.0879,  0.0889,
         0.0773,  0.0921,  0.1063,  0.0549,  0.0549,  0.0750,  0.0823,  0.0877,
         0.0869,  0.0202,  0.0821,  0.0838,  0.1005,  0.0932,  0.0936,  0.0879,
         0.1054,  0.0963,  0.0470,  0.0982,  0.1065,  0.1022,  0.0945,  0.0970,
         0.1013, -0.0469,  0.0846,  0.0930,  0.0919,  0.0027,  0.0620, -0.0168,
         0.0843, -0.0447,  0.0729,  0.1014,  0.0791,  0.0283,  0.0784,  0.0870,
         0.0927,  0.0840,  0.0997,  0.0802, -0.0125,  0.1015,  0.1073,  0.0951,
         0.0342,  0.0898,  0.0896,  0.0799,  0.0893,  0.0967,  0.0454,  0.0172,
         0.0887,  0.1048,  0.0494,  0.0933,  0.1021, -0.0319,  0.0273,  0.0957,
         0.0862,  0.0663,  0.0396,  0.0983,  0.0965,  0.0244,  0.0946,  0.1006,
         0.1064,  0.0993,  0.0909,  0.0553,  0.0991,  0.0952,  0.0892, -0.0460,
         0.0609,  0.0839,  0.1073,  0.0813,  0.0383,  0.0867,  0.0891,  0.1082,
         0.0897,  0.0734,  0.0992,  0.0127,  0.0926,  0.0832,  0.0928,  0.0977,
         0.0622,  0.0972,  0.0774,  0.1068,  0.0677,  0.1087,  0.0883,  0.0483,
         0.0542,  0.0435,  0.0801,  0.0927,  0.0924,  0.0933,  0.0242,  0.0678,
         0.0838,  0.1003,  0.1013,  0.0867,  0.0967,  0.1052,  0.0454,  0.0958,
         0.0954,  0.0973,  0.0362,  0.1041,  0.1002,  0.1067,  0.0545,  0.0886,
         0.1104,  0.1134,  0.1118,  0.1092,  0.1100,  0.1093,  0.1009,  0.1036,
         0.1176,  0.0999,  0.1136,  0.1145,  0.1029,  0.1092,  0.0966,  0.1048,
         0.1075,  0.1081,  0.1048,  0.1060,  0.1047,  0.1140,  0.1099,  0.1034,
         0.1051,  0.1212,  0.1126,  0.1100,  0.1163,  0.1005,  0.1019,  0.1152,
         0.1038,  0.1064,  0.1006,  0.1050,  0.1112,  0.1019,  0.0929,  0.1063,
         0.1112,  0.1065,  0.1007,  0.1097,  0.1055,  0.1197,  0.1088,  0.1010,
         0.1082,  0.1096,  0.1004,  0.0958,  0.1051,  0.1011,  0.1035,  0.1096,
         0.1144,  0.1087,  0.0966,  0.1080,  0.0991,  0.1151,  0.1145,  0.1094,
         0.1133,  0.1067,  0.1126,  0.1074,  0.1086,  0.1079,  0.1012,  0.1063,
         0.1006,  0.1076,  0.1143,  0.1023,  0.1050,  0.1002,  0.1134,  0.0968,
         0.0925,  0.1085,  0.1124,  0.0969,  0.1085,  0.1047,  0.1047,  0.1069,
         0.0990,  0.0889,  0.1032,  0.1149,  0.1013,  0.1149,  0.1011,  0.1050,
         0.1063,  0.0794,  0.1049,  0.1056,  0.1026,  0.1077,  0.1009,  0.1103,
         0.1080,  0.1084,  0.1145,  0.1040,  0.1110,  0.1098,  0.1107,  0.1064,
         0.1071,  0.1071,  0.1080,  0.0947,  0.1046,  0.1022,  0.1160,  0.1200,
         0.1051,  0.0853,  0.1051,  0.1008,  0.1110,  0.1047,  0.1107,  0.1097],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.1250, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:42,922 :: INFO :: Epoch 25: loss tensor(91.0699, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0974,  0.0646,  0.1241,  0.0956,  0.0815,  0.1124,  0.0877,  0.0923,
         0.0693,  0.0823,  0.1102,  0.0915,  0.0697,  0.1098,  0.1043,  0.0743,
         0.0574,  0.0845,  0.0681,  0.1053,  0.0858,  0.0695,  0.1020,  0.1096,
         0.0983,  0.0591,  0.0588,  0.0973,  0.1287,  0.0650,  0.0997,  0.0866,
         0.0946,  0.0887,  0.0811,  0.0670,  0.0678,  0.1074,  0.0748,  0.1091,
         0.0865,  0.0808,  0.1030,  0.0806,  0.0629,  0.0805,  0.0766,  0.0948,
         0.1071,  0.0546,  0.0923,  0.0670,  0.0678,  0.0522,  0.1172,  0.0994,
         0.0995,  0.1320,  0.1030,  0.1068,  0.0578,  0.0879,  0.0907,  0.0782,
         0.0859,  0.0668,  0.1016,  0.0992,  0.0893,  0.0859,  0.0680,  0.0874,
         0.0580,  0.1062,  0.0872,  0.0702,  0.0860,  0.0895,  0.0725,  0.0774,
         0.1181,  0.1269,  0.0922,  0.1086,  0.0757,  0.1106,  0.0830,  0.0979,
         0.0722,  0.0441,  0.0840,  0.1140,  0.0903,  0.0764,  0.1096,  0.0923,
         0.0775,  0.0925,  0.1001,  0.0800,  0.0876,  0.0566,  0.0934,  0.0856,
         0.1187,  0.1115,  0.0892,  0.0683,  0.0755,  0.0997,  0.1255,  0.0686,
         0.1133,  0.0894,  0.0884,  0.0779,  0.0802,  0.1079,  0.0481,  0.0979,
         0.0991,  0.0900,  0.0884,  0.0570,  0.0662,  0.0860,  0.1031,  0.1002,
         0.0872,  0.1054,  0.1257,  0.0503,  0.0530,  0.0798,  0.0943,  0.1074,
         0.1142,  0.0222,  0.0953,  0.1034,  0.1138,  0.1146,  0.1108,  0.1165,
         0.1260,  0.1086,  0.0446,  0.1254,  0.1272,  0.1257,  0.1182,  0.1174,
         0.1156, -0.0497,  0.0994,  0.1203,  0.1131,  0.0179,  0.0606, -0.0054,
         0.0905, -0.0453,  0.0747,  0.1204,  0.0837,  0.0266,  0.0978,  0.1060,
         0.1145,  0.0970,  0.1156,  0.0843, -0.0020,  0.1201,  0.1271,  0.1212,
         0.0332,  0.0975,  0.1056,  0.0951,  0.1024,  0.1211,  0.0419,  0.0349,
         0.0954,  0.1222,  0.0687,  0.1054,  0.1198, -0.0292,  0.0516,  0.1159,
         0.0917,  0.0890,  0.0363,  0.1113,  0.1199,  0.0207,  0.1078,  0.1227,
         0.1254,  0.1219,  0.1015,  0.0538,  0.1152,  0.1205,  0.1082, -0.0479,
         0.0565,  0.1000,  0.1249,  0.0819,  0.0350,  0.1066,  0.0992,  0.1320,
         0.1076,  0.0931,  0.1110,  0.0297,  0.1116,  0.0965,  0.1051,  0.1174,
         0.0637,  0.1159,  0.0835,  0.1302,  0.0644,  0.1288,  0.0958,  0.0422,
         0.0556,  0.0402,  0.0816,  0.1187,  0.1134,  0.1124,  0.0232,  0.0691,
         0.0962,  0.1219,  0.1266,  0.0978,  0.1195,  0.1244,  0.0687,  0.1132,
         0.1136,  0.1192,  0.0607,  0.1209,  0.1239,  0.1310,  0.0810,  0.1014,
         0.1411,  0.1428,  0.1440,  0.1301,  0.1356,  0.1385,  0.1271,  0.1296,
         0.1483,  0.1267,  0.1398,  0.1401,  0.1242,  0.1288,  0.1182,  0.1315,
         0.1354,  0.1345,  0.1274,  0.1318,  0.1328,  0.1403,  0.1319,  0.1267,
         0.1298,  0.1531,  0.1362,  0.1414,  0.1483,  0.1164,  0.1286,  0.1419,
         0.1198,  0.1341,  0.1255,  0.1314,  0.1441,  0.1203,  0.1133,  0.1266,
         0.1359,  0.1316,  0.1159,  0.1311,  0.1366,  0.1503,  0.1394,  0.1334,
         0.1363,  0.1413,  0.1171,  0.1192,  0.1291,  0.1282,  0.1309,  0.1401,
         0.1438,  0.1356,  0.1219,  0.1305,  0.1211,  0.1467,  0.1398,  0.1395,
         0.1369,  0.1364,  0.1366,  0.1329,  0.1370,  0.1284,  0.1331,  0.1354,
         0.1246,  0.1330,  0.1399,  0.1273,  0.1323,  0.1261,  0.1430,  0.1121,
         0.1038,  0.1341,  0.1401,  0.1122,  0.1334,  0.1296,  0.1300,  0.1336,
         0.1225,  0.1002,  0.1197,  0.1425,  0.1266,  0.1418,  0.1242,  0.1306,
         0.1327,  0.0900,  0.1309,  0.1289,  0.1343,  0.1328,  0.1225,  0.1356,
         0.1357,  0.1411,  0.1448,  0.1364,  0.1361,  0.1313,  0.1371,  0.1345,
         0.1289,  0.1334,  0.1351,  0.1272,  0.1254,  0.1238,  0.1479,  0.1502,
         0.1254,  0.0925,  0.1266,  0.1320,  0.1334,  0.1318,  0.1390,  0.1318],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.7505, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:47,391 :: INFO :: Epoch 30: loss tensor(89.5797, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1029,  0.0626,  0.1410,  0.0998,  0.0864,  0.1253,  0.0899,  0.0977,
         0.0723,  0.0850,  0.1206,  0.0983,  0.0721,  0.1193,  0.1095,  0.0762,
         0.0552,  0.0859,  0.0660,  0.1156,  0.0908,  0.0668,  0.1072,  0.1225,
         0.1061,  0.0606,  0.0549,  0.1051,  0.1454,  0.0641,  0.1074,  0.0868,
         0.1000,  0.0889,  0.0831,  0.0656,  0.0675,  0.1194,  0.0778,  0.1141,
         0.0924,  0.0839,  0.1062,  0.0843,  0.0624,  0.0777,  0.0778,  0.1052,
         0.1145,  0.0528,  0.0918,  0.0660,  0.0699,  0.0517,  0.1293,  0.1058,
         0.1063,  0.1534,  0.1117,  0.1172,  0.0578,  0.0913,  0.0979,  0.0802,
         0.0867,  0.0668,  0.1053,  0.1040,  0.0922,  0.0888,  0.0674,  0.0916,
         0.0549,  0.1242,  0.0903,  0.0687,  0.0868,  0.0955,  0.0714,  0.0785,
         0.1310,  0.1477,  0.0957,  0.1216,  0.0753,  0.1178,  0.0829,  0.1027,
         0.0763,  0.0422,  0.0885,  0.1278,  0.0940,  0.0744,  0.1187,  0.0940,
         0.0773,  0.0929,  0.1088,  0.0806,  0.0879,  0.0572,  0.0971,  0.0954,
         0.1317,  0.1207,  0.0912,  0.0652,  0.0755,  0.1060,  0.1409,  0.0701,
         0.1206,  0.0918,  0.0924,  0.0798,  0.0814,  0.1215,  0.0481,  0.1056,
         0.1045,  0.0935,  0.0889,  0.0572,  0.0675,  0.0920,  0.1114,  0.1083,
         0.0938,  0.1136,  0.1402,  0.0448,  0.0477,  0.0784,  0.1008,  0.1223,
         0.1384,  0.0210,  0.1044,  0.1177,  0.1205,  0.1318,  0.1215,  0.1440,
         0.1418,  0.1138,  0.0403,  0.1497,  0.1430,  0.1445,  0.1380,  0.1342,
         0.1238, -0.0513,  0.1081,  0.1443,  0.1300,  0.0320,  0.0577,  0.0085,
         0.0915, -0.0430,  0.0773,  0.1344,  0.0862,  0.0246,  0.1126,  0.1201,
         0.1316,  0.1047,  0.1251,  0.0834,  0.0091,  0.1335,  0.1418,  0.1441,
         0.0305,  0.0982,  0.1159,  0.1068,  0.1099,  0.1416,  0.0380,  0.0502,
         0.0954,  0.1341,  0.0840,  0.1095,  0.1314, -0.0253,  0.0759,  0.1311,
         0.0926,  0.1073,  0.0338,  0.1170,  0.1396,  0.0181,  0.1149,  0.1399,
         0.1394,  0.1401,  0.1064,  0.0503,  0.1246,  0.1428,  0.1220, -0.0477,
         0.0515,  0.1107,  0.1371,  0.0777,  0.0313,  0.1214,  0.1018,  0.1525,
         0.1198,  0.1083,  0.1162,  0.0447,  0.1267,  0.1030,  0.1134,  0.1341,
         0.0605,  0.1294,  0.0843,  0.1506,  0.0590,  0.1438,  0.0963,  0.0362,
         0.0582,  0.0372,  0.0785,  0.1409,  0.1293,  0.1268,  0.0216,  0.0686,
         0.1020,  0.1392,  0.1484,  0.1048,  0.1384,  0.1394,  0.0886,  0.1247,
         0.1269,  0.1370,  0.0840,  0.1332,  0.1435,  0.1514,  0.1039,  0.1076,
         0.1706,  0.1700,  0.1745,  0.1466,  0.1589,  0.1663,  0.1494,  0.1520,
         0.1783,  0.1519,  0.1634,  0.1628,  0.1408,  0.1420,  0.1352,  0.1549,
         0.1625,  0.1581,  0.1453,  0.1552,  0.1589,  0.1640,  0.1498,  0.1455,
         0.1502,  0.1833,  0.1562,  0.1708,  0.1786,  0.1248,  0.1515,  0.1656,
         0.1311,  0.1592,  0.1488,  0.1544,  0.1759,  0.1343,  0.1286,  0.1444,
         0.1569,  0.1552,  0.1244,  0.1476,  0.1664,  0.1797,  0.1694,  0.1641,
         0.1625,  0.1713,  0.1268,  0.1419,  0.1484,  0.1537,  0.1572,  0.1694,
         0.1710,  0.1610,  0.1456,  0.1479,  0.1395,  0.1771,  0.1612,  0.1683,
         0.1561,  0.1657,  0.1561,  0.1557,  0.1637,  0.1446,  0.1636,  0.1630,
         0.1442,  0.1547,  0.1618,  0.1491,  0.1563,  0.1482,  0.1704,  0.1214,
         0.1090,  0.1561,  0.1669,  0.1198,  0.1541,  0.1507,  0.1523,  0.1571,
         0.1425,  0.1053,  0.1322,  0.1681,  0.1494,  0.1656,  0.1426,  0.1532,
         0.1553,  0.0960,  0.1543,  0.1478,  0.1642,  0.1565,  0.1394,  0.1573,
         0.1622,  0.1724,  0.1743,  0.1676,  0.1592,  0.1482,  0.1607,  0.1608,
         0.1465,  0.1561,  0.1590,  0.1577,  0.1405,  0.1411,  0.1779,  0.1784,
         0.1397,  0.0928,  0.1430,  0.1619,  0.1515,  0.1555,  0.1646,  0.1497],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.1322, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:51,892 :: INFO :: Epoch 35: loss tensor(92.2125, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1053,  0.0609,  0.1541,  0.1016,  0.0873,  0.1380,  0.0914,  0.0982,
         0.0759,  0.0838,  0.1298,  0.1003,  0.0700,  0.1271,  0.1127,  0.0754,
         0.0521,  0.0896,  0.0659,  0.1218,  0.0943,  0.0659,  0.1104,  0.1322,
         0.1107,  0.0585,  0.0501,  0.1084,  0.1594,  0.0626,  0.1113,  0.0855,
         0.1053,  0.0887,  0.0840,  0.0648,  0.0664,  0.1273,  0.0784,  0.1193,
         0.0953,  0.0864,  0.1085,  0.0835,  0.0591,  0.0767,  0.0758,  0.1124,
         0.1220,  0.0520,  0.0910,  0.0624,  0.0708,  0.0479,  0.1391,  0.1093,
         0.1126,  0.1709,  0.1176,  0.1242,  0.0559,  0.0932,  0.1047,  0.0789,
         0.0875,  0.0667,  0.1082,  0.1047,  0.0911,  0.0894,  0.0680,  0.0946,
         0.0514,  0.1386,  0.0903,  0.0675,  0.0876,  0.1006,  0.0712,  0.0807,
         0.1415,  0.1674,  0.0953,  0.1311,  0.0758,  0.1236,  0.0802,  0.1066,
         0.0780,  0.0426,  0.0875,  0.1377,  0.0940,  0.0727,  0.1264,  0.0961,
         0.0753,  0.0933,  0.1136,  0.0836,  0.0893,  0.0548,  0.0973,  0.1014,
         0.1430,  0.1307,  0.0896,  0.0631,  0.0760,  0.1093,  0.1539,  0.0676,
         0.1257,  0.0916,  0.0932,  0.0796,  0.0829,  0.1346,  0.0462,  0.1097,
         0.1078,  0.0953,  0.0910,  0.0601,  0.0653,  0.0944,  0.1176,  0.1145,
         0.0934,  0.1159,  0.1496,  0.0390,  0.0425,  0.0749,  0.1006,  0.1317,
         0.1591,  0.0226,  0.1072,  0.1265,  0.1200,  0.1443,  0.1252,  0.1653,
         0.1527,  0.1131,  0.0352,  0.1696,  0.1533,  0.1585,  0.1536,  0.1404,
         0.1256, -0.0522,  0.1115,  0.1655,  0.1426,  0.0432,  0.0534,  0.0202,
         0.0875, -0.0405,  0.0719,  0.1432,  0.0838,  0.0234,  0.1224,  0.1288,
         0.1444,  0.1060,  0.1285,  0.0781,  0.0181,  0.1417,  0.1519,  0.1632,
         0.0274,  0.0930,  0.1205,  0.1137,  0.1116,  0.1580,  0.0315,  0.0619,
         0.0904,  0.1385,  0.0946,  0.1070,  0.1363, -0.0218,  0.0941,  0.1410,
         0.0891,  0.1213,  0.0317,  0.1172,  0.1508,  0.0156,  0.1160,  0.1517,
         0.1478,  0.1503,  0.1063,  0.0442,  0.1277,  0.1578,  0.1305, -0.0471,
         0.0458,  0.1155,  0.1405,  0.0706,  0.0262,  0.1309,  0.0991,  0.1673,
         0.1257,  0.1177,  0.1143,  0.0569,  0.1332,  0.1034,  0.1129,  0.1447,
         0.0566,  0.1334,  0.0813,  0.1644,  0.0538,  0.1536,  0.0908,  0.0315,
         0.0557,  0.0330,  0.0721,  0.1579,  0.1404,  0.1365,  0.0188,  0.0638,
         0.1017,  0.1517,  0.1663,  0.1058,  0.1532,  0.1498,  0.1049,  0.1302,
         0.1350,  0.1505,  0.1008,  0.1405,  0.1574,  0.1673,  0.1232,  0.1073,
         0.1980,  0.1949,  0.2031,  0.1589,  0.1806,  0.1907,  0.1683,  0.1707,
         0.2067,  0.1754,  0.1848,  0.1827,  0.1533,  0.1510,  0.1487,  0.1751,
         0.1874,  0.1791,  0.1593,  0.1755,  0.1833,  0.1853,  0.1641,  0.1606,
         0.1668,  0.2114,  0.1719,  0.1979,  0.2073,  0.1266,  0.1713,  0.1861,
         0.1389,  0.1814,  0.1695,  0.1742,  0.2057,  0.1454,  0.1394,  0.1598,
         0.1739,  0.1772,  0.1273,  0.1608,  0.1940,  0.2071,  0.1973,  0.1925,
         0.1867,  0.1995,  0.1313,  0.1633,  0.1648,  0.1771,  0.1818,  0.1966,
         0.1963,  0.1841,  0.1674,  0.1612,  0.1561,  0.2058,  0.1794,  0.1951,
         0.1713,  0.1934,  0.1718,  0.1761,  0.1882,  0.1574,  0.1921,  0.1887,
         0.1598,  0.1739,  0.1805,  0.1683,  0.1773,  0.1663,  0.1960,  0.1246,
         0.1102,  0.1751,  0.1915,  0.1206,  0.1709,  0.1682,  0.1710,  0.1777,
         0.1592,  0.1081,  0.1414,  0.1911,  0.1695,  0.1864,  0.1569,  0.1743,
         0.1747,  0.1004,  0.1748,  0.1626,  0.1918,  0.1792,  0.1535,  0.1755,
         0.1870,  0.2019,  0.2006,  0.1971,  0.1811,  0.1612,  0.1816,  0.1854,
         0.1604,  0.1756,  0.1802,  0.1862,  0.1498,  0.1554,  0.2059,  0.2048,
         0.1495,  0.0900,  0.1539,  0.1904,  0.1656,  0.1764,  0.1877,  0.1636],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.2951, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:56,424 :: INFO :: Epoch 40: loss tensor(90.5855, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1064,  0.0595,  0.1653,  0.1024,  0.0879,  0.1439,  0.0897,  0.0970,
         0.0776,  0.0820,  0.1331,  0.1023,  0.0673,  0.1325,  0.1145,  0.0762,
         0.0508,  0.0913,  0.0626,  0.1250,  0.0954,  0.0644,  0.1124,  0.1394,
         0.1138,  0.0557,  0.0470,  0.1091,  0.1698,  0.0601,  0.1155,  0.0843,
         0.1067,  0.0859,  0.0842,  0.0618,  0.0652,  0.1323,  0.0779,  0.1188,
         0.0982,  0.0830,  0.1049,  0.0833,  0.0599,  0.0722,  0.0734,  0.1191,
         0.1215,  0.0507,  0.0876,  0.0596,  0.0711,  0.0457,  0.1425,  0.1115,
         0.1138,  0.1860,  0.1192,  0.1248,  0.0565,  0.0882,  0.1079,  0.0786,
         0.0872,  0.0640,  0.1070,  0.1064,  0.0906,  0.0868,  0.0680,  0.0946,
         0.0462,  0.1500,  0.0890,  0.0652,  0.0876,  0.1033,  0.0710,  0.0802,
         0.1508,  0.1804,  0.0951,  0.1388,  0.0734,  0.1237,  0.0770,  0.1093,
         0.0808,  0.0403,  0.0861,  0.1456,  0.0931,  0.0709,  0.1274,  0.0957,
         0.0738,  0.0907,  0.1158,  0.0826,  0.0867,  0.0541,  0.0972,  0.1061,
         0.1503,  0.1318,  0.0891,  0.0596,  0.0723,  0.1118,  0.1620,  0.0691,
         0.1265,  0.0905,  0.0931,  0.0781,  0.0810,  0.1466,  0.0451,  0.1122,
         0.1078,  0.0950,  0.0874,  0.0588,  0.0635,  0.0950,  0.1227,  0.1189,
         0.0912,  0.1151,  0.1544,  0.0326,  0.0353,  0.0712,  0.0968,  0.1367,
         0.1758,  0.0223,  0.1046,  0.1296,  0.1141,  0.1524,  0.1235,  0.1836,
         0.1589,  0.1086,  0.0308,  0.1853,  0.1588,  0.1679,  0.1646,  0.1403,
         0.1220, -0.0532,  0.1093,  0.1834,  0.1507,  0.0504,  0.0497,  0.0300,
         0.0809, -0.0369,  0.0689,  0.1470,  0.0807,  0.0230,  0.1267,  0.1328,
         0.1520,  0.1030,  0.1271,  0.0723,  0.0243,  0.1453,  0.1579,  0.1780,
         0.0241,  0.0845,  0.1191,  0.1179,  0.1094,  0.1699,  0.0303,  0.0688,
         0.0831,  0.1386,  0.0995,  0.1014,  0.1374, -0.0192,  0.1084,  0.1455,
         0.0843,  0.1296,  0.0287,  0.1143,  0.1553,  0.0128,  0.1123,  0.1575,
         0.1516,  0.1533,  0.1028,  0.0382,  0.1248,  0.1668,  0.1334, -0.0462,
         0.0411,  0.1158,  0.1370,  0.0632,  0.0226,  0.1352,  0.0919,  0.1773,
         0.1262,  0.1215,  0.1087,  0.0641,  0.1320,  0.0983,  0.1069,  0.1497,
         0.0525,  0.1315,  0.0758,  0.1737,  0.0464,  0.1583,  0.0819,  0.0271,
         0.0534,  0.0288,  0.0651,  0.1701,  0.1459,  0.1419,  0.0173,  0.0582,
         0.0975,  0.1593,  0.1803,  0.1052,  0.1635,  0.1563,  0.1161,  0.1306,
         0.1383,  0.1596,  0.1141,  0.1437,  0.1655,  0.1786,  0.1381,  0.1019,
         0.2241,  0.2180,  0.2294,  0.1679,  0.1995,  0.2125,  0.1843,  0.1861,
         0.2328,  0.1964,  0.2045,  0.2005,  0.1608,  0.1548,  0.1592,  0.1923,
         0.2098,  0.1978,  0.1689,  0.1941,  0.2050,  0.2044,  0.1757,  0.1722,
         0.1797,  0.2370,  0.1839,  0.2239,  0.2341,  0.1237,  0.1875,  0.2043,
         0.1434,  0.2010,  0.1889,  0.1910,  0.2333,  0.1521,  0.1476,  0.1728,
         0.1870,  0.1961,  0.1261,  0.1691,  0.2202,  0.2314,  0.2229,  0.2184,
         0.2093,  0.2261,  0.1312,  0.1826,  0.1776,  0.1983,  0.2030,  0.2211,
         0.2198,  0.2045,  0.1866,  0.1693,  0.1701,  0.2319,  0.1941,  0.2198,
         0.1829,  0.2186,  0.1839,  0.1938,  0.2104,  0.1665,  0.2187,  0.2127,
         0.1730,  0.1905,  0.1964,  0.1860,  0.1958,  0.1810,  0.2194,  0.1207,
         0.1096,  0.1906,  0.2131,  0.1153,  0.1838,  0.1822,  0.1871,  0.1962,
         0.1726,  0.1119,  0.1473,  0.2132,  0.1867,  0.2046,  0.1670,  0.1930,
         0.1913,  0.1035,  0.1931,  0.1732,  0.2166,  0.1985,  0.1643,  0.1905,
         0.2098,  0.2293,  0.2235,  0.2246,  0.1991,  0.1699,  0.2000,  0.2078,
         0.1715,  0.1920,  0.1989,  0.2119,  0.1544,  0.1654,  0.2311,  0.2284,
         0.1527,  0.0880,  0.1594,  0.2168,  0.1755,  0.1944,  0.2085,  0.1747],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.2710, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:50:56,424 :: INFO :: ----- frontend -----
2023-05-14 11:50:56,424 :: INFO :: Environment 0
2023-05-14 11:51:05,799 :: INFO :: Epoch 5: loss tensor(862.3271, device='cuda:0'), U.norm 13.155776977539062, V.norm 16.976316452026367, MLP.norm 2.161973476409912
2023-05-14 11:51:06,034 :: INFO :: Epoch 10: loss tensor(835.4331, device='cuda:0'), U.norm 10.344199180603027, V.norm 16.31230926513672, MLP.norm 3.684821605682373
2023-05-14 11:51:06,284 :: INFO :: Epoch 15: loss tensor(799.4367, device='cuda:0'), U.norm 8.642373085021973, V.norm 15.98916244506836, MLP.norm 5.457673072814941
2023-05-14 11:51:06,518 :: INFO :: Epoch 20: loss tensor(759.8979, device='cuda:0'), U.norm 7.481273174285889, V.norm 15.77355670928955, MLP.norm 7.28778600692749
2023-05-14 11:51:06,534 :: INFO :: Environment 1
2023-05-14 11:51:15,863 :: INFO :: Epoch 5: loss tensor(817.2145, device='cuda:0'), U.norm 13.158382415771484, V.norm 16.8641414642334, MLP.norm 2.128559112548828
2023-05-14 11:51:16,113 :: INFO :: Epoch 10: loss tensor(793.9090, device='cuda:0'), U.norm 10.348541259765625, V.norm 16.158742904663086, MLP.norm 3.565873861312866
2023-05-14 11:51:16,348 :: INFO :: Epoch 15: loss tensor(761.1099, device='cuda:0'), U.norm 8.648702621459961, V.norm 15.817923545837402, MLP.norm 5.264416694641113
2023-05-14 11:51:16,582 :: INFO :: Epoch 20: loss tensor(724.8175, device='cuda:0'), U.norm 7.49007511138916, V.norm 15.59144115447998, MLP.norm 7.040818691253662
2023-05-14 11:51:16,598 :: INFO :: Environment 2
2023-05-14 11:51:25,911 :: INFO :: Epoch 5: loss tensor(862.4912, device='cuda:0'), U.norm 13.163126945495605, V.norm 16.97532844543457, MLP.norm 2.196568489074707
2023-05-14 11:51:26,145 :: INFO :: Epoch 10: loss tensor(834.0500, device='cuda:0'), U.norm 10.35927963256836, V.norm 16.31488800048828, MLP.norm 3.759512424468994
2023-05-14 11:51:26,380 :: INFO :: Epoch 15: loss tensor(795.4255, device='cuda:0'), U.norm 8.666510581970215, V.norm 15.991965293884277, MLP.norm 5.543092250823975
2023-05-14 11:51:26,630 :: INFO :: Epoch 20: loss tensor(753.6703, device='cuda:0'), U.norm 7.515398979187012, V.norm 15.773478507995605, MLP.norm 7.397922515869141
2023-05-14 11:51:26,630 :: INFO :: Environment 3
2023-05-14 11:51:35,756 :: INFO :: Epoch 5: loss tensor(826.3110, device='cuda:0'), U.norm 13.15280532836914, V.norm 16.907209396362305, MLP.norm 2.109475612640381
2023-05-14 11:51:36,006 :: INFO :: Epoch 10: loss tensor(802.4795, device='cuda:0'), U.norm 10.337864875793457, V.norm 16.221914291381836, MLP.norm 3.547759532928467
2023-05-14 11:51:36,240 :: INFO :: Epoch 15: loss tensor(769.5023, device='cuda:0'), U.norm 8.632200241088867, V.norm 15.891691207885742, MLP.norm 5.260838508605957
2023-05-14 11:51:36,475 :: INFO :: Epoch 20: loss tensor(733.5453, device='cuda:0'), U.norm 7.466549873352051, V.norm 15.672425270080566, MLP.norm 7.079549312591553
2023-05-14 11:51:36,475 :: INFO :: Environment 4
2023-05-14 11:51:45,835 :: INFO :: Epoch 5: loss tensor(908.2769, device='cuda:0'), U.norm 13.158679962158203, V.norm 17.065765380859375, MLP.norm 2.1815946102142334
2023-05-14 11:51:46,070 :: INFO :: Epoch 10: loss tensor(877.3198, device='cuda:0'), U.norm 10.35146713256836, V.norm 16.44272804260254, MLP.norm 3.776681661605835
2023-05-14 11:51:46,304 :: INFO :: Epoch 15: loss tensor(835.9316, device='cuda:0'), U.norm 8.65503215789795, V.norm 16.137847900390625, MLP.norm 5.618162155151367
2023-05-14 11:51:46,538 :: INFO :: Epoch 20: loss tensor(792.4758, device='cuda:0'), U.norm 7.499731540679932, V.norm 15.93362808227539, MLP.norm 7.522815704345703
2023-05-14 11:51:46,554 :: INFO :: Environment 5
2023-05-14 11:51:55,883 :: INFO :: Epoch 5: loss tensor(833.6611, device='cuda:0'), U.norm 13.15429973602295, V.norm 16.92267417907715, MLP.norm 2.155954360961914
2023-05-14 11:51:56,117 :: INFO :: Epoch 10: loss tensor(807.0616, device='cuda:0'), U.norm 10.340778350830078, V.norm 16.241533279418945, MLP.norm 3.678621292114258
2023-05-14 11:51:56,352 :: INFO :: Epoch 15: loss tensor(772.3653, device='cuda:0'), U.norm 8.63588809967041, V.norm 15.909481048583984, MLP.norm 5.447432041168213
2023-05-14 11:51:56,586 :: INFO :: Epoch 20: loss tensor(735.4410, device='cuda:0'), U.norm 7.470270156860352, V.norm 15.687193870544434, MLP.norm 7.25771427154541
2023-05-14 11:51:56,586 :: INFO :: Environment 6
2023-05-14 11:52:05,931 :: INFO :: Epoch 5: loss tensor(847.8476, device='cuda:0'), U.norm 13.155625343322754, V.norm 16.941373825073242, MLP.norm 2.1412928104400635
2023-05-14 11:52:06,165 :: INFO :: Epoch 10: loss tensor(823.6730, device='cuda:0'), U.norm 10.343557357788086, V.norm 16.268247604370117, MLP.norm 3.6164438724517822
2023-05-14 11:52:06,400 :: INFO :: Epoch 15: loss tensor(790.5472, device='cuda:0'), U.norm 8.641006469726562, V.norm 15.942864418029785, MLP.norm 5.3524250984191895
2023-05-14 11:52:06,634 :: INFO :: Epoch 20: loss tensor(754.7573, device='cuda:0'), U.norm 7.478701114654541, V.norm 15.727655410766602, MLP.norm 7.130383491516113
2023-05-14 11:52:06,650 :: INFO :: Environment 7
2023-05-14 11:52:15,995 :: INFO :: Epoch 5: loss tensor(812.3940, device='cuda:0'), U.norm 13.153653144836426, V.norm 16.879247665405273, MLP.norm 2.1428325176239014
2023-05-14 11:52:16,229 :: INFO :: Epoch 10: loss tensor(786.3772, device='cuda:0'), U.norm 10.339301109313965, V.norm 16.17441749572754, MLP.norm 3.6164956092834473
2023-05-14 11:52:16,463 :: INFO :: Epoch 15: loss tensor(750.4847, device='cuda:0'), U.norm 8.63416576385498, V.norm 15.830181121826172, MLP.norm 5.354515075683594
2023-05-14 11:52:16,698 :: INFO :: Epoch 20: loss tensor(712.1422, device='cuda:0'), U.norm 7.46897554397583, V.norm 15.599248886108398, MLP.norm 7.157201766967773
2023-05-14 11:52:16,698 :: INFO :: Environment 8
2023-05-14 11:52:26,074 :: INFO :: Epoch 5: loss tensor(832.3536, device='cuda:0'), U.norm 13.154511451721191, V.norm 16.922985076904297, MLP.norm 2.1491899490356445
2023-05-14 11:52:26,308 :: INFO :: Epoch 10: loss tensor(806.8356, device='cuda:0'), U.norm 10.341568946838379, V.norm 16.239912033081055, MLP.norm 3.6339025497436523
2023-05-14 11:52:26,543 :: INFO :: Epoch 15: loss tensor(772.1823, device='cuda:0'), U.norm 8.638470649719238, V.norm 15.909099578857422, MLP.norm 5.360058784484863
2023-05-14 11:52:26,777 :: INFO :: Epoch 20: loss tensor(734.0560, device='cuda:0'), U.norm 7.4757537841796875, V.norm 15.687662124633789, MLP.norm 7.154903411865234
2023-05-14 11:52:26,793 :: INFO :: Environment 9
2023-05-14 11:52:36,122 :: INFO :: Epoch 5: loss tensor(868.3099, device='cuda:0'), U.norm 13.159477233886719, V.norm 16.992008209228516, MLP.norm 2.1748456954956055
2023-05-14 11:52:36,388 :: INFO :: Epoch 10: loss tensor(838.2117, device='cuda:0'), U.norm 10.352446556091309, V.norm 16.339902877807617, MLP.norm 3.7412526607513428
2023-05-14 11:52:36,622 :: INFO :: Epoch 15: loss tensor(797.8234, device='cuda:0'), U.norm 8.656582832336426, V.norm 16.02290153503418, MLP.norm 5.554213523864746
2023-05-14 11:52:36,857 :: INFO :: Epoch 20: loss tensor(753.4183, device='cuda:0'), U.norm 7.501742362976074, V.norm 15.810760498046875, MLP.norm 7.442318439483643
2023-05-14 11:52:36,872 :: INFO :: Ite = 1, Delta = 4097
2023-05-14 11:52:36,872 :: INFO :: ----- backend -----
2023-05-14 11:52:41,357 :: INFO :: Epoch 5: loss tensor(103.7297, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.4338e-03,  1.2871e-02,  1.2299e-02,  1.6275e-02,  1.1638e-02,
         5.1440e-03,  8.2691e-03,  1.8875e-02,  4.8772e-03,  1.8707e-02,
         2.9025e-03,  1.6314e-02,  8.8469e-03,  1.6401e-02,  2.1442e-02,
         1.9162e-02,  2.1900e-02,  2.0235e-02,  1.9117e-02,  1.3808e-02,
         1.2973e-02,  1.9346e-02,  1.7889e-02,  1.8779e-03,  1.7814e-02,
         1.1589e-02,  1.1787e-02,  1.0933e-02,  2.1092e-02,  1.6352e-02,
         1.9969e-02,  2.4452e-02,  9.5807e-03,  1.8056e-02, -1.4676e-03,
        -7.6297e-03,  2.0823e-02,  2.1079e-02,  1.0379e-02,  1.7660e-02,
         5.5725e-03,  6.8188e-04,  2.1113e-02,  1.7783e-02,  1.2611e-02,
         2.0406e-02,  1.3435e-02,  1.5639e-03,  1.6436e-02,  1.4394e-02,
         2.0141e-02,  1.1375e-02,  1.5183e-02,  1.3125e-02,  8.4794e-03,
         1.9872e-02,  9.6443e-03,  1.7532e-02,  3.2456e-03, -2.8494e-04,
         1.8620e-02,  1.8808e-02,  1.5016e-03,  1.4830e-02,  1.3715e-02,
         4.0084e-03,  2.3928e-02,  1.3074e-02,  2.4622e-02,  9.3283e-03,
         2.0554e-02,  5.5527e-03,  2.2805e-02,  7.0903e-03,  1.7969e-02,
         5.1664e-03,  1.9607e-02, -1.5826e-05,  1.1627e-02,  1.9303e-02,
         1.5717e-02,  2.5185e-03,  2.0219e-02,  1.2345e-02,  1.6116e-03,
         1.7119e-02,  2.3677e-02,  9.4571e-03,  7.4308e-03, -2.9450e-03,
         9.5602e-03,  1.6905e-02,  1.6973e-02,  1.9023e-02,  6.3263e-03,
         1.8724e-02,  2.2848e-02,  1.6920e-02,  9.0103e-03,  1.3170e-02,
        -4.5445e-04,  1.4018e-02,  1.5844e-02, -1.4649e-03,  1.1566e-02,
         8.4035e-06,  1.6867e-02,  1.5359e-02,  1.7720e-02,  1.8160e-02,
         4.7972e-03,  1.5473e-02,  1.5131e-02,  1.7816e-02,  5.6501e-03,
         4.7005e-03,  1.4398e-02,  1.1295e-02,  1.3157e-02,  1.0475e-02,
         1.7135e-02,  1.3148e-02,  7.7874e-03,  7.0862e-03,  5.2640e-03,
         1.5113e-02, -7.5746e-05,  1.7615e-02,  1.9529e-02,  1.8647e-02,
         1.8878e-02,  2.4619e-02,  2.3863e-02,  2.0318e-02,  7.9610e-03,
        -5.4320e-03, -1.5977e-02,  1.5991e-02,  1.1440e-02, -7.3399e-03,
         2.2033e-02, -7.3558e-03,  9.8134e-03,  1.2169e-02,  1.6684e-02,
         2.5866e-02,  1.4591e-02, -1.8095e-03,  1.2746e-02, -2.7040e-04,
        -1.2727e-02,  1.9299e-02,  2.0988e-02, -2.1842e-02,  3.7177e-03,
        -1.4914e-02, -5.9416e-03, -2.0774e-02,  2.5307e-02, -1.7715e-02,
         2.5811e-02, -1.9132e-02,  2.2087e-02,  7.0505e-03,  2.5670e-02,
         1.5822e-02, -1.3072e-02, -2.0633e-03, -5.5207e-03,  1.2689e-02,
         2.0251e-02,  1.9088e-02, -2.0254e-02,  1.7739e-02,  2.2346e-02,
        -1.3294e-02,  1.9399e-02,  2.0444e-02,  3.3119e-03,  4.0247e-03,
         1.2983e-02, -1.1737e-02,  1.9387e-02, -2.0302e-02,  2.1701e-02,
         2.2132e-02, -1.9397e-02,  1.4094e-02,  2.2150e-02, -2.0901e-02,
        -8.6810e-03,  1.3048e-03,  2.5574e-02, -1.9144e-02,  1.6232e-02,
         2.0503e-02,  1.6198e-02,  1.8657e-02,  2.2267e-02,  7.0934e-03,
         2.1869e-02,  1.3277e-02,  2.5251e-02,  2.1125e-02,  1.5345e-02,
         1.0132e-02, -5.3639e-03, -2.0529e-02,  2.2172e-02,  1.9691e-03,
         2.5262e-02,  2.5174e-02,  1.8187e-02, -1.1670e-02,  1.7762e-02,
         1.8601e-02,  9.1932e-04, -1.3240e-02,  2.5723e-02, -2.0511e-02,
         1.5800e-02,  6.0565e-03,  2.5411e-02,  2.3138e-02,  1.8932e-02,
         1.5963e-02,  1.4343e-02,  1.7478e-02,  1.4997e-02,  2.2350e-02,
         1.8626e-02,  2.1391e-02,  2.5361e-02,  2.0847e-02,  2.5824e-02,
        -5.7381e-03, -1.0581e-03,  2.3104e-03,  1.2800e-02,  1.9845e-02,
         7.6523e-03,  7.7493e-03,  2.2723e-03,  2.5981e-02, -5.6374e-03,
         1.8385e-02, -2.0059e-02,  1.0938e-02,  3.9343e-03, -1.2845e-03,
        -1.1770e-02,  2.5306e-02,  8.8160e-03,  1.0689e-02, -1.9845e-02,
         1.0949e-02,  1.0387e-02,  1.7052e-02,  1.5195e-02,  2.4777e-02,
         2.1863e-02,  1.4625e-02,  4.8175e-03,  9.5956e-03,  2.0035e-02,
         1.3345e-02,  2.2486e-02,  2.4739e-02,  1.4522e-02,  1.9826e-02,
         1.0974e-02,  9.4002e-03,  9.6673e-03,  1.6416e-02,  1.5555e-02,
         1.0191e-02,  1.2661e-02,  1.8282e-02,  2.1754e-02,  1.3380e-02,
         1.3416e-02,  2.0958e-02,  2.1661e-02,  1.5769e-02,  1.5429e-02,
         1.8276e-02,  6.8943e-03,  1.4655e-02,  2.1970e-02,  9.2212e-03,
         6.0340e-03,  1.1078e-02,  1.1820e-02,  1.8600e-02,  9.7590e-03,
         2.4956e-02,  1.8521e-02,  1.3226e-02,  2.1070e-02,  1.9830e-02,
         1.4123e-02,  2.5301e-02,  9.4360e-03,  1.5288e-03,  1.7349e-02,
         1.6887e-02,  1.8794e-02,  1.0644e-02,  1.3707e-02,  2.7633e-03,
         1.1672e-02,  1.0553e-02,  1.6476e-02,  1.6141e-02,  7.4798e-03,
         1.8610e-02,  1.3646e-02,  1.9126e-02,  1.8266e-02,  1.6473e-02,
         2.5075e-02,  7.6276e-03,  2.5114e-02,  1.7131e-02,  1.4154e-02,
         2.5207e-02,  6.7684e-03,  8.6308e-03,  1.2383e-02,  1.3481e-02,
         1.6230e-02,  7.6581e-03,  5.5464e-03,  5.6741e-03,  1.5865e-02,
         1.8269e-02,  1.4589e-02,  1.4512e-02,  2.0716e-02,  1.4808e-02,
         1.6712e-02,  1.2187e-02,  1.1185e-02,  1.7355e-02,  9.7018e-03,
         1.9193e-02,  1.6570e-02,  1.9732e-02,  1.0408e-02,  1.3264e-02,
         1.2685e-02,  1.5980e-02,  1.1222e-02,  7.2733e-03,  9.2871e-03,
         1.3761e-02,  8.4854e-03,  2.1002e-02,  1.4150e-02,  1.6508e-02,
         1.7583e-02,  1.2604e-02,  2.0963e-02,  7.3537e-03,  2.4598e-02,
         2.5618e-02,  1.6800e-02,  1.6135e-02,  2.0163e-02,  1.1046e-02,
         1.0140e-02,  2.0382e-05,  1.5343e-02,  1.3986e-02,  1.6002e-02,
         2.5528e-02,  1.8784e-02,  1.8702e-02,  1.5462e-02,  1.0593e-02,
         2.5186e-02,  7.4029e-03,  9.0984e-03,  2.1275e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.4603, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:52:45,842 :: INFO :: Epoch 10: loss tensor(102.4020, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0350,  0.0395,  0.0406,  0.0454,  0.0387,  0.0339,  0.0370,  0.0464,
         0.0323,  0.0457,  0.0314,  0.0424,  0.0362,  0.0457,  0.0490,  0.0423,
         0.0467,  0.0472,  0.0443,  0.0427,  0.0401,  0.0454,  0.0457,  0.0306,
         0.0445,  0.0382,  0.0374,  0.0396,  0.0506,  0.0434,  0.0479,  0.0507,
         0.0379,  0.0458,  0.0244,  0.0168,  0.0452,  0.0494,  0.0383,  0.0456,
         0.0326,  0.0275,  0.0480,  0.0444,  0.0393,  0.0466,  0.0395,  0.0284,
         0.0452,  0.0390,  0.0470,  0.0388,  0.0400,  0.0378,  0.0383,  0.0480,
         0.0373,  0.0468,  0.0322,  0.0286,  0.0435,  0.0443,  0.0290,  0.0421,
         0.0415,  0.0294,  0.0490,  0.0406,  0.0511,  0.0379,  0.0460,  0.0335,
         0.0451,  0.0353,  0.0448,  0.0325,  0.0457,  0.0275,  0.0382,  0.0455,
         0.0448,  0.0319,  0.0469,  0.0421,  0.0294,  0.0457,  0.0495,  0.0387,
         0.0340,  0.0223,  0.0379,  0.0458,  0.0437,  0.0446,  0.0356,  0.0461,
         0.0493,  0.0447,  0.0366,  0.0409,  0.0273,  0.0384,  0.0429,  0.0258,
         0.0413,  0.0285,  0.0426,  0.0415,  0.0452,  0.0463,  0.0326,  0.0415,
         0.0448,  0.0455,  0.0337,  0.0310,  0.0419,  0.0403,  0.0362,  0.0391,
         0.0454,  0.0404,  0.0351,  0.0339,  0.0322,  0.0404,  0.0252,  0.0462,
         0.0457,  0.0469,  0.0482,  0.0452,  0.0468,  0.0422,  0.0369,  0.0216,
        -0.0040,  0.0203,  0.0398,  0.0181,  0.0514,  0.0181,  0.0394,  0.0416,
         0.0468,  0.0537,  0.0369,  0.0278,  0.0423,  0.0291,  0.0055,  0.0482,
         0.0497, -0.0385,  0.0327, -0.0014,  0.0198, -0.0335,  0.0458, -0.0187,
         0.0514, -0.0300,  0.0440,  0.0363,  0.0513,  0.0267,  0.0043,  0.0262,
         0.0213,  0.0411,  0.0496,  0.0462, -0.0312,  0.0474,  0.0518,  0.0047,
         0.0272,  0.0489,  0.0323,  0.0315,  0.0418,  0.0088,  0.0343, -0.0305,
         0.0491,  0.0518, -0.0256,  0.0438,  0.0517, -0.0349,  0.0148,  0.0302,
         0.0518, -0.0234,  0.0385,  0.0494,  0.0465,  0.0233,  0.0504,  0.0376,
         0.0512,  0.0443,  0.0528,  0.0372,  0.0445,  0.0413,  0.0209, -0.0344,
         0.0450,  0.0310,  0.0551,  0.0508,  0.0285,  0.0084,  0.0459,  0.0494,
         0.0300,  0.0039,  0.0540, -0.0325,  0.0445,  0.0350,  0.0520,  0.0509,
         0.0420,  0.0459,  0.0419,  0.0483,  0.0413,  0.0517,  0.0462,  0.0404,
         0.0448,  0.0378,  0.0522,  0.0220,  0.0280,  0.0311,  0.0161,  0.0434,
         0.0364,  0.0379,  0.0325,  0.0528,  0.0210,  0.0475, -0.0296,  0.0402,
         0.0328,  0.0270,  0.0092,  0.0539,  0.0399,  0.0411, -0.0294,  0.0400,
         0.0407,  0.0474,  0.0469,  0.0540,  0.0510,  0.0455,  0.0347,  0.0400,
         0.0498,  0.0426,  0.0519,  0.0543,  0.0441,  0.0488,  0.0403,  0.0398,
         0.0394,  0.0459,  0.0453,  0.0397,  0.0426,  0.0481,  0.0507,  0.0428,
         0.0432,  0.0517,  0.0510,  0.0466,  0.0465,  0.0467,  0.0372,  0.0443,
         0.0504,  0.0392,  0.0342,  0.0412,  0.0428,  0.0477,  0.0392,  0.0533,
         0.0481,  0.0423,  0.0496,  0.0490,  0.0449,  0.0554,  0.0401,  0.0321,
         0.0472,  0.0475,  0.0475,  0.0398,  0.0427,  0.0320,  0.0418,  0.0412,
         0.0471,  0.0458,  0.0369,  0.0481,  0.0426,  0.0503,  0.0478,  0.0468,
         0.0547,  0.0375,  0.0545,  0.0465,  0.0443,  0.0539,  0.0381,  0.0389,
         0.0421,  0.0433,  0.0456,  0.0371,  0.0355,  0.0356,  0.0463,  0.0471,
         0.0430,  0.0443,  0.0498,  0.0439,  0.0466,  0.0421,  0.0410,  0.0478,
         0.0389,  0.0462,  0.0448,  0.0496,  0.0399,  0.0431,  0.0428,  0.0456,
         0.0412,  0.0357,  0.0383,  0.0433,  0.0395,  0.0493,  0.0432,  0.0463,
         0.0473,  0.0442,  0.0515,  0.0382,  0.0538,  0.0548,  0.0467,  0.0459,
         0.0491,  0.0413,  0.0401,  0.0308,  0.0448,  0.0434,  0.0462,  0.0561,
         0.0478,  0.0460,  0.0449,  0.0410,  0.0542,  0.0376,  0.0391,  0.0505],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.8891, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:52:50,342 :: INFO :: Epoch 15: loss tensor(99.0554, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0593,  0.0597,  0.0689,  0.0697,  0.0592,  0.0608,  0.0621,  0.0699,
         0.0534,  0.0668,  0.0580,  0.0646,  0.0583,  0.0711,  0.0719,  0.0601,
         0.0630,  0.0674,  0.0637,  0.0688,  0.0624,  0.0645,  0.0699,  0.0588,
         0.0679,  0.0563,  0.0536,  0.0645,  0.0795,  0.0632,  0.0723,  0.0708,
         0.0615,  0.0688,  0.0492,  0.0405,  0.0628,  0.0752,  0.0602,  0.0689,
         0.0570,  0.0514,  0.0708,  0.0664,  0.0563,  0.0653,  0.0590,  0.0544,
         0.0706,  0.0553,  0.0685,  0.0585,  0.0599,  0.0520,  0.0666,  0.0737,
         0.0607,  0.0762,  0.0593,  0.0562,  0.0582,  0.0645,  0.0525,  0.0635,
         0.0641,  0.0488,  0.0686,  0.0636,  0.0729,  0.0619,  0.0648,  0.0577,
         0.0596,  0.0626,  0.0656,  0.0552,  0.0661,  0.0536,  0.0565,  0.0657,
         0.0719,  0.0611,  0.0679,  0.0701,  0.0528,  0.0699,  0.0700,  0.0654,
         0.0551,  0.0418,  0.0628,  0.0724,  0.0638,  0.0630,  0.0628,  0.0691,
         0.0676,  0.0690,  0.0606,  0.0638,  0.0515,  0.0506,  0.0645,  0.0529,
         0.0700,  0.0553,  0.0640,  0.0611,  0.0666,  0.0689,  0.0608,  0.0598,
         0.0719,  0.0670,  0.0587,  0.0529,  0.0629,  0.0676,  0.0478,  0.0652,
         0.0689,  0.0639,  0.0581,  0.0535,  0.0542,  0.0640,  0.0506,  0.0709,
         0.0684,  0.0727,  0.0768,  0.0550,  0.0614,  0.0589,  0.0635,  0.0507,
         0.0240,  0.0225,  0.0660,  0.0472,  0.0795,  0.0476,  0.0681,  0.0721,
         0.0764,  0.0789,  0.0476,  0.0607,  0.0716,  0.0603,  0.0342,  0.0761,
         0.0767, -0.0505,  0.0608,  0.0267,  0.0489, -0.0381,  0.0554, -0.0089,
         0.0722, -0.0353,  0.0578,  0.0652,  0.0707,  0.0283,  0.0305,  0.0553,
         0.0514,  0.0674,  0.0771,  0.0681, -0.0345,  0.0758,  0.0804,  0.0344,
         0.0320,  0.0745,  0.0607,  0.0574,  0.0681,  0.0383,  0.0405, -0.0307,
         0.0729,  0.0803, -0.0196,  0.0715,  0.0802, -0.0432,  0.0449,  0.0598,
         0.0735, -0.0134,  0.0497,  0.0759,  0.0771,  0.0244,  0.0766,  0.0686,
         0.0796,  0.0760,  0.0775,  0.0449,  0.0725,  0.0734,  0.0498, -0.0435,
         0.0570,  0.0594,  0.0844,  0.0712,  0.0324,  0.0363,  0.0714,  0.0805,
         0.0588,  0.0304,  0.0793, -0.0362,  0.0719,  0.0623,  0.0753,  0.0775,
         0.0561,  0.0753,  0.0656,  0.0790,  0.0584,  0.0807,  0.0700,  0.0450,
         0.0534,  0.0437,  0.0734,  0.0544,  0.0584,  0.0597,  0.0189,  0.0586,
         0.0629,  0.0684,  0.0644,  0.0761,  0.0512,  0.0755, -0.0275,  0.0685,
         0.0613,  0.0570,  0.0394,  0.0812,  0.0716,  0.0725, -0.0250,  0.0669,
         0.0732,  0.0788,  0.0805,  0.0826,  0.0802,  0.0774,  0.0657,  0.0712,
         0.0811,  0.0723,  0.0814,  0.0841,  0.0732,  0.0771,  0.0690,  0.0711,
         0.0703,  0.0754,  0.0752,  0.0694,  0.0731,  0.0783,  0.0795,  0.0724,
         0.0735,  0.0845,  0.0806,  0.0792,  0.0801,  0.0737,  0.0687,  0.0746,
         0.0765,  0.0705,  0.0626,  0.0719,  0.0763,  0.0756,  0.0683,  0.0804,
         0.0778,  0.0718,  0.0757,  0.0776,  0.0770,  0.0870,  0.0732,  0.0657,
         0.0776,  0.0802,  0.0742,  0.0682,  0.0714,  0.0629,  0.0721,  0.0732,
         0.0790,  0.0758,  0.0674,  0.0776,  0.0704,  0.0835,  0.0778,  0.0783,
         0.0844,  0.0697,  0.0840,  0.0767,  0.0756,  0.0820,  0.0715,  0.0700,
         0.0718,  0.0730,  0.0753,  0.0670,  0.0669,  0.0664,  0.0780,  0.0749,
         0.0690,  0.0748,  0.0801,  0.0721,  0.0768,  0.0727,  0.0714,  0.0790,
         0.0686,  0.0697,  0.0720,  0.0804,  0.0694,  0.0739,  0.0732,  0.0753,
         0.0720,  0.0608,  0.0680,  0.0729,  0.0723,  0.0770,  0.0711,  0.0768,
         0.0773,  0.0779,  0.0837,  0.0716,  0.0830,  0.0837,  0.0769,  0.0764,
         0.0773,  0.0724,  0.0712,  0.0645,  0.0731,  0.0722,  0.0786,  0.0882,
         0.0766,  0.0699,  0.0739,  0.0732,  0.0833,  0.0687,  0.0705,  0.0787],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.0844, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:52:54,874 :: INFO :: Epoch 20: loss tensor(98.1017, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0773,  0.0694,  0.0957,  0.0893,  0.0721,  0.0816,  0.0802,  0.0888,
         0.0632,  0.0806,  0.0801,  0.0809,  0.0714,  0.0902,  0.0872,  0.0700,
         0.0677,  0.0792,  0.0750,  0.0909,  0.0792,  0.0725,  0.0888,  0.0826,
         0.0879,  0.0639,  0.0593,  0.0842,  0.1051,  0.0743,  0.0912,  0.0825,
         0.0781,  0.0840,  0.0669,  0.0576,  0.0684,  0.0970,  0.0736,  0.0843,
         0.0749,  0.0674,  0.0858,  0.0818,  0.0646,  0.0738,  0.0709,  0.0740,
         0.0893,  0.0602,  0.0808,  0.0665,  0.0711,  0.0537,  0.0910,  0.0939,
         0.0752,  0.1047,  0.0808,  0.0774,  0.0620,  0.0782,  0.0657,  0.0782,
         0.0772,  0.0592,  0.0806,  0.0788,  0.0886,  0.0768,  0.0745,  0.0743,
         0.0634,  0.0857,  0.0801,  0.0689,  0.0785,  0.0748,  0.0655,  0.0776,
         0.0960,  0.0870,  0.0823,  0.0952,  0.0666,  0.0871,  0.0830,  0.0852,
         0.0664,  0.0491,  0.0820,  0.0960,  0.0771,  0.0714,  0.0842,  0.0847,
         0.0765,  0.0847,  0.0794,  0.0803,  0.0663,  0.0537,  0.0800,  0.0748,
         0.0949,  0.0764,  0.0776,  0.0706,  0.0802,  0.0879,  0.0861,  0.0686,
         0.0935,  0.0807,  0.0779,  0.0668,  0.0750,  0.0900,  0.0500,  0.0877,
         0.0866,  0.0804,  0.0714,  0.0619,  0.0666,  0.0806,  0.0712,  0.0907,
         0.0854,  0.0949,  0.1031,  0.0570,  0.0681,  0.0682,  0.0860,  0.0775,
         0.0541,  0.0244,  0.0880,  0.0741,  0.1034,  0.0745,  0.0933,  0.1019,
         0.1038,  0.1006,  0.0518,  0.0926,  0.0987,  0.0892,  0.0621,  0.1005,
         0.1003, -0.0598,  0.0851,  0.0558,  0.0756, -0.0384,  0.0594,  0.0065,
         0.0869, -0.0364,  0.0665,  0.0912,  0.0845,  0.0269,  0.0542,  0.0816,
         0.0793,  0.0901,  0.1003,  0.0842, -0.0337,  0.1018,  0.1068,  0.0643,
         0.0345,  0.0956,  0.0856,  0.0785,  0.0905,  0.0667,  0.0415, -0.0261,
         0.0919,  0.1047, -0.0087,  0.0943,  0.1043, -0.0490,  0.0751,  0.0867,
         0.0898,  0.0022,  0.0509,  0.0972,  0.1051,  0.0233,  0.0996,  0.0976,
         0.1058,  0.1046,  0.0977,  0.0476,  0.0974,  0.1031,  0.0757, -0.0500,
         0.0592,  0.0844,  0.1097,  0.0853,  0.0327,  0.0611,  0.0928,  0.1091,
         0.0848,  0.0555,  0.0997, -0.0354,  0.0956,  0.0854,  0.0924,  0.1017,
         0.0625,  0.1005,  0.0833,  0.1075,  0.0650,  0.1076,  0.0893,  0.0441,
         0.0562,  0.0480,  0.0878,  0.0856,  0.0863,  0.0853,  0.0177,  0.0683,
         0.0858,  0.0967,  0.0952,  0.0966,  0.0792,  0.1016, -0.0194,  0.0944,
         0.0866,  0.0846,  0.0698,  0.1059,  0.1013,  0.1023, -0.0119,  0.0899,
         0.1052,  0.1098,  0.1142,  0.1076,  0.1078,  0.1098,  0.0949,  0.1003,
         0.1129,  0.1021,  0.1097,  0.1122,  0.0993,  0.1022,  0.0948,  0.1009,
         0.0999,  0.1032,  0.1021,  0.0973,  0.1036,  0.1070,  0.1056,  0.0992,
         0.1016,  0.1178,  0.1076,  0.1114,  0.1139,  0.0956,  0.0986,  0.1031,
         0.0982,  0.1004,  0.0896,  0.1009,  0.1106,  0.0994,  0.0940,  0.1043,
         0.1050,  0.0992,  0.0963,  0.1027,  0.1090,  0.1192,  0.1065,  0.0996,
         0.1082,  0.1130,  0.0964,  0.0948,  0.0969,  0.0922,  0.1015,  0.1054,
         0.1108,  0.1041,  0.0958,  0.1045,  0.0963,  0.1169,  0.1057,  0.1100,
         0.1118,  0.1017,  0.1109,  0.1050,  0.1061,  0.1074,  0.1050,  0.1003,
         0.0989,  0.1003,  0.1026,  0.0945,  0.0967,  0.0952,  0.1092,  0.0969,
         0.0896,  0.1033,  0.1099,  0.0954,  0.1050,  0.1014,  0.0994,  0.1098,
         0.0954,  0.0873,  0.0948,  0.1100,  0.0974,  0.1032,  0.1010,  0.1034,
         0.1008,  0.0810,  0.0968,  0.1001,  0.1051,  0.1030,  0.0961,  0.1056,
         0.1069,  0.1122,  0.1160,  0.1054,  0.1105,  0.1097,  0.1057,  0.1065,
         0.1025,  0.1018,  0.1007,  0.0982,  0.0971,  0.0977,  0.1112,  0.1205,
         0.1009,  0.0871,  0.0991,  0.1054,  0.1098,  0.0982,  0.1007,  0.1040],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.0276, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:52:59,390 :: INFO :: Epoch 25: loss tensor(96.2305, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0860,  0.0717,  0.1183,  0.0999,  0.0753,  0.0965,  0.0911,  0.0984,
         0.0673,  0.0861,  0.0959,  0.0896,  0.0772,  0.1022,  0.0961,  0.0706,
         0.0677,  0.0844,  0.0761,  0.1077,  0.0894,  0.0731,  0.0993,  0.0995,
         0.0998,  0.0636,  0.0566,  0.0976,  0.1261,  0.0760,  0.1020,  0.0865,
         0.0876,  0.0914,  0.0767,  0.0654,  0.0679,  0.1125,  0.0780,  0.0925,
         0.0844,  0.0751,  0.0948,  0.0896,  0.0646,  0.0751,  0.0733,  0.0872,
         0.1010,  0.0583,  0.0856,  0.0650,  0.0753,  0.0509,  0.1099,  0.1060,
         0.0836,  0.1302,  0.0948,  0.0920,  0.0593,  0.0839,  0.0729,  0.0842,
         0.0823,  0.0606,  0.0869,  0.0857,  0.0961,  0.0834,  0.0769,  0.0829,
         0.0591,  0.1047,  0.0841,  0.0726,  0.0835,  0.0889,  0.0657,  0.0820,
         0.1135,  0.1080,  0.0876,  0.1137,  0.0708,  0.0963,  0.0873,  0.0976,
         0.0704,  0.0496,  0.0936,  0.1154,  0.0792,  0.0732,  0.0975,  0.0937,
         0.0754,  0.0909,  0.0910,  0.0874,  0.0723,  0.0507,  0.0863,  0.0900,
         0.1137,  0.0892,  0.0839,  0.0711,  0.0849,  0.0966,  0.1064,  0.0695,
         0.1089,  0.0874,  0.0899,  0.0711,  0.0786,  0.1086,  0.0472,  0.1028,
         0.0952,  0.0872,  0.0757,  0.0638,  0.0702,  0.0909,  0.0844,  0.1040,
         0.0927,  0.1101,  0.1239,  0.0530,  0.0677,  0.0721,  0.1013,  0.0979,
         0.0797,  0.0232,  0.1025,  0.0941,  0.1183,  0.0946,  0.1110,  0.1295,
         0.1261,  0.1148,  0.0496,  0.1205,  0.1194,  0.1121,  0.0834,  0.1177,
         0.1164, -0.0678,  0.1014,  0.0807,  0.0961, -0.0371,  0.0540,  0.0212,
         0.0915, -0.0349,  0.0696,  0.1098,  0.0895,  0.0240,  0.0713,  0.1016,
         0.1013,  0.1053,  0.1161,  0.0915, -0.0316,  0.1222,  0.1276,  0.0890,
         0.0323,  0.1082,  0.1026,  0.0934,  0.1052,  0.0894,  0.0370, -0.0206,
         0.1019,  0.1219,  0.0008,  0.1063,  0.1214, -0.0528,  0.1022,  0.1071,
         0.0976,  0.0162,  0.0481,  0.1101,  0.1266,  0.0206,  0.1159,  0.1206,
         0.1263,  0.1267,  0.1109,  0.0442,  0.1149,  0.1273,  0.0940, -0.0545,
         0.0554,  0.1029,  0.1257,  0.0887,  0.0297,  0.0785,  0.1060,  0.1327,
         0.1040,  0.0746,  0.1106, -0.0331,  0.1127,  0.1009,  0.1005,  0.1212,
         0.0626,  0.1172,  0.0912,  0.1306,  0.0590,  0.1283,  0.0986,  0.0385,
         0.0581,  0.0430,  0.0925,  0.1115,  0.1082,  0.1046,  0.0161,  0.0702,
         0.1004,  0.1196,  0.1214,  0.1100,  0.1014,  0.1222, -0.0101,  0.1135,
         0.1053,  0.1064,  0.0966,  0.1243,  0.1256,  0.1272,  0.0028,  0.1050,
         0.1360,  0.1389,  0.1468,  0.1288,  0.1334,  0.1405,  0.1210,  0.1264,
         0.1437,  0.1302,  0.1360,  0.1373,  0.1210,  0.1210,  0.1174,  0.1281,
         0.1285,  0.1280,  0.1253,  0.1232,  0.1324,  0.1335,  0.1279,  0.1219,
         0.1262,  0.1500,  0.1306,  0.1420,  0.1468,  0.1112,  0.1256,  0.1292,
         0.1147,  0.1279,  0.1146,  0.1263,  0.1442,  0.1193,  0.1152,  0.1252,
         0.1285,  0.1247,  0.1096,  0.1233,  0.1397,  0.1503,  0.1384,  0.1325,
         0.1369,  0.1447,  0.1126,  0.1194,  0.1184,  0.1200,  0.1297,  0.1365,
         0.1409,  0.1316,  0.1229,  0.1267,  0.1188,  0.1494,  0.1306,  0.1402,
         0.1356,  0.1330,  0.1338,  0.1308,  0.1351,  0.1287,  0.1371,  0.1294,
         0.1227,  0.1251,  0.1271,  0.1192,  0.1241,  0.1204,  0.1391,  0.1132,
         0.1046,  0.1288,  0.1388,  0.1116,  0.1299,  0.1265,  0.1250,  0.1382,
         0.1184,  0.0999,  0.1131,  0.1378,  0.1231,  0.1299,  0.1248,  0.1287,
         0.1269,  0.0960,  0.1234,  0.1232,  0.1368,  0.1273,  0.1170,  0.1311,
         0.1348,  0.1455,  0.1470,  0.1387,  0.1360,  0.1320,  0.1319,  0.1355,
         0.1233,  0.1283,  0.1277,  0.1306,  0.1168,  0.1193,  0.1429,  0.1513,
         0.1200,  0.0948,  0.1193,  0.1364,  0.1321,  0.1251,  0.1290,  0.1252],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.6824, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:53:03,922 :: INFO :: Epoch 30: loss tensor(95.2539, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0883,  0.0700,  0.1368,  0.1031,  0.0761,  0.1049,  0.0958,  0.1023,
         0.0686,  0.0856,  0.1043,  0.0929,  0.0803,  0.1068,  0.0981,  0.0704,
         0.0671,  0.0839,  0.0737,  0.1185,  0.0935,  0.0690,  0.1041,  0.1092,
         0.1067,  0.0627,  0.0523,  0.1055,  0.1423,  0.0753,  0.1085,  0.0855,
         0.0909,  0.0921,  0.0809,  0.0658,  0.0649,  0.1240,  0.0787,  0.0945,
         0.0911,  0.0763,  0.0970,  0.0928,  0.0617,  0.0714,  0.0706,  0.0966,
         0.1062,  0.0565,  0.0825,  0.0626,  0.0774,  0.0480,  0.1223,  0.1125,
         0.0866,  0.1521,  0.1017,  0.0996,  0.0557,  0.0845,  0.0750,  0.0844,
         0.0813,  0.0574,  0.0879,  0.0868,  0.0987,  0.0828,  0.0749,  0.0851,
         0.0541,  0.1192,  0.0824,  0.0709,  0.0824,  0.0945,  0.0625,  0.0807,
         0.1252,  0.1227,  0.0883,  0.1283,  0.0690,  0.0993,  0.0857,  0.1033,
         0.0707,  0.0467,  0.0995,  0.1276,  0.0775,  0.0712,  0.1042,  0.0961,
         0.0722,  0.0905,  0.0969,  0.0889,  0.0721,  0.0467,  0.0870,  0.1015,
         0.1255,  0.0943,  0.0846,  0.0667,  0.0850,  0.0997,  0.1214,  0.0690,
         0.1176,  0.0895,  0.0947,  0.0699,  0.0772,  0.1243,  0.0443,  0.1106,
         0.0953,  0.0890,  0.0739,  0.0620,  0.0694,  0.0976,  0.0906,  0.1107,
         0.0957,  0.1201,  0.1399,  0.0475,  0.0621,  0.0721,  0.1102,  0.1133,
         0.1013,  0.0222,  0.1108,  0.1085,  0.1260,  0.1104,  0.1211,  0.1541,
         0.1431,  0.1228,  0.0444,  0.1441,  0.1342,  0.1290,  0.1002,  0.1289,
         0.1255, -0.0740,  0.1108,  0.1018,  0.1121, -0.0342,  0.0511,  0.0354,
         0.0904, -0.0304,  0.0696,  0.1228,  0.0910,  0.0215,  0.0832,  0.1164,
         0.1180,  0.1141,  0.1262,  0.0927, -0.0277,  0.1374,  0.1433,  0.1092,
         0.0290,  0.1135,  0.1127,  0.1040,  0.1130,  0.1067,  0.0326, -0.0142,
         0.1048,  0.1324,  0.0097,  0.1091,  0.1324, -0.0547,  0.1257,  0.1218,
         0.0991,  0.0283,  0.0414,  0.1154,  0.1419,  0.0200,  0.1260,  0.1383,
         0.1417,  0.1431,  0.1175,  0.0396,  0.1246,  0.1460,  0.1072, -0.0565,
         0.0489,  0.1156,  0.1326,  0.0863,  0.0256,  0.0902,  0.1110,  0.1520,
         0.1171,  0.0891,  0.1138, -0.0295,  0.1225,  0.1095,  0.1016,  0.1350,
         0.0596,  0.1265,  0.0933,  0.1487,  0.0534,  0.1428,  0.0997,  0.0335,
         0.0550,  0.0377,  0.0911,  0.1319,  0.1245,  0.1194,  0.0147,  0.0701,
         0.1076,  0.1375,  0.1440,  0.1184,  0.1189,  0.1384, -0.0004,  0.1263,
         0.1187,  0.1240,  0.1192,  0.1379,  0.1449,  0.1478,  0.0170,  0.1122,
         0.1649,  0.1658,  0.1772,  0.1444,  0.1566,  0.1684,  0.1432,  0.1487,
         0.1729,  0.1560,  0.1599,  0.1593,  0.1375,  0.1342,  0.1362,  0.1518,
         0.1546,  0.1503,  0.1436,  0.1463,  0.1589,  0.1569,  0.1460,  0.1399,
         0.1465,  0.1798,  0.1495,  0.1712,  0.1773,  0.1190,  0.1490,  0.1520,
         0.1258,  0.1523,  0.1371,  0.1482,  0.1761,  0.1343,  0.1313,  0.1426,
         0.1481,  0.1479,  0.1155,  0.1390,  0.1685,  0.1791,  0.1690,  0.1629,
         0.1640,  0.1745,  0.1208,  0.1410,  0.1360,  0.1454,  0.1550,  0.1655,
         0.1687,  0.1568,  0.1479,  0.1438,  0.1377,  0.1801,  0.1516,  0.1681,
         0.1548,  0.1623,  0.1519,  0.1536,  0.1622,  0.1455,  0.1672,  0.1565,
         0.1421,  0.1461,  0.1474,  0.1406,  0.1481,  0.1414,  0.1664,  0.1223,
         0.1134,  0.1505,  0.1655,  0.1203,  0.1507,  0.1475,  0.1477,  0.1639,
         0.1382,  0.1074,  0.1265,  0.1632,  0.1458,  0.1534,  0.1438,  0.1513,
         0.1492,  0.1048,  0.1470,  0.1412,  0.1662,  0.1480,  0.1339,  0.1530,
         0.1603,  0.1772,  0.1759,  0.1702,  0.1585,  0.1494,  0.1550,  0.1622,
         0.1393,  0.1514,  0.1512,  0.1608,  0.1295,  0.1356,  0.1725,  0.1800,
         0.1331,  0.0943,  0.1339,  0.1656,  0.1501,  0.1486,  0.1543,  0.1415],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.0838, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:53:08,391 :: INFO :: Epoch 35: loss tensor(92.6558, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0905,  0.0689,  0.1531,  0.1034,  0.0746,  0.1099,  0.0993,  0.1031,
         0.0703,  0.0853,  0.1095,  0.0919,  0.0808,  0.1099,  0.0998,  0.0676,
         0.0654,  0.0842,  0.0731,  0.1257,  0.0961,  0.0675,  0.1066,  0.1162,
         0.1113,  0.0607,  0.0497,  0.1093,  0.1557,  0.0742,  0.1115,  0.0838,
         0.0938,  0.0932,  0.0826,  0.0675,  0.0641,  0.1293,  0.0776,  0.0966,
         0.0950,  0.0764,  0.0978,  0.0939,  0.0593,  0.0687,  0.0683,  0.1021,
         0.1091,  0.0549,  0.0798,  0.0589,  0.0780,  0.0464,  0.1306,  0.1151,
         0.0887,  0.1713,  0.1065,  0.1059,  0.0533,  0.0836,  0.0750,  0.0843,
         0.0817,  0.0563,  0.0879,  0.0865,  0.0983,  0.0829,  0.0756,  0.0866,
         0.0513,  0.1297,  0.0799,  0.0700,  0.0831,  0.0978,  0.0603,  0.0804,
         0.1322,  0.1346,  0.0870,  0.1388,  0.0674,  0.0998,  0.0837,  0.1072,
         0.0693,  0.0456,  0.1009,  0.1356,  0.0761,  0.0694,  0.1075,  0.0975,
         0.0673,  0.0900,  0.0995,  0.0914,  0.0723,  0.0422,  0.0828,  0.1072,
         0.1339,  0.0983,  0.0835,  0.0646,  0.0857,  0.0996,  0.1340,  0.0673,
         0.1236,  0.0910,  0.0966,  0.0687,  0.0759,  0.1372,  0.0420,  0.1158,
         0.0950,  0.0872,  0.0725,  0.0634,  0.0666,  0.1017,  0.0961,  0.1130,
         0.0962,  0.1241,  0.1502,  0.0416,  0.0574,  0.0700,  0.1124,  0.1227,
         0.1189,  0.0225,  0.1136,  0.1181,  0.1270,  0.1213,  0.1243,  0.1761,
         0.1548,  0.1239,  0.0404,  0.1636,  0.1430,  0.1409,  0.1120,  0.1364,
         0.1279, -0.0793,  0.1149,  0.1198,  0.1229, -0.0306,  0.0464,  0.0490,
         0.0863, -0.0241,  0.0684,  0.1306,  0.0878,  0.0194,  0.0915,  0.1254,
         0.1298,  0.1168,  0.1297,  0.0891, -0.0229,  0.1467,  0.1531,  0.1245,
         0.0271,  0.1118,  0.1171,  0.1114,  0.1144,  0.1195,  0.0285, -0.0078,
         0.1009,  0.1368,  0.0169,  0.1063,  0.1386, -0.0558,  0.1463,  0.1305,
         0.0954,  0.0375,  0.0382,  0.1151,  0.1527,  0.0164,  0.1302,  0.1502,
         0.1513,  0.1544,  0.1190,  0.0352,  0.1269,  0.1605,  0.1153, -0.0571,
         0.0423,  0.1217,  0.1334,  0.0793,  0.0230,  0.0967,  0.1095,  0.1668,
         0.1233,  0.0995,  0.1118, -0.0252,  0.1280,  0.1124,  0.1014,  0.1450,
         0.0551,  0.1310,  0.0903,  0.1619,  0.0508,  0.1518,  0.0945,  0.0294,
         0.0556,  0.0352,  0.0847,  0.1471,  0.1356,  0.1284,  0.0152,  0.0659,
         0.1076,  0.1502,  0.1621,  0.1220,  0.1313,  0.1484,  0.0087,  0.1330,
         0.1260,  0.1364,  0.1367,  0.1454,  0.1593,  0.1638,  0.0300,  0.1126,
         0.1923,  0.1904,  0.2059,  0.1554,  0.1774,  0.1942,  0.1618,  0.1676,
         0.2000,  0.1791,  0.1816,  0.1790,  0.1501,  0.1421,  0.1511,  0.1729,
         0.1783,  0.1697,  0.1572,  0.1656,  0.1835,  0.1774,  0.1606,  0.1531,
         0.1631,  0.2069,  0.1642,  0.1986,  0.2066,  0.1211,  0.1691,  0.1713,
         0.1332,  0.1738,  0.1568,  0.1667,  0.2060,  0.1452,  0.1422,  0.1575,
         0.1636,  0.1690,  0.1155,  0.1505,  0.1958,  0.2051,  0.1979,  0.1914,
         0.1886,  0.2027,  0.1224,  0.1611,  0.1491,  0.1685,  0.1783,  0.1926,
         0.1942,  0.1795,  0.1708,  0.1565,  0.1540,  0.2092,  0.1690,  0.1940,
         0.1705,  0.1895,  0.1656,  0.1736,  0.1872,  0.1584,  0.1959,  0.1815,
         0.1577,  0.1633,  0.1634,  0.1580,  0.1689,  0.1583,  0.1916,  0.1246,
         0.1186,  0.1687,  0.1897,  0.1225,  0.1677,  0.1647,  0.1666,  0.1870,
         0.1542,  0.1121,  0.1347,  0.1861,  0.1665,  0.1741,  0.1586,  0.1716,
         0.1679,  0.1103,  0.1672,  0.1544,  0.1939,  0.1672,  0.1470,  0.1716,
         0.1844,  0.2071,  0.2025,  0.1997,  0.1792,  0.1627,  0.1753,  0.1868,
         0.1516,  0.1711,  0.1717,  0.1893,  0.1371,  0.1485,  0.1999,  0.2069,
         0.1401,  0.0915,  0.1425,  0.1931,  0.1640,  0.1693,  0.1772,  0.1539],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.2739, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:53:12,892 :: INFO :: Epoch 40: loss tensor(92.7643, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0891,  0.0672,  0.1649,  0.1043,  0.0703,  0.1137,  0.1013,  0.1009,
         0.0711,  0.0820,  0.1143,  0.0880,  0.0772,  0.1103,  0.0995,  0.0647,
         0.0639,  0.0847,  0.0718,  0.1310,  0.0958,  0.0657,  0.1058,  0.1206,
         0.1117,  0.0569,  0.0464,  0.1114,  0.1663,  0.0718,  0.1098,  0.0820,
         0.0965,  0.0936,  0.0815,  0.0669,  0.0599,  0.1301,  0.0752,  0.0979,
         0.0952,  0.0756,  0.0983,  0.0905,  0.0557,  0.0669,  0.0649,  0.1036,
         0.1129,  0.0529,  0.0772,  0.0547,  0.0760,  0.0439,  0.1388,  0.1159,
         0.0894,  0.1902,  0.1101,  0.1079,  0.0492,  0.0830,  0.0746,  0.0823,
         0.0807,  0.0525,  0.0873,  0.0849,  0.0946,  0.0816,  0.0765,  0.0872,
         0.0490,  0.1361,  0.0760,  0.0697,  0.0823,  0.1013,  0.0594,  0.0808,
         0.1363,  0.1441,  0.0822,  0.1465,  0.0669,  0.1004,  0.0785,  0.1097,
         0.0665,  0.0444,  0.0995,  0.1408,  0.0727,  0.0679,  0.1098,  0.0989,
         0.0620,  0.0902,  0.1017,  0.0931,  0.0718,  0.0379,  0.0781,  0.1102,
         0.1407,  0.1017,  0.0794,  0.0617,  0.0858,  0.0960,  0.1459,  0.0665,
         0.1283,  0.0925,  0.0971,  0.0657,  0.0736,  0.1466,  0.0406,  0.1198,
         0.0946,  0.0841,  0.0721,  0.0654,  0.0632,  0.1024,  0.0984,  0.1136,
         0.0947,  0.1232,  0.1538,  0.0354,  0.0518,  0.0668,  0.1093,  0.1250,
         0.1308,  0.0203,  0.1117,  0.1195,  0.1214,  0.1248,  0.1203,  0.1947,
         0.1603,  0.1196,  0.0327,  0.1788,  0.1441,  0.1457,  0.1161,  0.1388,
         0.1231, -0.0842,  0.1118,  0.1316,  0.1272, -0.0289,  0.0418,  0.0585,
         0.0806, -0.0179,  0.0656,  0.1307,  0.0838,  0.0177,  0.0924,  0.1277,
         0.1333,  0.1150,  0.1279,  0.0840, -0.0199,  0.1498,  0.1560,  0.1331,
         0.0240,  0.1046,  0.1133,  0.1146,  0.1103,  0.1252,  0.0244, -0.0048,
         0.0930,  0.1354,  0.0190,  0.0976,  0.1392, -0.0574,  0.1640,  0.1313,
         0.0892,  0.0409,  0.0349,  0.1105,  0.1584,  0.0152,  0.1295,  0.1550,
         0.1543,  0.1590,  0.1157,  0.0299,  0.1210,  0.1692,  0.1151, -0.0577,
         0.0362,  0.1213,  0.1269,  0.0704,  0.0182,  0.0952,  0.1028,  0.1757,
         0.1219,  0.1023,  0.1049, -0.0230,  0.1293,  0.1104,  0.0995,  0.1515,
         0.0494,  0.1304,  0.0851,  0.1697,  0.0413,  0.1531,  0.0841,  0.0239,
         0.0532,  0.0292,  0.0771,  0.1562,  0.1389,  0.1312,  0.0133,  0.0611,
         0.1023,  0.1557,  0.1749,  0.1226,  0.1366,  0.1524,  0.0139,  0.1320,
         0.1263,  0.1420,  0.1508,  0.1472,  0.1677,  0.1739,  0.0393,  0.1064,
         0.2175,  0.2121,  0.2319,  0.1637,  0.1953,  0.2172,  0.1779,  0.1837,
         0.2261,  0.1999,  0.2015,  0.1964,  0.1596,  0.1464,  0.1628,  0.1915,
         0.2006,  0.1862,  0.1669,  0.1838,  0.2053,  0.1967,  0.1727,  0.1623,
         0.1769,  0.2311,  0.1752,  0.2232,  0.2339,  0.1205,  0.1864,  0.1881,
         0.1397,  0.1930,  0.1754,  0.1823,  0.2343,  0.1539,  0.1493,  0.1702,
         0.1754,  0.1883,  0.1113,  0.1582,  0.2207,  0.2294,  0.2254,  0.2174,
         0.2108,  0.2290,  0.1211,  0.1793,  0.1583,  0.1899,  0.1995,  0.2171,
         0.2185,  0.2008,  0.1927,  0.1653,  0.1677,  0.2355,  0.1832,  0.2177,
         0.1832,  0.2157,  0.1760,  0.1912,  0.2100,  0.1684,  0.2221,  0.2049,
         0.1701,  0.1782,  0.1764,  0.1724,  0.1871,  0.1723,  0.2156,  0.1235,
         0.1218,  0.1840,  0.2118,  0.1207,  0.1813,  0.1789,  0.1839,  0.2089,
         0.1673,  0.1184,  0.1423,  0.2074,  0.1844,  0.1923,  0.1703,  0.1896,
         0.1839,  0.1149,  0.1862,  0.1638,  0.2188,  0.1843,  0.1583,  0.1874,
         0.2066,  0.2346,  0.2266,  0.2280,  0.1978,  0.1730,  0.1932,  0.2093,
         0.1602,  0.1883,  0.1897,  0.2146,  0.1407,  0.1579,  0.2250,  0.2305,
         0.1431,  0.0880,  0.1466,  0.2185,  0.1741,  0.1872,  0.1980,  0.1637],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.2731, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:53:12,892 :: INFO :: ----- frontend -----
2023-05-14 11:53:12,892 :: INFO :: Environment 0
2023-05-14 11:53:22,299 :: INFO :: Epoch 5: loss tensor(819.6264, device='cuda:0'), U.norm 13.157378196716309, V.norm 16.900407791137695, MLP.norm 2.1528615951538086
2023-05-14 11:53:22,533 :: INFO :: Epoch 10: loss tensor(793.3817, device='cuda:0'), U.norm 10.346855163574219, V.norm 16.203771591186523, MLP.norm 3.645454168319702
2023-05-14 11:53:22,768 :: INFO :: Epoch 15: loss tensor(757.7604, device='cuda:0'), U.norm 8.645944595336914, V.norm 15.863945960998535, MLP.norm 5.39034366607666
2023-05-14 11:53:23,033 :: INFO :: Epoch 20: loss tensor(719.0093, device='cuda:0'), U.norm 7.485395431518555, V.norm 15.634498596191406, MLP.norm 7.195570468902588
2023-05-14 11:53:23,033 :: INFO :: Environment 1
2023-05-14 11:53:32,362 :: INFO :: Epoch 5: loss tensor(844.2407, device='cuda:0'), U.norm 13.15710735321045, V.norm 16.94295310974121, MLP.norm 2.1504149436950684
2023-05-14 11:53:32,597 :: INFO :: Epoch 10: loss tensor(817.5204, device='cuda:0'), U.norm 10.346294403076172, V.norm 16.272541046142578, MLP.norm 3.672517776489258
2023-05-14 11:53:32,815 :: INFO :: Epoch 15: loss tensor(780.8675, device='cuda:0'), U.norm 8.645423889160156, V.norm 15.949427604675293, MLP.norm 5.46820068359375
2023-05-14 11:53:33,050 :: INFO :: Epoch 20: loss tensor(742.0845, device='cuda:0'), U.norm 7.484918117523193, V.norm 15.73427677154541, MLP.norm 7.334491729736328
2023-05-14 11:53:33,065 :: INFO :: Environment 2
2023-05-14 11:53:42,223 :: INFO :: Epoch 5: loss tensor(820.2214, device='cuda:0'), U.norm 13.153295516967773, V.norm 16.908416748046875, MLP.norm 2.149163246154785
2023-05-14 11:53:42,442 :: INFO :: Epoch 10: loss tensor(795.0360, device='cuda:0'), U.norm 10.339111328125, V.norm 16.225685119628906, MLP.norm 3.6108460426330566
2023-05-14 11:53:42,676 :: INFO :: Epoch 15: loss tensor(761.1776, device='cuda:0'), U.norm 8.634183883666992, V.norm 15.899477005004883, MLP.norm 5.3226470947265625
2023-05-14 11:53:42,910 :: INFO :: Epoch 20: loss tensor(724.5779, device='cuda:0'), U.norm 7.469303131103516, V.norm 15.6834716796875, MLP.norm 7.100047588348389
2023-05-14 11:53:42,910 :: INFO :: Environment 3
2023-05-14 11:53:52,099 :: INFO :: Epoch 5: loss tensor(826.3180, device='cuda:0'), U.norm 13.152653694152832, V.norm 16.902101516723633, MLP.norm 2.1615734100341797
2023-05-14 11:53:52,317 :: INFO :: Epoch 10: loss tensor(800.6137, device='cuda:0'), U.norm 10.337213516235352, V.norm 16.20921516418457, MLP.norm 3.6317238807678223
2023-05-14 11:53:52,568 :: INFO :: Epoch 15: loss tensor(766.5892, device='cuda:0'), U.norm 8.630749702453613, V.norm 15.871315002441406, MLP.norm 5.342294216156006
2023-05-14 11:53:52,787 :: INFO :: Epoch 20: loss tensor(728.8770, device='cuda:0'), U.norm 7.464149475097656, V.norm 15.646202087402344, MLP.norm 7.124120235443115
2023-05-14 11:53:52,802 :: INFO :: Environment 4
2023-05-14 11:54:02,131 :: INFO :: Epoch 5: loss tensor(818.0364, device='cuda:0'), U.norm 13.157976150512695, V.norm 16.87990951538086, MLP.norm 2.139059066772461
2023-05-14 11:54:02,366 :: INFO :: Epoch 10: loss tensor(793.7701, device='cuda:0'), U.norm 10.3471040725708, V.norm 16.179473876953125, MLP.norm 3.60837984085083
2023-05-14 11:54:02,585 :: INFO :: Epoch 15: loss tensor(760.0176, device='cuda:0'), U.norm 8.645429611206055, V.norm 15.841654777526855, MLP.norm 5.3425517082214355
2023-05-14 11:54:02,819 :: INFO :: Epoch 20: loss tensor(723.2702, device='cuda:0'), U.norm 7.483734607696533, V.norm 15.61652660369873, MLP.norm 7.147558689117432
2023-05-14 11:54:02,835 :: INFO :: Environment 5
2023-05-14 11:54:12,007 :: INFO :: Epoch 5: loss tensor(874.1997, device='cuda:0'), U.norm 13.153634071350098, V.norm 16.997554779052734, MLP.norm 2.166262149810791
2023-05-14 11:54:12,226 :: INFO :: Epoch 10: loss tensor(844.0916, device='cuda:0'), U.norm 10.342141151428223, V.norm 16.345706939697266, MLP.norm 3.7132580280303955
2023-05-14 11:54:12,461 :: INFO :: Epoch 15: loss tensor(804.0648, device='cuda:0'), U.norm 8.640814781188965, V.norm 16.02611541748047, MLP.norm 5.50089693069458
2023-05-14 11:54:12,679 :: INFO :: Epoch 20: loss tensor(760.0397, device='cuda:0'), U.norm 7.480120658874512, V.norm 15.811235427856445, MLP.norm 7.343900203704834
2023-05-14 11:54:12,695 :: INFO :: Environment 6
2023-05-14 11:54:21,837 :: INFO :: Epoch 5: loss tensor(868.5411, device='cuda:0'), U.norm 13.15440559387207, V.norm 16.986692428588867, MLP.norm 2.1787397861480713
2023-05-14 11:54:22,071 :: INFO :: Epoch 10: loss tensor(840.4733, device='cuda:0'), U.norm 10.341808319091797, V.norm 16.32988929748535, MLP.norm 3.70973539352417
2023-05-14 11:54:22,290 :: INFO :: Epoch 15: loss tensor(803.5237, device='cuda:0'), U.norm 8.638718605041504, V.norm 16.011354446411133, MLP.norm 5.491358280181885
2023-05-14 11:54:22,524 :: INFO :: Epoch 20: loss tensor(764.4689, device='cuda:0'), U.norm 7.4752960205078125, V.norm 15.797942161560059, MLP.norm 7.297282695770264
2023-05-14 11:54:22,540 :: INFO :: Environment 7
2023-05-14 11:54:31,682 :: INFO :: Epoch 5: loss tensor(888.1865, device='cuda:0'), U.norm 13.158488273620605, V.norm 17.020158767700195, MLP.norm 2.187605619430542
2023-05-14 11:54:31,916 :: INFO :: Epoch 10: loss tensor(858.5439, device='cuda:0'), U.norm 10.349748611450195, V.norm 16.37565040588379, MLP.norm 3.7753281593322754
2023-05-14 11:54:32,150 :: INFO :: Epoch 15: loss tensor(819.1268, device='cuda:0'), U.norm 8.651154518127441, V.norm 16.059999465942383, MLP.norm 5.604085445404053
2023-05-14 11:54:32,369 :: INFO :: Epoch 20: loss tensor(776.7793, device='cuda:0'), U.norm 7.493258476257324, V.norm 15.84745979309082, MLP.norm 7.492385387420654
2023-05-14 11:54:32,384 :: INFO :: Environment 8
2023-05-14 11:54:41,573 :: INFO :: Epoch 5: loss tensor(861.6096, device='cuda:0'), U.norm 13.157584190368652, V.norm 16.971467971801758, MLP.norm 2.150035858154297
2023-05-14 11:54:41,808 :: INFO :: Epoch 10: loss tensor(836.2471, device='cuda:0'), U.norm 10.347319602966309, V.norm 16.31495475769043, MLP.norm 3.6554343700408936
2023-05-14 11:54:42,027 :: INFO :: Epoch 15: loss tensor(801.5457, device='cuda:0'), U.norm 8.646941184997559, V.norm 15.999195098876953, MLP.norm 5.439456462860107
2023-05-14 11:54:42,261 :: INFO :: Epoch 20: loss tensor(763.7692, device='cuda:0'), U.norm 7.4871745109558105, V.norm 15.789463996887207, MLP.norm 7.3015313148498535
2023-05-14 11:54:42,277 :: INFO :: Environment 9
2023-05-14 11:54:51,418 :: INFO :: Epoch 5: loss tensor(849.7642, device='cuda:0'), U.norm 13.157105445861816, V.norm 16.948396682739258, MLP.norm 2.171873092651367
2023-05-14 11:54:51,652 :: INFO :: Epoch 10: loss tensor(823.2672, device='cuda:0'), U.norm 10.34717082977295, V.norm 16.275964736938477, MLP.norm 3.699317693710327
2023-05-14 11:54:51,887 :: INFO :: Epoch 15: loss tensor(788.1033, device='cuda:0'), U.norm 8.647194862365723, V.norm 15.948731422424316, MLP.norm 5.487752437591553
2023-05-14 11:54:52,106 :: INFO :: Epoch 20: loss tensor(749.6849, device='cuda:0'), U.norm 7.487585067749023, V.norm 15.729522705078125, MLP.norm 7.35638952255249
2023-05-14 11:54:52,121 :: INFO :: Ite = 1, Delta = 4047
2023-05-14 11:54:52,121 :: INFO :: ----- backend -----
2023-05-14 11:54:56,731 :: INFO :: Epoch 5: loss tensor(149.1976, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 9.4141e-03,  1.1457e-02,  1.4719e-02,  1.4832e-02,  8.8199e-03,
         1.4962e-03,  9.1483e-03,  1.7170e-02,  5.4448e-03,  1.7491e-02,
         1.4847e-03,  1.3927e-02,  8.0234e-03,  1.7552e-02,  2.3785e-02,
         1.8932e-02,  1.8143e-02,  1.8724e-02,  1.8997e-02,  1.2256e-02,
         1.1492e-02,  1.9468e-02,  2.1202e-02,  3.2711e-03,  1.8625e-02,
         1.0679e-02,  1.1223e-02,  8.8912e-03,  2.4697e-02,  1.4642e-02,
         1.9619e-02,  2.4130e-02,  1.1246e-02,  1.8624e-02,  1.6934e-03,
        -3.2580e-03,  1.9711e-02,  1.8160e-02,  4.7779e-03,  2.0265e-02,
         1.0334e-02,  1.8711e-03,  2.5427e-02,  1.6482e-02,  1.1313e-02,
         1.8662e-02,  1.4319e-02,  2.6537e-03,  1.4260e-02,  1.4260e-02,
         2.3569e-02,  1.0934e-02,  1.3632e-02,  1.4416e-02,  1.1297e-02,
         2.0072e-02,  8.8793e-03,  1.7987e-02,  2.1194e-03,  6.1916e-04,
         1.7566e-02,  1.8971e-02, -2.1495e-04,  1.4259e-02,  1.6093e-02,
         4.4990e-03,  2.5182e-02,  1.1236e-02,  2.5013e-02,  6.9574e-03,
         2.1762e-02,  4.9982e-03,  2.2878e-02,  5.0461e-03,  1.8784e-02,
         6.1173e-03,  2.0885e-02,  6.3988e-04,  1.2977e-02,  1.8600e-02,
         1.7296e-02,  4.0561e-03,  2.0886e-02,  1.1879e-02,  1.7732e-03,
         1.6795e-02,  2.4518e-02,  1.3435e-02,  2.1329e-03, -2.9030e-03,
         1.0489e-02,  1.7233e-02,  1.4147e-02,  1.8074e-02,  4.0508e-03,
         1.8695e-02,  2.4478e-02,  1.4502e-02,  4.0624e-03,  1.1929e-02,
         4.1272e-03,  1.3208e-02,  1.6751e-02, -2.2196e-03,  8.4588e-03,
        -8.2191e-03,  1.7727e-02,  1.6654e-02,  1.7671e-02,  1.8696e-02,
         6.3534e-03,  1.6131e-02,  2.1725e-02,  1.8407e-02,  2.9465e-03,
         4.8271e-03,  1.3017e-02,  1.3105e-02,  1.2691e-02,  7.2742e-03,
         1.6918e-02,  1.4713e-02,  8.5647e-03,  7.6752e-03,  3.1853e-03,
         1.6292e-02,  3.9286e-03,  1.6107e-02,  2.0072e-02,  1.8387e-02,
         2.2817e-02,  2.4755e-02,  2.3112e-02,  1.9659e-02,  7.6120e-03,
        -1.3615e-03, -3.8860e-03,  1.4345e-02,  1.0741e-02, -6.0080e-03,
         2.5098e-02, -1.0386e-04,  1.1937e-02,  1.3149e-02,  2.1365e-02,
         2.5465e-02,  1.4671e-02,  1.3755e-02,  2.2477e-02,  1.3560e-02,
        -1.5527e-03,  1.9573e-02,  2.5257e-02, -2.0362e-02,  3.1794e-03,
        -2.5938e-03, -1.5407e-04, -2.0117e-02,  2.4706e-02, -1.8551e-02,
         2.5557e-02, -1.8832e-02,  2.3175e-02,  1.3810e-02,  2.5143e-02,
         1.5806e-02, -1.1544e-02,  1.8785e-04,  8.2158e-04,  1.2030e-02,
         2.1262e-02,  1.9256e-02, -1.9952e-02,  1.9532e-02,  2.5757e-02,
         4.0188e-03,  1.9033e-02,  2.1040e-02,  5.5044e-03,  1.5621e-03,
         1.4718e-02,  2.9841e-03,  2.3643e-02, -1.9619e-02,  2.2162e-02,
         2.2584e-02, -1.7417e-02,  1.6434e-02,  2.4818e-02, -2.0454e-02,
        -9.0221e-03,  8.3762e-03,  2.5253e-02, -1.6522e-02,  1.6675e-02,
         2.1059e-02,  1.7563e-02,  1.5691e-02,  2.1866e-02,  1.3354e-02,
         2.5398e-02,  1.6033e-02,  2.1853e-02,  2.2297e-02,  1.8770e-02,
         1.5734e-02, -3.4401e-03, -1.9889e-02,  2.2267e-02,  3.1224e-03,
         2.5773e-02,  2.5114e-02,  1.6061e-02, -5.3102e-03,  1.8383e-02,
         2.1213e-02,  5.1700e-03, -1.3058e-02,  2.5050e-02, -1.9563e-02,
         1.6947e-02,  4.0769e-03,  2.5113e-02,  2.1446e-02,  2.0773e-02,
         1.6955e-02,  1.3519e-02,  1.7977e-02,  1.5438e-02,  2.5630e-02,
         2.1956e-02,  2.0450e-02,  2.3228e-02,  2.0002e-02,  2.4803e-02,
         1.0019e-02,  3.3078e-03,  5.9738e-03,  1.1617e-02,  2.0175e-02,
         9.2308e-03,  1.3197e-02,  1.1826e-02,  2.4960e-02,  5.1960e-03,
         2.0757e-02, -1.8729e-02,  1.3794e-02,  9.5078e-03,  6.3338e-03,
         6.4263e-06,  2.5226e-02,  1.2327e-02,  2.0129e-02, -1.6644e-02,
         1.2913e-02,  1.4347e-02,  1.3318e-02,  1.2139e-02,  2.5697e-02,
         2.5349e-02,  1.5271e-02,  9.4170e-03,  1.2406e-02,  1.7718e-02,
         1.4643e-02,  1.8485e-02,  2.0348e-02,  1.5557e-02,  2.5361e-02,
         1.0404e-02,  1.2531e-02,  1.3797e-02,  1.4641e-02,  1.8170e-02,
         1.2273e-02,  1.4271e-02,  2.2017e-02,  2.5454e-02,  1.4874e-02,
         1.7248e-02,  1.7215e-02,  2.3583e-02,  7.3687e-03,  1.4264e-02,
         2.1135e-02,  1.1432e-02,  1.7474e-02,  2.5591e-02,  1.1761e-02,
         1.0781e-02,  1.5395e-02,  1.1859e-02,  2.0004e-02,  9.8960e-03,
         2.6099e-02,  1.9902e-02,  1.0419e-02,  2.2054e-02,  2.5319e-02,
         1.2248e-02,  2.5733e-02,  6.6589e-03, -1.1339e-04,  2.2541e-02,
         1.3247e-02,  2.0461e-02,  1.0719e-02,  1.6315e-02,  3.9830e-03,
         1.1336e-02,  1.4825e-02,  1.8964e-02,  1.7410e-02,  8.7657e-03,
         2.0743e-02,  1.5758e-02,  1.6403e-02,  2.5154e-02,  1.2326e-02,
         2.6289e-02,  1.7517e-03,  2.6298e-02,  1.5522e-02,  1.5793e-02,
         2.5580e-02,  5.3248e-03,  1.0762e-02,  1.3362e-02,  1.5189e-02,
         2.1623e-02,  6.4311e-03,  1.0865e-02,  9.2176e-03,  1.6288e-02,
         1.9560e-02,  1.4942e-02,  1.7397e-02,  2.0607e-02,  1.7105e-02,
         1.9051e-02,  1.4952e-02,  1.1031e-02,  1.6864e-02,  1.2415e-02,
         1.9451e-02,  2.1458e-02,  1.4745e-02,  1.0241e-02,  2.0197e-02,
         1.5480e-02,  1.3082e-02,  1.4526e-02,  8.6613e-03,  1.3404e-02,
         1.7191e-02,  5.2469e-03,  2.2138e-02,  1.7157e-02,  1.8044e-02,
         1.5594e-02,  7.0230e-03,  2.0608e-02,  4.6254e-03,  2.4933e-02,
         2.6198e-02,  2.0048e-02,  1.4385e-02,  2.2338e-02,  1.6063e-02,
         1.6555e-02, -4.1128e-03,  1.7408e-02,  1.5734e-02,  1.3958e-02,
         2.2549e-02,  2.0810e-02,  2.0703e-02,  1.8810e-02,  4.0346e-03,
         2.6367e-02,  1.1809e-02,  1.4630e-02,  2.1239e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.5402, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:01,310 :: INFO :: Epoch 10: loss tensor(146.9503, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0386,  0.0393,  0.0447,  0.0436,  0.0368,  0.0309,  0.0392,  0.0433,
         0.0336,  0.0448,  0.0312,  0.0418,  0.0351,  0.0465,  0.0517,  0.0430,
         0.0439,  0.0465,  0.0440,  0.0421,  0.0386,  0.0463,  0.0499,  0.0337,
         0.0470,  0.0364,  0.0347,  0.0385,  0.0558,  0.0400,  0.0478,  0.0505,
         0.0411,  0.0471,  0.0304,  0.0231,  0.0461,  0.0469,  0.0326,  0.0501,
         0.0387,  0.0290,  0.0545,  0.0431,  0.0381,  0.0459,  0.0403,  0.0317,
         0.0438,  0.0398,  0.0519,  0.0386,  0.0378,  0.0379,  0.0421,  0.0490,
         0.0369,  0.0485,  0.0324,  0.0305,  0.0432,  0.0431,  0.0280,  0.0431,
         0.0444,  0.0306,  0.0535,  0.0403,  0.0513,  0.0353,  0.0471,  0.0345,
         0.0444,  0.0335,  0.0463,  0.0346,  0.0484,  0.0291,  0.0402,  0.0435,
         0.0467,  0.0351,  0.0474,  0.0423,  0.0306,  0.0466,  0.0502,  0.0436,
         0.0297,  0.0219,  0.0388,  0.0458,  0.0414,  0.0458,  0.0349,  0.0484,
         0.0514,  0.0441,  0.0321,  0.0392,  0.0332,  0.0373,  0.0443,  0.0239,
         0.0392,  0.0161,  0.0444,  0.0431,  0.0435,  0.0458,  0.0363,  0.0427,
         0.0526,  0.0473,  0.0315,  0.0315,  0.0412,  0.0421,  0.0338,  0.0363,
         0.0453,  0.0426,  0.0364,  0.0348,  0.0303,  0.0404,  0.0328,  0.0453,
         0.0477,  0.0459,  0.0513,  0.0461,  0.0495,  0.0448,  0.0359,  0.0264,
         0.0249,  0.0184,  0.0399,  0.0190,  0.0530,  0.0279,  0.0414,  0.0436,
         0.0509,  0.0519,  0.0371,  0.0449,  0.0515,  0.0445,  0.0262,  0.0497,
         0.0530, -0.0353,  0.0312,  0.0264,  0.0275, -0.0326,  0.0454, -0.0290,
         0.0519, -0.0329,  0.0471,  0.0427,  0.0503,  0.0280,  0.0051,  0.0282,
         0.0294,  0.0405,  0.0507,  0.0455, -0.0329,  0.0479,  0.0551,  0.0338,
         0.0287,  0.0488,  0.0340,  0.0266,  0.0431,  0.0327,  0.0434, -0.0295,
         0.0487,  0.0521, -0.0196,  0.0460,  0.0554, -0.0349,  0.0133,  0.0374,
         0.0508, -0.0119,  0.0405,  0.0491,  0.0486,  0.0209,  0.0500,  0.0432,
         0.0536,  0.0470,  0.0486,  0.0393,  0.0470,  0.0471,  0.0224, -0.0357,
         0.0454,  0.0318,  0.0549,  0.0499,  0.0308,  0.0199,  0.0466,  0.0518,
         0.0338,  0.0030,  0.0519, -0.0307,  0.0469,  0.0335,  0.0536,  0.0502,
         0.0460,  0.0473,  0.0407,  0.0489,  0.0405,  0.0544,  0.0486,  0.0376,
         0.0422,  0.0387,  0.0492,  0.0411,  0.0326,  0.0343,  0.0191,  0.0445,
         0.0379,  0.0427,  0.0422,  0.0510,  0.0344,  0.0493, -0.0227,  0.0425,
         0.0381,  0.0353,  0.0291,  0.0528,  0.0428,  0.0502, -0.0080,  0.0415,
         0.0459,  0.0450,  0.0443,  0.0562,  0.0559,  0.0466,  0.0409,  0.0438,
         0.0492,  0.0454,  0.0498,  0.0515,  0.0466,  0.0559,  0.0410,  0.0441,
         0.0447,  0.0459,  0.0492,  0.0437,  0.0454,  0.0530,  0.0560,  0.0461,
         0.0484,  0.0494,  0.0543,  0.0392,  0.0463,  0.0514,  0.0429,  0.0486,
         0.0557,  0.0432,  0.0415,  0.0468,  0.0440,  0.0502,  0.0407,  0.0560,
         0.0510,  0.0407,  0.0525,  0.0560,  0.0442,  0.0574,  0.0381,  0.0311,
         0.0533,  0.0450,  0.0506,  0.0416,  0.0474,  0.0341,  0.0430,  0.0463,
         0.0503,  0.0482,  0.0395,  0.0515,  0.0461,  0.0486,  0.0561,  0.0439,
         0.0570,  0.0320,  0.0570,  0.0463,  0.0470,  0.0557,  0.0373,  0.0421,
         0.0444,  0.0464,  0.0527,  0.0372,  0.0424,  0.0407,  0.0477,  0.0500,
         0.0450,  0.0485,  0.0516,  0.0476,  0.0501,  0.0463,  0.0421,  0.0477,
         0.0435,  0.0490,  0.0516,  0.0459,  0.0412,  0.0513,  0.0469,  0.0445,
         0.0460,  0.0384,  0.0444,  0.0482,  0.0373,  0.0523,  0.0481,  0.0491,
         0.0470,  0.0393,  0.0524,  0.0363,  0.0558,  0.0562,  0.0510,  0.0458,
         0.0531,  0.0474,  0.0480,  0.0264,  0.0481,  0.0467,  0.0457,  0.0542,
         0.0511,  0.0496,  0.0497,  0.0355,  0.0568,  0.0433,  0.0462,  0.0519],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0708, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:06,013 :: INFO :: Epoch 15: loss tensor(142.8621, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0633,  0.0600,  0.0732,  0.0678,  0.0598,  0.0593,  0.0645,  0.0654,
         0.0558,  0.0657,  0.0602,  0.0646,  0.0584,  0.0708,  0.0743,  0.0601,
         0.0612,  0.0670,  0.0594,  0.0682,  0.0607,  0.0636,  0.0741,  0.0630,
         0.0708,  0.0530,  0.0498,  0.0644,  0.0854,  0.0583,  0.0722,  0.0706,
         0.0675,  0.0692,  0.0560,  0.0455,  0.0633,  0.0740,  0.0561,  0.0757,
         0.0615,  0.0536,  0.0795,  0.0646,  0.0570,  0.0658,  0.0607,  0.0588,
         0.0697,  0.0558,  0.0744,  0.0581,  0.0561,  0.0510,  0.0710,  0.0737,
         0.0609,  0.0775,  0.0597,  0.0584,  0.0584,  0.0624,  0.0533,  0.0660,
         0.0665,  0.0485,  0.0753,  0.0649,  0.0713,  0.0581,  0.0639,  0.0602,
         0.0578,  0.0616,  0.0674,  0.0556,  0.0706,  0.0533,  0.0593,  0.0637,
         0.0736,  0.0662,  0.0682,  0.0693,  0.0542,  0.0728,  0.0697,  0.0690,
         0.0516,  0.0406,  0.0623,  0.0711,  0.0618,  0.0655,  0.0646,  0.0737,
         0.0701,  0.0686,  0.0558,  0.0602,  0.0572,  0.0503,  0.0671,  0.0505,
         0.0675,  0.0427,  0.0656,  0.0614,  0.0626,  0.0685,  0.0647,  0.0630,
         0.0806,  0.0692,  0.0562,  0.0519,  0.0610,  0.0688,  0.0476,  0.0615,
         0.0686,  0.0653,  0.0601,  0.0534,  0.0532,  0.0625,  0.0581,  0.0694,
         0.0714,  0.0711,  0.0785,  0.0549,  0.0650,  0.0598,  0.0614,  0.0543,
         0.0567,  0.0219,  0.0673,  0.0459,  0.0782,  0.0563,  0.0694,  0.0749,
         0.0794,  0.0753,  0.0474,  0.0768,  0.0794,  0.0749,  0.0555,  0.0782,
         0.0782, -0.0457,  0.0572,  0.0572,  0.0553, -0.0377,  0.0557, -0.0308,
         0.0732, -0.0407,  0.0632,  0.0697,  0.0694,  0.0278,  0.0284,  0.0561,
         0.0579,  0.0668,  0.0783,  0.0660, -0.0395,  0.0748,  0.0834,  0.0647,
         0.0332,  0.0727,  0.0607,  0.0501,  0.0683,  0.0626,  0.0512, -0.0304,
         0.0707,  0.0797, -0.0102,  0.0728,  0.0832, -0.0439,  0.0443,  0.0654,
         0.0712,  0.0052,  0.0512,  0.0742,  0.0790,  0.0225,  0.0758,  0.0728,
         0.0805,  0.0775,  0.0720,  0.0439,  0.0734,  0.0786,  0.0483, -0.0469,
         0.0575,  0.0592,  0.0828,  0.0689,  0.0325,  0.0467,  0.0715,  0.0821,
         0.0610,  0.0272,  0.0759, -0.0340,  0.0756,  0.0610,  0.0774,  0.0772,
         0.0590,  0.0761,  0.0637,  0.0795,  0.0567,  0.0823,  0.0708,  0.0413,
         0.0504,  0.0414,  0.0674,  0.0729,  0.0614,  0.0615,  0.0219,  0.0602,
         0.0640,  0.0715,  0.0729,  0.0741,  0.0637,  0.0766, -0.0139,  0.0692,
         0.0654,  0.0641,  0.0616,  0.0795,  0.0732,  0.0802,  0.0153,  0.0674,
         0.0799,  0.0781,  0.0788,  0.0863,  0.0866,  0.0796,  0.0732,  0.0760,
         0.0828,  0.0768,  0.0814,  0.0826,  0.0768,  0.0847,  0.0711,  0.0763,
         0.0766,  0.0771,  0.0798,  0.0756,  0.0774,  0.0849,  0.0854,  0.0772,
         0.0797,  0.0836,  0.0850,  0.0734,  0.0810,  0.0797,  0.0752,  0.0805,
         0.0834,  0.0757,  0.0725,  0.0789,  0.0786,  0.0793,  0.0705,  0.0848,
         0.0820,  0.0718,  0.0808,  0.0858,  0.0776,  0.0908,  0.0720,  0.0658,
         0.0852,  0.0786,  0.0787,  0.0720,  0.0782,  0.0668,  0.0757,  0.0798,
         0.0834,  0.0798,  0.0710,  0.0819,  0.0759,  0.0828,  0.0873,  0.0768,
         0.0874,  0.0657,  0.0876,  0.0773,  0.0795,  0.0846,  0.0715,  0.0747,
         0.0756,  0.0778,  0.0840,  0.0688,  0.0749,  0.0730,  0.0807,  0.0788,
         0.0723,  0.0801,  0.0835,  0.0765,  0.0817,  0.0778,  0.0745,  0.0798,
         0.0749,  0.0743,  0.0799,  0.0776,  0.0729,  0.0832,  0.0779,  0.0762,
         0.0780,  0.0641,  0.0764,  0.0793,  0.0715,  0.0818,  0.0783,  0.0802,
         0.0788,  0.0741,  0.0856,  0.0708,  0.0870,  0.0851,  0.0828,  0.0781,
         0.0826,  0.0793,  0.0802,  0.0612,  0.0779,  0.0772,  0.0798,  0.0875,
         0.0800,  0.0731,  0.0800,  0.0690,  0.0869,  0.0758,  0.0789,  0.0813],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3328, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:10,592 :: INFO :: Epoch 20: loss tensor(140.3347, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0811,  0.0683,  0.0990,  0.0834,  0.0758,  0.0814,  0.0816,  0.0822,
         0.0654,  0.0785,  0.0846,  0.0816,  0.0736,  0.0891,  0.0905,  0.0686,
         0.0654,  0.0763,  0.0669,  0.0907,  0.0769,  0.0680,  0.0922,  0.0881,
         0.0912,  0.0581,  0.0542,  0.0857,  0.1114,  0.0660,  0.0917,  0.0812,
         0.0860,  0.0826,  0.0736,  0.0575,  0.0686,  0.0967,  0.0710,  0.0941,
         0.0758,  0.0696,  0.0951,  0.0794,  0.0675,  0.0720,  0.0722,  0.0789,
         0.0881,  0.0598,  0.0874,  0.0663,  0.0657,  0.0518,  0.0956,  0.0933,
         0.0752,  0.1055,  0.0807,  0.0797,  0.0615,  0.0736,  0.0677,  0.0821,
         0.0782,  0.0578,  0.0861,  0.0816,  0.0844,  0.0707,  0.0709,  0.0764,
         0.0608,  0.0864,  0.0810,  0.0648,  0.0835,  0.0713,  0.0665,  0.0739,
         0.0973,  0.0949,  0.0816,  0.0919,  0.0668,  0.0920,  0.0817,  0.0875,
         0.0634,  0.0464,  0.0787,  0.0925,  0.0739,  0.0743,  0.0877,  0.0898,
         0.0767,  0.0835,  0.0740,  0.0729,  0.0698,  0.0537,  0.0822,  0.0705,
         0.0912,  0.0643,  0.0793,  0.0685,  0.0722,  0.0851,  0.0898,  0.0726,
         0.1022,  0.0830,  0.0747,  0.0639,  0.0709,  0.0897,  0.0518,  0.0815,
         0.0858,  0.0812,  0.0739,  0.0581,  0.0674,  0.0775,  0.0784,  0.0888,
         0.0882,  0.0918,  0.1031,  0.0558,  0.0684,  0.0696,  0.0822,  0.0790,
         0.0872,  0.0260,  0.0897,  0.0700,  0.0980,  0.0817,  0.0932,  0.1046,
         0.1054,  0.0940,  0.0501,  0.1073,  0.1046,  0.1030,  0.0827,  0.1016,
         0.0993, -0.0534,  0.0788,  0.0872,  0.0805, -0.0374,  0.0593, -0.0271,
         0.0867, -0.0444,  0.0738,  0.0936,  0.0822,  0.0264,  0.0495,  0.0808,
         0.0843,  0.0884,  0.1011,  0.0791, -0.0414,  0.0985,  0.1089,  0.0940,
         0.0342,  0.0912,  0.0835,  0.0683,  0.0893,  0.0906,  0.0550, -0.0255,
         0.0864,  0.1032,  0.0037,  0.0934,  0.1060, -0.0495,  0.0735,  0.0905,
         0.0858,  0.0250,  0.0520,  0.0942,  0.1059,  0.0204,  0.0972,  0.0995,
         0.1045,  0.1049,  0.0908,  0.0454,  0.0964,  0.1074,  0.0712, -0.0549,
         0.0600,  0.0829,  0.1055,  0.0808,  0.0326,  0.0704,  0.0911,  0.1095,
         0.0847,  0.0500,  0.0940, -0.0317,  0.0994,  0.0830,  0.0930,  0.1011,
         0.0639,  0.1001,  0.0795,  0.1072,  0.0613,  0.1076,  0.0877,  0.0400,
         0.0529,  0.0432,  0.0780,  0.1026,  0.0873,  0.0855,  0.0200,  0.0681,
         0.0849,  0.0979,  0.1019,  0.0931,  0.0906,  0.1017,  0.0014,  0.0928,
         0.0893,  0.0906,  0.0920,  0.1028,  0.1009,  0.1081,  0.0418,  0.0886,
         0.1131,  0.1110,  0.1131,  0.1126,  0.1152,  0.1123,  0.1035,  0.1055,
         0.1162,  0.1071,  0.1120,  0.1121,  0.1026,  0.1098,  0.0979,  0.1064,
         0.1073,  0.1072,  0.1074,  0.1050,  0.1084,  0.1146,  0.1118,  0.1054,
         0.1088,  0.1178,  0.1128,  0.1074,  0.1151,  0.1025,  0.1054,  0.1104,
         0.1056,  0.1061,  0.1025,  0.1092,  0.1133,  0.1046,  0.0972,  0.1101,
         0.1102,  0.1011,  0.1041,  0.1121,  0.1111,  0.1239,  0.1060,  0.1004,
         0.1163,  0.1118,  0.1022,  0.0999,  0.1057,  0.0979,  0.1077,  0.1131,
         0.1154,  0.1095,  0.1012,  0.1087,  0.1025,  0.1167,  0.1161,  0.1093,
         0.1149,  0.0993,  0.1156,  0.1062,  0.1107,  0.1108,  0.1054,  0.1063,
         0.1044,  0.1067,  0.1126,  0.0981,  0.1050,  0.1029,  0.1129,  0.1025,
         0.0929,  0.1093,  0.1143,  0.0994,  0.1104,  0.1067,  0.1041,  0.1102,
         0.1030,  0.0934,  0.1035,  0.1080,  0.1030,  0.1133,  0.1064,  0.1066,
         0.1081,  0.0831,  0.1073,  0.1073,  0.1055,  0.1095,  0.1063,  0.1086,
         0.1101,  0.1088,  0.1184,  0.1055,  0.1154,  0.1106,  0.1126,  0.1096,
         0.1098,  0.1089,  0.1107,  0.0958,  0.1027,  0.1032,  0.1139,  0.1204,
         0.1046,  0.0890,  0.1061,  0.1022,  0.1139,  0.1063,  0.1101,  0.1077],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.2867, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:15,249 :: INFO :: Epoch 25: loss tensor(138.7399, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0923,  0.0708,  0.1207,  0.0922,  0.0841,  0.1000,  0.0916,  0.0893,
         0.0702,  0.0848,  0.1046,  0.0923,  0.0813,  0.1003,  0.0998,  0.0723,
         0.0671,  0.0799,  0.0674,  0.1071,  0.0849,  0.0697,  0.1048,  0.1076,
         0.1043,  0.0593,  0.0539,  0.1004,  0.1338,  0.0674,  0.1049,  0.0854,
         0.0971,  0.0892,  0.0844,  0.0628,  0.0703,  0.1127,  0.0803,  0.1060,
         0.0854,  0.0799,  0.1055,  0.0880,  0.0719,  0.0751,  0.0775,  0.0942,
         0.1011,  0.0622,  0.0917,  0.0678,  0.0712,  0.0497,  0.1154,  0.1059,
         0.0845,  0.1304,  0.0951,  0.0959,  0.0619,  0.0785,  0.0777,  0.0916,
         0.0824,  0.0596,  0.0904,  0.0917,  0.0891,  0.0779,  0.0738,  0.0857,
         0.0587,  0.1071,  0.0871,  0.0685,  0.0888,  0.0837,  0.0675,  0.0787,
         0.1153,  0.1193,  0.0871,  0.1090,  0.0728,  0.1045,  0.0859,  0.0997,
         0.0695,  0.0475,  0.0870,  0.1075,  0.0781,  0.0778,  0.1058,  0.0992,
         0.0768,  0.0910,  0.0863,  0.0789,  0.0760,  0.0536,  0.0886,  0.0859,
         0.1101,  0.0821,  0.0867,  0.0709,  0.0756,  0.0927,  0.1110,  0.0760,
         0.1186,  0.0907,  0.0849,  0.0689,  0.0744,  0.1084,  0.0526,  0.0961,
         0.0960,  0.0893,  0.0809,  0.0609,  0.0732,  0.0870,  0.0917,  0.1024,
         0.0970,  0.1065,  0.1228,  0.0515,  0.0656,  0.0735,  0.0960,  0.0982,
         0.1142,  0.0288,  0.1059,  0.0889,  0.1092,  0.1020,  0.1099,  0.1327,
         0.1267,  0.1051,  0.0483,  0.1346,  0.1246,  0.1266,  0.1054,  0.1190,
         0.1135, -0.0593,  0.0937,  0.1143,  0.1004, -0.0346,  0.0565, -0.0188,
         0.0909, -0.0443,  0.0785,  0.1120,  0.0886,  0.0244,  0.0658,  0.0998,
         0.1064,  0.1022,  0.1175,  0.0844, -0.0404,  0.1168,  0.1295,  0.1199,
         0.0335,  0.1016,  0.0998,  0.0813,  0.1029,  0.1145,  0.0515, -0.0188,
         0.0938,  0.1200,  0.0157,  0.1044,  0.1225, -0.0531,  0.1007,  0.1102,
         0.0926,  0.0419,  0.0492,  0.1066,  0.1278,  0.0172,  0.1119,  0.1213,
         0.1233,  0.1267,  0.1027,  0.0420,  0.1124,  0.1319,  0.0890, -0.0604,
         0.0561,  0.1006,  0.1200,  0.0838,  0.0308,  0.0881,  0.1031,  0.1325,
         0.1020,  0.0685,  0.1035, -0.0270,  0.1172,  0.0977,  0.1009,  0.1203,
         0.0640,  0.1173,  0.0868,  0.1304,  0.0568,  0.1279,  0.0964,  0.0345,
         0.0546,  0.0388,  0.0802,  0.1283,  0.1081,  0.1041,  0.0192,  0.0701,
         0.0978,  0.1197,  0.1276,  0.1068,  0.1132,  0.1221,  0.0172,  0.1102,
         0.1075,  0.1127,  0.1195,  0.1202,  0.1238,  0.1320,  0.0659,  0.1021,
         0.1450,  0.1419,  0.1459,  0.1345,  0.1416,  0.1434,  0.1308,  0.1319,
         0.1486,  0.1359,  0.1407,  0.1388,  0.1235,  0.1296,  0.1200,  0.1338,
         0.1369,  0.1346,  0.1311,  0.1315,  0.1373,  0.1421,  0.1347,  0.1300,
         0.1348,  0.1503,  0.1367,  0.1396,  0.1484,  0.1183,  0.1328,  0.1380,
         0.1231,  0.1340,  0.1307,  0.1364,  0.1474,  0.1255,  0.1189,  0.1321,
         0.1349,  0.1286,  0.1200,  0.1343,  0.1428,  0.1552,  0.1392,  0.1337,
         0.1454,  0.1438,  0.1181,  0.1266,  0.1290,  0.1270,  0.1375,  0.1444,
         0.1460,  0.1373,  0.1292,  0.1314,  0.1257,  0.1497,  0.1419,  0.1404,
         0.1388,  0.1317,  0.1399,  0.1325,  0.1404,  0.1327,  0.1378,  0.1362,
         0.1292,  0.1323,  0.1381,  0.1245,  0.1327,  0.1295,  0.1433,  0.1192,
         0.1062,  0.1355,  0.1439,  0.1146,  0.1356,  0.1324,  0.1303,  0.1381,
         0.1279,  0.1067,  0.1222,  0.1365,  0.1298,  0.1408,  0.1310,  0.1341,
         0.1353,  0.0968,  0.1359,  0.1314,  0.1381,  0.1349,  0.1300,  0.1340,
         0.1394,  0.1423,  0.1501,  0.1386,  0.1417,  0.1312,  0.1398,  0.1395,
         0.1326,  0.1356,  0.1386,  0.1288,  0.1217,  0.1249,  0.1467,  0.1518,
         0.1227,  0.0960,  0.1274,  0.1344,  0.1370,  0.1342,  0.1392,  0.1300],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.9413, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:19,858 :: INFO :: Epoch 30: loss tensor(135.5181, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0958,  0.0682,  0.1368,  0.0952,  0.0893,  0.1132,  0.0929,  0.0934,
         0.0725,  0.0882,  0.1171,  0.0977,  0.0854,  0.1054,  0.1027,  0.0724,
         0.0678,  0.0782,  0.0661,  0.1187,  0.0901,  0.0660,  0.1099,  0.1203,
         0.1129,  0.0591,  0.0522,  0.1103,  0.1489,  0.0652,  0.1119,  0.0829,
         0.1018,  0.0897,  0.0875,  0.0625,  0.0669,  0.1270,  0.0861,  0.1095,
         0.0913,  0.0857,  0.1093,  0.0926,  0.0722,  0.0742,  0.0785,  0.1035,
         0.1076,  0.0635,  0.0876,  0.0640,  0.0751,  0.0469,  0.1279,  0.1117,
         0.0878,  0.1510,  0.1014,  0.1031,  0.0601,  0.0811,  0.0814,  0.0942,
         0.0817,  0.0589,  0.0907,  0.0941,  0.0904,  0.0794,  0.0713,  0.0875,
         0.0553,  0.1263,  0.0873,  0.0659,  0.0886,  0.0902,  0.0637,  0.0791,
         0.1271,  0.1380,  0.0884,  0.1211,  0.0726,  0.1100,  0.0857,  0.1034,
         0.0711,  0.0462,  0.0902,  0.1177,  0.0799,  0.0754,  0.1161,  0.1019,
         0.0734,  0.0923,  0.0962,  0.0803,  0.0783,  0.0509,  0.0892,  0.0996,
         0.1213,  0.0921,  0.0867,  0.0681,  0.0766,  0.0964,  0.1257,  0.0774,
         0.1258,  0.0921,  0.0880,  0.0695,  0.0736,  0.1236,  0.0524,  0.1039,
         0.1003,  0.0919,  0.0845,  0.0627,  0.0759,  0.0942,  0.0990,  0.1110,
         0.1014,  0.1157,  0.1368,  0.0464,  0.0595,  0.0733,  0.1033,  0.1113,
         0.1365,  0.0312,  0.1164,  0.1009,  0.1118,  0.1162,  0.1187,  0.1585,
         0.1421,  0.1101,  0.0435,  0.1582,  0.1381,  0.1435,  0.1215,  0.1326,
         0.1200, -0.0647,  0.1006,  0.1365,  0.1142, -0.0316,  0.0551, -0.0094,
         0.0912, -0.0413,  0.0808,  0.1234,  0.0913,  0.0210,  0.0759,  0.1129,
         0.1221,  0.1105,  0.1262,  0.0841, -0.0385,  0.1293,  0.1441,  0.1410,
         0.0326,  0.1039,  0.1088,  0.0893,  0.1096,  0.1324,  0.0447, -0.0128,
         0.0940,  0.1297,  0.0234,  0.1069,  0.1308, -0.0553,  0.1257,  0.1232,
         0.0936,  0.0522,  0.0461,  0.1113,  0.1451,  0.0163,  0.1200,  0.1368,
         0.1361,  0.1426,  0.1085,  0.0369,  0.1207,  0.1523,  0.1000, -0.0639,
         0.0511,  0.1120,  0.1269,  0.0808,  0.0256,  0.0984,  0.1062,  0.1506,
         0.1125,  0.0817,  0.1060, -0.0227,  0.1301,  0.1050,  0.1061,  0.1358,
         0.0594,  0.1285,  0.0874,  0.1498,  0.0549,  0.1417,  0.0965,  0.0290,
         0.0534,  0.0348,  0.0777,  0.1494,  0.1222,  0.1166,  0.0169,  0.0695,
         0.1035,  0.1360,  0.1490,  0.1166,  0.1302,  0.1381,  0.0292,  0.1202,
         0.1194,  0.1291,  0.1443,  0.1325,  0.1415,  0.1510,  0.0837,  0.1080,
         0.1755,  0.1706,  0.1774,  0.1515,  0.1666,  0.1721,  0.1542,  0.1546,
         0.1797,  0.1636,  0.1673,  0.1620,  0.1390,  0.1415,  0.1373,  0.1578,
         0.1648,  0.1590,  0.1493,  0.1555,  0.1654,  0.1668,  0.1525,  0.1494,
         0.1565,  0.1814,  0.1564,  0.1698,  0.1799,  0.1259,  0.1565,  0.1625,
         0.1359,  0.1592,  0.1557,  0.1599,  0.1802,  0.1407,  0.1351,  0.1513,
         0.1554,  0.1548,  0.1263,  0.1515,  0.1727,  0.1849,  0.1713,  0.1656,
         0.1731,  0.1744,  0.1254,  0.1528,  0.1470,  0.1541,  0.1660,  0.1743,
         0.1748,  0.1644,  0.1554,  0.1488,  0.1463,  0.1811,  0.1638,  0.1696,
         0.1578,  0.1626,  0.1597,  0.1553,  0.1683,  0.1505,  0.1689,  0.1646,
         0.1494,  0.1537,  0.1594,  0.1472,  0.1573,  0.1517,  0.1713,  0.1272,
         0.1121,  0.1579,  0.1720,  0.1215,  0.1571,  0.1538,  0.1528,  0.1644,
         0.1499,  0.1154,  0.1360,  0.1626,  0.1545,  0.1647,  0.1501,  0.1590,
         0.1585,  0.1063,  0.1615,  0.1512,  0.1691,  0.1597,  0.1478,  0.1554,
         0.1669,  0.1745,  0.1800,  0.1711,  0.1671,  0.1458,  0.1644,  0.1673,
         0.1500,  0.1584,  0.1630,  0.1600,  0.1339,  0.1430,  0.1781,  0.1817,
         0.1343,  0.0919,  0.1425,  0.1653,  0.1553,  0.1588,  0.1657,  0.1470],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.3492, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:24,500 :: INFO :: Epoch 35: loss tensor(136.2051, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0980,  0.0652,  0.1509,  0.0978,  0.0904,  0.1216,  0.0941,  0.0936,
         0.0732,  0.0872,  0.1268,  0.0984,  0.0829,  0.1094,  0.1051,  0.0697,
         0.0635,  0.0799,  0.0655,  0.1274,  0.0918,  0.0652,  0.1132,  0.1302,
         0.1184,  0.0539,  0.0478,  0.1159,  0.1621,  0.0623,  0.1169,  0.0803,
         0.1066,  0.0909,  0.0889,  0.0612,  0.0633,  0.1333,  0.0860,  0.1131,
         0.0923,  0.0878,  0.1105,  0.0924,  0.0697,  0.0701,  0.0773,  0.1077,
         0.1133,  0.0608,  0.0854,  0.0602,  0.0732,  0.0418,  0.1377,  0.1169,
         0.0889,  0.1702,  0.1083,  0.1056,  0.0563,  0.0823,  0.0829,  0.0957,
         0.0824,  0.0560,  0.0904,  0.0953,  0.0886,  0.0785,  0.0717,  0.0898,
         0.0516,  0.1374,  0.0887,  0.0637,  0.0894,  0.0970,  0.0618,  0.0789,
         0.1372,  0.1530,  0.0874,  0.1307,  0.0713,  0.1127,  0.0837,  0.1062,
         0.0701,  0.0437,  0.0886,  0.1253,  0.0792,  0.0740,  0.1224,  0.1045,
         0.0677,  0.0935,  0.1011,  0.0824,  0.0763,  0.0478,  0.0873,  0.1046,
         0.1310,  0.1005,  0.0859,  0.0667,  0.0770,  0.0975,  0.1399,  0.0741,
         0.1309,  0.0935,  0.0902,  0.0676,  0.0740,  0.1315,  0.0501,  0.1103,
         0.1016,  0.0917,  0.0858,  0.0641,  0.0737,  0.0963,  0.1047,  0.1172,
         0.0998,  0.1191,  0.1452,  0.0419,  0.0543,  0.0715,  0.1048,  0.1177,
         0.1542,  0.0325,  0.1199,  0.1065,  0.1082,  0.1241,  0.1204,  0.1805,
         0.1526,  0.1092,  0.0388,  0.1781,  0.1459,  0.1551,  0.1316,  0.1386,
         0.1201, -0.0696,  0.1015,  0.1543,  0.1225, -0.0297,  0.0503, -0.0010,
         0.0860, -0.0376,  0.0803,  0.1284,  0.0899,  0.0204,  0.0802,  0.1200,
         0.1318,  0.1123,  0.1300,  0.0805, -0.0368,  0.1361,  0.1530,  0.1571,
         0.0281,  0.1001,  0.1113,  0.0935,  0.1112,  0.1451,  0.0407, -0.0088,
         0.0887,  0.1330,  0.0272,  0.1033,  0.1340, -0.0573,  0.1455,  0.1301,
         0.0903,  0.0568,  0.0437,  0.1110,  0.1557,  0.0134,  0.1213,  0.1466,
         0.1429,  0.1518,  0.1092,  0.0328,  0.1227,  0.1663,  0.1047, -0.0665,
         0.0442,  0.1170,  0.1254,  0.0740,  0.0225,  0.1025,  0.1039,  0.1629,
         0.1160,  0.0886,  0.1017, -0.0197,  0.1352,  0.1065,  0.1054,  0.1461,
         0.0558,  0.1327,  0.0836,  0.1637,  0.0492,  0.1503,  0.0917,  0.0253,
         0.0506,  0.0317,  0.0710,  0.1655,  0.1300,  0.1233,  0.0151,  0.0672,
         0.1030,  0.1465,  0.1664,  0.1218,  0.1416,  0.1487,  0.0370,  0.1240,
         0.1252,  0.1398,  0.1642,  0.1389,  0.1533,  0.1653,  0.0957,  0.1078,
         0.2043,  0.1968,  0.2062,  0.1640,  0.1904,  0.1988,  0.1743,  0.1743,
         0.2080,  0.1892,  0.1916,  0.1832,  0.1517,  0.1483,  0.1509,  0.1795,
         0.1906,  0.1805,  0.1631,  0.1759,  0.1917,  0.1883,  0.1676,  0.1650,
         0.1749,  0.2094,  0.1731,  0.1983,  0.2085,  0.1292,  0.1767,  0.1831,
         0.1458,  0.1810,  0.1786,  0.1806,  0.2111,  0.1526,  0.1471,  0.1693,
         0.1730,  0.1790,  0.1279,  0.1650,  0.2007,  0.2113,  0.2008,  0.1936,
         0.1990,  0.2022,  0.1268,  0.1760,  0.1593,  0.1781,  0.1914,  0.2026,
         0.2006,  0.1899,  0.1790,  0.1617,  0.1644,  0.2093,  0.1823,  0.1968,
         0.1731,  0.1910,  0.1755,  0.1761,  0.1943,  0.1655,  0.1976,  0.1909,
         0.1669,  0.1705,  0.1766,  0.1662,  0.1791,  0.1706,  0.1964,  0.1295,
         0.1154,  0.1769,  0.1977,  0.1229,  0.1753,  0.1719,  0.1712,  0.1868,
         0.1698,  0.1228,  0.1459,  0.1870,  0.1760,  0.1859,  0.1653,  0.1813,
         0.1779,  0.1143,  0.1832,  0.1670,  0.1973,  0.1829,  0.1628,  0.1734,
         0.1920,  0.2041,  0.2063,  0.2019,  0.1911,  0.1556,  0.1865,  0.1934,
         0.1642,  0.1779,  0.1845,  0.1890,  0.1394,  0.1579,  0.2078,  0.2084,
         0.1398,  0.0872,  0.1516,  0.1938,  0.1703,  0.1809,  0.1897,  0.1607],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.5441, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:29,204 :: INFO :: Epoch 40: loss tensor(133.8011, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0982,  0.0649,  0.1627,  0.0965,  0.0924,  0.1244,  0.0919,  0.0943,
         0.0755,  0.0879,  0.1313,  0.1011,  0.0851,  0.1106,  0.1064,  0.0706,
         0.0641,  0.0787,  0.0641,  0.1339,  0.0954,  0.0640,  0.1143,  0.1352,
         0.1229,  0.0534,  0.0447,  0.1201,  0.1697,  0.0601,  0.1226,  0.0784,
         0.1053,  0.0886,  0.0900,  0.0613,  0.0638,  0.1398,  0.0876,  0.1112,
         0.0969,  0.0857,  0.1087,  0.0943,  0.0687,  0.0673,  0.0775,  0.1140,
         0.1133,  0.0616,  0.0810,  0.0577,  0.0756,  0.0407,  0.1414,  0.1189,
         0.0854,  0.1882,  0.1093,  0.1059,  0.0558,  0.0814,  0.0808,  0.0965,
         0.0800,  0.0540,  0.0861,  0.0968,  0.0884,  0.0757,  0.0713,  0.0883,
         0.0493,  0.1507,  0.0872,  0.0613,  0.0878,  0.1007,  0.0600,  0.0757,
         0.1443,  0.1615,  0.0869,  0.1369,  0.0687,  0.1104,  0.0818,  0.1063,
         0.0727,  0.0424,  0.0886,  0.1312,  0.0780,  0.0721,  0.1202,  0.1021,
         0.0652,  0.0901,  0.1047,  0.0832,  0.0726,  0.0472,  0.0846,  0.1119,
         0.1353,  0.1056,  0.0856,  0.0632,  0.0752,  0.0975,  0.1495,  0.0755,
         0.1294,  0.0931,  0.0913,  0.0655,  0.0731,  0.1426,  0.0504,  0.1156,
         0.1014,  0.0912,  0.0812,  0.0621,  0.0715,  0.1009,  0.1068,  0.1209,
         0.0972,  0.1193,  0.1486,  0.0365,  0.0480,  0.0672,  0.1036,  0.1198,
         0.1678,  0.0316,  0.1207,  0.1078,  0.0997,  0.1281,  0.1157,  0.1991,
         0.1575,  0.1053,  0.0345,  0.1935,  0.1478,  0.1608,  0.1370,  0.1402,
         0.1150, -0.0742,  0.0985,  0.1692,  0.1268, -0.0276,  0.0484,  0.0076,
         0.0812, -0.0325,  0.0764,  0.1285,  0.0882,  0.0197,  0.0823,  0.1232,
         0.1370,  0.1108,  0.1281,  0.0764, -0.0346,  0.1376,  0.1564,  0.1689,
         0.0283,  0.0936,  0.1091,  0.0967,  0.1089,  0.1530,  0.0361, -0.0054,
         0.0819,  0.1308,  0.0285,  0.0956,  0.1313, -0.0588,  0.1613,  0.1313,
         0.0859,  0.0593,  0.0408,  0.1064,  0.1595,  0.0121,  0.1186,  0.1498,
         0.1449,  0.1541,  0.1073,  0.0265,  0.1183,  0.1739,  0.1056, -0.0680,
         0.0421,  0.1182,  0.1172,  0.0672,  0.0186,  0.1022,  0.0974,  0.1693,
         0.1147,  0.0920,  0.0938, -0.0169,  0.1355,  0.1032,  0.1005,  0.1526,
         0.0511,  0.1297,  0.0784,  0.1717,  0.0432,  0.1523,  0.0837,  0.0208,
         0.0490,  0.0269,  0.0650,  0.1758,  0.1323,  0.1264,  0.0142,  0.0616,
         0.0989,  0.1517,  0.1795,  0.1234,  0.1486,  0.1555,  0.0429,  0.1219,
         0.1269,  0.1464,  0.1794,  0.1417,  0.1588,  0.1743,  0.1049,  0.1031,
         0.2316,  0.2203,  0.2328,  0.1723,  0.2104,  0.2228,  0.1913,  0.1901,
         0.2343,  0.2104,  0.2139,  0.2010,  0.1589,  0.1497,  0.1616,  0.1979,
         0.2133,  0.1998,  0.1717,  0.1933,  0.2149,  0.2078,  0.1795,  0.1761,
         0.1897,  0.2345,  0.1845,  0.2250,  0.2356,  0.1256,  0.1936,  0.2009,
         0.1526,  0.2001,  0.2003,  0.1980,  0.2391,  0.1598,  0.1554,  0.1843,
         0.1854,  0.2012,  0.1232,  0.1737,  0.2265,  0.2353,  0.2280,  0.2198,
         0.2222,  0.2280,  0.1244,  0.1974,  0.1668,  0.2001,  0.2164,  0.2279,
         0.2250,  0.2120,  0.2014,  0.1698,  0.1793,  0.2358,  0.1968,  0.2217,
         0.1844,  0.2178,  0.1873,  0.1936,  0.2175,  0.1765,  0.2240,  0.2152,
         0.1813,  0.1850,  0.1898,  0.1831,  0.1975,  0.1850,  0.2204,  0.1245,
         0.1150,  0.1923,  0.2197,  0.1176,  0.1887,  0.1857,  0.1872,  0.2070,
         0.1861,  0.1317,  0.1544,  0.2083,  0.1950,  0.2041,  0.1767,  0.2025,
         0.1941,  0.1195,  0.2022,  0.1776,  0.2233,  0.2016,  0.1742,  0.1877,
         0.2153,  0.2315,  0.2298,  0.2296,  0.2111,  0.1611,  0.2051,  0.2174,
         0.1746,  0.1939,  0.2029,  0.2155,  0.1396,  0.1680,  0.2344,  0.2324,
         0.1393,  0.0818,  0.1533,  0.2205,  0.1804,  0.1996,  0.2112,  0.1699],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.5448, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:55:29,219 :: INFO :: ----- frontend -----
2023-05-14 11:55:29,219 :: INFO :: Environment 0
2023-05-14 11:55:38,626 :: INFO :: Epoch 5: loss tensor(872.4337, device='cuda:0'), U.norm 13.160419464111328, V.norm 16.989267349243164, MLP.norm 2.1471712589263916
2023-05-14 11:55:38,877 :: INFO :: Epoch 10: loss tensor(844.5738, device='cuda:0'), U.norm 10.353906631469727, V.norm 16.334211349487305, MLP.norm 3.687282085418701
2023-05-14 11:55:39,111 :: INFO :: Epoch 15: loss tensor(806.8676, device='cuda:0'), U.norm 8.658121109008789, V.norm 16.015670776367188, MLP.norm 5.461165904998779
2023-05-14 11:55:39,345 :: INFO :: Epoch 20: loss tensor(764.5051, device='cuda:0'), U.norm 7.503838062286377, V.norm 15.80218505859375, MLP.norm 7.280041217803955
2023-05-14 11:55:39,361 :: INFO :: Environment 1
2023-05-14 11:55:48,659 :: INFO :: Epoch 5: loss tensor(834.2830, device='cuda:0'), U.norm 13.157909393310547, V.norm 16.919570922851562, MLP.norm 2.1535513401031494
2023-05-14 11:55:48,909 :: INFO :: Epoch 10: loss tensor(808.9242, device='cuda:0'), U.norm 10.348106384277344, V.norm 16.237977981567383, MLP.norm 3.6423861980438232
2023-05-14 11:55:49,143 :: INFO :: Epoch 15: loss tensor(774.6664, device='cuda:0'), U.norm 8.64848518371582, V.norm 15.907767295837402, MLP.norm 5.405402183532715
2023-05-14 11:55:49,378 :: INFO :: Epoch 20: loss tensor(735.4246, device='cuda:0'), U.norm 7.490084171295166, V.norm 15.68552017211914, MLP.norm 7.251530647277832
2023-05-14 11:55:49,378 :: INFO :: Environment 2
2023-05-14 11:55:58,707 :: INFO :: Epoch 5: loss tensor(818.6771, device='cuda:0'), U.norm 13.154535293579102, V.norm 16.9044246673584, MLP.norm 2.1640806198120117
2023-05-14 11:55:58,925 :: INFO :: Epoch 10: loss tensor(794.7525, device='cuda:0'), U.norm 10.340197563171387, V.norm 16.214157104492188, MLP.norm 3.621035099029541
2023-05-14 11:55:59,160 :: INFO :: Epoch 15: loss tensor(761.5674, device='cuda:0'), U.norm 8.635114669799805, V.norm 15.88118839263916, MLP.norm 5.329277038574219
2023-05-14 11:55:59,394 :: INFO :: Epoch 20: loss tensor(724.0529, device='cuda:0'), U.norm 7.470247745513916, V.norm 15.659173965454102, MLP.norm 7.1298828125
2023-05-14 11:55:59,394 :: INFO :: Environment 3
2023-05-14 11:56:08,583 :: INFO :: Epoch 5: loss tensor(834.2490, device='cuda:0'), U.norm 13.151701927185059, V.norm 16.908588409423828, MLP.norm 2.169654130935669
2023-05-14 11:56:08,817 :: INFO :: Epoch 10: loss tensor(807.5171, device='cuda:0'), U.norm 10.336000442504883, V.norm 16.218780517578125, MLP.norm 3.672962188720703
2023-05-14 11:56:09,036 :: INFO :: Epoch 15: loss tensor(771.1919, device='cuda:0'), U.norm 8.629554748535156, V.norm 15.884642601013184, MLP.norm 5.452750205993652
2023-05-14 11:56:09,270 :: INFO :: Epoch 20: loss tensor(731.1052, device='cuda:0'), U.norm 7.463144302368164, V.norm 15.660301208496094, MLP.norm 7.299526214599609
2023-05-14 11:56:09,286 :: INFO :: Environment 4
2023-05-14 11:56:18,443 :: INFO :: Epoch 5: loss tensor(795.3126, device='cuda:0'), U.norm 13.1524019241333, V.norm 16.851119995117188, MLP.norm 2.125669240951538
2023-05-14 11:56:18,677 :: INFO :: Epoch 10: loss tensor(770.8865, device='cuda:0'), U.norm 10.336531639099121, V.norm 16.13738441467285, MLP.norm 3.5897958278656006
2023-05-14 11:56:18,896 :: INFO :: Epoch 15: loss tensor(736.8066, device='cuda:0'), U.norm 8.629650115966797, V.norm 15.793164253234863, MLP.norm 5.30629301071167
2023-05-14 11:56:19,131 :: INFO :: Epoch 20: loss tensor(699.5607, device='cuda:0'), U.norm 7.462396621704102, V.norm 15.562251091003418, MLP.norm 7.093813896179199
2023-05-14 11:56:19,146 :: INFO :: Environment 5
2023-05-14 11:56:28,288 :: INFO :: Epoch 5: loss tensor(851.2611, device='cuda:0'), U.norm 13.15776252746582, V.norm 16.954914093017578, MLP.norm 2.140559196472168
2023-05-14 11:56:28,522 :: INFO :: Epoch 10: loss tensor(823.6106, device='cuda:0'), U.norm 10.3482666015625, V.norm 16.286964416503906, MLP.norm 3.638461112976074
2023-05-14 11:56:28,757 :: INFO :: Epoch 15: loss tensor(787.0288, device='cuda:0'), U.norm 8.648719787597656, V.norm 15.962791442871094, MLP.norm 5.377204418182373
2023-05-14 11:56:28,991 :: INFO :: Epoch 20: loss tensor(747.8922, device='cuda:0'), U.norm 7.489583492279053, V.norm 15.746247291564941, MLP.norm 7.158073902130127
2023-05-14 11:56:29,007 :: INFO :: Environment 6
2023-05-14 11:56:38,367 :: INFO :: Epoch 5: loss tensor(914.9210, device='cuda:0'), U.norm 13.159664154052734, V.norm 17.054969787597656, MLP.norm 2.2084407806396484
2023-05-14 11:56:38,586 :: INFO :: Epoch 10: loss tensor(886.6674, device='cuda:0'), U.norm 10.353306770324707, V.norm 16.42768096923828, MLP.norm 3.8345446586608887
2023-05-14 11:56:38,820 :: INFO :: Epoch 15: loss tensor(848.9009, device='cuda:0'), U.norm 8.657620429992676, V.norm 16.123882293701172, MLP.norm 5.747263431549072
2023-05-14 11:56:39,055 :: INFO :: Epoch 20: loss tensor(808.5169, device='cuda:0'), U.norm 7.503299713134766, V.norm 15.920339584350586, MLP.norm 7.714569091796875
2023-05-14 11:56:39,071 :: INFO :: Environment 7
2023-05-14 11:56:48,212 :: INFO :: Epoch 5: loss tensor(832.2985, device='cuda:0'), U.norm 13.155111312866211, V.norm 16.929672241210938, MLP.norm 2.1862130165100098
2023-05-14 11:56:48,446 :: INFO :: Epoch 10: loss tensor(805.4031, device='cuda:0'), U.norm 10.342451095581055, V.norm 16.254108428955078, MLP.norm 3.6986005306243896
2023-05-14 11:56:48,666 :: INFO :: Epoch 15: loss tensor(770.3020, device='cuda:0'), U.norm 8.638396263122559, V.norm 15.928383827209473, MLP.norm 5.427331924438477
2023-05-14 11:56:48,900 :: INFO :: Epoch 20: loss tensor(733.0959, device='cuda:0'), U.norm 7.473891735076904, V.norm 15.71088981628418, MLP.norm 7.196764945983887
2023-05-14 11:56:48,916 :: INFO :: Environment 8
2023-05-14 11:56:58,073 :: INFO :: Epoch 5: loss tensor(850.6559, device='cuda:0'), U.norm 13.157753944396973, V.norm 16.953397750854492, MLP.norm 2.147352695465088
2023-05-14 11:56:58,291 :: INFO :: Epoch 10: loss tensor(824.5618, device='cuda:0'), U.norm 10.348118782043457, V.norm 16.283273696899414, MLP.norm 3.62650990486145
2023-05-14 11:56:58,526 :: INFO :: Epoch 15: loss tensor(788.9240, device='cuda:0'), U.norm 8.648581504821777, V.norm 15.956757545471191, MLP.norm 5.36584997177124
2023-05-14 11:56:58,745 :: INFO :: Epoch 20: loss tensor(749.9104, device='cuda:0'), U.norm 7.490480422973633, V.norm 15.7378511428833, MLP.norm 7.183225631713867
2023-05-14 11:56:58,760 :: INFO :: Environment 9
2023-05-14 11:57:07,918 :: INFO :: Epoch 5: loss tensor(866.0062, device='cuda:0'), U.norm 13.158231735229492, V.norm 16.97605323791504, MLP.norm 2.1858842372894287
2023-05-14 11:57:08,152 :: INFO :: Epoch 10: loss tensor(838.0690, device='cuda:0'), U.norm 10.349800109863281, V.norm 16.315773010253906, MLP.norm 3.7344584465026855
2023-05-14 11:57:08,387 :: INFO :: Epoch 15: loss tensor(800.9081, device='cuda:0'), U.norm 8.651708602905273, V.norm 15.995903968811035, MLP.norm 5.534128189086914
2023-05-14 11:57:08,605 :: INFO :: Epoch 20: loss tensor(760.4777, device='cuda:0'), U.norm 7.4942169189453125, V.norm 15.78191089630127, MLP.norm 7.423047065734863
2023-05-14 11:57:08,620 :: INFO :: Ite = 1, Delta = 4047
2023-05-14 11:57:08,620 :: INFO :: ----- backend -----
2023-05-14 11:57:14,246 :: INFO :: Epoch 5: loss tensor(85.7390, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.5977e-02,  1.5605e-02,  2.5006e-02,  1.9376e-02,  9.1594e-03,
         9.1673e-03,  1.5688e-02,  1.8619e-02,  8.0327e-03,  2.1370e-02,
         7.9873e-03,  1.4645e-02,  3.9518e-03,  2.2050e-02,  2.6249e-02,
         2.0368e-02,  2.0360e-02,  1.9766e-02,  1.7112e-02,  2.0343e-02,
         1.1206e-02,  2.2478e-02,  2.5695e-02,  9.4520e-03,  1.8863e-02,
         1.1151e-02,  1.4389e-02,  1.4527e-02,  3.0822e-02,  1.9151e-02,
         2.2199e-02,  2.5391e-02,  1.6509e-02,  2.1928e-02,  5.8114e-03,
        -4.2060e-04,  2.0364e-02,  1.2491e-02,  9.1650e-03,  2.2041e-02,
         1.0205e-02,  7.2938e-03,  2.7054e-02,  1.3876e-02,  1.2027e-02,
         2.5037e-02,  1.4973e-02,  1.0807e-02,  1.7377e-02,  1.6753e-02,
         2.4812e-02,  1.5212e-02,  1.7207e-02,  1.1481e-02,  2.4085e-02,
         2.4150e-02,  1.4814e-02,  2.6080e-02,  1.0403e-02,  8.9747e-03,
         1.8983e-02,  1.8566e-02,  5.9851e-03,  1.8268e-02,  1.9912e-02,
         6.3330e-03,  2.7097e-02,  1.6089e-02,  2.6225e-02,  9.6565e-03,
         2.4599e-02,  9.6995e-03,  2.2794e-02,  5.3896e-03,  2.2419e-02,
         1.0525e-02,  2.0441e-02,  8.8302e-03,  1.6428e-02,  1.6740e-02,
         1.8200e-02,  1.0983e-02,  2.3420e-02,  1.7449e-02,  8.5304e-04,
         2.4072e-02,  2.4866e-02,  2.0197e-02,  1.0933e-02, -7.6801e-05,
         8.0984e-03,  2.2389e-02,  1.7392e-02,  2.3617e-02,  1.5060e-02,
         2.0595e-02,  2.5858e-02,  2.2233e-02,  1.2989e-02,  1.3727e-02,
         8.0787e-03,  1.2999e-02,  1.8961e-02, -2.2861e-03,  2.0207e-02,
         5.1864e-03,  1.9962e-02,  2.0121e-02,  1.5934e-02,  2.0316e-02,
         1.9126e-02,  1.5620e-02,  2.6877e-02,  2.0823e-02,  8.0485e-03,
         7.3306e-03,  1.9937e-02,  1.5274e-02,  1.1141e-02,  1.6108e-02,
         2.1138e-02,  1.3472e-02,  8.8422e-03,  1.1024e-02,  6.0963e-03,
         1.0242e-02,  1.2992e-02,  2.0392e-02,  2.3125e-02,  1.9889e-02,
         1.9067e-02,  2.5727e-02,  2.9133e-02,  2.4049e-02,  9.4286e-03,
        -5.0596e-03, -1.3564e-02,  1.5593e-02,  1.4730e-02, -4.9986e-03,
         2.5066e-02, -8.3229e-03,  1.4975e-02,  1.5101e-02,  2.1223e-02,
         2.7373e-02,  1.8205e-02,  4.7078e-03,  2.2101e-02,  1.3986e-02,
        -1.1252e-02,  2.3803e-02,  2.8569e-02, -1.9179e-02,  6.8026e-03,
        -9.2999e-03, -9.4914e-03, -2.2440e-02,  2.2337e-02, -2.2114e-02,
         2.8305e-02, -2.3722e-02,  2.3370e-02,  1.4850e-02,  2.6714e-02,
         1.3965e-02, -1.4595e-02, -2.5630e-03,  1.8957e-04,  1.5515e-02,
         2.6052e-02,  2.3228e-02, -2.3129e-02,  2.0397e-02,  2.9615e-02,
        -1.1942e-02,  1.5680e-02,  2.2854e-02,  8.0150e-03,  1.8453e-03,
         1.6449e-02, -5.0038e-03,  2.1011e-02, -2.1879e-02,  2.2619e-02,
         2.5273e-02, -1.9393e-02,  2.2043e-02,  2.5994e-02, -2.2294e-02,
        -1.4261e-02,  2.2356e-03,  2.7295e-02, -1.9166e-02,  2.0587e-02,
         2.4112e-02,  1.9201e-02,  1.2837e-02,  2.9202e-02,  1.1836e-02,
         2.3763e-02,  1.7051e-02,  2.8003e-02,  2.1981e-02,  1.7847e-02,
         1.4625e-02, -2.1242e-03, -2.3679e-02,  2.3511e-02,  1.1404e-03,
         2.8909e-02,  2.5804e-02,  1.5535e-02, -1.0919e-02,  2.2902e-02,
         2.2784e-02,  2.3285e-03, -1.5465e-02,  2.6828e-02, -2.1619e-02,
         2.0250e-02,  1.3707e-02,  2.8519e-02,  2.7236e-02,  2.4270e-02,
         2.0691e-02,  1.7414e-02,  2.0925e-02,  1.9151e-02,  2.9487e-02,
         2.3190e-02,  2.2124e-02,  2.5259e-02,  2.0799e-02,  2.6257e-02,
         2.6430e-03,  3.9452e-04, -2.2523e-03,  1.1445e-02,  2.2009e-02,
         1.1264e-02,  8.9496e-03,  4.7023e-03,  2.7606e-02, -6.7408e-03,
         1.3425e-02, -2.1164e-02,  1.5099e-02,  3.6818e-03, -1.9535e-03,
        -1.1995e-02,  2.6292e-02,  1.1602e-02,  1.7271e-02, -2.0430e-02,
         1.5385e-02,  1.9289e-02,  1.3936e-02,  1.5553e-02,  3.0978e-02,
         2.6966e-02,  2.0957e-02,  1.4422e-02,  1.8762e-02,  2.3563e-02,
         2.0180e-02,  2.7850e-02,  3.0094e-02,  2.2851e-02,  3.0334e-02,
         1.5628e-02,  1.9303e-02,  2.6765e-02,  1.9012e-02,  2.4934e-02,
         2.2265e-02,  1.7215e-02,  3.0435e-02,  3.0987e-02,  2.2274e-02,
         2.2430e-02,  2.5428e-02,  3.1009e-02,  7.4894e-03,  1.9550e-02,
         2.7355e-02,  1.7516e-02,  2.8860e-02,  3.0453e-02,  1.9097e-02,
         2.1257e-02,  2.0326e-02,  1.8855e-02,  2.6776e-02,  1.7657e-02,
         3.0721e-02,  2.7562e-02,  2.1572e-02,  2.6644e-02,  3.0325e-02,
         1.3120e-02,  3.1018e-02,  1.9470e-02,  6.2948e-03,  2.2334e-02,
         1.8162e-02,  2.7092e-02,  2.0221e-02,  2.1655e-02,  1.7149e-02,
         1.5312e-02,  2.2306e-02,  2.7528e-02,  2.4620e-02,  1.5500e-02,
         2.7127e-02,  1.9819e-02,  2.2992e-02,  3.0870e-02,  2.0564e-02,
         3.1108e-02,  1.7616e-02,  3.1124e-02,  2.3488e-02,  2.5190e-02,
         3.1192e-02,  9.9461e-03,  1.8798e-02,  1.8366e-02,  2.1269e-02,
         3.0547e-02,  1.9347e-02,  1.7885e-02,  1.3895e-02,  2.8591e-02,
         2.4622e-02,  2.3198e-02,  2.5362e-02,  2.7612e-02,  2.3424e-02,
         2.5207e-02,  2.0237e-02,  2.0693e-02,  2.2609e-02,  1.9085e-02,
         2.5503e-02,  2.9960e-02,  2.3023e-02,  1.8738e-02,  2.8415e-02,
         2.0002e-02,  1.7004e-02,  2.1705e-02,  1.6822e-02,  1.8540e-02,
         2.8078e-02,  9.4577e-03,  2.7551e-02,  2.0283e-02,  2.7757e-02,
         2.3423e-02,  8.0976e-03,  2.7015e-02,  1.1246e-02,  2.8254e-02,
         3.1172e-02,  2.4838e-02,  2.2465e-02,  3.0820e-02,  2.1720e-02,
         2.2312e-02, -1.0029e-05,  2.4233e-02,  2.1722e-02,  2.4074e-02,
         3.1064e-02,  2.7315e-02,  2.7577e-02,  2.4902e-02,  1.3582e-02,
         3.1065e-02,  1.6634e-02,  2.2889e-02,  2.6223e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.8118, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:19,856 :: INFO :: Epoch 10: loss tensor(87.3410, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0463,  0.0425,  0.0588,  0.0500,  0.0353,  0.0419,  0.0484,  0.0453,
         0.0365,  0.0478,  0.0409,  0.0422,  0.0288,  0.0530,  0.0549,  0.0428,
         0.0431,  0.0478,  0.0411,  0.0518,  0.0392,  0.0467,  0.0551,  0.0446,
         0.0481,  0.0349,  0.0354,  0.0464,  0.0656,  0.0443,  0.0520,  0.0521,
         0.0471,  0.0516,  0.0365,  0.0291,  0.0431,  0.0406,  0.0359,  0.0529,
         0.0392,  0.0355,  0.0552,  0.0391,  0.0382,  0.0499,  0.0396,  0.0412,
         0.0490,  0.0398,  0.0556,  0.0427,  0.0372,  0.0333,  0.0587,  0.0556,
         0.0434,  0.0606,  0.0443,  0.0422,  0.0407,  0.0444,  0.0373,  0.0472,
         0.0489,  0.0317,  0.0546,  0.0451,  0.0525,  0.0396,  0.0484,  0.0413,
         0.0424,  0.0311,  0.0507,  0.0407,  0.0463,  0.0407,  0.0412,  0.0411,
         0.0505,  0.0473,  0.0502,  0.0505,  0.0311,  0.0557,  0.0499,  0.0518,
         0.0374,  0.0255,  0.0377,  0.0544,  0.0450,  0.0483,  0.0494,  0.0504,
         0.0502,  0.0528,  0.0429,  0.0418,  0.0380,  0.0329,  0.0480,  0.0242,
         0.0553,  0.0373,  0.0464,  0.0446,  0.0410,  0.0489,  0.0534,  0.0393,
         0.0601,  0.0497,  0.0395,  0.0359,  0.0485,  0.0448,  0.0296,  0.0488,
         0.0518,  0.0433,  0.0367,  0.0369,  0.0330,  0.0337,  0.0445,  0.0514,
         0.0491,  0.0498,  0.0512,  0.0424,  0.0539,  0.0467,  0.0412,  0.0236,
         0.0079,  0.0223,  0.0469,  0.0261,  0.0564,  0.0184,  0.0487,  0.0510,
         0.0550,  0.0539,  0.0359,  0.0410,  0.0557,  0.0509,  0.0102,  0.0571,
         0.0590, -0.0318,  0.0398,  0.0209,  0.0146, -0.0357,  0.0367, -0.0270,
         0.0541, -0.0380,  0.0432,  0.0481,  0.0504,  0.0247,  0.0026,  0.0286,
         0.0335,  0.0471,  0.0588,  0.0498, -0.0363,  0.0519,  0.0626,  0.0084,
         0.0229,  0.0519,  0.0407,  0.0316,  0.0466,  0.0263,  0.0345, -0.0327,
         0.0497,  0.0564, -0.0264,  0.0553,  0.0584, -0.0367,  0.0111,  0.0344,
         0.0520, -0.0217,  0.0431,  0.0521,  0.0536,  0.0172,  0.0606,  0.0468,
         0.0552,  0.0523,  0.0574,  0.0369,  0.0500,  0.0502,  0.0295, -0.0395,
         0.0431,  0.0324,  0.0611,  0.0475,  0.0267,  0.0114,  0.0545,  0.0571,
         0.0337,  0.0017,  0.0543, -0.0336,  0.0531,  0.0480,  0.0560,  0.0598,
         0.0481,  0.0532,  0.0450,  0.0552,  0.0418,  0.0627,  0.0518,  0.0370,
         0.0416,  0.0362,  0.0506,  0.0386,  0.0335,  0.0272,  0.0157,  0.0427,
         0.0427,  0.0428,  0.0398,  0.0553,  0.0206,  0.0450, -0.0300,  0.0481,
         0.0353,  0.0291,  0.0139,  0.0530,  0.0464,  0.0529, -0.0201,  0.0469,
         0.0574,  0.0498,  0.0536,  0.0665,  0.0618,  0.0568,  0.0518,  0.0557,
         0.0605,  0.0552,  0.0637,  0.0660,  0.0588,  0.0657,  0.0507,  0.0564,
         0.0626,  0.0552,  0.0609,  0.0584,  0.0520,  0.0667,  0.0659,  0.0585,
         0.0590,  0.0630,  0.0669,  0.0430,  0.0576,  0.0621,  0.0547,  0.0653,
         0.0642,  0.0562,  0.0574,  0.0572,  0.0576,  0.0616,  0.0535,  0.0644,
         0.0637,  0.0572,  0.0609,  0.0656,  0.0496,  0.0681,  0.0573,  0.0437,
         0.0579,  0.0554,  0.0619,  0.0553,  0.0579,  0.0539,  0.0508,  0.0597,
         0.0645,  0.0605,  0.0520,  0.0629,  0.0542,  0.0604,  0.0671,  0.0570,
         0.0668,  0.0552,  0.0669,  0.0597,  0.0617,  0.0658,  0.0478,  0.0558,
         0.0545,  0.0576,  0.0667,  0.0558,  0.0551,  0.0511,  0.0655,  0.0595,
         0.0572,  0.0619,  0.0633,  0.0588,  0.0617,  0.0569,  0.0574,  0.0581,
         0.0554,  0.0584,  0.0641,  0.0592,  0.0545,  0.0652,  0.0564,  0.0524,
         0.0586,  0.0507,  0.0551,  0.0643,  0.0470,  0.0623,  0.0556,  0.0642,
         0.0595,  0.0460,  0.0640,  0.0495,  0.0643,  0.0663,  0.0610,  0.0586,
         0.0660,  0.0585,  0.0593,  0.0362,  0.0600,  0.0572,  0.0614,  0.0674,
         0.0626,  0.0608,  0.0607,  0.0503,  0.0664,  0.0540,  0.0601,  0.0617],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.5655, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:25,529 :: INFO :: Epoch 15: loss tensor(85.1415, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0683,  0.0563,  0.0901,  0.0726,  0.0514,  0.0711,  0.0728,  0.0646,
         0.0550,  0.0661,  0.0697,  0.0626,  0.0455,  0.0779,  0.0767,  0.0553,
         0.0559,  0.0650,  0.0548,  0.0789,  0.0579,  0.0588,  0.0785,  0.0754,
         0.0717,  0.0475,  0.0458,  0.0722,  0.0973,  0.0585,  0.0751,  0.0686,
         0.0673,  0.0714,  0.0591,  0.0501,  0.0546,  0.0637,  0.0543,  0.0776,
         0.0610,  0.0571,  0.0771,  0.0551,  0.0534,  0.0652,  0.0560,  0.0657,
         0.0752,  0.0512,  0.0764,  0.0576,  0.0517,  0.0425,  0.0892,  0.0809,
         0.0637,  0.0937,  0.0722,  0.0682,  0.0484,  0.0647,  0.0601,  0.0674,
         0.0679,  0.0455,  0.0729,  0.0667,  0.0719,  0.0613,  0.0615,  0.0624,
         0.0526,  0.0536,  0.0695,  0.0608,  0.0654,  0.0673,  0.0531,  0.0586,
         0.0779,  0.0815,  0.0691,  0.0793,  0.0528,  0.0793,  0.0667,  0.0767,
         0.0524,  0.0397,  0.0603,  0.0819,  0.0633,  0.0607,  0.0783,  0.0720,
         0.0645,  0.0729,  0.0670,  0.0612,  0.0595,  0.0398,  0.0679,  0.0464,
         0.0870,  0.0659,  0.0653,  0.0593,  0.0571,  0.0703,  0.0849,  0.0549,
         0.0883,  0.0717,  0.0638,  0.0566,  0.0677,  0.0702,  0.0373,  0.0761,
         0.0755,  0.0650,  0.0578,  0.0510,  0.0510,  0.0525,  0.0710,  0.0760,
         0.0660,  0.0736,  0.0804,  0.0449,  0.0630,  0.0633,  0.0670,  0.0531,
         0.0415,  0.0253,  0.0729,  0.0574,  0.0833,  0.0486,  0.0779,  0.0865,
         0.0862,  0.0742,  0.0414,  0.0782,  0.0871,  0.0862,  0.0409,  0.0874,
         0.0846, -0.0409,  0.0680,  0.0571,  0.0435, -0.0412,  0.0402, -0.0187,
         0.0703, -0.0452,  0.0562,  0.0782,  0.0638,  0.0237,  0.0283,  0.0588,
         0.0665,  0.0722,  0.0850,  0.0667, -0.0412,  0.0799,  0.0924,  0.0401,
         0.0266,  0.0738,  0.0690,  0.0576,  0.0705,  0.0602,  0.0379, -0.0342,
         0.0686,  0.0835, -0.0241,  0.0820,  0.0862, -0.0455,  0.0459,  0.0651,
         0.0675, -0.0106,  0.0483,  0.0743,  0.0868,  0.0184,  0.0868,  0.0797,
         0.0836,  0.0861,  0.0806,  0.0425,  0.0782,  0.0851,  0.0598, -0.0497,
         0.0490,  0.0602,  0.0908,  0.0599,  0.0311,  0.0397,  0.0796,  0.0898,
         0.0625,  0.0285,  0.0765, -0.0379,  0.0817,  0.0769,  0.0775,  0.0895,
         0.0576,  0.0821,  0.0623,  0.0876,  0.0528,  0.0937,  0.0730,  0.0395,
         0.0491,  0.0406,  0.0650,  0.0751,  0.0655,  0.0563,  0.0185,  0.0553,
         0.0675,  0.0746,  0.0749,  0.0768,  0.0520,  0.0733, -0.0276,  0.0776,
         0.0644,  0.0610,  0.0488,  0.0761,  0.0801,  0.0876, -0.0012,  0.0728,
         0.0964,  0.0859,  0.0935,  0.0991,  0.0929,  0.0928,  0.0884,  0.0917,
         0.0979,  0.0877,  0.0984,  0.1009,  0.0919,  0.0974,  0.0817,  0.0927,
         0.0968,  0.0894,  0.0947,  0.0929,  0.0838,  0.1020,  0.0978,  0.0929,
         0.0943,  0.1018,  0.1012,  0.0809,  0.0970,  0.0919,  0.0912,  0.1010,
         0.0918,  0.0927,  0.0912,  0.0932,  0.0978,  0.0926,  0.0862,  0.0932,
         0.0982,  0.0905,  0.0901,  0.0981,  0.0872,  0.1060,  0.0962,  0.0828,
         0.0921,  0.0936,  0.0920,  0.0869,  0.0917,  0.0903,  0.0850,  0.0973,
         0.1018,  0.0949,  0.0869,  0.0964,  0.0837,  0.0989,  0.1022,  0.0939,
         0.1004,  0.0931,  0.1009,  0.0948,  0.0975,  0.0976,  0.0867,  0.0924,
         0.0881,  0.0921,  0.1015,  0.0906,  0.0918,  0.0874,  0.1029,  0.0904,
         0.0837,  0.0972,  0.0979,  0.0901,  0.0968,  0.0923,  0.0926,  0.0924,
         0.0895,  0.0830,  0.0921,  0.0949,  0.0872,  0.1015,  0.0908,  0.0858,
         0.0945,  0.0765,  0.0909,  0.0987,  0.0853,  0.0933,  0.0876,  0.0992,
         0.0940,  0.0860,  0.1007,  0.0894,  0.0979,  0.0985,  0.0959,  0.0938,
         0.0985,  0.0945,  0.0958,  0.0764,  0.0922,  0.0895,  0.1000,  0.1042,
         0.0940,  0.0865,  0.0937,  0.0879,  0.0997,  0.0908,  0.0973,  0.0939],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.9383, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:31,185 :: INFO :: Epoch 20: loss tensor(84.0069, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0795,  0.0590,  0.1178,  0.0857,  0.0586,  0.0922,  0.0856,  0.0765,
         0.0634,  0.0745,  0.0906,  0.0758,  0.0538,  0.0922,  0.0890,  0.0607,
         0.0588,  0.0715,  0.0577,  0.0992,  0.0678,  0.0614,  0.0939,  0.0982,
         0.0903,  0.0502,  0.0470,  0.0903,  0.1232,  0.0628,  0.0907,  0.0752,
         0.0772,  0.0803,  0.0702,  0.0599,  0.0559,  0.0800,  0.0630,  0.0921,
         0.0727,  0.0683,  0.0884,  0.0635,  0.0575,  0.0676,  0.0623,  0.0822,
         0.0924,  0.0539,  0.0847,  0.0603,  0.0590,  0.0421,  0.1126,  0.0986,
         0.0730,  0.1251,  0.0896,  0.0830,  0.0475,  0.0756,  0.0714,  0.0768,
         0.0755,  0.0494,  0.0807,  0.0781,  0.0831,  0.0717,  0.0645,  0.0726,
         0.0541,  0.0718,  0.0789,  0.0682,  0.0736,  0.0856,  0.0544,  0.0653,
         0.0991,  0.1097,  0.0788,  0.1013,  0.0611,  0.0911,  0.0739,  0.0907,
         0.0572,  0.0429,  0.0739,  0.1042,  0.0731,  0.0628,  0.0971,  0.0816,
         0.0687,  0.0802,  0.0818,  0.0704,  0.0688,  0.0410,  0.0772,  0.0610,
         0.1117,  0.0859,  0.0741,  0.0619,  0.0636,  0.0832,  0.1110,  0.0613,
         0.1083,  0.0833,  0.0775,  0.0672,  0.0752,  0.0887,  0.0375,  0.0959,
         0.0899,  0.0771,  0.0684,  0.0540,  0.0596,  0.0650,  0.0886,  0.0925,
         0.0728,  0.0884,  0.1033,  0.0407,  0.0602,  0.0696,  0.0840,  0.0763,
         0.0733,  0.0259,  0.0891,  0.0808,  0.0999,  0.0728,  0.0974,  0.1193,
         0.1107,  0.0852,  0.0401,  0.1118,  0.1120,  0.1152,  0.0678,  0.1099,
         0.1012, -0.0472,  0.0859,  0.0896,  0.0677, -0.0420,  0.0379, -0.0040,
         0.0751, -0.0459,  0.0618,  0.1007,  0.0682,  0.0226,  0.0493,  0.0820,
         0.0931,  0.0874,  0.1011,  0.0730, -0.0413,  0.1013,  0.1149,  0.0703,
         0.0240,  0.0856,  0.0883,  0.0764,  0.0861,  0.0889,  0.0364, -0.0308,
         0.0770,  0.1020, -0.0171,  0.0961,  0.1046, -0.0508,  0.0795,  0.0890,
         0.0742,  0.0050,  0.0456,  0.0872,  0.1133,  0.0165,  0.1029,  0.1060,
         0.1058,  0.1126,  0.0950,  0.0412,  0.0979,  0.1143,  0.0822, -0.0556,
         0.0470,  0.0800,  0.1105,  0.0630,  0.0281,  0.0622,  0.0936,  0.1172,
         0.0841,  0.0509,  0.0901, -0.0377,  0.1013,  0.0955,  0.0881,  0.1127,
         0.0586,  0.1018,  0.0679,  0.1143,  0.0513,  0.1173,  0.0833,  0.0371,
         0.0506,  0.0378,  0.0689,  0.1069,  0.0905,  0.0788,  0.0165,  0.0594,
         0.0821,  0.1004,  0.1056,  0.0898,  0.0786,  0.0957, -0.0192,  0.0993,
         0.0861,  0.0868,  0.0831,  0.0932,  0.1078,  0.1171,  0.0226,  0.0895,
         0.1343,  0.1196,  0.1326,  0.1254,  0.1201,  0.1275,  0.1208,  0.1233,
         0.1345,  0.1175,  0.1298,  0.1317,  0.1187,  0.1214,  0.1061,  0.1252,
         0.1287,  0.1193,  0.1226,  0.1233,  0.1133,  0.1338,  0.1242,  0.1214,
         0.1249,  0.1404,  0.1300,  0.1172,  0.1359,  0.1122,  0.1239,  0.1329,
         0.1093,  0.1259,  0.1210,  0.1253,  0.1379,  0.1162,  0.1117,  0.1153,
         0.1274,  0.1201,  0.1091,  0.1240,  0.1235,  0.1431,  0.1346,  0.1216,
         0.1243,  0.1305,  0.1130,  0.1145,  0.1196,  0.1243,  0.1174,  0.1341,
         0.1371,  0.1265,  0.1191,  0.1236,  0.1068,  0.1370,  0.1329,  0.1289,
         0.1288,  0.1304,  0.1299,  0.1256,  0.1309,  0.1232,  0.1242,  0.1271,
         0.1160,  0.1218,  0.1319,  0.1211,  0.1247,  0.1192,  0.1385,  0.1123,
         0.0986,  0.1285,  0.1311,  0.1126,  0.1270,  0.1227,  0.1232,  0.1238,
         0.1178,  0.0966,  0.1106,  0.1272,  0.1155,  0.1342,  0.1195,  0.1141,
         0.1267,  0.0912,  0.1237,  0.1281,  0.1224,  0.1206,  0.1127,  0.1300,
         0.1260,  0.1253,  0.1363,  0.1291,  0.1281,  0.1245,  0.1270,  0.1270,
         0.1253,  0.1263,  0.1286,  0.1157,  0.1164,  0.1149,  0.1377,  0.1400,
         0.1177,  0.1002,  0.1201,  0.1241,  0.1279,  0.1238,  0.1320,  0.1202],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.9106, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:36,843 :: INFO :: Epoch 25: loss tensor(82.7213, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0820,  0.0583,  0.1404,  0.0917,  0.0604,  0.1072,  0.0903,  0.0807,
         0.0683,  0.0761,  0.1042,  0.0814,  0.0578,  0.0985,  0.0942,  0.0619,
         0.0583,  0.0732,  0.0569,  0.1124,  0.0715,  0.0597,  0.1007,  0.1124,
         0.1006,  0.0494,  0.0443,  0.1002,  0.1425,  0.0618,  0.0988,  0.0756,
         0.0807,  0.0817,  0.0743,  0.0625,  0.0539,  0.0909,  0.0661,  0.0994,
         0.0783,  0.0735,  0.0944,  0.0654,  0.0578,  0.0660,  0.0627,  0.0929,
         0.1025,  0.0535,  0.0846,  0.0576,  0.0616,  0.0403,  0.1284,  0.1076,
         0.0766,  0.1518,  0.0993,  0.0914,  0.0458,  0.0801,  0.0761,  0.0784,
         0.0761,  0.0487,  0.0840,  0.0824,  0.0878,  0.0756,  0.0622,  0.0759,
         0.0505,  0.0854,  0.0808,  0.0679,  0.0748,  0.0962,  0.0517,  0.0667,
         0.1114,  0.1317,  0.0821,  0.1158,  0.0621,  0.0947,  0.0735,  0.0956,
         0.0577,  0.0432,  0.0808,  0.1200,  0.0753,  0.0610,  0.1076,  0.0843,
         0.0677,  0.0798,  0.0908,  0.0734,  0.0731,  0.0379,  0.0777,  0.0705,
         0.1290,  0.0991,  0.0754,  0.0591,  0.0649,  0.0892,  0.1311,  0.0625,
         0.1198,  0.0874,  0.0838,  0.0710,  0.0774,  0.1037,  0.0361,  0.1084,
         0.0953,  0.0808,  0.0733,  0.0549,  0.0615,  0.0723,  0.0985,  0.1017,
         0.0740,  0.0952,  0.1184,  0.0350,  0.0532,  0.0718,  0.0919,  0.0912,
         0.0987,  0.0250,  0.0963,  0.0955,  0.1053,  0.0897,  0.1063,  0.1474,
         0.1273,  0.0884,  0.0361,  0.1387,  0.1279,  0.1364,  0.0875,  0.1231,
         0.1077, -0.0524,  0.0935,  0.1160,  0.0848, -0.0409,  0.0360,  0.0103,
         0.0736, -0.0435,  0.0633,  0.1146,  0.0679,  0.0192,  0.0632,  0.0969,
         0.1119,  0.0935,  0.1078,  0.0720, -0.0393,  0.1149,  0.1293,  0.0939,
         0.0237,  0.0879,  0.0973,  0.0889,  0.0931,  0.1090,  0.0346, -0.0256,
         0.0772,  0.1113, -0.0096,  0.0983,  0.1139, -0.0540,  0.1061,  0.1041,
         0.0745,  0.0184,  0.0397,  0.0922,  0.1312,  0.0152,  0.1095,  0.1237,
         0.1200,  0.1304,  0.1007,  0.0359,  0.1066,  0.1357,  0.0959, -0.0590,
         0.0429,  0.0914,  0.1182,  0.0606,  0.0245,  0.0761,  0.0965,  0.1371,
         0.0967,  0.0670,  0.0948, -0.0355,  0.1120,  0.1035,  0.0912,  0.1289,
         0.0550,  0.1115,  0.0662,  0.1337,  0.0463,  0.1326,  0.0830,  0.0326,
         0.0491,  0.0347,  0.0663,  0.1308,  0.1068,  0.0939,  0.0154,  0.0604,
         0.0866,  0.1185,  0.1300,  0.0961,  0.0978,  0.1113, -0.0098,  0.1110,
         0.0995,  0.1057,  0.1089,  0.1034,  0.1278,  0.1394,  0.0437,  0.0957,
         0.1698,  0.1505,  0.1691,  0.1449,  0.1443,  0.1589,  0.1487,  0.1501,
         0.1690,  0.1452,  0.1590,  0.1585,  0.1390,  0.1378,  0.1242,  0.1535,
         0.1581,  0.1460,  0.1446,  0.1499,  0.1411,  0.1622,  0.1450,  0.1439,
         0.1505,  0.1765,  0.1530,  0.1514,  0.1728,  0.1235,  0.1524,  0.1611,
         0.1186,  0.1555,  0.1485,  0.1528,  0.1760,  0.1335,  0.1308,  0.1325,
         0.1512,  0.1471,  0.1178,  0.1440,  0.1575,  0.1775,  0.1711,  0.1577,
         0.1547,  0.1653,  0.1247,  0.1397,  0.1418,  0.1547,  0.1475,  0.1691,
         0.1706,  0.1557,  0.1487,  0.1446,  0.1253,  0.1728,  0.1590,  0.1616,
         0.1517,  0.1659,  0.1536,  0.1525,  0.1622,  0.1432,  0.1596,  0.1598,
         0.1387,  0.1462,  0.1574,  0.1475,  0.1538,  0.1461,  0.1717,  0.1248,
         0.1052,  0.1552,  0.1619,  0.1260,  0.1522,  0.1482,  0.1491,  0.1515,
         0.1412,  0.1039,  0.1225,  0.1563,  0.1409,  0.1633,  0.1429,  0.1386,
         0.1544,  0.0981,  0.1536,  0.1520,  0.1573,  0.1454,  0.1316,  0.1562,
         0.1555,  0.1629,  0.1690,  0.1667,  0.1555,  0.1448,  0.1541,  0.1575,
         0.1468,  0.1538,  0.1576,  0.1526,  0.1324,  0.1333,  0.1736,  0.1731,
         0.1337,  0.1031,  0.1392,  0.1590,  0.1503,  0.1530,  0.1636,  0.1400],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.5443, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:42,499 :: INFO :: Epoch 30: loss tensor(81.0970, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 8.1467e-02,  5.7159e-02,  1.5887e-01,  9.1907e-02,  5.9753e-02,
         1.1637e-01,  9.0545e-02,  8.0328e-02,  7.1856e-02,  7.4485e-02,
         1.1069e-01,  8.2721e-02,  5.9924e-02,  9.9797e-02,  9.4796e-02,
         6.2475e-02,  5.6167e-02,  7.2266e-02,  5.5186e-02,  1.2071e-01,
         7.1466e-02,  5.6533e-02,  1.0279e-01,  1.2049e-01,  1.0709e-01,
         4.7490e-02,  4.0847e-02,  1.0416e-01,  1.5642e-01,  5.9672e-02,
         1.0327e-01,  7.3204e-02,  8.2155e-02,  8.0731e-02,  7.4735e-02,
         6.2369e-02,  5.0887e-02,  9.7661e-02,  6.6191e-02,  1.0190e-01,
         8.1947e-02,  7.4826e-02,  9.6538e-02,  6.5111e-02,  5.6809e-02,
         6.1454e-02,  6.1691e-02,  1.0038e-01,  1.0565e-01,  5.1802e-02,
         7.9705e-02,  5.3661e-02,  6.2254e-02,  3.8224e-02,  1.3738e-01,
         1.1088e-01,  7.6182e-02,  1.7578e-01,  1.0253e-01,  9.3468e-02,
         4.3046e-02,  8.0761e-02,  7.6892e-02,  7.7558e-02,  7.4457e-02,
         4.7004e-02,  8.2365e-02,  8.2587e-02,  8.8892e-02,  7.5495e-02,
         6.0376e-02,  7.5997e-02,  4.6412e-02,  9.6337e-02,  7.9921e-02,
         6.5620e-02,  7.3531e-02,  1.0247e-01,  4.7147e-02,  6.4855e-02,
         1.1904e-01,  1.4777e-01,  8.2094e-02,  1.2576e-01,  6.0542e-02,
         9.3756e-02,  7.0081e-02,  9.6522e-02,  5.6625e-02,  4.2381e-02,
         8.2275e-02,  1.3014e-01,  7.5729e-02,  5.8002e-02,  1.1092e-01,
         8.3822e-02,  6.4876e-02,  7.6571e-02,  9.6339e-02,  7.3736e-02,
         7.2336e-02,  3.5026e-02,  7.6260e-02,  7.7343e-02,  1.4088e-01,
         1.0504e-01,  7.3574e-02,  5.4779e-02,  6.4693e-02,  9.1065e-02,
         1.4566e-01,  6.2775e-02,  1.2484e-01,  8.9513e-02,  8.4966e-02,
         7.0346e-02,  7.7171e-02,  1.1651e-01,  3.4510e-02,  1.1660e-01,
         9.4924e-02,  8.1061e-02,  7.4761e-02,  5.4367e-02,  5.9851e-02,
         7.6668e-02,  1.0405e-01,  1.0577e-01,  7.2733e-02,  9.7134e-02,
         1.2658e-01,  2.9310e-02,  4.8156e-02,  7.0979e-02,  9.3528e-02,
         9.9750e-02,  1.1960e-01,  2.6558e-02,  9.7638e-02,  1.0262e-01,
         1.0229e-01,  1.0097e-01,  1.0629e-01,  1.7130e-01,  1.3640e-01,
         8.5800e-02,  3.2352e-02,  1.6044e-01,  1.3581e-01,  1.4970e-01,
         1.0174e-01,  1.2886e-01,  1.0594e-01, -5.6238e-02,  9.3765e-02,
         1.3769e-01,  9.5603e-02, -3.8478e-02,  3.3553e-02,  2.2762e-02,
         6.9310e-02, -3.9657e-02,  6.3589e-02,  1.2131e-01,  6.6092e-02,
         1.6905e-02,  7.2031e-02,  1.0500e-01,  1.2406e-01,  9.3382e-02,
         1.0826e-01,  6.8101e-02, -3.6135e-02,  1.2133e-01,  1.3633e-01,
         1.1324e-01,  2.0637e-02,  8.4308e-02,  9.8686e-02,  9.7071e-02,
         9.3102e-02,  1.2354e-01,  2.8932e-02, -1.9921e-02,  7.2431e-02,
         1.1438e-01, -2.3950e-03,  9.4398e-02,  1.1633e-01, -5.5755e-02,
         1.2805e-01,  1.1177e-01,  7.0974e-02,  2.9458e-02,  3.6350e-02,
         9.2580e-02,  1.4168e-01,  1.3849e-02,  1.0893e-01,  1.3371e-01,
         1.2716e-01,  1.3935e-01,  1.0072e-01,  3.1016e-02,  1.0656e-01,
         1.4972e-01,  1.0238e-01, -6.0875e-02,  3.8444e-02,  9.6652e-02,
         1.1618e-01,  5.5679e-02,  2.0113e-02,  8.3846e-02,  9.2122e-02,
         1.5007e-01,  1.0202e-01,  7.7912e-02,  9.2383e-02, -3.2151e-02,
         1.1572e-01,  1.0383e-01,  9.0791e-02,  1.3944e-01,  5.0104e-02,
         1.1343e-01,  6.1660e-02,  1.4727e-01,  4.1046e-02,  1.3979e-01,
         7.6779e-02,  2.8252e-02,  4.8162e-02,  3.0546e-02,  6.0953e-02,
         1.4798e-01,  1.1575e-01,  1.0293e-01,  1.4508e-02,  5.7561e-02,
         8.4357e-02,  1.2973e-01,  1.4907e-01,  9.9083e-02,  1.1177e-01,
         1.2140e-01, -7.6279e-07,  1.1447e-01,  1.0660e-01,  1.1910e-01,
         1.2937e-01,  1.0857e-01,  1.4054e-01,  1.5494e-01,  6.1183e-02,
         9.4807e-02,  2.0183e-01,  1.7833e-01,  2.0306e-01,  1.5753e-01,
         1.6506e-01,  1.8613e-01,  1.7186e-01,  1.7176e-01,  2.0094e-01,
         1.6875e-01,  1.8501e-01,  1.8157e-01,  1.5354e-01,  1.4670e-01,
         1.3800e-01,  1.7762e-01,  1.8403e-01,  1.6899e-01,  1.5967e-01,
         1.7224e-01,  1.6473e-01,  1.8614e-01,  1.6122e-01,  1.5977e-01,
         1.7098e-01,  2.0883e-01,  1.7017e-01,  1.8303e-01,  2.0700e-01,
         1.2673e-01,  1.7610e-01,  1.8436e-01,  1.2292e-01,  1.8086e-01,
         1.7273e-01,  1.7565e-01,  2.1145e-01,  1.4410e-01,  1.4323e-01,
         1.4542e-01,  1.6892e-01,  1.7113e-01,  1.1860e-01,  1.5752e-01,
         1.8938e-01,  2.0897e-01,  2.0538e-01,  1.9093e-01,  1.8151e-01,
         1.9743e-01,  1.2760e-01,  1.6293e-01,  1.5784e-01,  1.8113e-01,
         1.7490e-01,  2.0083e-01,  2.0102e-01,  1.8111e-01,  1.7505e-01,
         1.5867e-01,  1.3917e-01,  2.0571e-01,  1.7981e-01,  1.9220e-01,
         1.6837e-01,  1.9827e-01,  1.7150e-01,  1.7532e-01,  1.9006e-01,
         1.5759e-01,  1.9262e-01,  1.8946e-01,  1.5665e-01,  1.6565e-01,
         1.7769e-01,  1.6962e-01,  1.7856e-01,  1.6761e-01,  2.0163e-01,
         1.2816e-01,  1.0545e-01,  1.7701e-01,  1.8886e-01,  1.2978e-01,
         1.7196e-01,  1.6839e-01,  1.6935e-01,  1.7449e-01,  1.5881e-01,
         1.0791e-01,  1.2954e-01,  1.8267e-01,  1.6234e-01,  1.8806e-01,
         1.6009e-01,  1.5939e-01,  1.7732e-01,  1.0083e-01,  1.7914e-01,
         1.6999e-01,  1.8976e-01,  1.6699e-01,  1.4400e-01,  1.7743e-01,
         1.8258e-01,  1.9786e-01,  1.9821e-01,  2.0092e-01,  1.7916e-01,
         1.5837e-01,  1.7706e-01,  1.8609e-01,  1.6305e-01,  1.7650e-01,
         1.8197e-01,  1.8667e-01,  1.3918e-01,  1.4536e-01,  2.0592e-01,
         2.0299e-01,  1.4188e-01,  1.0132e-01,  1.5026e-01,  1.9168e-01,
         1.6682e-01,  1.7796e-01,  1.9195e-01,  1.5522e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(13.8918, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:48,125 :: INFO :: Epoch 35: loss tensor(82.2476, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0786,  0.0574,  0.1741,  0.0898,  0.0582,  0.1214,  0.0905,  0.0785,
         0.0754,  0.0726,  0.1147,  0.0826,  0.0611,  0.1002,  0.0947,  0.0623,
         0.0553,  0.0725,  0.0538,  0.1267,  0.0711,  0.0544,  0.1023,  0.1242,
         0.1124,  0.0466,  0.0389,  0.1059,  0.1660,  0.0577,  0.1064,  0.0710,
         0.0829,  0.0791,  0.0749,  0.0627,  0.0489,  0.1009,  0.0653,  0.1032,
         0.0840,  0.0754,  0.0958,  0.0636,  0.0553,  0.0584,  0.0606,  0.1054,
         0.1078,  0.0523,  0.0758,  0.0509,  0.0627,  0.0366,  0.1426,  0.1127,
         0.0757,  0.1979,  0.1036,  0.0936,  0.0419,  0.0805,  0.0772,  0.0767,
         0.0730,  0.0446,  0.0807,  0.0816,  0.0877,  0.0744,  0.0592,  0.0749,
         0.0443,  0.1047,  0.0769,  0.0644,  0.0728,  0.1069,  0.0448,  0.0637,
         0.1222,  0.1592,  0.0803,  0.1338,  0.0592,  0.0924,  0.0667,  0.0945,
         0.0557,  0.0418,  0.0832,  0.1360,  0.0764,  0.0553,  0.1117,  0.0831,
         0.0629,  0.0749,  0.0989,  0.0754,  0.0705,  0.0329,  0.0724,  0.0818,
         0.1485,  0.1100,  0.0705,  0.0516,  0.0637,  0.0890,  0.1579,  0.0634,
         0.1267,  0.0910,  0.0844,  0.0684,  0.0769,  0.1275,  0.0344,  0.1206,
         0.0934,  0.0792,  0.0750,  0.0556,  0.0580,  0.0809,  0.1059,  0.1068,
         0.0699,  0.0948,  0.1282,  0.0247,  0.0424,  0.0672,  0.0907,  0.1023,
         0.1332,  0.0245,  0.0949,  0.1031,  0.0946,  0.1058,  0.1000,  0.1899,
         0.1386,  0.0801,  0.0273,  0.1750,  0.1358,  0.1548,  0.1089,  0.1296,
         0.0990, -0.0595,  0.0887,  0.1532,  0.1000, -0.0364,  0.0299,  0.0320,
         0.0631, -0.0353,  0.0612,  0.1207,  0.0628,  0.0154,  0.0747,  0.1072,
         0.1289,  0.0893,  0.1037,  0.0631, -0.0333,  0.1221,  0.1372,  0.1256,
         0.0188,  0.0775,  0.0940,  0.1011,  0.0893,  0.1303,  0.0237, -0.0156,
         0.0653,  0.1126,  0.0028,  0.0854,  0.1139, -0.0571,  0.1425,  0.1124,
         0.0661,  0.0363,  0.0327,  0.0890,  0.1441,  0.0123,  0.1037,  0.1359,
         0.1283,  0.1396,  0.0967,  0.0263,  0.1002,  0.1560,  0.1024, -0.0621,
         0.0334,  0.0962,  0.1078,  0.0495,  0.0166,  0.0848,  0.0837,  0.1568,
         0.1007,  0.0826,  0.0865, -0.0292,  0.1135,  0.0993,  0.0866,  0.1446,
         0.0446,  0.1098,  0.0557,  0.1544,  0.0353,  0.1397,  0.0678,  0.0249,
         0.0459,  0.0258,  0.0551,  0.1570,  0.1172,  0.1059,  0.0136,  0.0519,
         0.0778,  0.1341,  0.1617,  0.0980,  0.1190,  0.1261,  0.0080,  0.1114,
         0.1075,  0.1255,  0.1419,  0.1101,  0.1462,  0.1634,  0.0727,  0.0891,
         0.2308,  0.2026,  0.2336,  0.1644,  0.1824,  0.2089,  0.1907,  0.1890,
         0.2306,  0.1888,  0.2079,  0.2011,  0.1628,  0.1481,  0.1468,  0.1982,
         0.2068,  0.1881,  0.1687,  0.1908,  0.1836,  0.2066,  0.1724,  0.1706,
         0.1864,  0.2362,  0.1818,  0.2123,  0.2376,  0.1225,  0.1956,  0.2035,
         0.1230,  0.2027,  0.1944,  0.1939,  0.2435,  0.1509,  0.1502,  0.1560,
         0.1812,  0.1928,  0.1128,  0.1657,  0.2182,  0.2365,  0.2369,  0.2204,
         0.2051,  0.2264,  0.1234,  0.1831,  0.1677,  0.2038,  0.1997,  0.2290,
         0.2285,  0.2041,  0.1991,  0.1668,  0.1498,  0.2349,  0.1960,  0.2195,
         0.1796,  0.2284,  0.1836,  0.1946,  0.2152,  0.1673,  0.2222,  0.2165,
         0.1687,  0.1801,  0.1928,  0.1881,  0.1997,  0.1840,  0.2285,  0.1248,
         0.1035,  0.1944,  0.2121,  0.1269,  0.1869,  0.1838,  0.1851,  0.1930,
         0.1724,  0.1126,  0.1340,  0.2060,  0.1801,  0.2090,  0.1717,  0.1770,
         0.1957,  0.1004,  0.2002,  0.1824,  0.2184,  0.1847,  0.1509,  0.1945,
         0.2060,  0.2296,  0.2222,  0.2322,  0.1996,  0.1663,  0.1962,  0.2112,
         0.1736,  0.1950,  0.2023,  0.2174,  0.1397,  0.1527,  0.2350,  0.2287,
         0.1428,  0.0947,  0.1543,  0.2215,  0.1780,  0.1990,  0.2170,  0.1647],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.9902, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:53,673 :: INFO :: Epoch 40: loss tensor(78.6838, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0751,  0.0572,  0.1867,  0.0881,  0.0552,  0.1238,  0.0903,  0.0750,
         0.0801,  0.0705,  0.1172,  0.0817,  0.0607,  0.1002,  0.0950,  0.0612,
         0.0539,  0.0736,  0.0525,  0.1303,  0.0694,  0.0534,  0.1010,  0.1267,
         0.1149,  0.0448,  0.0360,  0.1062,  0.1738,  0.0555,  0.1076,  0.0704,
         0.0847,  0.0775,  0.0764,  0.0622,  0.0465,  0.1008,  0.0626,  0.1040,
         0.0844,  0.0758,  0.0947,  0.0603,  0.0530,  0.0556,  0.0598,  0.1095,
         0.1092,  0.0516,  0.0738,  0.0470,  0.0621,  0.0353,  0.1455,  0.1126,
         0.0741,  0.2184,  0.1042,  0.0939,  0.0409,  0.0786,  0.0786,  0.0753,
         0.0726,  0.0422,  0.0812,  0.0819,  0.0847,  0.0726,  0.0595,  0.0762,
         0.0421,  0.1104,  0.0731,  0.0631,  0.0716,  0.1100,  0.0426,  0.0627,
         0.1239,  0.1675,  0.0772,  0.1386,  0.0594,  0.0911,  0.0638,  0.0935,
         0.0539,  0.0418,  0.0832,  0.1410,  0.0740,  0.0541,  0.1101,  0.0831,
         0.0601,  0.0735,  0.0995,  0.0759,  0.0697,  0.0311,  0.0676,  0.0836,
         0.1537,  0.1122,  0.0671,  0.0478,  0.0617,  0.0867,  0.1667,  0.0625,
         0.1282,  0.0925,  0.0831,  0.0656,  0.0769,  0.1362,  0.0354,  0.1239,
         0.0910,  0.0773,  0.0731,  0.0574,  0.0543,  0.0832,  0.1050,  0.1072,
         0.0661,  0.0899,  0.1256,  0.0214,  0.0380,  0.0626,  0.0848,  0.1004,
         0.1421,  0.0250,  0.0894,  0.0999,  0.0855,  0.1062,  0.0914,  0.2032,
         0.1363,  0.0726,  0.0245,  0.1848,  0.1318,  0.1552,  0.1113,  0.1262,
         0.0900, -0.0624,  0.0810,  0.1644,  0.1008, -0.0347,  0.0265,  0.0367,
         0.0558, -0.0316,  0.0592,  0.1163,  0.0586,  0.0142,  0.0731,  0.1052,
         0.1291,  0.0831,  0.0990,  0.0576, -0.0315,  0.1190,  0.1351,  0.1333,
         0.0160,  0.0693,  0.0862,  0.1009,  0.0833,  0.1327,  0.0204, -0.0128,
         0.0571,  0.1085,  0.0062,  0.0773,  0.1107, -0.0584,  0.1502,  0.1086,
         0.0597,  0.0399,  0.0295,  0.0841,  0.1411,  0.0099,  0.0968,  0.1329,
         0.1254,  0.1345,  0.0907,  0.0220,  0.0911,  0.1559,  0.0988, -0.0634,
         0.0276,  0.0920,  0.0966,  0.0432,  0.0136,  0.0818,  0.0750,  0.1594,
         0.0950,  0.0827,  0.0786, -0.0268,  0.1067,  0.0934,  0.0804,  0.1446,
         0.0409,  0.1031,  0.0491,  0.1574,  0.0309,  0.1353,  0.0585,  0.0215,
         0.0411,  0.0240,  0.0486,  0.1602,  0.1144,  0.1046,  0.0120,  0.0484,
         0.0701,  0.1335,  0.1695,  0.0957,  0.1219,  0.1266,  0.0142,  0.1052,
         0.1044,  0.1277,  0.1493,  0.1092,  0.1462,  0.1669,  0.0804,  0.0811,
         0.2570,  0.2228,  0.2609,  0.1687,  0.1974,  0.2273,  0.2068,  0.2028,
         0.2561,  0.2041,  0.2283,  0.2179,  0.1686,  0.1474,  0.1531,  0.2161,
         0.2244,  0.2039,  0.1734,  0.2058,  0.1983,  0.2250,  0.1809,  0.1772,
         0.1989,  0.2585,  0.1896,  0.2384,  0.2651,  0.1155,  0.2118,  0.2192,
         0.1228,  0.2217,  0.2136,  0.2091,  0.2719,  0.1552,  0.1530,  0.1655,
         0.1886,  0.2107,  0.1069,  0.1711,  0.2447,  0.2597,  0.2642,  0.2453,
         0.2258,  0.2514,  0.1193,  0.2002,  0.1735,  0.2225,  0.2201,  0.2534,
         0.2537,  0.2235,  0.2194,  0.1708,  0.1576,  0.2600,  0.2089,  0.2438,
         0.1874,  0.2539,  0.1925,  0.2104,  0.2369,  0.1735,  0.2487,  0.2413,
         0.1796,  0.1919,  0.2049,  0.2036,  0.2179,  0.1967,  0.2535,  0.1183,
         0.1029,  0.2083,  0.2297,  0.1195,  0.1975,  0.1954,  0.1976,  0.2075,
         0.1824,  0.1218,  0.1375,  0.2262,  0.1951,  0.2269,  0.1797,  0.1921,
         0.2105,  0.1010,  0.2179,  0.1904,  0.2435,  0.1986,  0.1570,  0.2081,
         0.2259,  0.2580,  0.2408,  0.2599,  0.2158,  0.1712,  0.2124,  0.2335,
         0.1827,  0.2103,  0.2194,  0.2450,  0.1363,  0.1576,  0.2602,  0.2500,
         0.1407,  0.0916,  0.1533,  0.2481,  0.1860,  0.2170,  0.2393,  0.1712],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.9044, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:57:53,673 :: INFO :: ----- frontend -----
2023-05-14 11:57:53,673 :: INFO :: Environment 0
2023-05-14 11:58:02,955 :: INFO :: Epoch 5: loss tensor(881.4721, device='cuda:0'), U.norm 13.15874195098877, V.norm 17.00807762145996, MLP.norm 2.157327175140381
2023-05-14 11:58:03,189 :: INFO :: Epoch 10: loss tensor(854.0940, device='cuda:0'), U.norm 10.350268363952637, V.norm 16.361806869506836, MLP.norm 3.687227725982666
2023-05-14 11:58:03,408 :: INFO :: Epoch 15: loss tensor(816.1046, device='cuda:0'), U.norm 8.652461051940918, V.norm 16.047555923461914, MLP.norm 5.484522819519043
2023-05-14 11:58:03,642 :: INFO :: Epoch 20: loss tensor(774.3674, device='cuda:0'), U.norm 7.496087551116943, V.norm 15.83730411529541, MLP.norm 7.369174480438232
2023-05-14 11:58:03,658 :: INFO :: Environment 1
2023-05-14 11:58:12,815 :: INFO :: Epoch 5: loss tensor(861.6689, device='cuda:0'), U.norm 13.150662422180176, V.norm 16.98006248474121, MLP.norm 2.1826107501983643
2023-05-14 11:58:13,050 :: INFO :: Epoch 10: loss tensor(833.0711, device='cuda:0'), U.norm 10.334940910339355, V.norm 16.3210391998291, MLP.norm 3.7296195030212402
2023-05-14 11:58:13,284 :: INFO :: Epoch 15: loss tensor(795.8115, device='cuda:0'), U.norm 8.628238677978516, V.norm 15.999728202819824, MLP.norm 5.540801525115967
2023-05-14 11:58:13,503 :: INFO :: Epoch 20: loss tensor(756.0594, device='cuda:0'), U.norm 7.460814952850342, V.norm 15.783926963806152, MLP.norm 7.39212703704834
2023-05-14 11:58:13,519 :: INFO :: Environment 2
2023-05-14 11:58:22,660 :: INFO :: Epoch 5: loss tensor(853.4083, device='cuda:0'), U.norm 13.157573699951172, V.norm 16.963350296020508, MLP.norm 2.150998830795288
2023-05-14 11:58:22,879 :: INFO :: Epoch 10: loss tensor(827.2849, device='cuda:0'), U.norm 10.347888946533203, V.norm 16.300973892211914, MLP.norm 3.6608660221099854
2023-05-14 11:58:23,113 :: INFO :: Epoch 15: loss tensor(790.5398, device='cuda:0'), U.norm 8.648810386657715, V.norm 15.981082916259766, MLP.norm 5.475324630737305
2023-05-14 11:58:23,348 :: INFO :: Epoch 20: loss tensor(751.0878, device='cuda:0'), U.norm 7.490684986114502, V.norm 15.768373489379883, MLP.norm 7.3728227615356445
2023-05-14 11:58:23,363 :: INFO :: Environment 3
2023-05-14 11:58:32,520 :: INFO :: Epoch 5: loss tensor(822.5018, device='cuda:0'), U.norm 13.154585838317871, V.norm 16.9035587310791, MLP.norm 2.1419460773468018
2023-05-14 11:58:32,740 :: INFO :: Epoch 10: loss tensor(798.8077, device='cuda:0'), U.norm 10.34073543548584, V.norm 16.214548110961914, MLP.norm 3.588602304458618
2023-05-14 11:58:32,974 :: INFO :: Epoch 15: loss tensor(765.9825, device='cuda:0'), U.norm 8.63633918762207, V.norm 15.881271362304688, MLP.norm 5.3015971183776855
2023-05-14 11:58:33,208 :: INFO :: Epoch 20: loss tensor(728.9960, device='cuda:0'), U.norm 7.472156524658203, V.norm 15.657316207885742, MLP.norm 7.094146728515625
2023-05-14 11:58:33,208 :: INFO :: Environment 4
2023-05-14 11:58:42,553 :: INFO :: Epoch 5: loss tensor(794.5137, device='cuda:0'), U.norm 13.150727272033691, V.norm 16.848163604736328, MLP.norm 2.113906145095825
2023-05-14 11:58:42,772 :: INFO :: Epoch 10: loss tensor(772.5640, device='cuda:0'), U.norm 10.332527160644531, V.norm 16.13765525817871, MLP.norm 3.522254467010498
2023-05-14 11:58:43,006 :: INFO :: Epoch 15: loss tensor(740.5620, device='cuda:0'), U.norm 8.62260627746582, V.norm 15.799271583557129, MLP.norm 5.196192741394043
2023-05-14 11:58:43,241 :: INFO :: Epoch 20: loss tensor(706.0726, device='cuda:0'), U.norm 7.451902389526367, V.norm 15.5747709274292, MLP.norm 6.946295261383057
2023-05-14 11:58:43,256 :: INFO :: Environment 5
2023-05-14 11:58:52,413 :: INFO :: Epoch 5: loss tensor(828.8560, device='cuda:0'), U.norm 13.157404899597168, V.norm 16.902793884277344, MLP.norm 2.1495213508605957
2023-05-14 11:58:52,648 :: INFO :: Epoch 10: loss tensor(802.7192, device='cuda:0'), U.norm 10.346981048583984, V.norm 16.208951950073242, MLP.norm 3.6520769596099854
2023-05-14 11:58:52,867 :: INFO :: Epoch 15: loss tensor(766.7315, device='cuda:0'), U.norm 8.646684646606445, V.norm 15.870580673217773, MLP.norm 5.425486087799072
2023-05-14 11:58:53,101 :: INFO :: Epoch 20: loss tensor(727.1312, device='cuda:0'), U.norm 7.487207412719727, V.norm 15.644513130187988, MLP.norm 7.256050109863281
2023-05-14 11:58:53,116 :: INFO :: Environment 6
2023-05-14 11:59:02,274 :: INFO :: Epoch 5: loss tensor(890.0808, device='cuda:0'), U.norm 13.159114837646484, V.norm 17.02518653869629, MLP.norm 2.1913552284240723
2023-05-14 11:59:02,508 :: INFO :: Epoch 10: loss tensor(860.5853, device='cuda:0'), U.norm 10.351750373840332, V.norm 16.3865909576416, MLP.norm 3.7444846630096436
2023-05-14 11:59:02,743 :: INFO :: Epoch 15: loss tensor(820.7631, device='cuda:0'), U.norm 8.65546989440918, V.norm 16.07668685913086, MLP.norm 5.522173881530762
2023-05-14 11:59:02,977 :: INFO :: Epoch 20: loss tensor(778.1060, device='cuda:0'), U.norm 7.50040864944458, V.norm 15.868854522705078, MLP.norm 7.3918914794921875
2023-05-14 11:59:02,993 :: INFO :: Environment 7
2023-05-14 11:59:12,119 :: INFO :: Epoch 5: loss tensor(875.6406, device='cuda:0'), U.norm 13.160367012023926, V.norm 17.006669998168945, MLP.norm 2.193547248840332
2023-05-14 11:59:12,353 :: INFO :: Epoch 10: loss tensor(845.6903, device='cuda:0'), U.norm 10.353033065795898, V.norm 16.359783172607422, MLP.norm 3.7455761432647705
2023-05-14 11:59:12,572 :: INFO :: Epoch 15: loss tensor(806.3422, device='cuda:0'), U.norm 8.656027793884277, V.norm 16.043596267700195, MLP.norm 5.527278423309326
2023-05-14 11:59:12,806 :: INFO :: Epoch 20: loss tensor(763.5040, device='cuda:0'), U.norm 7.499754905700684, V.norm 15.830223083496094, MLP.norm 7.364436149597168
2023-05-14 11:59:12,822 :: INFO :: Environment 8
2023-05-14 11:59:22,183 :: INFO :: Epoch 5: loss tensor(858.1889, device='cuda:0'), U.norm 13.160173416137695, V.norm 16.96763038635254, MLP.norm 2.164609909057617
2023-05-14 11:59:22,401 :: INFO :: Epoch 10: loss tensor(831.1993, device='cuda:0'), U.norm 10.352840423583984, V.norm 16.30337142944336, MLP.norm 3.686633586883545
2023-05-14 11:59:22,635 :: INFO :: Epoch 15: loss tensor(795.1243, device='cuda:0'), U.norm 8.655997276306152, V.norm 15.979120254516602, MLP.norm 5.452118396759033
2023-05-14 11:59:22,870 :: INFO :: Epoch 20: loss tensor(754.4544, device='cuda:0'), U.norm 7.500272750854492, V.norm 15.760210990905762, MLP.norm 7.270869731903076
2023-05-14 11:59:22,870 :: INFO :: Environment 9
2023-05-14 11:59:32,043 :: INFO :: Epoch 5: loss tensor(804.3691, device='cuda:0'), U.norm 13.155034065246582, V.norm 16.860334396362305, MLP.norm 2.1395583152770996
2023-05-14 11:59:32,277 :: INFO :: Epoch 10: loss tensor(781.7766, device='cuda:0'), U.norm 10.340930938720703, V.norm 16.151660919189453, MLP.norm 3.5726919174194336
2023-05-14 11:59:32,511 :: INFO :: Epoch 15: loss tensor(750.0955, device='cuda:0'), U.norm 8.636029243469238, V.norm 15.811389923095703, MLP.norm 5.216920852661133
2023-05-14 11:59:32,746 :: INFO :: Epoch 20: loss tensor(715.3887, device='cuda:0'), U.norm 7.471365451812744, V.norm 15.585046768188477, MLP.norm 6.939393997192383
2023-05-14 11:59:32,761 :: INFO :: Ite = 1, Delta = 4028
2023-05-14 11:59:32,761 :: INFO :: ----- backend -----
2023-05-14 11:59:37,465 :: INFO :: Epoch 5: loss tensor(156.4784, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.6484e-03,  1.0527e-02,  8.2967e-03,  1.2257e-02,  9.7045e-03,
         1.7838e-03,  6.3731e-03,  1.5496e-02,  2.4631e-03,  1.6420e-02,
         4.1987e-03,  9.9798e-03,  7.3642e-03,  1.3441e-02,  1.8863e-02,
         1.5247e-02,  1.4624e-02,  1.5803e-02,  1.3156e-02,  1.0897e-02,
         6.0600e-03,  1.8708e-02,  1.6234e-02, -3.3425e-03,  1.2249e-02,
         8.5185e-03,  8.4535e-03,  5.7401e-03,  1.7398e-02,  1.1725e-02,
         1.4437e-02,  2.3889e-02,  1.0893e-02,  1.2668e-02, -5.1438e-03,
        -5.7074e-03,  1.6344e-02,  1.9701e-02,  4.4528e-03,  1.3440e-02,
         9.2658e-04,  3.0945e-04,  2.0163e-02,  1.1421e-02,  1.0756e-02,
         1.7877e-02,  1.2008e-02,  3.0218e-03,  1.2643e-02,  1.2096e-02,
         1.6154e-02,  8.6551e-03,  7.8233e-03,  9.9856e-03,  7.2128e-03,
         1.7719e-02,  8.3455e-03,  1.4033e-02,  4.5719e-04, -4.4455e-03,
         1.5971e-02,  1.1511e-02,  3.3774e-04,  1.2363e-02,  1.2736e-02,
         4.1401e-04,  2.2564e-02,  6.5826e-03,  2.4444e-02,  2.8800e-03,
         1.8264e-02,  3.1727e-03,  2.2580e-02,  2.9967e-03,  1.8473e-02,
         1.7764e-03,  1.9759e-02, -2.6548e-03,  1.1982e-02,  1.4686e-02,
         1.1869e-02,  1.2025e-03,  1.9726e-02,  1.1490e-02, -3.3636e-03,
         1.3790e-02,  1.9931e-02,  5.1287e-03,  6.0531e-03, -2.9767e-03,
         6.5900e-03,  1.4289e-02,  1.5713e-02,  1.9488e-02, -4.8407e-04,
         1.8783e-02,  2.2154e-02,  1.1504e-02,  1.4788e-03,  7.9258e-03,
        -1.5250e-03,  1.4069e-02,  1.2153e-02, -4.2288e-03,  5.5465e-03,
        -2.4140e-03,  1.1925e-02,  1.1815e-02,  1.1745e-02,  1.4084e-02,
         3.9458e-03,  1.3027e-02,  1.0371e-02,  1.4236e-02, -2.4193e-03,
        -3.1365e-04,  9.8688e-03,  5.2082e-03,  1.0702e-02,  1.8361e-03,
         1.1206e-02,  9.9811e-03,  7.9911e-04,  4.2668e-03,  4.5615e-03,
         1.1154e-02, -5.2554e-04,  1.5385e-02,  1.8115e-02,  1.9381e-02,
         2.6033e-02,  2.5758e-02,  2.2919e-02,  1.7789e-02,  9.5524e-03,
         3.7770e-03,  5.7916e-03,  1.4201e-02,  9.6552e-03, -4.3066e-03,
         2.5924e-02,  5.3638e-03,  1.3394e-02,  3.4542e-03,  2.2967e-02,
         2.5842e-02,  1.5825e-02,  1.8611e-02,  2.6383e-02,  1.6298e-02,
         1.0044e-02,  1.6711e-02,  2.6280e-02, -2.1685e-02,  3.7350e-03,
         3.6180e-03,  2.0702e-03, -1.8833e-02,  2.2999e-02, -1.7679e-02,
         2.3117e-02, -1.8752e-02,  2.4158e-02,  1.8200e-02,  2.5391e-02,
         1.4082e-02, -9.3911e-03,  5.4607e-04,  4.6389e-03,  1.1097e-02,
         2.1482e-02,  2.0318e-02, -1.8415e-02,  2.2246e-02,  2.6286e-02,
         1.5828e-02,  1.4516e-02,  2.2929e-02,  7.6106e-03, -4.5614e-04,
         1.6511e-02,  1.4248e-02,  2.3211e-02, -1.8049e-02,  2.3204e-02,
         2.3347e-02, -1.5005e-02,  1.8161e-02,  2.1974e-02, -2.0472e-02,
        -1.5818e-02,  1.3543e-02,  2.5633e-02, -1.2388e-02,  1.6609e-02,
         2.1702e-02,  1.1368e-02,  1.4619e-02,  1.8156e-02,  1.6067e-02,
         2.6109e-02,  1.4441e-02,  1.9291e-02,  2.2416e-02,  2.0658e-02,
         1.2360e-02,  3.0201e-04, -1.9318e-02,  2.4462e-02,  5.6466e-03,
         2.6054e-02,  2.5731e-02,  1.4974e-02,  1.7930e-03,  1.8537e-02,
         2.1236e-02,  9.0492e-03, -1.3021e-02,  2.5872e-02, -1.8572e-02,
         1.4953e-02,  4.7179e-03,  2.2708e-02,  1.4941e-02,  2.0691e-02,
         1.6690e-02,  1.4353e-02,  1.8182e-02,  1.6832e-02,  2.6375e-02,
         2.2980e-02,  2.0290e-02,  2.1815e-02,  1.8618e-02,  2.5399e-02,
         1.5015e-02,  7.3947e-03,  8.8523e-03,  8.6675e-03,  2.1187e-02,
         1.0499e-02,  1.8438e-02,  1.5530e-02,  2.2970e-02,  1.3330e-02,
         2.6040e-02, -1.6379e-02,  1.5391e-02,  1.4498e-02,  9.9509e-03,
        -9.4254e-03,  2.6063e-02,  1.3519e-02,  2.3231e-02, -1.1728e-02,
         1.5200e-02,  1.4429e-02,  1.4556e-02,  1.2382e-02,  1.8652e-02,
         1.3728e-02,  1.5777e-02,  5.3197e-04,  3.9516e-03,  1.6215e-02,
         8.7007e-03,  1.9141e-02,  1.1942e-02,  1.1700e-02,  1.9389e-02,
         7.4981e-03,  6.5946e-03,  8.8586e-03,  1.2235e-02,  1.5054e-02,
         9.1239e-03,  8.2152e-03,  1.6327e-02,  1.9126e-02,  1.4178e-02,
         9.3709e-03,  1.0569e-02,  1.8657e-02,  9.4362e-03,  1.7031e-02,
         1.8510e-02,  1.9667e-03,  1.4986e-02,  1.8088e-02,  5.3086e-03,
         1.1001e-02,  8.5072e-03,  7.2597e-03,  1.5672e-02,  7.4299e-03,
         2.2433e-02,  1.6434e-02,  1.1427e-02,  1.9342e-02,  1.7917e-02,
         5.3668e-03,  2.6173e-02,  1.2760e-02,  8.5798e-04,  1.4059e-02,
         1.1901e-02,  1.6002e-02,  4.3368e-03,  1.4294e-02,  8.4665e-03,
         1.2380e-02,  6.9201e-03,  1.6456e-02,  1.1512e-02,  5.6565e-03,
         1.6432e-02,  8.6342e-03,  1.7812e-02,  1.4230e-02,  9.3539e-03,
         1.8418e-02,  9.5926e-03,  2.3320e-02,  1.0674e-02,  1.2918e-02,
         2.5326e-02,  4.2576e-03,  7.6182e-03,  1.0157e-02,  1.2673e-02,
         1.4706e-02,  8.4334e-03,  3.8188e-03,  2.3383e-03,  1.6213e-02,
         1.6659e-02,  1.2204e-02,  1.1898e-02,  1.7721e-02,  1.3988e-02,
         1.3655e-02,  9.1864e-03,  6.8134e-03,  8.8784e-03,  8.5933e-03,
         1.8035e-02,  1.5976e-02,  1.0772e-02,  9.7064e-03,  8.0727e-03,
         9.7607e-03,  1.3787e-02,  7.2291e-03,  3.5551e-03,  1.0991e-02,
         1.1418e-02, -1.4380e-05,  1.4666e-02,  1.4479e-02,  1.4091e-02,
         1.3339e-02,  9.0037e-03,  1.9728e-02, -1.3169e-03,  1.6898e-02,
         2.3392e-02,  1.1730e-02,  8.6131e-03,  1.8460e-02,  8.1535e-03,
         4.6686e-03, -2.5934e-03,  1.5303e-02,  1.2694e-02,  9.8200e-03,
         2.2694e-02,  1.7336e-02,  1.8194e-02,  1.3847e-02, -1.6697e-03,
         2.0270e-02,  2.6621e-03,  8.7842e-03,  1.6842e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.5259, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:59:42,060 :: INFO :: Epoch 10: loss tensor(150.8043, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0357,  0.0374,  0.0374,  0.0394,  0.0368,  0.0291,  0.0354,  0.0418,
         0.0293,  0.0430,  0.0333,  0.0348,  0.0341,  0.0415,  0.0450,  0.0392,
         0.0382,  0.0424,  0.0351,  0.0399,  0.0302,  0.0438,  0.0444,  0.0248,
         0.0381,  0.0350,  0.0321,  0.0332,  0.0479,  0.0367,  0.0420,  0.0481,
         0.0394,  0.0403,  0.0200,  0.0174,  0.0403,  0.0481,  0.0318,  0.0425,
         0.0274,  0.0267,  0.0477,  0.0368,  0.0360,  0.0436,  0.0374,  0.0311,
         0.0414,  0.0359,  0.0436,  0.0345,  0.0321,  0.0332,  0.0370,  0.0456,
         0.0341,  0.0431,  0.0281,  0.0223,  0.0397,  0.0352,  0.0269,  0.0396,
         0.0408,  0.0240,  0.0482,  0.0339,  0.0491,  0.0288,  0.0440,  0.0318,
         0.0437,  0.0311,  0.0442,  0.0281,  0.0460,  0.0222,  0.0375,  0.0413,
         0.0400,  0.0305,  0.0465,  0.0405,  0.0229,  0.0429,  0.0451,  0.0334,
         0.0330,  0.0210,  0.0341,  0.0431,  0.0419,  0.0456,  0.0279,  0.0470,
         0.0476,  0.0397,  0.0267,  0.0341,  0.0243,  0.0356,  0.0388,  0.0215,
         0.0349,  0.0232,  0.0378,  0.0365,  0.0374,  0.0399,  0.0320,  0.0378,
         0.0391,  0.0414,  0.0217,  0.0228,  0.0355,  0.0330,  0.0317,  0.0287,
         0.0374,  0.0369,  0.0273,  0.0298,  0.0320,  0.0345,  0.0251,  0.0438,
         0.0453,  0.0482,  0.0557,  0.0457,  0.0486,  0.0362,  0.0389,  0.0340,
         0.0370,  0.0218,  0.0390,  0.0228,  0.0545,  0.0354,  0.0435,  0.0328,
         0.0532,  0.0541,  0.0373,  0.0498,  0.0560,  0.0470,  0.0406,  0.0463,
         0.0552, -0.0371,  0.0329,  0.0340,  0.0316, -0.0279,  0.0421, -0.0291,
         0.0498, -0.0331,  0.0455,  0.0480,  0.0511,  0.0246,  0.0112,  0.0299,
         0.0345,  0.0406,  0.0511,  0.0471, -0.0290,  0.0521,  0.0560,  0.0467,
         0.0259,  0.0516,  0.0372,  0.0251,  0.0457,  0.0449,  0.0382, -0.0236,
         0.0515,  0.0525, -0.0062,  0.0467,  0.0506, -0.0336, -0.0130,  0.0438,
         0.0528,  0.0050,  0.0398,  0.0499,  0.0413,  0.0224,  0.0474,  0.0465,
         0.0556,  0.0447,  0.0473,  0.0346,  0.0501,  0.0428,  0.0288, -0.0343,
         0.0470,  0.0356,  0.0548,  0.0527,  0.0251,  0.0310,  0.0478,  0.0517,
         0.0393,  0.0043,  0.0539, -0.0245,  0.0442,  0.0342,  0.0494,  0.0433,
         0.0433,  0.0460,  0.0424,  0.0486,  0.0401,  0.0556,  0.0510,  0.0367,
         0.0349,  0.0344,  0.0521,  0.0459,  0.0379,  0.0388,  0.0140,  0.0445,
         0.0401,  0.0488,  0.0466,  0.0505,  0.0438,  0.0559, -0.0087,  0.0452,
         0.0445,  0.0401,  0.0121,  0.0552,  0.0441,  0.0536,  0.0118,  0.0448,
         0.0462,  0.0459,  0.0447,  0.0490,  0.0442,  0.0471,  0.0313,  0.0352,
         0.0473,  0.0391,  0.0502,  0.0428,  0.0423,  0.0494,  0.0378,  0.0380,
         0.0394,  0.0432,  0.0460,  0.0400,  0.0389,  0.0475,  0.0496,  0.0452,
         0.0403,  0.0423,  0.0493,  0.0409,  0.0490,  0.0486,  0.0330,  0.0462,
         0.0475,  0.0366,  0.0416,  0.0397,  0.0388,  0.0458,  0.0379,  0.0520,
         0.0473,  0.0418,  0.0488,  0.0480,  0.0369,  0.0574,  0.0442,  0.0326,
         0.0453,  0.0436,  0.0458,  0.0346,  0.0451,  0.0393,  0.0431,  0.0382,
         0.0479,  0.0419,  0.0356,  0.0470,  0.0379,  0.0497,  0.0452,  0.0407,
         0.0492,  0.0407,  0.0541,  0.0416,  0.0440,  0.0556,  0.0362,  0.0388,
         0.0408,  0.0437,  0.0456,  0.0397,  0.0351,  0.0334,  0.0480,  0.0464,
         0.0418,  0.0431,  0.0482,  0.0441,  0.0449,  0.0403,  0.0376,  0.0396,
         0.0395,  0.0467,  0.0458,  0.0417,  0.0405,  0.0391,  0.0407,  0.0442,
         0.0383,  0.0326,  0.0419,  0.0424,  0.0313,  0.0446,  0.0448,  0.0452,
         0.0445,  0.0414,  0.0509,  0.0294,  0.0470,  0.0537,  0.0428,  0.0394,
         0.0486,  0.0394,  0.0357,  0.0286,  0.0458,  0.0434,  0.0411,  0.0540,
         0.0476,  0.0459,  0.0447,  0.0286,  0.0506,  0.0338,  0.0402,  0.0471],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0501, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:59:46,591 :: INFO :: Epoch 15: loss tensor(147.5219, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0594,  0.0554,  0.0655,  0.0621,  0.0569,  0.0549,  0.0597,  0.0629,
         0.0477,  0.0624,  0.0617,  0.0561,  0.0536,  0.0652,  0.0674,  0.0576,
         0.0490,  0.0609,  0.0509,  0.0651,  0.0516,  0.0583,  0.0682,  0.0529,
         0.0617,  0.0513,  0.0444,  0.0585,  0.0776,  0.0531,  0.0658,  0.0632,
         0.0626,  0.0607,  0.0439,  0.0399,  0.0551,  0.0728,  0.0536,  0.0668,
         0.0504,  0.0481,  0.0685,  0.0567,  0.0534,  0.0569,  0.0538,  0.0568,
         0.0661,  0.0486,  0.0655,  0.0516,  0.0515,  0.0435,  0.0656,  0.0706,
         0.0551,  0.0715,  0.0541,  0.0487,  0.0511,  0.0514,  0.0473,  0.0593,
         0.0629,  0.0407,  0.0679,  0.0570,  0.0663,  0.0477,  0.0601,  0.0565,
         0.0522,  0.0585,  0.0624,  0.0485,  0.0652,  0.0451,  0.0527,  0.0596,
         0.0651,  0.0597,  0.0669,  0.0672,  0.0449,  0.0660,  0.0624,  0.0586,
         0.0525,  0.0369,  0.0572,  0.0694,  0.0588,  0.0606,  0.0544,  0.0697,
         0.0639,  0.0627,  0.0480,  0.0553,  0.0455,  0.0420,  0.0578,  0.0454,
         0.0625,  0.0472,  0.0585,  0.0525,  0.0552,  0.0602,  0.0591,  0.0531,
         0.0669,  0.0611,  0.0445,  0.0426,  0.0545,  0.0572,  0.0389,  0.0540,
         0.0588,  0.0576,  0.0472,  0.0461,  0.0522,  0.0525,  0.0486,  0.0674,
         0.0677,  0.0744,  0.0846,  0.0532,  0.0637,  0.0503,  0.0660,  0.0639,
         0.0694,  0.0265,  0.0663,  0.0529,  0.0810,  0.0657,  0.0723,  0.0647,
         0.0828,  0.0795,  0.0482,  0.0819,  0.0852,  0.0780,  0.0715,  0.0748,
         0.0826, -0.0483,  0.0616,  0.0660,  0.0619, -0.0281,  0.0487, -0.0343,
         0.0697, -0.0427,  0.0580,  0.0772,  0.0703,  0.0253,  0.0387,  0.0597,
         0.0652,  0.0677,  0.0786,  0.0680, -0.0326,  0.0806,  0.0845,  0.0782,
         0.0288,  0.0772,  0.0664,  0.0505,  0.0725,  0.0759,  0.0441, -0.0184,
         0.0761,  0.0798,  0.0148,  0.0734,  0.0783, -0.0410,  0.0059,  0.0737,
         0.0748,  0.0316,  0.0502,  0.0739,  0.0719,  0.0231,  0.0746,  0.0769,
         0.0842,  0.0752,  0.0730,  0.0430,  0.0782,  0.0740,  0.0588, -0.0452,
         0.0545,  0.0645,  0.0829,  0.0727,  0.0312,  0.0609,  0.0746,  0.0823,
         0.0686,  0.0314,  0.0786, -0.0203,  0.0719,  0.0626,  0.0708,  0.0702,
         0.0549,  0.0741,  0.0657,  0.0784,  0.0557,  0.0842,  0.0759,  0.0400,
         0.0410,  0.0421,  0.0726,  0.0777,  0.0685,  0.0680,  0.0160,  0.0577,
         0.0673,  0.0788,  0.0778,  0.0747,  0.0743,  0.0845,  0.0134,  0.0742,
         0.0733,  0.0702,  0.0421,  0.0817,  0.0747,  0.0844,  0.0433,  0.0722,
         0.0800,  0.0784,  0.0790,  0.0785,  0.0746,  0.0798,  0.0640,  0.0676,
         0.0796,  0.0703,  0.0815,  0.0741,  0.0724,  0.0784,  0.0678,  0.0705,
         0.0711,  0.0745,  0.0768,  0.0716,  0.0703,  0.0793,  0.0798,  0.0762,
         0.0720,  0.0766,  0.0803,  0.0740,  0.0830,  0.0764,  0.0656,  0.0782,
         0.0744,  0.0693,  0.0730,  0.0720,  0.0731,  0.0746,  0.0683,  0.0797,
         0.0785,  0.0723,  0.0763,  0.0775,  0.0702,  0.0902,  0.0778,  0.0669,
         0.0772,  0.0771,  0.0738,  0.0649,  0.0751,  0.0717,  0.0740,  0.0720,
         0.0808,  0.0725,  0.0663,  0.0773,  0.0661,  0.0837,  0.0767,  0.0734,
         0.0799,  0.0734,  0.0849,  0.0730,  0.0765,  0.0857,  0.0703,  0.0715,
         0.0712,  0.0749,  0.0767,  0.0713,  0.0678,  0.0659,  0.0809,  0.0742,
         0.0699,  0.0749,  0.0799,  0.0729,  0.0767,  0.0721,  0.0690,  0.0702,
         0.0708,  0.0696,  0.0734,  0.0735,  0.0717,  0.0714,  0.0719,  0.0750,
         0.0705,  0.0585,  0.0735,  0.0736,  0.0653,  0.0737,  0.0748,  0.0767,
         0.0764,  0.0760,  0.0834,  0.0640,  0.0771,  0.0833,  0.0746,  0.0711,
         0.0781,  0.0716,  0.0681,  0.0633,  0.0755,  0.0733,  0.0752,  0.0870,
         0.0767,  0.0687,  0.0751,  0.0617,  0.0807,  0.0666,  0.0730,  0.0760],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3245, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:59:51,232 :: INFO :: Epoch 20: loss tensor(145.2805, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0744,  0.0598,  0.0904,  0.0765,  0.0707,  0.0749,  0.0750,  0.0782,
         0.0549,  0.0730,  0.0843,  0.0700,  0.0643,  0.0792,  0.0794,  0.0663,
         0.0501,  0.0684,  0.0591,  0.0860,  0.0665,  0.0601,  0.0831,  0.0751,
         0.0794,  0.0584,  0.0495,  0.0781,  0.1032,  0.0599,  0.0822,  0.0685,
         0.0768,  0.0721,  0.0587,  0.0526,  0.0589,  0.0922,  0.0667,  0.0811,
         0.0655,  0.0618,  0.0796,  0.0687,  0.0597,  0.0590,  0.0622,  0.0754,
         0.0821,  0.0493,  0.0771,  0.0564,  0.0642,  0.0451,  0.0902,  0.0888,
         0.0683,  0.0974,  0.0728,  0.0671,  0.0528,  0.0627,  0.0579,  0.0689,
         0.0753,  0.0502,  0.0767,  0.0687,  0.0740,  0.0579,  0.0657,  0.0702,
         0.0519,  0.0827,  0.0691,  0.0574,  0.0729,  0.0633,  0.0545,  0.0687,
         0.0852,  0.0852,  0.0774,  0.0884,  0.0555,  0.0804,  0.0716,  0.0761,
         0.0612,  0.0404,  0.0736,  0.0904,  0.0681,  0.0637,  0.0729,  0.0816,
         0.0691,  0.0753,  0.0638,  0.0683,  0.0562,  0.0422,  0.0665,  0.0639,
         0.0856,  0.0634,  0.0690,  0.0599,  0.0653,  0.0735,  0.0832,  0.0578,
         0.0886,  0.0714,  0.0604,  0.0561,  0.0660,  0.0745,  0.0418,  0.0742,
         0.0738,  0.0706,  0.0579,  0.0502,  0.0635,  0.0659,  0.0673,  0.0852,
         0.0830,  0.0960,  0.1105,  0.0504,  0.0663,  0.0575,  0.0877,  0.0906,
         0.1002,  0.0315,  0.0881,  0.0804,  0.1019,  0.0936,  0.0960,  0.0960,
         0.1092,  0.0990,  0.0507,  0.1125,  0.1113,  0.1059,  0.1004,  0.0995,
         0.1053, -0.0558,  0.0862,  0.0968,  0.0895, -0.0212,  0.0529, -0.0329,
         0.0808, -0.0473,  0.0692,  0.1036,  0.0818,  0.0230,  0.0648,  0.0860,
         0.0936,  0.0896,  0.0999,  0.0798, -0.0298,  0.1056,  0.1089,  0.1081,
         0.0330,  0.0961,  0.0914,  0.0718,  0.0936,  0.1047,  0.0453, -0.0058,
         0.0934,  0.1023,  0.0379,  0.0922,  0.0993, -0.0431,  0.0314,  0.1003,
         0.0888,  0.0588,  0.0502,  0.0918,  0.0997,  0.0243,  0.0972,  0.1041,
         0.1093,  0.1024,  0.0933,  0.0429,  0.1016,  0.1027,  0.0859, -0.0516,
         0.0541,  0.0895,  0.1057,  0.0834,  0.0317,  0.0876,  0.0947,  0.1105,
         0.0941,  0.0583,  0.0970, -0.0082,  0.0946,  0.0860,  0.0851,  0.0933,
         0.0555,  0.0976,  0.0804,  0.1058,  0.0607,  0.1090,  0.0935,  0.0378,
         0.0437,  0.0416,  0.0840,  0.1074,  0.0961,  0.0943,  0.0168,  0.0659,
         0.0886,  0.1061,  0.1072,  0.0944,  0.1025,  0.1099,  0.0394,  0.0992,
         0.0989,  0.0980,  0.0724,  0.1042,  0.1027,  0.1126,  0.0744,  0.0943,
         0.1134,  0.1107,  0.1132,  0.1033,  0.1022,  0.1116,  0.0941,  0.0976,
         0.1120,  0.0999,  0.1111,  0.1031,  0.0987,  0.1027,  0.0946,  0.1008,
         0.1006,  0.1044,  0.1040,  0.1002,  0.0997,  0.1086,  0.1071,  0.1036,
         0.1008,  0.1106,  0.1084,  0.1070,  0.1167,  0.0981,  0.0961,  0.1078,
         0.0944,  0.1002,  0.1020,  0.1020,  0.1071,  0.0979,  0.0950,  0.1026,
         0.1066,  0.1002,  0.0974,  0.1023,  0.1033,  0.1228,  0.1107,  0.1015,
         0.1082,  0.1106,  0.0972,  0.0934,  0.1012,  0.1017,  0.1041,  0.1048,
         0.1124,  0.1008,  0.0946,  0.1036,  0.0899,  0.1175,  0.1055,  0.1055,
         0.1075,  0.1057,  0.1126,  0.1023,  0.1074,  0.1130,  0.1042,  0.1027,
         0.0982,  0.1029,  0.1048,  0.1002,  0.0985,  0.0956,  0.1127,  0.0946,
         0.0908,  0.1039,  0.1104,  0.0958,  0.1055,  0.1011,  0.0978,  0.0990,
         0.0989,  0.0846,  0.0941,  0.1041,  0.1014,  0.1016,  0.1000,  0.1036,
         0.1005,  0.0767,  0.1034,  0.1013,  0.0993,  0.0999,  0.1010,  0.1055,
         0.1072,  0.1104,  0.1154,  0.0979,  0.1042,  0.1093,  0.1041,  0.1018,
         0.1046,  0.1016,  0.0989,  0.0980,  0.1001,  0.0985,  0.1087,  0.1195,
         0.1012,  0.0833,  0.1011,  0.0947,  0.1074,  0.0973,  0.1044,  0.1022],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.2840, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 11:59:55,874 :: INFO :: Epoch 25: loss tensor(144.1785, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0810,  0.0594,  0.1102,  0.0820,  0.0739,  0.0902,  0.0830,  0.0859,
         0.0588,  0.0778,  0.1026,  0.0769,  0.0655,  0.0863,  0.0853,  0.0690,
         0.0501,  0.0703,  0.0602,  0.1001,  0.0737,  0.0616,  0.0901,  0.0905,
         0.0917,  0.0584,  0.0470,  0.0895,  0.1241,  0.0607,  0.0921,  0.0683,
         0.0865,  0.0770,  0.0659,  0.0568,  0.0581,  0.1036,  0.0716,  0.0909,
         0.0743,  0.0682,  0.0867,  0.0726,  0.0604,  0.0589,  0.0619,  0.0877,
         0.0930,  0.0493,  0.0811,  0.0551,  0.0690,  0.0428,  0.1101,  0.0997,
         0.0768,  0.1197,  0.0856,  0.0796,  0.0505,  0.0669,  0.0646,  0.0718,
         0.0801,  0.0532,  0.0816,  0.0745,  0.0747,  0.0607,  0.0660,  0.0776,
         0.0478,  0.0998,  0.0703,  0.0577,  0.0758,  0.0745,  0.0539,  0.0739,
         0.0987,  0.1069,  0.0780,  0.1023,  0.0606,  0.0868,  0.0745,  0.0858,
         0.0639,  0.0404,  0.0814,  0.1043,  0.0688,  0.0640,  0.0860,  0.0884,
         0.0674,  0.0809,  0.0715,  0.0730,  0.0628,  0.0389,  0.0669,  0.0753,
         0.1014,  0.0747,  0.0730,  0.0592,  0.0685,  0.0791,  0.1020,  0.0575,
         0.1049,  0.0750,  0.0690,  0.0600,  0.0715,  0.0874,  0.0396,  0.0879,
         0.0807,  0.0749,  0.0639,  0.0535,  0.0652,  0.0720,  0.0790,  0.0938,
         0.0914,  0.1108,  0.1309,  0.0448,  0.0641,  0.0622,  0.1014,  0.1112,
         0.1275,  0.0342,  0.1034,  0.1022,  0.1142,  0.1165,  0.1117,  0.1257,
         0.1304,  0.1091,  0.0493,  0.1403,  0.1319,  0.1291,  0.1246,  0.1183,
         0.1210, -0.0610,  0.1040,  0.1239,  0.1116, -0.0124,  0.0533, -0.0272,
         0.0840, -0.0481,  0.0736,  0.1243,  0.0860,  0.0207,  0.0855,  0.1065,
         0.1171,  0.1041,  0.1138,  0.0825, -0.0239,  0.1240,  0.1273,  0.1339,
         0.0328,  0.1054,  0.1101,  0.0865,  0.1059,  0.1293,  0.0425,  0.0077,
         0.1007,  0.1171,  0.0563,  0.1018,  0.1139, -0.0425,  0.0581,  0.1213,
         0.0922,  0.0804,  0.0468,  0.1003,  0.1234,  0.0211,  0.1133,  0.1264,
         0.1282,  0.1248,  0.1062,  0.0402,  0.1175,  0.1279,  0.1071, -0.0552,
         0.0493,  0.1080,  0.1209,  0.0829,  0.0308,  0.1079,  0.1068,  0.1344,
         0.1125,  0.0805,  0.1052,  0.0049,  0.1118,  0.1027,  0.0929,  0.1116,
         0.0534,  0.1144,  0.0860,  0.1291,  0.0603,  0.1289,  0.1014,  0.0314,
         0.0452,  0.0411,  0.0848,  0.1335,  0.1185,  0.1147,  0.0175,  0.0695,
         0.1016,  0.1282,  0.1327,  0.1089,  0.1260,  0.1298,  0.0621,  0.1174,
         0.1182,  0.1208,  0.1010,  0.1179,  0.1257,  0.1366,  0.1011,  0.1077,
         0.1448,  0.1413,  0.1462,  0.1229,  0.1273,  0.1417,  0.1200,  0.1234,
         0.1436,  0.1287,  0.1387,  0.1294,  0.1195,  0.1202,  0.1177,  0.1274,
         0.1284,  0.1315,  0.1260,  0.1257,  0.1278,  0.1355,  0.1304,  0.1258,
         0.1254,  0.1435,  0.1316,  0.1389,  0.1490,  0.1119,  0.1228,  0.1343,
         0.1069,  0.1280,  0.1278,  0.1286,  0.1408,  0.1150,  0.1163,  0.1203,
         0.1307,  0.1262,  0.1079,  0.1214,  0.1350,  0.1542,  0.1427,  0.1349,
         0.1377,  0.1432,  0.1131,  0.1205,  0.1223,  0.1299,  0.1322,  0.1365,
         0.1421,  0.1266,  0.1205,  0.1243,  0.1098,  0.1503,  0.1300,  0.1362,
         0.1302,  0.1371,  0.1361,  0.1286,  0.1364,  0.1358,  0.1368,  0.1320,
         0.1202,  0.1272,  0.1288,  0.1256,  0.1255,  0.1210,  0.1431,  0.1065,
         0.1036,  0.1289,  0.1394,  0.1095,  0.1304,  0.1251,  0.1235,  0.1256,
         0.1223,  0.0928,  0.1084,  0.1328,  0.1277,  0.1280,  0.1236,  0.1290,
         0.1269,  0.0887,  0.1310,  0.1240,  0.1319,  0.1241,  0.1225,  0.1304,
         0.1359,  0.1439,  0.1463,  0.1313,  0.1290,  0.1304,  0.1304,  0.1310,
         0.1260,  0.1279,  0.1265,  0.1313,  0.1188,  0.1172,  0.1413,  0.1506,
         0.1197,  0.0882,  0.1212,  0.1271,  0.1288,  0.1243,  0.1335,  0.1240],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.9428, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:00:00,546 :: INFO :: Epoch 30: loss tensor(144.2412, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0845,  0.0588,  0.1269,  0.0850,  0.0790,  0.0989,  0.0844,  0.0913,
         0.0604,  0.0807,  0.1131,  0.0836,  0.0672,  0.0910,  0.0881,  0.0715,
         0.0495,  0.0698,  0.0591,  0.1094,  0.0786,  0.0589,  0.0956,  0.1006,
         0.1011,  0.0591,  0.0471,  0.0988,  0.1398,  0.0595,  0.0981,  0.0686,
         0.0897,  0.0779,  0.0697,  0.0596,  0.0575,  0.1133,  0.0747,  0.0933,
         0.0811,  0.0690,  0.0875,  0.0759,  0.0613,  0.0558,  0.0629,  0.0985,
         0.0962,  0.0503,  0.0800,  0.0532,  0.0723,  0.0426,  0.1228,  0.1062,
         0.0784,  0.1403,  0.0910,  0.0851,  0.0506,  0.0685,  0.0651,  0.0734,
         0.0811,  0.0541,  0.0799,  0.0771,  0.0778,  0.0596,  0.0666,  0.0790,
         0.0451,  0.1141,  0.0704,  0.0570,  0.0746,  0.0807,  0.0518,  0.0700,
         0.1073,  0.1219,  0.0802,  0.1118,  0.0602,  0.0863,  0.0748,  0.0900,
         0.0667,  0.0383,  0.0876,  0.1157,  0.0691,  0.0633,  0.0896,  0.0891,
         0.0654,  0.0801,  0.0787,  0.0751,  0.0624,  0.0383,  0.0671,  0.0839,
         0.1116,  0.0796,  0.0756,  0.0573,  0.0697,  0.0841,  0.1155,  0.0588,
         0.1137,  0.0764,  0.0748,  0.0629,  0.0726,  0.0967,  0.0391,  0.0990,
         0.0848,  0.0778,  0.0635,  0.0523,  0.0666,  0.0750,  0.0875,  0.0994,
         0.0943,  0.1191,  0.1452,  0.0386,  0.0578,  0.0627,  0.1079,  0.1261,
         0.1510,  0.0340,  0.1122,  0.1174,  0.1176,  0.1340,  0.1183,  0.1519,
         0.1458,  0.1121,  0.0461,  0.1643,  0.1463,  0.1462,  0.1440,  0.1308,
         0.1290, -0.0649,  0.1136,  0.1471,  0.1279, -0.0036,  0.0543, -0.0194,
         0.0832, -0.0468,  0.0750,  0.1391,  0.0881,  0.0184,  0.0996,  0.1209,
         0.1350,  0.1113,  0.1187,  0.0811, -0.0175,  0.1362,  0.1387,  0.1557,
         0.0321,  0.1062,  0.1215,  0.0955,  0.1109,  0.1493,  0.0373,  0.0193,
         0.1002,  0.1246,  0.0691,  0.1027,  0.1213, -0.0408,  0.0823,  0.1357,
         0.0900,  0.0964,  0.0411,  0.1029,  0.1406,  0.0211,  0.1215,  0.1427,
         0.1409,  0.1409,  0.1123,  0.0362,  0.1249,  0.1475,  0.1213, -0.0574,
         0.0460,  0.1200,  0.1279,  0.0775,  0.0272,  0.1218,  0.1094,  0.1531,
         0.1239,  0.0962,  0.1065,  0.0164,  0.1220,  0.1111,  0.0954,  0.1240,
         0.0490,  0.1236,  0.0847,  0.1468,  0.0561,  0.1422,  0.1005,  0.0287,
         0.0437,  0.0374,  0.0807,  0.1548,  0.1349,  0.1295,  0.0157,  0.0678,
         0.1065,  0.1448,  0.1541,  0.1186,  0.1448,  0.1446,  0.0800,  0.1280,
         0.1317,  0.1382,  0.1255,  0.1259,  0.1433,  0.1554,  0.1231,  0.1132,
         0.1747,  0.1689,  0.1776,  0.1364,  0.1492,  0.1688,  0.1416,  0.1450,
         0.1737,  0.1555,  0.1636,  0.1518,  0.1344,  0.1299,  0.1361,  0.1507,
         0.1546,  0.1557,  0.1429,  0.1474,  0.1534,  0.1596,  0.1493,  0.1429,
         0.1447,  0.1741,  0.1499,  0.1681,  0.1802,  0.1184,  0.1458,  0.1572,
         0.1136,  0.1526,  0.1509,  0.1512,  0.1727,  0.1261,  0.1324,  0.1345,
         0.1502,  0.1496,  0.1105,  0.1354,  0.1644,  0.1833,  0.1730,  0.1662,
         0.1647,  0.1737,  0.1198,  0.1458,  0.1382,  0.1561,  0.1581,  0.1659,
         0.1697,  0.1507,  0.1447,  0.1387,  0.1247,  0.1815,  0.1505,  0.1645,
         0.1477,  0.1670,  0.1544,  0.1511,  0.1630,  0.1535,  0.1675,  0.1589,
         0.1369,  0.1479,  0.1485,  0.1474,  0.1490,  0.1414,  0.1714,  0.1100,
         0.1094,  0.1498,  0.1663,  0.1135,  0.1508,  0.1445,  0.1447,  0.1484,
         0.1408,  0.0964,  0.1177,  0.1584,  0.1510,  0.1511,  0.1420,  0.1516,
         0.1496,  0.0954,  0.1549,  0.1409,  0.1623,  0.1464,  0.1393,  0.1512,
         0.1626,  0.1758,  0.1753,  0.1629,  0.1508,  0.1461,  0.1534,  0.1577,
         0.1423,  0.1504,  0.1507,  0.1625,  0.1319,  0.1309,  0.1718,  0.1800,
         0.1307,  0.0845,  0.1348,  0.1577,  0.1453,  0.1479,  0.1598,  0.1404],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.3421, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:00:05,203 :: INFO :: Epoch 35: loss tensor(139.0305, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0839,  0.0562,  0.1394,  0.0850,  0.0776,  0.1056,  0.0844,  0.0920,
         0.0620,  0.0793,  0.1210,  0.0845,  0.0657,  0.0912,  0.0888,  0.0706,
         0.0477,  0.0711,  0.0579,  0.1146,  0.0806,  0.0564,  0.0952,  0.1066,
         0.1066,  0.0585,  0.0440,  0.1034,  0.1510,  0.0586,  0.1024,  0.0656,
         0.0928,  0.0761,  0.0704,  0.0593,  0.0554,  0.1174,  0.0733,  0.0949,
         0.0839,  0.0689,  0.0879,  0.0745,  0.0599,  0.0539,  0.0604,  0.1033,
         0.0977,  0.0474,  0.0766,  0.0509,  0.0729,  0.0400,  0.1321,  0.1087,
         0.0817,  0.1576,  0.0928,  0.0893,  0.0478,  0.0659,  0.0661,  0.0724,
         0.0812,  0.0526,  0.0803,  0.0766,  0.0768,  0.0583,  0.0642,  0.0792,
         0.0412,  0.1243,  0.0688,  0.0562,  0.0746,  0.0844,  0.0496,  0.0735,
         0.1136,  0.1332,  0.0786,  0.1183,  0.0598,  0.0858,  0.0721,  0.0912,
         0.0667,  0.0375,  0.0887,  0.1226,  0.0670,  0.0618,  0.0913,  0.0875,
         0.0635,  0.0790,  0.0807,  0.0768,  0.0624,  0.0337,  0.0641,  0.0878,
         0.1174,  0.0825,  0.0740,  0.0541,  0.0711,  0.0830,  0.1254,  0.0563,
         0.1186,  0.0736,  0.0771,  0.0612,  0.0717,  0.1042,  0.0388,  0.1050,
         0.0854,  0.0775,  0.0645,  0.0527,  0.0638,  0.0756,  0.0910,  0.1010,
         0.0875,  0.1202,  0.1530,  0.0324,  0.0504,  0.0609,  0.1072,  0.1338,
         0.1707,  0.0356,  0.1125,  0.1249,  0.1133,  0.1452,  0.1173,  0.1729,
         0.1546,  0.1072,  0.0408,  0.1841,  0.1544,  0.1578,  0.1582,  0.1322,
         0.1296, -0.0686,  0.1150,  0.1671,  0.1370,  0.0020,  0.0502, -0.0126,
         0.0756, -0.0453,  0.0746,  0.1473,  0.0846,  0.0155,  0.1057,  0.1271,
         0.1466,  0.1095,  0.1172,  0.0750, -0.0131,  0.1411,  0.1432,  0.1731,
         0.0283,  0.0999,  0.1247,  0.0978,  0.1090,  0.1647,  0.0336,  0.0260,
         0.0933,  0.1256,  0.0738,  0.0974,  0.1229, -0.0399,  0.1010,  0.1432,
         0.0835,  0.1049,  0.0365,  0.1005,  0.1494,  0.0173,  0.1209,  0.1526,
         0.1467,  0.1487,  0.1109,  0.0304,  0.1235,  0.1598,  0.1279, -0.0595,
         0.0412,  0.1243,  0.1253,  0.0697,  0.0241,  0.1280,  0.1045,  0.1655,
         0.1275,  0.1047,  0.1005,  0.0236,  0.1220,  0.1109,  0.0909,  0.1292,
         0.0429,  0.1237,  0.0779,  0.1580,  0.0484,  0.1489,  0.0927,  0.0236,
         0.0409,  0.0330,  0.0728,  0.1710,  0.1447,  0.1371,  0.0147,  0.0647,
         0.1035,  0.1555,  0.1709,  0.1214,  0.1583,  0.1529,  0.0921,  0.1304,
         0.1383,  0.1493,  0.1446,  0.1279,  0.1541,  0.1690,  0.1410,  0.1109,
         0.2032,  0.1945,  0.2064,  0.1453,  0.1692,  0.1925,  0.1604,  0.1634,
         0.2015,  0.1796,  0.1871,  0.1722,  0.1458,  0.1351,  0.1518,  0.1713,
         0.1779,  0.1781,  0.1556,  0.1666,  0.1765,  0.1806,  0.1658,  0.1563,
         0.1607,  0.2010,  0.1647,  0.1953,  0.2088,  0.1197,  0.1657,  0.1772,
         0.1177,  0.1747,  0.1722,  0.1709,  0.2021,  0.1337,  0.1453,  0.1451,
         0.1662,  0.1707,  0.1065,  0.1449,  0.1925,  0.2096,  0.2008,  0.1946,
         0.1897,  0.2013,  0.1224,  0.1689,  0.1496,  0.1800,  0.1816,  0.1933,
         0.1950,  0.1722,  0.1672,  0.1488,  0.1364,  0.2095,  0.1683,  0.1911,
         0.1612,  0.1943,  0.1694,  0.1719,  0.1875,  0.1688,  0.1960,  0.1840,
         0.1508,  0.1652,  0.1649,  0.1665,  0.1701,  0.1585,  0.1973,  0.1080,
         0.1103,  0.1678,  0.1907,  0.1122,  0.1673,  0.1605,  0.1625,  0.1674,
         0.1571,  0.0992,  0.1249,  0.1823,  0.1720,  0.1713,  0.1568,  0.1723,
         0.1691,  0.1011,  0.1753,  0.1543,  0.1903,  0.1657,  0.1528,  0.1694,
         0.1869,  0.2051,  0.2010,  0.1927,  0.1698,  0.1578,  0.1732,  0.1821,
         0.1558,  0.1701,  0.1720,  0.1916,  0.1391,  0.1402,  0.2003,  0.2059,
         0.1374,  0.0798,  0.1426,  0.1868,  0.1575,  0.1687,  0.1835,  0.1536],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.5079, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:00:09,859 :: INFO :: Epoch 40: loss tensor(140.6614, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0824,  0.0547,  0.1486,  0.0819,  0.0760,  0.1112,  0.0837,  0.0909,
         0.0636,  0.0778,  0.1252,  0.0840,  0.0683,  0.0899,  0.0868,  0.0698,
         0.0481,  0.0700,  0.0560,  0.1166,  0.0823,  0.0550,  0.0937,  0.1086,
         0.1097,  0.0570,  0.0429,  0.1043,  0.1583,  0.0574,  0.1047,  0.0607,
         0.0936,  0.0739,  0.0692,  0.0597,  0.0538,  0.1203,  0.0739,  0.0954,
         0.0873,  0.0689,  0.0869,  0.0742,  0.0584,  0.0518,  0.0576,  0.1077,
         0.0981,  0.0494,  0.0719,  0.0465,  0.0756,  0.0414,  0.1373,  0.1092,
         0.0833,  0.1713,  0.0925,  0.0903,  0.0461,  0.0633,  0.0674,  0.0698,
         0.0785,  0.0504,  0.0784,  0.0736,  0.0733,  0.0567,  0.0618,  0.0784,
         0.0381,  0.1345,  0.0636,  0.0543,  0.0723,  0.0865,  0.0469,  0.0734,
         0.1172,  0.1413,  0.0768,  0.1212,  0.0599,  0.0826,  0.0687,  0.0913,
         0.0672,  0.0364,  0.0880,  0.1272,  0.0646,  0.0607,  0.0905,  0.0857,
         0.0599,  0.0766,  0.0804,  0.0769,  0.0609,  0.0318,  0.0591,  0.0918,
         0.1194,  0.0836,  0.0704,  0.0513,  0.0680,  0.0801,  0.1310,  0.0565,
         0.1206,  0.0703,  0.0772,  0.0577,  0.0692,  0.1116,  0.0399,  0.1079,
         0.0840,  0.0746,  0.0623,  0.0534,  0.0615,  0.0776,  0.0925,  0.1013,
         0.0864,  0.1187,  0.1558,  0.0272,  0.0448,  0.0572,  0.1034,  0.1368,
         0.1868,  0.0344,  0.1101,  0.1285,  0.1051,  0.1528,  0.1107,  0.1907,
         0.1582,  0.1006,  0.0363,  0.2001,  0.1564,  0.1631,  0.1681,  0.1321,
         0.1244, -0.0708,  0.1123,  0.1835,  0.1429,  0.0087,  0.0462, -0.0049,
         0.0700, -0.0421,  0.0722,  0.1506,  0.0817,  0.0147,  0.1102,  0.1296,
         0.1538,  0.1055,  0.1119,  0.0699, -0.0072,  0.1412,  0.1421,  0.1867,
         0.0271,  0.0911,  0.1229,  0.0999,  0.1022,  0.1758,  0.0266,  0.0333,
         0.0842,  0.1218,  0.0789,  0.0888,  0.1199, -0.0374,  0.1171,  0.1453,
         0.0757,  0.1118,  0.0316,  0.0949,  0.1537,  0.0155,  0.1175,  0.1571,
         0.1472,  0.1515,  0.1077,  0.0259,  0.1163,  0.1670,  0.1306, -0.0600,
         0.0353,  0.1241,  0.1181,  0.0610,  0.0210,  0.1304,  0.0958,  0.1729,
         0.1254,  0.1110,  0.0926,  0.0319,  0.1201,  0.1077,  0.0870,  0.1311,
         0.0390,  0.1204,  0.0717,  0.1637,  0.0433,  0.1495,  0.0816,  0.0200,
         0.0386,  0.0289,  0.0656,  0.1822,  0.1496,  0.1410,  0.0145,  0.0606,
         0.0966,  0.1607,  0.1833,  0.1229,  0.1679,  0.1562,  0.1031,  0.1274,
         0.1402,  0.1570,  0.1590,  0.1265,  0.1595,  0.1773,  0.1552,  0.1036,
         0.2287,  0.2176,  0.2322,  0.1503,  0.1857,  0.2139,  0.1763,  0.1783,
         0.2268,  0.2006,  0.2081,  0.1894,  0.1523,  0.1355,  0.1647,  0.1888,
         0.1974,  0.1973,  0.1633,  0.1835,  0.1972,  0.1992,  0.1788,  0.1665,
         0.1729,  0.2237,  0.1752,  0.2199,  0.2349,  0.1161,  0.1826,  0.1937,
         0.1198,  0.1936,  0.1921,  0.1871,  0.2284,  0.1385,  0.1553,  0.1541,
         0.1776,  0.1888,  0.1002,  0.1492,  0.2181,  0.2326,  0.2266,  0.2200,
         0.2128,  0.2261,  0.1222,  0.1897,  0.1558,  0.2015,  0.2021,  0.2169,
         0.2185,  0.1894,  0.1871,  0.1535,  0.1460,  0.2343,  0.1824,  0.2149,
         0.1707,  0.2190,  0.1803,  0.1895,  0.2094,  0.1807,  0.2219,  0.2068,
         0.1609,  0.1789,  0.1771,  0.1833,  0.1877,  0.1713,  0.2207,  0.1032,
         0.1105,  0.1826,  0.2119,  0.1048,  0.1798,  0.1724,  0.1775,  0.1839,
         0.1698,  0.1058,  0.1312,  0.2035,  0.1900,  0.1887,  0.1676,  0.1908,
         0.1854,  0.1069,  0.1921,  0.1633,  0.2150,  0.1805,  0.1637,  0.1838,
         0.2078,  0.2319,  0.2229,  0.2184,  0.1853,  0.1654,  0.1896,  0.2031,
         0.1658,  0.1864,  0.1904,  0.2178,  0.1406,  0.1451,  0.2252,  0.2288,
         0.1390,  0.0769,  0.1445,  0.2130,  0.1642,  0.1862,  0.2041,  0.1630],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.4789, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:02:38,139 :: INFO :: log info to logs/tiktok_InvRL.log
2023-05-14 12:02:38,139 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='InvRL', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 12:02:39,686 :: INFO :: torch.Size([76083, 384])
2023-05-14 12:02:51,234 :: INFO :: ----- frontend -----
2023-05-14 12:02:51,234 :: INFO :: Environment 0
2023-05-14 12:03:01,938 :: INFO :: Epoch 5: loss tensor(820.6893, device='cuda:0'), U.norm 13.154335975646973, V.norm 16.90321159362793, MLP.norm 2.1432113647460938
2023-05-14 12:03:02,157 :: INFO :: Epoch 10: loss tensor(796.5191, device='cuda:0'), U.norm 10.340478897094727, V.norm 16.213722229003906, MLP.norm 3.6101739406585693
2023-05-14 12:03:02,391 :: INFO :: Epoch 15: loss tensor(763.9460, device='cuda:0'), U.norm 8.636116027832031, V.norm 15.882392883300781, MLP.norm 5.333689212799072
2023-05-14 12:03:02,610 :: INFO :: Epoch 20: loss tensor(726.9431, device='cuda:0'), U.norm 7.472327709197998, V.norm 15.661770820617676, MLP.norm 7.121977806091309
2023-05-14 12:03:02,626 :: INFO :: Environment 1
2023-05-14 12:03:12,002 :: INFO :: Epoch 5: loss tensor(822.5140, device='cuda:0'), U.norm 13.158339500427246, V.norm 16.898292541503906, MLP.norm 2.165095806121826
2023-05-14 12:03:12,220 :: INFO :: Epoch 10: loss tensor(797.6489, device='cuda:0'), U.norm 10.348400115966797, V.norm 16.202116012573242, MLP.norm 3.6142029762268066
2023-05-14 12:03:12,455 :: INFO :: Epoch 15: loss tensor(764.4840, device='cuda:0'), U.norm 8.648263931274414, V.norm 15.864622116088867, MLP.norm 5.285494804382324
2023-05-14 12:03:12,674 :: INFO :: Epoch 20: loss tensor(726.3179, device='cuda:0'), U.norm 7.489304542541504, V.norm 15.638911247253418, MLP.norm 7.050005912780762
2023-05-14 12:03:12,689 :: INFO :: Environment 2
2023-05-14 12:03:22,064 :: INFO :: Epoch 5: loss tensor(821.6847, device='cuda:0'), U.norm 13.155658721923828, V.norm 16.889244079589844, MLP.norm 2.139667272567749
2023-05-14 12:03:22,299 :: INFO :: Epoch 10: loss tensor(797.0227, device='cuda:0'), U.norm 10.342985153198242, V.norm 16.193347930908203, MLP.norm 3.6063661575317383
2023-05-14 12:03:22,534 :: INFO :: Epoch 15: loss tensor(762.7552, device='cuda:0'), U.norm 8.63952350616455, V.norm 15.85777759552002, MLP.norm 5.345634937286377
2023-05-14 12:03:22,784 :: INFO :: Epoch 20: loss tensor(725.2324, device='cuda:0'), U.norm 7.475948810577393, V.norm 15.633336067199707, MLP.norm 7.148841381072998
2023-05-14 12:03:22,784 :: INFO :: Environment 3
2023-05-14 12:03:32,144 :: INFO :: Epoch 5: loss tensor(868.2234, device='cuda:0'), U.norm 13.159645080566406, V.norm 16.99323844909668, MLP.norm 2.149503469467163
2023-05-14 12:03:32,378 :: INFO :: Epoch 10: loss tensor(840.9801, device='cuda:0'), U.norm 10.352229118347168, V.norm 16.342863082885742, MLP.norm 3.6609532833099365
2023-05-14 12:03:32,612 :: INFO :: Epoch 15: loss tensor(803.5703, device='cuda:0'), U.norm 8.655778884887695, V.norm 16.02696418762207, MLP.norm 5.43019962310791
2023-05-14 12:03:32,847 :: INFO :: Epoch 20: loss tensor(761.7867, device='cuda:0'), U.norm 7.500767707824707, V.norm 15.81550121307373, MLP.norm 7.268904685974121
2023-05-14 12:03:32,862 :: INFO :: Environment 4
2023-05-14 12:03:42,379 :: INFO :: Epoch 5: loss tensor(885.9465, device='cuda:0'), U.norm 13.156479835510254, V.norm 17.014446258544922, MLP.norm 2.192281484603882
2023-05-14 12:03:42,613 :: INFO :: Epoch 10: loss tensor(857.0052, device='cuda:0'), U.norm 10.346564292907715, V.norm 16.368600845336914, MLP.norm 3.774351119995117
2023-05-14 12:03:42,848 :: INFO :: Epoch 15: loss tensor(817.7902, device='cuda:0'), U.norm 8.647082328796387, V.norm 16.055212020874023, MLP.norm 5.612770080566406
2023-05-14 12:03:43,082 :: INFO :: Epoch 20: loss tensor(775.2299, device='cuda:0'), U.norm 7.488407611846924, V.norm 15.845390319824219, MLP.norm 7.528124809265137
2023-05-14 12:03:43,082 :: INFO :: Environment 5
2023-05-14 12:03:52,458 :: INFO :: Epoch 5: loss tensor(854.8801, device='cuda:0'), U.norm 13.157281875610352, V.norm 16.957685470581055, MLP.norm 2.184299945831299
2023-05-14 12:03:52,677 :: INFO :: Epoch 10: loss tensor(828.5184, device='cuda:0'), U.norm 10.347418785095215, V.norm 16.29047203063965, MLP.norm 3.7023673057556152
2023-05-14 12:03:52,911 :: INFO :: Epoch 15: loss tensor(793.2852, device='cuda:0'), U.norm 8.647418975830078, V.norm 15.966443061828613, MLP.norm 5.464942455291748
2023-05-14 12:03:53,145 :: INFO :: Epoch 20: loss tensor(754.2217, device='cuda:0'), U.norm 7.488337993621826, V.norm 15.748204231262207, MLP.norm 7.318854331970215
2023-05-14 12:03:53,161 :: INFO :: Environment 6
2023-05-14 12:04:02,693 :: INFO :: Epoch 5: loss tensor(839.5480, device='cuda:0'), U.norm 13.157156944274902, V.norm 16.92407989501953, MLP.norm 2.175391435623169
2023-05-14 12:04:02,927 :: INFO :: Epoch 10: loss tensor(811.9303, device='cuda:0'), U.norm 10.347271919250488, V.norm 16.244548797607422, MLP.norm 3.7023205757141113
2023-05-14 12:04:03,177 :: INFO :: Epoch 15: loss tensor(775.5492, device='cuda:0'), U.norm 8.647202491760254, V.norm 15.915831565856934, MLP.norm 5.460270881652832
2023-05-14 12:04:03,412 :: INFO :: Epoch 20: loss tensor(736.0673, device='cuda:0'), U.norm 7.4876251220703125, V.norm 15.698244094848633, MLP.norm 7.273549556732178
2023-05-14 12:04:03,427 :: INFO :: Environment 7
2023-05-14 12:04:12,944 :: INFO :: Epoch 5: loss tensor(838.4651, device='cuda:0'), U.norm 13.157768249511719, V.norm 16.924800872802734, MLP.norm 2.144545078277588
2023-05-14 12:04:13,194 :: INFO :: Epoch 10: loss tensor(811.9263, device='cuda:0'), U.norm 10.34786605834961, V.norm 16.242048263549805, MLP.norm 3.6650185585021973
2023-05-14 12:04:13,428 :: INFO :: Epoch 15: loss tensor(776.1516, device='cuda:0'), U.norm 8.648435592651367, V.norm 15.909919738769531, MLP.norm 5.453350067138672
2023-05-14 12:04:13,662 :: INFO :: Epoch 20: loss tensor(737.4321, device='cuda:0'), U.norm 7.489965915679932, V.norm 15.687167167663574, MLP.norm 7.308760643005371
2023-05-14 12:04:13,678 :: INFO :: Environment 8
2023-05-14 12:04:23,210 :: INFO :: Epoch 5: loss tensor(859.8460, device='cuda:0'), U.norm 13.158031463623047, V.norm 16.97214698791504, MLP.norm 2.1555449962615967
2023-05-14 12:04:23,445 :: INFO :: Epoch 10: loss tensor(832.8674, device='cuda:0'), U.norm 10.34985065460205, V.norm 16.311452865600586, MLP.norm 3.6625757217407227
2023-05-14 12:04:23,679 :: INFO :: Epoch 15: loss tensor(796.1583, device='cuda:0'), U.norm 8.652542114257812, V.norm 15.99136734008789, MLP.norm 5.4376678466796875
2023-05-14 12:04:23,898 :: INFO :: Epoch 20: loss tensor(755.9276, device='cuda:0'), U.norm 7.496525287628174, V.norm 15.777645111083984, MLP.norm 7.29607629776001
2023-05-14 12:04:23,914 :: INFO :: Environment 9
2023-05-14 12:04:33,258 :: INFO :: Epoch 5: loss tensor(858.1609, device='cuda:0'), U.norm 13.15622329711914, V.norm 16.966672897338867, MLP.norm 2.160280466079712
2023-05-14 12:04:33,492 :: INFO :: Epoch 10: loss tensor(830.6180, device='cuda:0'), U.norm 10.345008850097656, V.norm 16.301469802856445, MLP.norm 3.6787986755371094
2023-05-14 12:04:33,727 :: INFO :: Epoch 15: loss tensor(793.7725, device='cuda:0'), U.norm 8.643503189086914, V.norm 15.978910446166992, MLP.norm 5.461211204528809
2023-05-14 12:04:33,945 :: INFO :: Epoch 20: loss tensor(753.0798, device='cuda:0'), U.norm 7.482150077819824, V.norm 15.762491226196289, MLP.norm 7.30648946762085
2023-05-14 12:04:33,961 :: INFO :: Ite = 1, Delta = 4067
2023-05-14 12:04:33,961 :: INFO :: ----- backend -----
2023-05-14 12:04:38,430 :: INFO :: Epoch 5: loss tensor(100.2005, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0151,  0.0159,  0.0220,  0.0177,  0.0118,  0.0128,  0.0129,  0.0189,
         0.0071,  0.0217,  0.0147,  0.0193,  0.0100,  0.0201,  0.0249,  0.0225,
         0.0218,  0.0220,  0.0223,  0.0191,  0.0157,  0.0215,  0.0223,  0.0082,
         0.0234,  0.0121,  0.0183,  0.0137,  0.0255,  0.0199,  0.0244,  0.0247,
         0.0134,  0.0239,  0.0053,  0.0045,  0.0235,  0.0201,  0.0108,  0.0255,
         0.0123,  0.0074,  0.0255,  0.0197,  0.0152,  0.0244,  0.0182,  0.0076,
         0.0221,  0.0180,  0.0242,  0.0145,  0.0226,  0.0152,  0.0215,  0.0221,
         0.0168,  0.0244,  0.0115,  0.0099,  0.0198,  0.0235,  0.0067,  0.0173,
         0.0181,  0.0089,  0.0255,  0.0167,  0.0247,  0.0127,  0.0243,  0.0108,
         0.0227,  0.0093,  0.0202,  0.0117,  0.0234,  0.0107,  0.0163,  0.0200,
         0.0211,  0.0147,  0.0220,  0.0150,  0.0056,  0.0230,  0.0241,  0.0168,
         0.0116,  0.0010,  0.0123,  0.0200,  0.0219,  0.0217,  0.0154,  0.0204,
         0.0224,  0.0232,  0.0169,  0.0191,  0.0082,  0.0151,  0.0192,  0.0018,
         0.0202,  0.0166,  0.0212,  0.0205,  0.0197,  0.0201,  0.0223,  0.0172,
         0.0247,  0.0232,  0.0110,  0.0131,  0.0183,  0.0164,  0.0156,  0.0161,
         0.0217,  0.0182,  0.0147,  0.0147,  0.0076,  0.0212,  0.0132,  0.0186,
         0.0174,  0.0202,  0.0253,  0.0241,  0.0234,  0.0198,  0.0098,  0.0043,
        -0.0055,  0.0135,  0.0109,  0.0025,  0.0225,  0.0083,  0.0110, -0.0010,
         0.0212,  0.0254,  0.0141,  0.0037,  0.0227,  0.0125,  0.0057,  0.0155,
         0.0252, -0.0208,  0.0090,  0.0008,  0.0077, -0.0186,  0.0244, -0.0182,
         0.0251, -0.0193,  0.0228,  0.0214,  0.0246,  0.0160, -0.0010,  0.0053,
         0.0079,  0.0127,  0.0205,  0.0197, -0.0186,  0.0205,  0.0255,  0.0027,
         0.0202,  0.0220,  0.0117,  0.0071,  0.0142,  0.0069,  0.0192, -0.0175,
         0.0219,  0.0220, -0.0135,  0.0168,  0.0201, -0.0194, -0.0159,  0.0119,
         0.0250, -0.0107,  0.0151,  0.0211,  0.0082,  0.0178,  0.0248,  0.0122,
         0.0250,  0.0094,  0.0247,  0.0228,  0.0206,  0.0036,  0.0083, -0.0198,
         0.0212,  0.0055,  0.0229,  0.0247,  0.0197,  0.0036,  0.0208,  0.0178,
         0.0078, -0.0050,  0.0247, -0.0178,  0.0130,  0.0102,  0.0222,  0.0219,
         0.0198,  0.0136,  0.0139,  0.0160,  0.0156,  0.0254,  0.0220,  0.0206,
         0.0243,  0.0182,  0.0249, -0.0001,  0.0072,  0.0121,  0.0146,  0.0189,
         0.0099,  0.0142,  0.0095,  0.0246,  0.0096,  0.0250, -0.0159,  0.0167,
         0.0150,  0.0128, -0.0150,  0.0251,  0.0090,  0.0166, -0.0161,  0.0134,
         0.0169,  0.0208,  0.0136,  0.0254,  0.0222,  0.0154,  0.0095,  0.0125,
         0.0254,  0.0109,  0.0252,  0.0254,  0.0169,  0.0249,  0.0124,  0.0129,
         0.0187,  0.0186,  0.0175,  0.0172,  0.0133,  0.0250,  0.0254,  0.0162,
         0.0157,  0.0239,  0.0252,  0.0134,  0.0193,  0.0212,  0.0100,  0.0249,
         0.0253,  0.0138,  0.0137,  0.0137,  0.0135,  0.0206,  0.0095,  0.0254,
         0.0222,  0.0211,  0.0214,  0.0252,  0.0099,  0.0259,  0.0163,  0.0032,
         0.0178,  0.0129,  0.0197,  0.0115,  0.0167,  0.0118,  0.0140,  0.0154,
         0.0212,  0.0200,  0.0102,  0.0211,  0.0145,  0.0186,  0.0253,  0.0142,
         0.0256,  0.0157,  0.0255,  0.0192,  0.0172,  0.0257,  0.0030,  0.0137,
         0.0136,  0.0176,  0.0253,  0.0139,  0.0128,  0.0088,  0.0201,  0.0187,
         0.0140,  0.0188,  0.0247,  0.0163,  0.0193,  0.0148,  0.0155,  0.0167,
         0.0118,  0.0185,  0.0250,  0.0245,  0.0127,  0.0243,  0.0128,  0.0169,
         0.0154,  0.0082,  0.0151,  0.0182,  0.0054,  0.0218,  0.0166,  0.0211,
         0.0160,  0.0095,  0.0208,  0.0068,  0.0254,  0.0255,  0.0206,  0.0149,
         0.0221,  0.0165,  0.0169, -0.0026,  0.0185,  0.0159,  0.0207,  0.0257,
         0.0210,  0.0159,  0.0193,  0.0057,  0.0255,  0.0125,  0.0185,  0.0245],
       device='cuda:0', requires_grad=True) MLP.norm tensor(3.4773, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:04:42,821 :: INFO :: Epoch 10: loss tensor(96.8955, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0439,  0.0424,  0.0511,  0.0460,  0.0377,  0.0416,  0.0414,  0.0453,
         0.0341,  0.0479,  0.0439,  0.0449,  0.0356,  0.0496,  0.0528,  0.0466,
         0.0442,  0.0492,  0.0461,  0.0473,  0.0414,  0.0463,  0.0498,  0.0381,
         0.0497,  0.0371,  0.0421,  0.0414,  0.0550,  0.0450,  0.0512,  0.0516,
         0.0421,  0.0509,  0.0338,  0.0322,  0.0476,  0.0469,  0.0370,  0.0540,
         0.0392,  0.0341,  0.0532,  0.0451,  0.0397,  0.0502,  0.0426,  0.0356,
         0.0498,  0.0421,  0.0514,  0.0407,  0.0441,  0.0378,  0.0503,  0.0500,
         0.0444,  0.0539,  0.0414,  0.0401,  0.0436,  0.0479,  0.0353,  0.0444,
         0.0465,  0.0346,  0.0528,  0.0460,  0.0500,  0.0409,  0.0484,  0.0390,
         0.0439,  0.0364,  0.0479,  0.0389,  0.0497,  0.0381,  0.0435,  0.0451,
         0.0503,  0.0446,  0.0486,  0.0442,  0.0343,  0.0518,  0.0491,  0.0456,
         0.0381,  0.0264,  0.0395,  0.0482,  0.0482,  0.0479,  0.0446,  0.0481,
         0.0476,  0.0507,  0.0443,  0.0457,  0.0371,  0.0374,  0.0468,  0.0286,
         0.0492,  0.0461,  0.0481,  0.0456,  0.0452,  0.0478,  0.0515,  0.0424,
         0.0534,  0.0497,  0.0386,  0.0390,  0.0448,  0.0446,  0.0361,  0.0440,
         0.0488,  0.0456,  0.0421,  0.0393,  0.0332,  0.0442,  0.0413,  0.0458,
         0.0430,  0.0477,  0.0537,  0.0447,  0.0455,  0.0447,  0.0380,  0.0335,
         0.0234,  0.0168,  0.0388,  0.0309,  0.0511,  0.0378,  0.0404,  0.0272,
         0.0505,  0.0521,  0.0351,  0.0346,  0.0517,  0.0431,  0.0356,  0.0448,
         0.0532, -0.0338,  0.0371,  0.0307,  0.0368, -0.0216,  0.0468, -0.0260,
         0.0511, -0.0327,  0.0464,  0.0500,  0.0500,  0.0289,  0.0260,  0.0341,
         0.0373,  0.0403,  0.0494,  0.0460, -0.0250,  0.0490,  0.0543,  0.0331,
         0.0289,  0.0492,  0.0403,  0.0341,  0.0423,  0.0373,  0.0367, -0.0162,
         0.0487,  0.0515,  0.0013,  0.0458,  0.0499, -0.0295, -0.0134,  0.0412,
         0.0503,  0.0111,  0.0349,  0.0493,  0.0384,  0.0238,  0.0516,  0.0424,
         0.0534,  0.0402,  0.0510,  0.0420,  0.0487,  0.0342,  0.0370, -0.0332,
         0.0453,  0.0341,  0.0527,  0.0497,  0.0331,  0.0325,  0.0479,  0.0486,
         0.0369,  0.0209,  0.0518, -0.0177,  0.0421,  0.0386,  0.0500,  0.0496,
         0.0433,  0.0435,  0.0407,  0.0469,  0.0409,  0.0543,  0.0487,  0.0403,
         0.0430,  0.0360,  0.0497,  0.0300,  0.0367,  0.0409,  0.0202,  0.0427,
         0.0382,  0.0439,  0.0401,  0.0499,  0.0394,  0.0532, -0.0049,  0.0450,
         0.0438,  0.0421, -0.0092,  0.0528,  0.0397,  0.0469, -0.0032,  0.0417,
         0.0469,  0.0511,  0.0446,  0.0550,  0.0519,  0.0460,  0.0401,  0.0431,
         0.0553,  0.0404,  0.0547,  0.0555,  0.0470,  0.0546,  0.0425,  0.0435,
         0.0482,  0.0487,  0.0477,  0.0472,  0.0434,  0.0549,  0.0549,  0.0462,
         0.0461,  0.0545,  0.0551,  0.0446,  0.0500,  0.0503,  0.0406,  0.0550,
         0.0545,  0.0444,  0.0428,  0.0443,  0.0442,  0.0497,  0.0390,  0.0542,
         0.0523,  0.0504,  0.0510,  0.0548,  0.0408,  0.0559,  0.0459,  0.0340,
         0.0475,  0.0435,  0.0496,  0.0407,  0.0468,  0.0416,  0.0444,  0.0459,
         0.0515,  0.0493,  0.0398,  0.0513,  0.0442,  0.0492,  0.0553,  0.0451,
         0.0555,  0.0453,  0.0553,  0.0491,  0.0472,  0.0549,  0.0339,  0.0441,
         0.0438,  0.0481,  0.0553,  0.0438,  0.0434,  0.0394,  0.0505,  0.0477,
         0.0437,  0.0490,  0.0534,  0.0459,  0.0497,  0.0453,  0.0457,  0.0465,
         0.0416,  0.0467,  0.0540,  0.0544,  0.0428,  0.0543,  0.0432,  0.0470,
         0.0458,  0.0367,  0.0448,  0.0484,  0.0364,  0.0511,  0.0464,  0.0512,
         0.0463,  0.0407,  0.0508,  0.0374,  0.0543,  0.0550,  0.0508,  0.0453,
         0.0519,  0.0468,  0.0473,  0.0274,  0.0488,  0.0462,  0.0508,  0.0561,
         0.0507,  0.0441,  0.0493,  0.0361,  0.0550,  0.0431,  0.0489,  0.0544],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.9713, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:04:47,259 :: INFO :: Epoch 15: loss tensor(95.9389, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0689,  0.0590,  0.0787,  0.0694,  0.0596,  0.0691,  0.0641,  0.0680,
         0.0539,  0.0680,  0.0705,  0.0670,  0.0554,  0.0755,  0.0769,  0.0645,
         0.0568,  0.0686,  0.0619,  0.0720,  0.0637,  0.0622,  0.0734,  0.0665,
         0.0716,  0.0528,  0.0564,  0.0659,  0.0832,  0.0607,  0.0741,  0.0716,
         0.0663,  0.0717,  0.0582,  0.0539,  0.0631,  0.0727,  0.0576,  0.0789,
         0.0634,  0.0570,  0.0769,  0.0650,  0.0563,  0.0685,  0.0623,  0.0616,
         0.0742,  0.0535,  0.0730,  0.0590,  0.0592,  0.0516,  0.0767,  0.0731,
         0.0681,  0.0817,  0.0675,  0.0682,  0.0564,  0.0677,  0.0598,  0.0650,
         0.0686,  0.0528,  0.0758,  0.0703,  0.0705,  0.0639,  0.0622,  0.0623,
         0.0568,  0.0636,  0.0679,  0.0588,  0.0700,  0.0618,  0.0616,  0.0631,
         0.0773,  0.0744,  0.0704,  0.0709,  0.0572,  0.0769,  0.0686,  0.0701,
         0.0572,  0.0425,  0.0617,  0.0746,  0.0696,  0.0665,  0.0712,  0.0700,
         0.0666,  0.0724,  0.0686,  0.0650,  0.0613,  0.0505,  0.0691,  0.0543,
         0.0761,  0.0729,  0.0708,  0.0618,  0.0636,  0.0716,  0.0791,  0.0604,
         0.0791,  0.0703,  0.0620,  0.0602,  0.0640,  0.0708,  0.0467,  0.0679,
         0.0717,  0.0684,  0.0653,  0.0523,  0.0529,  0.0643,  0.0675,  0.0698,
         0.0630,  0.0723,  0.0815,  0.0549,  0.0540,  0.0643,  0.0629,  0.0623,
         0.0562,  0.0186,  0.0632,  0.0591,  0.0784,  0.0671,  0.0693,  0.0576,
         0.0794,  0.0767,  0.0450,  0.0672,  0.0806,  0.0741,  0.0663,  0.0720,
         0.0795, -0.0416,  0.0631,  0.0628,  0.0658, -0.0119,  0.0589, -0.0249,
         0.0712, -0.0411,  0.0629,  0.0774,  0.0684,  0.0282,  0.0539,  0.0623,
         0.0665,  0.0648,  0.0773,  0.0669, -0.0215,  0.0770,  0.0824,  0.0652,
         0.0320,  0.0727,  0.0671,  0.0590,  0.0683,  0.0683,  0.0447, -0.0012,
         0.0720,  0.0804,  0.0257,  0.0725,  0.0785, -0.0326,  0.0035,  0.0700,
         0.0713,  0.0396,  0.0405,  0.0765,  0.0685,  0.0250,  0.0756,  0.0729,
         0.0815,  0.0710,  0.0736,  0.0529,  0.0760,  0.0656,  0.0649, -0.0416,
         0.0578,  0.0611,  0.0821,  0.0693,  0.0367,  0.0613,  0.0715,  0.0797,
         0.0652,  0.0487,  0.0780, -0.0045,  0.0690,  0.0636,  0.0738,  0.0752,
         0.0571,  0.0720,  0.0629,  0.0781,  0.0595,  0.0829,  0.0718,  0.0487,
         0.0500,  0.0431,  0.0690,  0.0623,  0.0661,  0.0687,  0.0240,  0.0591,
         0.0639,  0.0735,  0.0717,  0.0706,  0.0694,  0.0806,  0.0201,  0.0723,
         0.0714,  0.0711,  0.0107,  0.0801,  0.0712,  0.0780,  0.0253,  0.0676,
         0.0790,  0.0827,  0.0782,  0.0840,  0.0820,  0.0780,  0.0718,  0.0746,
         0.0864,  0.0709,  0.0851,  0.0860,  0.0768,  0.0839,  0.0714,  0.0752,
         0.0785,  0.0794,  0.0778,  0.0778,  0.0746,  0.0854,  0.0840,  0.0764,
         0.0770,  0.0879,  0.0852,  0.0775,  0.0830,  0.0782,  0.0723,  0.0861,
         0.0819,  0.0763,  0.0727,  0.0759,  0.0774,  0.0780,  0.0679,  0.0820,
         0.0831,  0.0794,  0.0791,  0.0841,  0.0735,  0.0879,  0.0771,  0.0676,
         0.0784,  0.0767,  0.0777,  0.0697,  0.0775,  0.0722,  0.0749,  0.0779,
         0.0835,  0.0797,  0.0692,  0.0814,  0.0734,  0.0823,  0.0862,  0.0776,
         0.0860,  0.0761,  0.0854,  0.0793,  0.0785,  0.0834,  0.0678,  0.0758,
         0.0738,  0.0790,  0.0859,  0.0743,  0.0753,  0.0711,  0.0824,  0.0750,
         0.0712,  0.0800,  0.0833,  0.0745,  0.0806,  0.0763,  0.0764,  0.0777,
         0.0720,  0.0718,  0.0812,  0.0854,  0.0732,  0.0856,  0.0737,  0.0772,
         0.0771,  0.0617,  0.0755,  0.0785,  0.0697,  0.0804,  0.0753,  0.0820,
         0.0778,  0.0746,  0.0828,  0.0707,  0.0836,  0.0840,  0.0817,  0.0763,
         0.0812,  0.0781,  0.0787,  0.0613,  0.0788,  0.0761,  0.0833,  0.0884,
         0.0798,  0.0693,  0.0792,  0.0686,  0.0846,  0.0750,  0.0805,  0.0837],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.2049, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:04:51,728 :: INFO :: Epoch 20: loss tensor(93.7275, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0867,  0.0651,  0.1039,  0.0860,  0.0750,  0.0935,  0.0794,  0.0843,
         0.0642,  0.0787,  0.0925,  0.0828,  0.0678,  0.0952,  0.0933,  0.0724,
         0.0594,  0.0790,  0.0688,  0.0920,  0.0797,  0.0684,  0.0909,  0.0906,
         0.0882,  0.0608,  0.0621,  0.0856,  0.1079,  0.0668,  0.0902,  0.0826,
         0.0835,  0.0844,  0.0734,  0.0652,  0.0679,  0.0937,  0.0714,  0.0968,
         0.0791,  0.0737,  0.0931,  0.0771,  0.0627,  0.0779,  0.0743,  0.0808,
         0.0937,  0.0557,  0.0862,  0.0665,  0.0674,  0.0566,  0.0992,  0.0892,
         0.0873,  0.1082,  0.0881,  0.0909,  0.0597,  0.0829,  0.0774,  0.0755,
         0.0802,  0.0640,  0.0915,  0.0876,  0.0835,  0.0804,  0.0668,  0.0775,
         0.0611,  0.0878,  0.0801,  0.0687,  0.0804,  0.0793,  0.0690,  0.0725,
         0.0997,  0.1023,  0.0849,  0.0922,  0.0706,  0.0970,  0.0792,  0.0876,
         0.0677,  0.0460,  0.0773,  0.0968,  0.0844,  0.0748,  0.0930,  0.0841,
         0.0755,  0.0855,  0.0889,  0.0766,  0.0776,  0.0571,  0.0839,  0.0743,
         0.0994,  0.0954,  0.0843,  0.0690,  0.0741,  0.0895,  0.1046,  0.0681,
         0.0988,  0.0842,  0.0793,  0.0745,  0.0757,  0.0924,  0.0502,  0.0865,
         0.0891,  0.0822,  0.0815,  0.0557,  0.0640,  0.0795,  0.0879,  0.0889,
         0.0773,  0.0921,  0.1063,  0.0549,  0.0549,  0.0750,  0.0823,  0.0877,
         0.0869,  0.0202,  0.0821,  0.0838,  0.1005,  0.0932,  0.0936,  0.0879,
         0.1054,  0.0963,  0.0470,  0.0982,  0.1065,  0.1022,  0.0945,  0.0970,
         0.1013, -0.0469,  0.0846,  0.0930,  0.0919,  0.0027,  0.0620, -0.0168,
         0.0843, -0.0447,  0.0729,  0.1014,  0.0791,  0.0283,  0.0784,  0.0870,
         0.0927,  0.0840,  0.0997,  0.0802, -0.0125,  0.1015,  0.1073,  0.0951,
         0.0342,  0.0898,  0.0896,  0.0799,  0.0893,  0.0967,  0.0454,  0.0172,
         0.0887,  0.1048,  0.0494,  0.0933,  0.1021, -0.0319,  0.0273,  0.0957,
         0.0862,  0.0663,  0.0396,  0.0983,  0.0965,  0.0244,  0.0946,  0.1006,
         0.1064,  0.0993,  0.0909,  0.0553,  0.0991,  0.0952,  0.0892, -0.0460,
         0.0609,  0.0839,  0.1073,  0.0813,  0.0383,  0.0867,  0.0891,  0.1082,
         0.0897,  0.0734,  0.0992,  0.0127,  0.0926,  0.0832,  0.0928,  0.0977,
         0.0622,  0.0972,  0.0774,  0.1068,  0.0677,  0.1087,  0.0883,  0.0483,
         0.0542,  0.0435,  0.0801,  0.0927,  0.0924,  0.0933,  0.0242,  0.0678,
         0.0838,  0.1003,  0.1013,  0.0867,  0.0967,  0.1052,  0.0454,  0.0958,
         0.0954,  0.0973,  0.0362,  0.1041,  0.1002,  0.1067,  0.0545,  0.0886,
         0.1104,  0.1134,  0.1118,  0.1092,  0.1100,  0.1093,  0.1009,  0.1036,
         0.1176,  0.0999,  0.1136,  0.1145,  0.1029,  0.1092,  0.0966,  0.1048,
         0.1075,  0.1081,  0.1048,  0.1060,  0.1047,  0.1140,  0.1099,  0.1034,
         0.1051,  0.1212,  0.1126,  0.1100,  0.1163,  0.1005,  0.1019,  0.1152,
         0.1038,  0.1064,  0.1006,  0.1050,  0.1112,  0.1019,  0.0929,  0.1063,
         0.1112,  0.1065,  0.1007,  0.1097,  0.1055,  0.1197,  0.1088,  0.1010,
         0.1082,  0.1096,  0.1004,  0.0958,  0.1051,  0.1011,  0.1035,  0.1096,
         0.1144,  0.1087,  0.0966,  0.1080,  0.0991,  0.1151,  0.1145,  0.1094,
         0.1133,  0.1067,  0.1126,  0.1074,  0.1086,  0.1079,  0.1012,  0.1063,
         0.1006,  0.1076,  0.1143,  0.1023,  0.1050,  0.1002,  0.1134,  0.0968,
         0.0925,  0.1085,  0.1124,  0.0969,  0.1085,  0.1047,  0.1047,  0.1069,
         0.0990,  0.0889,  0.1032,  0.1149,  0.1013,  0.1149,  0.1011,  0.1050,
         0.1063,  0.0794,  0.1049,  0.1056,  0.1026,  0.1077,  0.1009,  0.1103,
         0.1080,  0.1084,  0.1145,  0.1040,  0.1110,  0.1098,  0.1107,  0.1064,
         0.1071,  0.1071,  0.1080,  0.0947,  0.1046,  0.1022,  0.1160,  0.1200,
         0.1051,  0.0853,  0.1051,  0.1008,  0.1110,  0.1047,  0.1107,  0.1097],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.1250, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:04:56,150 :: INFO :: Epoch 25: loss tensor(91.0699, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0974,  0.0646,  0.1241,  0.0956,  0.0815,  0.1124,  0.0877,  0.0923,
         0.0693,  0.0823,  0.1102,  0.0915,  0.0697,  0.1098,  0.1043,  0.0743,
         0.0574,  0.0845,  0.0681,  0.1053,  0.0858,  0.0695,  0.1020,  0.1096,
         0.0983,  0.0591,  0.0588,  0.0973,  0.1287,  0.0650,  0.0997,  0.0866,
         0.0946,  0.0887,  0.0811,  0.0670,  0.0678,  0.1074,  0.0748,  0.1091,
         0.0865,  0.0808,  0.1030,  0.0806,  0.0629,  0.0805,  0.0766,  0.0948,
         0.1071,  0.0546,  0.0923,  0.0670,  0.0678,  0.0522,  0.1172,  0.0994,
         0.0995,  0.1320,  0.1030,  0.1068,  0.0578,  0.0879,  0.0907,  0.0782,
         0.0859,  0.0668,  0.1016,  0.0992,  0.0893,  0.0859,  0.0680,  0.0874,
         0.0580,  0.1062,  0.0872,  0.0702,  0.0860,  0.0895,  0.0725,  0.0774,
         0.1181,  0.1269,  0.0922,  0.1086,  0.0757,  0.1106,  0.0830,  0.0979,
         0.0722,  0.0441,  0.0840,  0.1140,  0.0903,  0.0764,  0.1096,  0.0923,
         0.0775,  0.0925,  0.1001,  0.0800,  0.0876,  0.0566,  0.0934,  0.0856,
         0.1187,  0.1115,  0.0892,  0.0683,  0.0755,  0.0997,  0.1255,  0.0686,
         0.1133,  0.0894,  0.0884,  0.0779,  0.0802,  0.1079,  0.0481,  0.0979,
         0.0991,  0.0900,  0.0884,  0.0570,  0.0662,  0.0860,  0.1031,  0.1002,
         0.0872,  0.1054,  0.1257,  0.0503,  0.0530,  0.0798,  0.0943,  0.1074,
         0.1142,  0.0222,  0.0953,  0.1034,  0.1138,  0.1146,  0.1108,  0.1165,
         0.1260,  0.1086,  0.0446,  0.1254,  0.1272,  0.1257,  0.1182,  0.1174,
         0.1156, -0.0497,  0.0994,  0.1203,  0.1131,  0.0179,  0.0606, -0.0054,
         0.0905, -0.0453,  0.0747,  0.1204,  0.0837,  0.0266,  0.0978,  0.1060,
         0.1145,  0.0970,  0.1156,  0.0843, -0.0020,  0.1201,  0.1271,  0.1212,
         0.0332,  0.0975,  0.1056,  0.0951,  0.1024,  0.1211,  0.0419,  0.0349,
         0.0954,  0.1222,  0.0687,  0.1054,  0.1198, -0.0292,  0.0516,  0.1159,
         0.0917,  0.0890,  0.0363,  0.1113,  0.1199,  0.0207,  0.1078,  0.1227,
         0.1254,  0.1219,  0.1015,  0.0538,  0.1152,  0.1205,  0.1082, -0.0479,
         0.0565,  0.1000,  0.1249,  0.0819,  0.0350,  0.1066,  0.0992,  0.1320,
         0.1076,  0.0931,  0.1110,  0.0297,  0.1116,  0.0965,  0.1051,  0.1174,
         0.0637,  0.1159,  0.0835,  0.1302,  0.0644,  0.1288,  0.0958,  0.0422,
         0.0556,  0.0402,  0.0816,  0.1187,  0.1134,  0.1124,  0.0232,  0.0691,
         0.0962,  0.1219,  0.1266,  0.0978,  0.1195,  0.1244,  0.0687,  0.1132,
         0.1136,  0.1192,  0.0607,  0.1209,  0.1239,  0.1310,  0.0810,  0.1014,
         0.1411,  0.1428,  0.1440,  0.1301,  0.1356,  0.1385,  0.1271,  0.1296,
         0.1483,  0.1267,  0.1398,  0.1401,  0.1242,  0.1288,  0.1182,  0.1315,
         0.1354,  0.1345,  0.1274,  0.1318,  0.1328,  0.1403,  0.1319,  0.1267,
         0.1298,  0.1531,  0.1362,  0.1414,  0.1483,  0.1164,  0.1286,  0.1419,
         0.1198,  0.1341,  0.1255,  0.1314,  0.1441,  0.1203,  0.1133,  0.1266,
         0.1359,  0.1316,  0.1159,  0.1311,  0.1366,  0.1503,  0.1394,  0.1334,
         0.1363,  0.1413,  0.1171,  0.1192,  0.1291,  0.1282,  0.1309,  0.1401,
         0.1438,  0.1356,  0.1219,  0.1305,  0.1211,  0.1467,  0.1398,  0.1395,
         0.1369,  0.1364,  0.1366,  0.1329,  0.1370,  0.1284,  0.1331,  0.1354,
         0.1246,  0.1330,  0.1399,  0.1273,  0.1323,  0.1261,  0.1430,  0.1121,
         0.1038,  0.1341,  0.1401,  0.1122,  0.1334,  0.1296,  0.1300,  0.1336,
         0.1225,  0.1002,  0.1197,  0.1425,  0.1266,  0.1418,  0.1242,  0.1306,
         0.1327,  0.0900,  0.1309,  0.1289,  0.1343,  0.1328,  0.1225,  0.1356,
         0.1357,  0.1411,  0.1448,  0.1364,  0.1361,  0.1313,  0.1371,  0.1345,
         0.1289,  0.1334,  0.1351,  0.1272,  0.1254,  0.1238,  0.1479,  0.1502,
         0.1254,  0.0925,  0.1266,  0.1320,  0.1334,  0.1318,  0.1390,  0.1318],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.7505, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:05:00,651 :: INFO :: Epoch 30: loss tensor(89.5797, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1029,  0.0626,  0.1410,  0.0998,  0.0864,  0.1253,  0.0899,  0.0977,
         0.0723,  0.0850,  0.1206,  0.0983,  0.0721,  0.1193,  0.1095,  0.0762,
         0.0552,  0.0859,  0.0660,  0.1156,  0.0908,  0.0668,  0.1072,  0.1225,
         0.1061,  0.0606,  0.0549,  0.1051,  0.1454,  0.0641,  0.1074,  0.0868,
         0.1000,  0.0889,  0.0831,  0.0656,  0.0675,  0.1194,  0.0778,  0.1141,
         0.0924,  0.0839,  0.1062,  0.0843,  0.0624,  0.0777,  0.0778,  0.1052,
         0.1145,  0.0528,  0.0918,  0.0660,  0.0699,  0.0517,  0.1293,  0.1058,
         0.1063,  0.1534,  0.1117,  0.1172,  0.0578,  0.0913,  0.0979,  0.0802,
         0.0867,  0.0668,  0.1053,  0.1040,  0.0922,  0.0888,  0.0674,  0.0916,
         0.0549,  0.1242,  0.0903,  0.0687,  0.0868,  0.0955,  0.0714,  0.0785,
         0.1310,  0.1477,  0.0957,  0.1216,  0.0753,  0.1178,  0.0829,  0.1027,
         0.0763,  0.0422,  0.0885,  0.1278,  0.0940,  0.0744,  0.1187,  0.0940,
         0.0773,  0.0929,  0.1088,  0.0806,  0.0879,  0.0572,  0.0971,  0.0954,
         0.1317,  0.1207,  0.0912,  0.0652,  0.0755,  0.1060,  0.1409,  0.0701,
         0.1206,  0.0918,  0.0924,  0.0798,  0.0814,  0.1215,  0.0481,  0.1056,
         0.1045,  0.0935,  0.0889,  0.0572,  0.0675,  0.0920,  0.1114,  0.1083,
         0.0938,  0.1136,  0.1402,  0.0448,  0.0477,  0.0784,  0.1008,  0.1223,
         0.1384,  0.0210,  0.1044,  0.1177,  0.1205,  0.1318,  0.1215,  0.1440,
         0.1418,  0.1138,  0.0403,  0.1497,  0.1430,  0.1445,  0.1380,  0.1342,
         0.1238, -0.0513,  0.1081,  0.1443,  0.1300,  0.0320,  0.0577,  0.0085,
         0.0915, -0.0430,  0.0773,  0.1344,  0.0862,  0.0246,  0.1126,  0.1201,
         0.1316,  0.1047,  0.1251,  0.0834,  0.0091,  0.1335,  0.1418,  0.1441,
         0.0305,  0.0982,  0.1159,  0.1068,  0.1099,  0.1416,  0.0380,  0.0502,
         0.0954,  0.1341,  0.0840,  0.1095,  0.1314, -0.0253,  0.0759,  0.1311,
         0.0926,  0.1073,  0.0338,  0.1170,  0.1396,  0.0181,  0.1149,  0.1399,
         0.1394,  0.1401,  0.1064,  0.0503,  0.1246,  0.1428,  0.1220, -0.0477,
         0.0515,  0.1107,  0.1371,  0.0777,  0.0313,  0.1214,  0.1018,  0.1525,
         0.1198,  0.1083,  0.1162,  0.0447,  0.1267,  0.1030,  0.1134,  0.1341,
         0.0605,  0.1294,  0.0843,  0.1506,  0.0590,  0.1438,  0.0963,  0.0362,
         0.0582,  0.0372,  0.0785,  0.1409,  0.1293,  0.1268,  0.0216,  0.0686,
         0.1020,  0.1392,  0.1484,  0.1048,  0.1384,  0.1394,  0.0886,  0.1247,
         0.1269,  0.1370,  0.0840,  0.1332,  0.1435,  0.1514,  0.1039,  0.1076,
         0.1706,  0.1700,  0.1745,  0.1466,  0.1589,  0.1663,  0.1494,  0.1520,
         0.1783,  0.1519,  0.1634,  0.1628,  0.1408,  0.1420,  0.1352,  0.1549,
         0.1625,  0.1581,  0.1453,  0.1552,  0.1589,  0.1640,  0.1498,  0.1455,
         0.1502,  0.1833,  0.1562,  0.1708,  0.1786,  0.1248,  0.1515,  0.1656,
         0.1311,  0.1592,  0.1488,  0.1544,  0.1759,  0.1343,  0.1286,  0.1444,
         0.1569,  0.1552,  0.1244,  0.1476,  0.1664,  0.1797,  0.1694,  0.1641,
         0.1625,  0.1713,  0.1268,  0.1419,  0.1484,  0.1537,  0.1572,  0.1694,
         0.1710,  0.1610,  0.1456,  0.1479,  0.1395,  0.1771,  0.1612,  0.1683,
         0.1561,  0.1657,  0.1561,  0.1557,  0.1637,  0.1446,  0.1636,  0.1630,
         0.1442,  0.1547,  0.1618,  0.1491,  0.1563,  0.1482,  0.1704,  0.1214,
         0.1090,  0.1561,  0.1669,  0.1198,  0.1541,  0.1507,  0.1523,  0.1571,
         0.1425,  0.1053,  0.1322,  0.1681,  0.1494,  0.1656,  0.1426,  0.1532,
         0.1553,  0.0960,  0.1543,  0.1478,  0.1642,  0.1565,  0.1394,  0.1573,
         0.1622,  0.1724,  0.1743,  0.1676,  0.1592,  0.1482,  0.1607,  0.1608,
         0.1465,  0.1561,  0.1590,  0.1577,  0.1405,  0.1411,  0.1779,  0.1784,
         0.1397,  0.0928,  0.1430,  0.1619,  0.1515,  0.1555,  0.1646,  0.1497],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.1322, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:05:05,136 :: INFO :: Epoch 35: loss tensor(92.2125, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1053,  0.0609,  0.1541,  0.1016,  0.0873,  0.1380,  0.0914,  0.0982,
         0.0759,  0.0838,  0.1298,  0.1003,  0.0700,  0.1271,  0.1127,  0.0754,
         0.0521,  0.0896,  0.0659,  0.1218,  0.0943,  0.0659,  0.1104,  0.1322,
         0.1107,  0.0585,  0.0501,  0.1084,  0.1594,  0.0626,  0.1113,  0.0855,
         0.1053,  0.0887,  0.0840,  0.0648,  0.0664,  0.1273,  0.0784,  0.1193,
         0.0953,  0.0864,  0.1085,  0.0835,  0.0591,  0.0767,  0.0758,  0.1124,
         0.1220,  0.0520,  0.0910,  0.0624,  0.0708,  0.0479,  0.1391,  0.1093,
         0.1126,  0.1709,  0.1176,  0.1242,  0.0559,  0.0932,  0.1047,  0.0789,
         0.0875,  0.0667,  0.1082,  0.1047,  0.0911,  0.0894,  0.0680,  0.0946,
         0.0514,  0.1386,  0.0903,  0.0675,  0.0876,  0.1006,  0.0712,  0.0807,
         0.1415,  0.1674,  0.0953,  0.1311,  0.0758,  0.1236,  0.0802,  0.1066,
         0.0780,  0.0426,  0.0875,  0.1377,  0.0940,  0.0727,  0.1264,  0.0961,
         0.0753,  0.0933,  0.1136,  0.0836,  0.0893,  0.0548,  0.0973,  0.1014,
         0.1430,  0.1307,  0.0896,  0.0631,  0.0760,  0.1093,  0.1539,  0.0676,
         0.1257,  0.0916,  0.0932,  0.0796,  0.0829,  0.1346,  0.0462,  0.1097,
         0.1078,  0.0953,  0.0910,  0.0601,  0.0653,  0.0944,  0.1176,  0.1145,
         0.0934,  0.1159,  0.1496,  0.0390,  0.0425,  0.0749,  0.1006,  0.1317,
         0.1591,  0.0226,  0.1072,  0.1265,  0.1200,  0.1443,  0.1252,  0.1653,
         0.1527,  0.1131,  0.0352,  0.1696,  0.1533,  0.1585,  0.1536,  0.1404,
         0.1256, -0.0522,  0.1115,  0.1655,  0.1426,  0.0432,  0.0534,  0.0202,
         0.0875, -0.0405,  0.0719,  0.1432,  0.0838,  0.0234,  0.1224,  0.1288,
         0.1444,  0.1060,  0.1285,  0.0781,  0.0181,  0.1417,  0.1519,  0.1632,
         0.0274,  0.0930,  0.1205,  0.1137,  0.1116,  0.1580,  0.0315,  0.0619,
         0.0904,  0.1385,  0.0946,  0.1070,  0.1363, -0.0218,  0.0941,  0.1410,
         0.0891,  0.1213,  0.0317,  0.1172,  0.1508,  0.0156,  0.1160,  0.1517,
         0.1478,  0.1503,  0.1063,  0.0442,  0.1277,  0.1578,  0.1305, -0.0471,
         0.0458,  0.1155,  0.1405,  0.0706,  0.0262,  0.1309,  0.0991,  0.1673,
         0.1257,  0.1177,  0.1143,  0.0569,  0.1332,  0.1034,  0.1129,  0.1447,
         0.0566,  0.1334,  0.0813,  0.1644,  0.0538,  0.1536,  0.0908,  0.0315,
         0.0557,  0.0330,  0.0721,  0.1579,  0.1404,  0.1365,  0.0188,  0.0638,
         0.1017,  0.1517,  0.1663,  0.1058,  0.1532,  0.1498,  0.1049,  0.1302,
         0.1350,  0.1505,  0.1008,  0.1405,  0.1574,  0.1673,  0.1232,  0.1073,
         0.1980,  0.1949,  0.2031,  0.1589,  0.1806,  0.1907,  0.1683,  0.1707,
         0.2067,  0.1754,  0.1848,  0.1827,  0.1533,  0.1510,  0.1487,  0.1751,
         0.1874,  0.1791,  0.1593,  0.1755,  0.1833,  0.1853,  0.1641,  0.1606,
         0.1668,  0.2114,  0.1719,  0.1979,  0.2073,  0.1266,  0.1713,  0.1861,
         0.1389,  0.1814,  0.1695,  0.1742,  0.2057,  0.1454,  0.1394,  0.1598,
         0.1739,  0.1772,  0.1273,  0.1608,  0.1940,  0.2071,  0.1973,  0.1925,
         0.1867,  0.1995,  0.1313,  0.1633,  0.1648,  0.1771,  0.1818,  0.1966,
         0.1963,  0.1841,  0.1674,  0.1612,  0.1561,  0.2058,  0.1794,  0.1951,
         0.1713,  0.1934,  0.1718,  0.1761,  0.1882,  0.1574,  0.1921,  0.1887,
         0.1598,  0.1739,  0.1805,  0.1683,  0.1773,  0.1663,  0.1960,  0.1246,
         0.1102,  0.1751,  0.1915,  0.1206,  0.1709,  0.1682,  0.1710,  0.1777,
         0.1592,  0.1081,  0.1414,  0.1911,  0.1695,  0.1864,  0.1569,  0.1743,
         0.1747,  0.1004,  0.1748,  0.1626,  0.1918,  0.1792,  0.1535,  0.1755,
         0.1870,  0.2019,  0.2006,  0.1971,  0.1811,  0.1612,  0.1816,  0.1854,
         0.1604,  0.1756,  0.1802,  0.1862,  0.1498,  0.1554,  0.2059,  0.2048,
         0.1495,  0.0900,  0.1539,  0.1904,  0.1656,  0.1764,  0.1877,  0.1636],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.2951, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:05:09,605 :: INFO :: Epoch 40: loss tensor(90.5855, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.1064,  0.0595,  0.1653,  0.1024,  0.0879,  0.1439,  0.0897,  0.0970,
         0.0776,  0.0820,  0.1331,  0.1023,  0.0673,  0.1325,  0.1145,  0.0762,
         0.0508,  0.0913,  0.0626,  0.1250,  0.0954,  0.0644,  0.1124,  0.1394,
         0.1138,  0.0557,  0.0470,  0.1091,  0.1698,  0.0601,  0.1155,  0.0843,
         0.1067,  0.0859,  0.0842,  0.0618,  0.0652,  0.1323,  0.0779,  0.1188,
         0.0982,  0.0830,  0.1049,  0.0833,  0.0599,  0.0722,  0.0734,  0.1191,
         0.1215,  0.0507,  0.0876,  0.0596,  0.0711,  0.0457,  0.1425,  0.1115,
         0.1138,  0.1860,  0.1192,  0.1248,  0.0565,  0.0882,  0.1079,  0.0786,
         0.0872,  0.0640,  0.1070,  0.1064,  0.0906,  0.0868,  0.0680,  0.0946,
         0.0462,  0.1500,  0.0890,  0.0652,  0.0876,  0.1033,  0.0710,  0.0802,
         0.1508,  0.1804,  0.0951,  0.1388,  0.0734,  0.1237,  0.0770,  0.1093,
         0.0808,  0.0403,  0.0861,  0.1456,  0.0931,  0.0709,  0.1274,  0.0957,
         0.0738,  0.0907,  0.1158,  0.0826,  0.0867,  0.0541,  0.0972,  0.1061,
         0.1503,  0.1318,  0.0891,  0.0596,  0.0723,  0.1118,  0.1620,  0.0691,
         0.1265,  0.0905,  0.0931,  0.0781,  0.0810,  0.1466,  0.0451,  0.1122,
         0.1078,  0.0950,  0.0874,  0.0588,  0.0635,  0.0950,  0.1227,  0.1189,
         0.0912,  0.1151,  0.1544,  0.0326,  0.0353,  0.0712,  0.0968,  0.1367,
         0.1758,  0.0223,  0.1046,  0.1296,  0.1141,  0.1524,  0.1235,  0.1836,
         0.1589,  0.1086,  0.0308,  0.1853,  0.1588,  0.1679,  0.1646,  0.1403,
         0.1220, -0.0532,  0.1093,  0.1834,  0.1507,  0.0504,  0.0497,  0.0300,
         0.0809, -0.0369,  0.0689,  0.1470,  0.0807,  0.0230,  0.1267,  0.1328,
         0.1520,  0.1030,  0.1271,  0.0723,  0.0243,  0.1453,  0.1579,  0.1780,
         0.0241,  0.0845,  0.1191,  0.1179,  0.1094,  0.1699,  0.0303,  0.0688,
         0.0831,  0.1386,  0.0995,  0.1014,  0.1374, -0.0192,  0.1084,  0.1455,
         0.0843,  0.1296,  0.0287,  0.1143,  0.1553,  0.0128,  0.1123,  0.1575,
         0.1516,  0.1533,  0.1028,  0.0382,  0.1248,  0.1668,  0.1334, -0.0462,
         0.0411,  0.1158,  0.1370,  0.0632,  0.0226,  0.1352,  0.0919,  0.1773,
         0.1262,  0.1215,  0.1087,  0.0641,  0.1320,  0.0983,  0.1069,  0.1497,
         0.0525,  0.1315,  0.0758,  0.1737,  0.0464,  0.1583,  0.0819,  0.0271,
         0.0534,  0.0288,  0.0651,  0.1701,  0.1459,  0.1419,  0.0173,  0.0582,
         0.0975,  0.1593,  0.1803,  0.1052,  0.1635,  0.1563,  0.1161,  0.1306,
         0.1383,  0.1596,  0.1141,  0.1437,  0.1655,  0.1786,  0.1381,  0.1019,
         0.2241,  0.2180,  0.2294,  0.1679,  0.1995,  0.2125,  0.1843,  0.1861,
         0.2328,  0.1964,  0.2045,  0.2005,  0.1608,  0.1548,  0.1592,  0.1923,
         0.2098,  0.1978,  0.1689,  0.1941,  0.2050,  0.2044,  0.1757,  0.1722,
         0.1797,  0.2370,  0.1839,  0.2239,  0.2341,  0.1237,  0.1875,  0.2043,
         0.1434,  0.2010,  0.1889,  0.1910,  0.2333,  0.1521,  0.1476,  0.1728,
         0.1870,  0.1961,  0.1261,  0.1691,  0.2202,  0.2314,  0.2229,  0.2184,
         0.2093,  0.2261,  0.1312,  0.1826,  0.1776,  0.1983,  0.2030,  0.2211,
         0.2198,  0.2045,  0.1866,  0.1693,  0.1701,  0.2319,  0.1941,  0.2198,
         0.1829,  0.2186,  0.1839,  0.1938,  0.2104,  0.1665,  0.2187,  0.2127,
         0.1730,  0.1905,  0.1964,  0.1860,  0.1958,  0.1810,  0.2194,  0.1207,
         0.1096,  0.1906,  0.2131,  0.1153,  0.1838,  0.1822,  0.1871,  0.1962,
         0.1726,  0.1119,  0.1473,  0.2132,  0.1867,  0.2046,  0.1670,  0.1930,
         0.1913,  0.1035,  0.1931,  0.1732,  0.2166,  0.1985,  0.1643,  0.1905,
         0.2098,  0.2293,  0.2235,  0.2246,  0.1991,  0.1699,  0.2000,  0.2078,
         0.1715,  0.1920,  0.1989,  0.2119,  0.1544,  0.1654,  0.2311,  0.2284,
         0.1527,  0.0880,  0.1594,  0.2168,  0.1755,  0.1944,  0.2085,  0.1747],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.2710, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:05:09,620 :: INFO :: ----- frontend -----
2023-05-14 12:05:09,620 :: INFO :: Environment 0
2023-05-14 12:05:19,074 :: INFO :: Epoch 5: loss tensor(862.3271, device='cuda:0'), U.norm 13.155776977539062, V.norm 16.976316452026367, MLP.norm 2.161973476409912
2023-05-14 12:05:19,308 :: INFO :: Epoch 10: loss tensor(835.4331, device='cuda:0'), U.norm 10.344199180603027, V.norm 16.31230926513672, MLP.norm 3.684821605682373
2023-05-14 12:05:19,543 :: INFO :: Epoch 15: loss tensor(799.4367, device='cuda:0'), U.norm 8.642373085021973, V.norm 15.98916244506836, MLP.norm 5.457673072814941
2023-05-14 12:05:19,777 :: INFO :: Epoch 20: loss tensor(759.8979, device='cuda:0'), U.norm 7.481273174285889, V.norm 15.77355670928955, MLP.norm 7.28778600692749
2023-05-14 12:05:19,793 :: INFO :: Environment 1
2023-05-14 12:05:29,122 :: INFO :: Epoch 5: loss tensor(817.2145, device='cuda:0'), U.norm 13.158382415771484, V.norm 16.8641414642334, MLP.norm 2.128559112548828
2023-05-14 12:05:29,356 :: INFO :: Epoch 10: loss tensor(793.9090, device='cuda:0'), U.norm 10.348541259765625, V.norm 16.158742904663086, MLP.norm 3.565873861312866
2023-05-14 12:05:29,591 :: INFO :: Epoch 15: loss tensor(761.1099, device='cuda:0'), U.norm 8.648702621459961, V.norm 15.817923545837402, MLP.norm 5.264416694641113
2023-05-14 12:05:29,825 :: INFO :: Epoch 20: loss tensor(724.8175, device='cuda:0'), U.norm 7.49007511138916, V.norm 15.59144115447998, MLP.norm 7.040818691253662
2023-05-14 12:05:29,841 :: INFO :: Environment 2
2023-05-14 12:05:39,185 :: INFO :: Epoch 5: loss tensor(862.4912, device='cuda:0'), U.norm 13.163126945495605, V.norm 16.97532844543457, MLP.norm 2.196568489074707
2023-05-14 12:05:39,419 :: INFO :: Epoch 10: loss tensor(834.0500, device='cuda:0'), U.norm 10.35927963256836, V.norm 16.31488800048828, MLP.norm 3.759512424468994
2023-05-14 12:05:39,654 :: INFO :: Epoch 15: loss tensor(795.4255, device='cuda:0'), U.norm 8.666510581970215, V.norm 15.991965293884277, MLP.norm 5.543092250823975
2023-05-14 12:05:39,888 :: INFO :: Epoch 20: loss tensor(753.6703, device='cuda:0'), U.norm 7.515398979187012, V.norm 15.773478507995605, MLP.norm 7.397922515869141
2023-05-14 12:05:39,904 :: INFO :: Environment 3
2023-05-14 12:05:49,233 :: INFO :: Epoch 5: loss tensor(826.3110, device='cuda:0'), U.norm 13.15280532836914, V.norm 16.907209396362305, MLP.norm 2.109475612640381
2023-05-14 12:05:49,467 :: INFO :: Epoch 10: loss tensor(802.4795, device='cuda:0'), U.norm 10.337864875793457, V.norm 16.221914291381836, MLP.norm 3.547759532928467
2023-05-14 12:05:49,701 :: INFO :: Epoch 15: loss tensor(769.5023, device='cuda:0'), U.norm 8.632200241088867, V.norm 15.891691207885742, MLP.norm 5.260838508605957
2023-05-14 12:05:49,951 :: INFO :: Epoch 20: loss tensor(733.5453, device='cuda:0'), U.norm 7.466549873352051, V.norm 15.672425270080566, MLP.norm 7.079549312591553
2023-05-14 12:05:49,967 :: INFO :: Environment 4
2023-05-14 12:05:59,296 :: INFO :: Epoch 5: loss tensor(908.2769, device='cuda:0'), U.norm 13.158679962158203, V.norm 17.065765380859375, MLP.norm 2.1815946102142334
2023-05-14 12:05:59,531 :: INFO :: Epoch 10: loss tensor(877.3198, device='cuda:0'), U.norm 10.35146713256836, V.norm 16.44272804260254, MLP.norm 3.776681661605835
2023-05-14 12:05:59,765 :: INFO :: Epoch 15: loss tensor(835.9316, device='cuda:0'), U.norm 8.65503215789795, V.norm 16.137847900390625, MLP.norm 5.618162155151367
2023-05-14 12:06:00,015 :: INFO :: Epoch 20: loss tensor(792.4758, device='cuda:0'), U.norm 7.499731540679932, V.norm 15.93362808227539, MLP.norm 7.522815704345703
2023-05-14 12:06:00,030 :: INFO :: Environment 5
2023-05-14 12:06:09,344 :: INFO :: Epoch 5: loss tensor(833.6611, device='cuda:0'), U.norm 13.15429973602295, V.norm 16.92267417907715, MLP.norm 2.155954360961914
2023-05-14 12:06:09,594 :: INFO :: Epoch 10: loss tensor(807.0616, device='cuda:0'), U.norm 10.340778350830078, V.norm 16.241533279418945, MLP.norm 3.678621292114258
2023-05-14 12:06:09,828 :: INFO :: Epoch 15: loss tensor(772.3653, device='cuda:0'), U.norm 8.63588809967041, V.norm 15.909481048583984, MLP.norm 5.447432041168213
2023-05-14 12:06:10,062 :: INFO :: Epoch 20: loss tensor(735.4410, device='cuda:0'), U.norm 7.470270156860352, V.norm 15.687193870544434, MLP.norm 7.25771427154541
2023-05-14 12:06:10,062 :: INFO :: Environment 6
2023-05-14 12:06:19,172 :: INFO :: Epoch 5: loss tensor(847.8476, device='cuda:0'), U.norm 13.155625343322754, V.norm 16.941373825073242, MLP.norm 2.1412928104400635
2023-05-14 12:06:19,407 :: INFO :: Epoch 10: loss tensor(823.6730, device='cuda:0'), U.norm 10.343557357788086, V.norm 16.268247604370117, MLP.norm 3.6164438724517822
2023-05-14 12:06:19,626 :: INFO :: Epoch 15: loss tensor(790.5472, device='cuda:0'), U.norm 8.641006469726562, V.norm 15.942864418029785, MLP.norm 5.3524250984191895
2023-05-14 12:06:19,860 :: INFO :: Epoch 20: loss tensor(754.7573, device='cuda:0'), U.norm 7.478701114654541, V.norm 15.727655410766602, MLP.norm 7.130383491516113
2023-05-14 12:06:19,876 :: INFO :: Environment 7
2023-05-14 12:06:29,048 :: INFO :: Epoch 5: loss tensor(812.3940, device='cuda:0'), U.norm 13.153653144836426, V.norm 16.879247665405273, MLP.norm 2.1428325176239014
2023-05-14 12:06:29,267 :: INFO :: Epoch 10: loss tensor(786.3772, device='cuda:0'), U.norm 10.339301109313965, V.norm 16.17441749572754, MLP.norm 3.6164956092834473
2023-05-14 12:06:29,502 :: INFO :: Epoch 15: loss tensor(750.4847, device='cuda:0'), U.norm 8.63416576385498, V.norm 15.830181121826172, MLP.norm 5.354515075683594
2023-05-14 12:06:29,720 :: INFO :: Epoch 20: loss tensor(712.1422, device='cuda:0'), U.norm 7.46897554397583, V.norm 15.599248886108398, MLP.norm 7.157201766967773
2023-05-14 12:06:29,736 :: INFO :: Environment 8
2023-05-14 12:06:38,893 :: INFO :: Epoch 5: loss tensor(832.3536, device='cuda:0'), U.norm 13.154511451721191, V.norm 16.922985076904297, MLP.norm 2.1491899490356445
2023-05-14 12:06:39,127 :: INFO :: Epoch 10: loss tensor(806.8356, device='cuda:0'), U.norm 10.341568946838379, V.norm 16.239912033081055, MLP.norm 3.6339025497436523
2023-05-14 12:06:39,362 :: INFO :: Epoch 15: loss tensor(772.1823, device='cuda:0'), U.norm 8.638470649719238, V.norm 15.909099578857422, MLP.norm 5.360058784484863
2023-05-14 12:06:39,580 :: INFO :: Epoch 20: loss tensor(734.0560, device='cuda:0'), U.norm 7.4757537841796875, V.norm 15.687662124633789, MLP.norm 7.154903411865234
2023-05-14 12:06:39,596 :: INFO :: Environment 9
2023-05-14 12:06:48,769 :: INFO :: Epoch 5: loss tensor(868.3099, device='cuda:0'), U.norm 13.159477233886719, V.norm 16.992008209228516, MLP.norm 2.1748456954956055
2023-05-14 12:06:48,987 :: INFO :: Epoch 10: loss tensor(838.2117, device='cuda:0'), U.norm 10.352446556091309, V.norm 16.339902877807617, MLP.norm 3.7412526607513428
2023-05-14 12:06:49,222 :: INFO :: Epoch 15: loss tensor(797.8234, device='cuda:0'), U.norm 8.656582832336426, V.norm 16.02290153503418, MLP.norm 5.554213523864746
2023-05-14 12:06:49,456 :: INFO :: Epoch 20: loss tensor(753.4183, device='cuda:0'), U.norm 7.501742362976074, V.norm 15.810760498046875, MLP.norm 7.442318439483643
2023-05-14 12:06:49,472 :: INFO :: Ite = 1, Delta = 4097
2023-05-14 12:06:49,472 :: INFO :: ----- backend -----
2023-05-14 12:06:53,847 :: INFO :: Epoch 5: loss tensor(103.7297, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.4338e-03,  1.2871e-02,  1.2299e-02,  1.6275e-02,  1.1638e-02,
         5.1440e-03,  8.2691e-03,  1.8875e-02,  4.8772e-03,  1.8707e-02,
         2.9025e-03,  1.6314e-02,  8.8469e-03,  1.6401e-02,  2.1442e-02,
         1.9162e-02,  2.1900e-02,  2.0235e-02,  1.9117e-02,  1.3808e-02,
         1.2973e-02,  1.9346e-02,  1.7889e-02,  1.8779e-03,  1.7814e-02,
         1.1589e-02,  1.1787e-02,  1.0933e-02,  2.1092e-02,  1.6352e-02,
         1.9969e-02,  2.4452e-02,  9.5807e-03,  1.8056e-02, -1.4676e-03,
        -7.6297e-03,  2.0823e-02,  2.1079e-02,  1.0379e-02,  1.7660e-02,
         5.5725e-03,  6.8188e-04,  2.1113e-02,  1.7783e-02,  1.2611e-02,
         2.0406e-02,  1.3435e-02,  1.5639e-03,  1.6436e-02,  1.4394e-02,
         2.0141e-02,  1.1375e-02,  1.5183e-02,  1.3125e-02,  8.4794e-03,
         1.9872e-02,  9.6443e-03,  1.7532e-02,  3.2456e-03, -2.8494e-04,
         1.8620e-02,  1.8808e-02,  1.5016e-03,  1.4830e-02,  1.3715e-02,
         4.0084e-03,  2.3928e-02,  1.3074e-02,  2.4622e-02,  9.3283e-03,
         2.0554e-02,  5.5527e-03,  2.2805e-02,  7.0903e-03,  1.7969e-02,
         5.1664e-03,  1.9607e-02, -1.5826e-05,  1.1627e-02,  1.9303e-02,
         1.5717e-02,  2.5185e-03,  2.0219e-02,  1.2345e-02,  1.6116e-03,
         1.7119e-02,  2.3677e-02,  9.4571e-03,  7.4308e-03, -2.9450e-03,
         9.5602e-03,  1.6905e-02,  1.6973e-02,  1.9023e-02,  6.3263e-03,
         1.8724e-02,  2.2848e-02,  1.6920e-02,  9.0103e-03,  1.3170e-02,
        -4.5445e-04,  1.4018e-02,  1.5844e-02, -1.4649e-03,  1.1566e-02,
         8.4035e-06,  1.6867e-02,  1.5359e-02,  1.7720e-02,  1.8160e-02,
         4.7972e-03,  1.5473e-02,  1.5131e-02,  1.7816e-02,  5.6501e-03,
         4.7005e-03,  1.4398e-02,  1.1295e-02,  1.3157e-02,  1.0475e-02,
         1.7135e-02,  1.3148e-02,  7.7874e-03,  7.0862e-03,  5.2640e-03,
         1.5113e-02, -7.5746e-05,  1.7615e-02,  1.9529e-02,  1.8647e-02,
         1.8878e-02,  2.4619e-02,  2.3863e-02,  2.0318e-02,  7.9610e-03,
        -5.4320e-03, -1.5977e-02,  1.5991e-02,  1.1440e-02, -7.3399e-03,
         2.2033e-02, -7.3558e-03,  9.8134e-03,  1.2169e-02,  1.6684e-02,
         2.5866e-02,  1.4591e-02, -1.8095e-03,  1.2746e-02, -2.7040e-04,
        -1.2727e-02,  1.9299e-02,  2.0988e-02, -2.1842e-02,  3.7177e-03,
        -1.4914e-02, -5.9416e-03, -2.0774e-02,  2.5307e-02, -1.7715e-02,
         2.5811e-02, -1.9132e-02,  2.2087e-02,  7.0505e-03,  2.5670e-02,
         1.5822e-02, -1.3072e-02, -2.0633e-03, -5.5207e-03,  1.2689e-02,
         2.0251e-02,  1.9088e-02, -2.0254e-02,  1.7739e-02,  2.2346e-02,
        -1.3294e-02,  1.9399e-02,  2.0444e-02,  3.3119e-03,  4.0247e-03,
         1.2983e-02, -1.1737e-02,  1.9387e-02, -2.0302e-02,  2.1701e-02,
         2.2132e-02, -1.9397e-02,  1.4094e-02,  2.2150e-02, -2.0901e-02,
        -8.6810e-03,  1.3048e-03,  2.5574e-02, -1.9144e-02,  1.6232e-02,
         2.0503e-02,  1.6198e-02,  1.8657e-02,  2.2267e-02,  7.0934e-03,
         2.1869e-02,  1.3277e-02,  2.5251e-02,  2.1125e-02,  1.5345e-02,
         1.0132e-02, -5.3639e-03, -2.0529e-02,  2.2172e-02,  1.9691e-03,
         2.5262e-02,  2.5174e-02,  1.8187e-02, -1.1670e-02,  1.7762e-02,
         1.8601e-02,  9.1932e-04, -1.3240e-02,  2.5723e-02, -2.0511e-02,
         1.5800e-02,  6.0565e-03,  2.5411e-02,  2.3138e-02,  1.8932e-02,
         1.5963e-02,  1.4343e-02,  1.7478e-02,  1.4997e-02,  2.2350e-02,
         1.8626e-02,  2.1391e-02,  2.5361e-02,  2.0847e-02,  2.5824e-02,
        -5.7381e-03, -1.0581e-03,  2.3104e-03,  1.2800e-02,  1.9845e-02,
         7.6523e-03,  7.7493e-03,  2.2723e-03,  2.5981e-02, -5.6374e-03,
         1.8385e-02, -2.0059e-02,  1.0938e-02,  3.9343e-03, -1.2845e-03,
        -1.1770e-02,  2.5306e-02,  8.8160e-03,  1.0689e-02, -1.9845e-02,
         1.0949e-02,  1.0387e-02,  1.7052e-02,  1.5195e-02,  2.4777e-02,
         2.1863e-02,  1.4625e-02,  4.8175e-03,  9.5956e-03,  2.0035e-02,
         1.3345e-02,  2.2486e-02,  2.4739e-02,  1.4522e-02,  1.9826e-02,
         1.0974e-02,  9.4002e-03,  9.6673e-03,  1.6416e-02,  1.5555e-02,
         1.0191e-02,  1.2661e-02,  1.8282e-02,  2.1754e-02,  1.3380e-02,
         1.3416e-02,  2.0958e-02,  2.1661e-02,  1.5769e-02,  1.5429e-02,
         1.8276e-02,  6.8943e-03,  1.4655e-02,  2.1970e-02,  9.2212e-03,
         6.0340e-03,  1.1078e-02,  1.1820e-02,  1.8600e-02,  9.7590e-03,
         2.4956e-02,  1.8521e-02,  1.3226e-02,  2.1070e-02,  1.9830e-02,
         1.4123e-02,  2.5301e-02,  9.4360e-03,  1.5288e-03,  1.7349e-02,
         1.6887e-02,  1.8794e-02,  1.0644e-02,  1.3707e-02,  2.7633e-03,
         1.1672e-02,  1.0553e-02,  1.6476e-02,  1.6141e-02,  7.4798e-03,
         1.8610e-02,  1.3646e-02,  1.9126e-02,  1.8266e-02,  1.6473e-02,
         2.5075e-02,  7.6276e-03,  2.5114e-02,  1.7131e-02,  1.4154e-02,
         2.5207e-02,  6.7684e-03,  8.6308e-03,  1.2383e-02,  1.3481e-02,
         1.6230e-02,  7.6581e-03,  5.5464e-03,  5.6741e-03,  1.5865e-02,
         1.8269e-02,  1.4589e-02,  1.4512e-02,  2.0716e-02,  1.4808e-02,
         1.6712e-02,  1.2187e-02,  1.1185e-02,  1.7355e-02,  9.7018e-03,
         1.9193e-02,  1.6570e-02,  1.9732e-02,  1.0408e-02,  1.3264e-02,
         1.2685e-02,  1.5980e-02,  1.1222e-02,  7.2733e-03,  9.2871e-03,
         1.3761e-02,  8.4854e-03,  2.1002e-02,  1.4150e-02,  1.6508e-02,
         1.7583e-02,  1.2604e-02,  2.0963e-02,  7.3537e-03,  2.4598e-02,
         2.5618e-02,  1.6800e-02,  1.6135e-02,  2.0163e-02,  1.1046e-02,
         1.0140e-02,  2.0382e-05,  1.5343e-02,  1.3986e-02,  1.6002e-02,
         2.5528e-02,  1.8784e-02,  1.8702e-02,  1.5462e-02,  1.0593e-02,
         2.5186e-02,  7.4029e-03,  9.0984e-03,  2.1275e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.4603, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:06:58,238 :: INFO :: Epoch 10: loss tensor(102.4020, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0350,  0.0395,  0.0406,  0.0454,  0.0387,  0.0339,  0.0370,  0.0464,
         0.0323,  0.0457,  0.0314,  0.0424,  0.0362,  0.0457,  0.0490,  0.0423,
         0.0467,  0.0472,  0.0443,  0.0427,  0.0401,  0.0454,  0.0457,  0.0306,
         0.0445,  0.0382,  0.0374,  0.0396,  0.0506,  0.0434,  0.0479,  0.0507,
         0.0379,  0.0458,  0.0244,  0.0168,  0.0452,  0.0494,  0.0383,  0.0456,
         0.0326,  0.0275,  0.0480,  0.0444,  0.0393,  0.0466,  0.0395,  0.0284,
         0.0452,  0.0390,  0.0470,  0.0388,  0.0400,  0.0378,  0.0383,  0.0480,
         0.0373,  0.0468,  0.0322,  0.0286,  0.0435,  0.0443,  0.0290,  0.0421,
         0.0415,  0.0294,  0.0490,  0.0406,  0.0511,  0.0379,  0.0460,  0.0335,
         0.0451,  0.0353,  0.0448,  0.0325,  0.0457,  0.0275,  0.0382,  0.0455,
         0.0448,  0.0319,  0.0469,  0.0421,  0.0294,  0.0457,  0.0495,  0.0387,
         0.0340,  0.0223,  0.0379,  0.0458,  0.0437,  0.0446,  0.0356,  0.0461,
         0.0493,  0.0447,  0.0366,  0.0409,  0.0273,  0.0384,  0.0429,  0.0258,
         0.0413,  0.0285,  0.0426,  0.0415,  0.0452,  0.0463,  0.0326,  0.0415,
         0.0448,  0.0455,  0.0337,  0.0310,  0.0419,  0.0403,  0.0362,  0.0391,
         0.0454,  0.0404,  0.0351,  0.0339,  0.0322,  0.0404,  0.0252,  0.0462,
         0.0457,  0.0469,  0.0482,  0.0452,  0.0468,  0.0422,  0.0369,  0.0216,
        -0.0040,  0.0203,  0.0398,  0.0181,  0.0514,  0.0181,  0.0394,  0.0416,
         0.0468,  0.0537,  0.0369,  0.0278,  0.0423,  0.0291,  0.0055,  0.0482,
         0.0497, -0.0385,  0.0327, -0.0014,  0.0198, -0.0335,  0.0458, -0.0187,
         0.0514, -0.0300,  0.0440,  0.0363,  0.0513,  0.0267,  0.0043,  0.0262,
         0.0213,  0.0411,  0.0496,  0.0462, -0.0312,  0.0474,  0.0518,  0.0047,
         0.0272,  0.0489,  0.0323,  0.0315,  0.0418,  0.0088,  0.0343, -0.0305,
         0.0491,  0.0518, -0.0256,  0.0438,  0.0517, -0.0349,  0.0148,  0.0302,
         0.0518, -0.0234,  0.0385,  0.0494,  0.0465,  0.0233,  0.0504,  0.0376,
         0.0512,  0.0443,  0.0528,  0.0372,  0.0445,  0.0413,  0.0209, -0.0344,
         0.0450,  0.0310,  0.0551,  0.0508,  0.0285,  0.0084,  0.0459,  0.0494,
         0.0300,  0.0039,  0.0540, -0.0325,  0.0445,  0.0350,  0.0520,  0.0509,
         0.0420,  0.0459,  0.0419,  0.0483,  0.0413,  0.0517,  0.0462,  0.0404,
         0.0448,  0.0378,  0.0522,  0.0220,  0.0280,  0.0311,  0.0161,  0.0434,
         0.0364,  0.0379,  0.0325,  0.0528,  0.0210,  0.0475, -0.0296,  0.0402,
         0.0328,  0.0270,  0.0092,  0.0539,  0.0399,  0.0411, -0.0294,  0.0400,
         0.0407,  0.0474,  0.0469,  0.0540,  0.0510,  0.0455,  0.0347,  0.0400,
         0.0498,  0.0426,  0.0519,  0.0543,  0.0441,  0.0488,  0.0403,  0.0398,
         0.0394,  0.0459,  0.0453,  0.0397,  0.0426,  0.0481,  0.0507,  0.0428,
         0.0432,  0.0517,  0.0510,  0.0466,  0.0465,  0.0467,  0.0372,  0.0443,
         0.0504,  0.0392,  0.0342,  0.0412,  0.0428,  0.0477,  0.0392,  0.0533,
         0.0481,  0.0423,  0.0496,  0.0490,  0.0449,  0.0554,  0.0401,  0.0321,
         0.0472,  0.0475,  0.0475,  0.0398,  0.0427,  0.0320,  0.0418,  0.0412,
         0.0471,  0.0458,  0.0369,  0.0481,  0.0426,  0.0503,  0.0478,  0.0468,
         0.0547,  0.0375,  0.0545,  0.0465,  0.0443,  0.0539,  0.0381,  0.0389,
         0.0421,  0.0433,  0.0456,  0.0371,  0.0355,  0.0356,  0.0463,  0.0471,
         0.0430,  0.0443,  0.0498,  0.0439,  0.0466,  0.0421,  0.0410,  0.0478,
         0.0389,  0.0462,  0.0448,  0.0496,  0.0399,  0.0431,  0.0428,  0.0456,
         0.0412,  0.0357,  0.0383,  0.0433,  0.0395,  0.0493,  0.0432,  0.0463,
         0.0473,  0.0442,  0.0515,  0.0382,  0.0538,  0.0548,  0.0467,  0.0459,
         0.0491,  0.0413,  0.0401,  0.0308,  0.0448,  0.0434,  0.0462,  0.0561,
         0.0478,  0.0460,  0.0449,  0.0410,  0.0542,  0.0376,  0.0391,  0.0505],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.8891, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:02,566 :: INFO :: Epoch 15: loss tensor(99.0554, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0593,  0.0597,  0.0689,  0.0697,  0.0592,  0.0608,  0.0621,  0.0699,
         0.0534,  0.0668,  0.0580,  0.0646,  0.0583,  0.0711,  0.0719,  0.0601,
         0.0630,  0.0674,  0.0637,  0.0688,  0.0624,  0.0645,  0.0699,  0.0588,
         0.0679,  0.0563,  0.0536,  0.0645,  0.0795,  0.0632,  0.0723,  0.0708,
         0.0615,  0.0688,  0.0492,  0.0405,  0.0628,  0.0752,  0.0602,  0.0689,
         0.0570,  0.0514,  0.0708,  0.0664,  0.0563,  0.0653,  0.0590,  0.0544,
         0.0706,  0.0553,  0.0685,  0.0585,  0.0599,  0.0520,  0.0666,  0.0737,
         0.0607,  0.0762,  0.0593,  0.0562,  0.0582,  0.0645,  0.0525,  0.0635,
         0.0641,  0.0488,  0.0686,  0.0636,  0.0729,  0.0619,  0.0648,  0.0577,
         0.0596,  0.0626,  0.0656,  0.0552,  0.0661,  0.0536,  0.0565,  0.0657,
         0.0719,  0.0611,  0.0679,  0.0701,  0.0528,  0.0699,  0.0700,  0.0654,
         0.0551,  0.0418,  0.0628,  0.0724,  0.0638,  0.0630,  0.0628,  0.0691,
         0.0676,  0.0690,  0.0606,  0.0638,  0.0515,  0.0506,  0.0645,  0.0529,
         0.0700,  0.0553,  0.0640,  0.0611,  0.0666,  0.0689,  0.0608,  0.0598,
         0.0719,  0.0670,  0.0587,  0.0529,  0.0629,  0.0676,  0.0478,  0.0652,
         0.0689,  0.0639,  0.0581,  0.0535,  0.0542,  0.0640,  0.0506,  0.0709,
         0.0684,  0.0727,  0.0768,  0.0550,  0.0614,  0.0589,  0.0635,  0.0507,
         0.0240,  0.0225,  0.0660,  0.0472,  0.0795,  0.0476,  0.0681,  0.0721,
         0.0764,  0.0789,  0.0476,  0.0607,  0.0716,  0.0603,  0.0342,  0.0761,
         0.0767, -0.0505,  0.0608,  0.0267,  0.0489, -0.0381,  0.0554, -0.0089,
         0.0722, -0.0353,  0.0578,  0.0652,  0.0707,  0.0283,  0.0305,  0.0553,
         0.0514,  0.0674,  0.0771,  0.0681, -0.0345,  0.0758,  0.0804,  0.0344,
         0.0320,  0.0745,  0.0607,  0.0574,  0.0681,  0.0383,  0.0405, -0.0307,
         0.0729,  0.0803, -0.0196,  0.0715,  0.0802, -0.0432,  0.0449,  0.0598,
         0.0735, -0.0134,  0.0497,  0.0759,  0.0771,  0.0244,  0.0766,  0.0686,
         0.0796,  0.0760,  0.0775,  0.0449,  0.0725,  0.0734,  0.0498, -0.0435,
         0.0570,  0.0594,  0.0844,  0.0712,  0.0324,  0.0363,  0.0714,  0.0805,
         0.0588,  0.0304,  0.0793, -0.0362,  0.0719,  0.0623,  0.0753,  0.0775,
         0.0561,  0.0753,  0.0656,  0.0790,  0.0584,  0.0807,  0.0700,  0.0450,
         0.0534,  0.0437,  0.0734,  0.0544,  0.0584,  0.0597,  0.0189,  0.0586,
         0.0629,  0.0684,  0.0644,  0.0761,  0.0512,  0.0755, -0.0275,  0.0685,
         0.0613,  0.0570,  0.0394,  0.0812,  0.0716,  0.0725, -0.0250,  0.0669,
         0.0732,  0.0788,  0.0805,  0.0826,  0.0802,  0.0774,  0.0657,  0.0712,
         0.0811,  0.0723,  0.0814,  0.0841,  0.0732,  0.0771,  0.0690,  0.0711,
         0.0703,  0.0754,  0.0752,  0.0694,  0.0731,  0.0783,  0.0795,  0.0724,
         0.0735,  0.0845,  0.0806,  0.0792,  0.0801,  0.0737,  0.0687,  0.0746,
         0.0765,  0.0705,  0.0626,  0.0719,  0.0763,  0.0756,  0.0683,  0.0804,
         0.0778,  0.0718,  0.0757,  0.0776,  0.0770,  0.0870,  0.0732,  0.0657,
         0.0776,  0.0802,  0.0742,  0.0682,  0.0714,  0.0629,  0.0721,  0.0732,
         0.0790,  0.0758,  0.0674,  0.0776,  0.0704,  0.0835,  0.0778,  0.0783,
         0.0844,  0.0697,  0.0840,  0.0767,  0.0756,  0.0820,  0.0715,  0.0700,
         0.0718,  0.0730,  0.0753,  0.0670,  0.0669,  0.0664,  0.0780,  0.0749,
         0.0690,  0.0748,  0.0801,  0.0721,  0.0768,  0.0727,  0.0714,  0.0790,
         0.0686,  0.0697,  0.0720,  0.0804,  0.0694,  0.0739,  0.0732,  0.0753,
         0.0720,  0.0608,  0.0680,  0.0729,  0.0723,  0.0770,  0.0711,  0.0768,
         0.0773,  0.0779,  0.0837,  0.0716,  0.0830,  0.0837,  0.0769,  0.0764,
         0.0773,  0.0724,  0.0712,  0.0645,  0.0731,  0.0722,  0.0786,  0.0882,
         0.0766,  0.0699,  0.0739,  0.0732,  0.0833,  0.0687,  0.0705,  0.0787],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.0844, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:06,879 :: INFO :: Epoch 20: loss tensor(98.1017, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0773,  0.0694,  0.0957,  0.0893,  0.0721,  0.0816,  0.0802,  0.0888,
         0.0632,  0.0806,  0.0801,  0.0809,  0.0714,  0.0902,  0.0872,  0.0700,
         0.0677,  0.0792,  0.0750,  0.0909,  0.0792,  0.0725,  0.0888,  0.0826,
         0.0879,  0.0639,  0.0593,  0.0842,  0.1051,  0.0743,  0.0912,  0.0825,
         0.0781,  0.0840,  0.0669,  0.0576,  0.0684,  0.0970,  0.0736,  0.0843,
         0.0749,  0.0674,  0.0858,  0.0818,  0.0646,  0.0738,  0.0709,  0.0740,
         0.0893,  0.0602,  0.0808,  0.0665,  0.0711,  0.0537,  0.0910,  0.0939,
         0.0752,  0.1047,  0.0808,  0.0774,  0.0620,  0.0782,  0.0657,  0.0782,
         0.0772,  0.0592,  0.0806,  0.0788,  0.0886,  0.0768,  0.0745,  0.0743,
         0.0634,  0.0857,  0.0801,  0.0689,  0.0785,  0.0748,  0.0655,  0.0776,
         0.0960,  0.0870,  0.0823,  0.0952,  0.0666,  0.0871,  0.0830,  0.0852,
         0.0664,  0.0491,  0.0820,  0.0960,  0.0771,  0.0714,  0.0842,  0.0847,
         0.0765,  0.0847,  0.0794,  0.0803,  0.0663,  0.0537,  0.0800,  0.0748,
         0.0949,  0.0764,  0.0776,  0.0706,  0.0802,  0.0879,  0.0861,  0.0686,
         0.0935,  0.0807,  0.0779,  0.0668,  0.0750,  0.0900,  0.0500,  0.0877,
         0.0866,  0.0804,  0.0714,  0.0619,  0.0666,  0.0806,  0.0712,  0.0907,
         0.0854,  0.0949,  0.1031,  0.0570,  0.0681,  0.0682,  0.0860,  0.0775,
         0.0541,  0.0244,  0.0880,  0.0741,  0.1034,  0.0745,  0.0933,  0.1019,
         0.1038,  0.1006,  0.0518,  0.0926,  0.0987,  0.0892,  0.0621,  0.1005,
         0.1003, -0.0598,  0.0851,  0.0558,  0.0756, -0.0384,  0.0594,  0.0065,
         0.0869, -0.0364,  0.0665,  0.0912,  0.0845,  0.0269,  0.0542,  0.0816,
         0.0793,  0.0901,  0.1003,  0.0842, -0.0337,  0.1018,  0.1068,  0.0643,
         0.0345,  0.0956,  0.0856,  0.0785,  0.0905,  0.0667,  0.0415, -0.0261,
         0.0919,  0.1047, -0.0087,  0.0943,  0.1043, -0.0490,  0.0751,  0.0867,
         0.0898,  0.0022,  0.0509,  0.0972,  0.1051,  0.0233,  0.0996,  0.0976,
         0.1058,  0.1046,  0.0977,  0.0476,  0.0974,  0.1031,  0.0757, -0.0500,
         0.0592,  0.0844,  0.1097,  0.0853,  0.0327,  0.0611,  0.0928,  0.1091,
         0.0848,  0.0555,  0.0997, -0.0354,  0.0956,  0.0854,  0.0924,  0.1017,
         0.0625,  0.1005,  0.0833,  0.1075,  0.0650,  0.1076,  0.0893,  0.0441,
         0.0562,  0.0480,  0.0878,  0.0856,  0.0863,  0.0853,  0.0177,  0.0683,
         0.0858,  0.0967,  0.0952,  0.0966,  0.0792,  0.1016, -0.0194,  0.0944,
         0.0866,  0.0846,  0.0698,  0.1059,  0.1013,  0.1023, -0.0119,  0.0899,
         0.1052,  0.1098,  0.1142,  0.1076,  0.1078,  0.1098,  0.0949,  0.1003,
         0.1129,  0.1021,  0.1097,  0.1122,  0.0993,  0.1022,  0.0948,  0.1009,
         0.0999,  0.1032,  0.1021,  0.0973,  0.1036,  0.1070,  0.1056,  0.0992,
         0.1016,  0.1178,  0.1076,  0.1114,  0.1139,  0.0956,  0.0986,  0.1031,
         0.0982,  0.1004,  0.0896,  0.1009,  0.1106,  0.0994,  0.0940,  0.1043,
         0.1050,  0.0992,  0.0963,  0.1027,  0.1090,  0.1192,  0.1065,  0.0996,
         0.1082,  0.1130,  0.0964,  0.0948,  0.0969,  0.0922,  0.1015,  0.1054,
         0.1108,  0.1041,  0.0958,  0.1045,  0.0963,  0.1169,  0.1057,  0.1100,
         0.1118,  0.1017,  0.1109,  0.1050,  0.1061,  0.1074,  0.1050,  0.1003,
         0.0989,  0.1003,  0.1026,  0.0945,  0.0967,  0.0952,  0.1092,  0.0969,
         0.0896,  0.1033,  0.1099,  0.0954,  0.1050,  0.1014,  0.0994,  0.1098,
         0.0954,  0.0873,  0.0948,  0.1100,  0.0974,  0.1032,  0.1010,  0.1034,
         0.1008,  0.0810,  0.0968,  0.1001,  0.1051,  0.1030,  0.0961,  0.1056,
         0.1069,  0.1122,  0.1160,  0.1054,  0.1105,  0.1097,  0.1057,  0.1065,
         0.1025,  0.1018,  0.1007,  0.0982,  0.0971,  0.0977,  0.1112,  0.1205,
         0.1009,  0.0871,  0.0991,  0.1054,  0.1098,  0.0982,  0.1007,  0.1040],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.0276, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:11,286 :: INFO :: Epoch 25: loss tensor(96.2305, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0860,  0.0717,  0.1183,  0.0999,  0.0753,  0.0965,  0.0911,  0.0984,
         0.0673,  0.0861,  0.0959,  0.0896,  0.0772,  0.1022,  0.0961,  0.0706,
         0.0677,  0.0844,  0.0761,  0.1077,  0.0894,  0.0731,  0.0993,  0.0995,
         0.0998,  0.0636,  0.0566,  0.0976,  0.1261,  0.0760,  0.1020,  0.0865,
         0.0876,  0.0914,  0.0767,  0.0654,  0.0679,  0.1125,  0.0780,  0.0925,
         0.0844,  0.0751,  0.0948,  0.0896,  0.0646,  0.0751,  0.0733,  0.0872,
         0.1010,  0.0583,  0.0856,  0.0650,  0.0753,  0.0509,  0.1099,  0.1060,
         0.0836,  0.1302,  0.0948,  0.0920,  0.0593,  0.0839,  0.0729,  0.0842,
         0.0823,  0.0606,  0.0869,  0.0857,  0.0961,  0.0834,  0.0769,  0.0829,
         0.0591,  0.1047,  0.0841,  0.0726,  0.0835,  0.0889,  0.0657,  0.0820,
         0.1135,  0.1080,  0.0876,  0.1137,  0.0708,  0.0963,  0.0873,  0.0976,
         0.0704,  0.0496,  0.0936,  0.1154,  0.0792,  0.0732,  0.0975,  0.0937,
         0.0754,  0.0909,  0.0910,  0.0874,  0.0723,  0.0507,  0.0863,  0.0900,
         0.1137,  0.0892,  0.0839,  0.0711,  0.0849,  0.0966,  0.1064,  0.0695,
         0.1089,  0.0874,  0.0899,  0.0711,  0.0786,  0.1086,  0.0472,  0.1028,
         0.0952,  0.0872,  0.0757,  0.0638,  0.0702,  0.0909,  0.0844,  0.1040,
         0.0927,  0.1101,  0.1239,  0.0530,  0.0677,  0.0721,  0.1013,  0.0979,
         0.0797,  0.0232,  0.1025,  0.0941,  0.1183,  0.0946,  0.1110,  0.1295,
         0.1261,  0.1148,  0.0496,  0.1205,  0.1194,  0.1121,  0.0834,  0.1177,
         0.1164, -0.0678,  0.1014,  0.0807,  0.0961, -0.0371,  0.0540,  0.0212,
         0.0915, -0.0349,  0.0696,  0.1098,  0.0895,  0.0240,  0.0713,  0.1016,
         0.1013,  0.1053,  0.1161,  0.0915, -0.0316,  0.1222,  0.1276,  0.0890,
         0.0323,  0.1082,  0.1026,  0.0934,  0.1052,  0.0894,  0.0370, -0.0206,
         0.1019,  0.1219,  0.0008,  0.1063,  0.1214, -0.0528,  0.1022,  0.1071,
         0.0976,  0.0162,  0.0481,  0.1101,  0.1266,  0.0206,  0.1159,  0.1206,
         0.1263,  0.1267,  0.1109,  0.0442,  0.1149,  0.1273,  0.0940, -0.0545,
         0.0554,  0.1029,  0.1257,  0.0887,  0.0297,  0.0785,  0.1060,  0.1327,
         0.1040,  0.0746,  0.1106, -0.0331,  0.1127,  0.1009,  0.1005,  0.1212,
         0.0626,  0.1172,  0.0912,  0.1306,  0.0590,  0.1283,  0.0986,  0.0385,
         0.0581,  0.0430,  0.0925,  0.1115,  0.1082,  0.1046,  0.0161,  0.0702,
         0.1004,  0.1196,  0.1214,  0.1100,  0.1014,  0.1222, -0.0101,  0.1135,
         0.1053,  0.1064,  0.0966,  0.1243,  0.1256,  0.1272,  0.0028,  0.1050,
         0.1360,  0.1389,  0.1468,  0.1288,  0.1334,  0.1405,  0.1210,  0.1264,
         0.1437,  0.1302,  0.1360,  0.1373,  0.1210,  0.1210,  0.1174,  0.1281,
         0.1285,  0.1280,  0.1253,  0.1232,  0.1324,  0.1335,  0.1279,  0.1219,
         0.1262,  0.1500,  0.1306,  0.1420,  0.1468,  0.1112,  0.1256,  0.1292,
         0.1147,  0.1279,  0.1146,  0.1263,  0.1442,  0.1193,  0.1152,  0.1252,
         0.1285,  0.1247,  0.1096,  0.1233,  0.1397,  0.1503,  0.1384,  0.1325,
         0.1369,  0.1447,  0.1126,  0.1194,  0.1184,  0.1200,  0.1297,  0.1365,
         0.1409,  0.1316,  0.1229,  0.1267,  0.1188,  0.1494,  0.1306,  0.1402,
         0.1356,  0.1330,  0.1338,  0.1308,  0.1351,  0.1287,  0.1371,  0.1294,
         0.1227,  0.1251,  0.1271,  0.1192,  0.1241,  0.1204,  0.1391,  0.1132,
         0.1046,  0.1288,  0.1388,  0.1116,  0.1299,  0.1265,  0.1250,  0.1382,
         0.1184,  0.0999,  0.1131,  0.1378,  0.1231,  0.1299,  0.1248,  0.1287,
         0.1269,  0.0960,  0.1234,  0.1232,  0.1368,  0.1273,  0.1170,  0.1311,
         0.1348,  0.1455,  0.1470,  0.1387,  0.1360,  0.1320,  0.1319,  0.1355,
         0.1233,  0.1283,  0.1277,  0.1306,  0.1168,  0.1193,  0.1429,  0.1513,
         0.1200,  0.0948,  0.1193,  0.1364,  0.1321,  0.1251,  0.1290,  0.1252],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.6824, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:15,661 :: INFO :: Epoch 30: loss tensor(95.2539, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0883,  0.0700,  0.1368,  0.1031,  0.0761,  0.1049,  0.0958,  0.1023,
         0.0686,  0.0856,  0.1043,  0.0929,  0.0803,  0.1068,  0.0981,  0.0704,
         0.0671,  0.0839,  0.0737,  0.1185,  0.0935,  0.0690,  0.1041,  0.1092,
         0.1067,  0.0627,  0.0523,  0.1055,  0.1423,  0.0753,  0.1085,  0.0855,
         0.0909,  0.0921,  0.0809,  0.0658,  0.0649,  0.1240,  0.0787,  0.0945,
         0.0911,  0.0763,  0.0970,  0.0928,  0.0617,  0.0714,  0.0706,  0.0966,
         0.1062,  0.0565,  0.0825,  0.0626,  0.0774,  0.0480,  0.1223,  0.1125,
         0.0866,  0.1521,  0.1017,  0.0996,  0.0557,  0.0845,  0.0750,  0.0844,
         0.0813,  0.0574,  0.0879,  0.0868,  0.0987,  0.0828,  0.0749,  0.0851,
         0.0541,  0.1192,  0.0824,  0.0709,  0.0824,  0.0945,  0.0625,  0.0807,
         0.1252,  0.1227,  0.0883,  0.1283,  0.0690,  0.0993,  0.0857,  0.1033,
         0.0707,  0.0467,  0.0995,  0.1276,  0.0775,  0.0712,  0.1042,  0.0961,
         0.0722,  0.0905,  0.0969,  0.0889,  0.0721,  0.0467,  0.0870,  0.1015,
         0.1255,  0.0943,  0.0846,  0.0667,  0.0850,  0.0997,  0.1214,  0.0690,
         0.1176,  0.0895,  0.0947,  0.0699,  0.0772,  0.1243,  0.0443,  0.1106,
         0.0953,  0.0890,  0.0739,  0.0620,  0.0694,  0.0976,  0.0906,  0.1107,
         0.0957,  0.1201,  0.1399,  0.0475,  0.0621,  0.0721,  0.1102,  0.1133,
         0.1013,  0.0222,  0.1108,  0.1085,  0.1260,  0.1104,  0.1211,  0.1541,
         0.1431,  0.1228,  0.0444,  0.1441,  0.1342,  0.1290,  0.1002,  0.1289,
         0.1255, -0.0740,  0.1108,  0.1018,  0.1121, -0.0342,  0.0511,  0.0354,
         0.0904, -0.0304,  0.0696,  0.1228,  0.0910,  0.0215,  0.0832,  0.1164,
         0.1180,  0.1141,  0.1262,  0.0927, -0.0277,  0.1374,  0.1433,  0.1092,
         0.0290,  0.1135,  0.1127,  0.1040,  0.1130,  0.1067,  0.0326, -0.0142,
         0.1048,  0.1324,  0.0097,  0.1091,  0.1324, -0.0547,  0.1257,  0.1218,
         0.0991,  0.0283,  0.0414,  0.1154,  0.1419,  0.0200,  0.1260,  0.1383,
         0.1417,  0.1431,  0.1175,  0.0396,  0.1246,  0.1460,  0.1072, -0.0565,
         0.0489,  0.1156,  0.1326,  0.0863,  0.0256,  0.0902,  0.1110,  0.1520,
         0.1171,  0.0891,  0.1138, -0.0295,  0.1225,  0.1095,  0.1016,  0.1350,
         0.0596,  0.1265,  0.0933,  0.1487,  0.0534,  0.1428,  0.0997,  0.0335,
         0.0550,  0.0377,  0.0911,  0.1319,  0.1245,  0.1194,  0.0147,  0.0701,
         0.1076,  0.1375,  0.1440,  0.1184,  0.1189,  0.1384, -0.0004,  0.1263,
         0.1187,  0.1240,  0.1192,  0.1379,  0.1449,  0.1478,  0.0170,  0.1122,
         0.1649,  0.1658,  0.1772,  0.1444,  0.1566,  0.1684,  0.1432,  0.1487,
         0.1729,  0.1560,  0.1599,  0.1593,  0.1375,  0.1342,  0.1362,  0.1518,
         0.1546,  0.1503,  0.1436,  0.1463,  0.1589,  0.1569,  0.1460,  0.1399,
         0.1465,  0.1798,  0.1495,  0.1712,  0.1773,  0.1190,  0.1490,  0.1520,
         0.1258,  0.1523,  0.1371,  0.1482,  0.1761,  0.1343,  0.1313,  0.1426,
         0.1481,  0.1479,  0.1155,  0.1390,  0.1685,  0.1791,  0.1690,  0.1629,
         0.1640,  0.1745,  0.1208,  0.1410,  0.1360,  0.1454,  0.1550,  0.1655,
         0.1687,  0.1568,  0.1479,  0.1438,  0.1377,  0.1801,  0.1516,  0.1681,
         0.1548,  0.1623,  0.1519,  0.1536,  0.1622,  0.1455,  0.1672,  0.1565,
         0.1421,  0.1461,  0.1474,  0.1406,  0.1481,  0.1414,  0.1664,  0.1223,
         0.1134,  0.1505,  0.1655,  0.1203,  0.1507,  0.1475,  0.1477,  0.1639,
         0.1382,  0.1074,  0.1265,  0.1632,  0.1458,  0.1534,  0.1438,  0.1513,
         0.1492,  0.1048,  0.1470,  0.1412,  0.1662,  0.1480,  0.1339,  0.1530,
         0.1603,  0.1772,  0.1759,  0.1702,  0.1585,  0.1494,  0.1550,  0.1622,
         0.1393,  0.1514,  0.1512,  0.1608,  0.1295,  0.1356,  0.1725,  0.1800,
         0.1331,  0.0943,  0.1339,  0.1656,  0.1501,  0.1486,  0.1543,  0.1415],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.0838, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:20,115 :: INFO :: Epoch 35: loss tensor(92.6558, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0905,  0.0689,  0.1531,  0.1034,  0.0746,  0.1099,  0.0993,  0.1031,
         0.0703,  0.0853,  0.1095,  0.0919,  0.0808,  0.1099,  0.0998,  0.0676,
         0.0654,  0.0842,  0.0731,  0.1257,  0.0961,  0.0675,  0.1066,  0.1162,
         0.1113,  0.0607,  0.0497,  0.1093,  0.1557,  0.0742,  0.1115,  0.0838,
         0.0938,  0.0932,  0.0826,  0.0675,  0.0641,  0.1293,  0.0776,  0.0966,
         0.0950,  0.0764,  0.0978,  0.0939,  0.0593,  0.0687,  0.0683,  0.1021,
         0.1091,  0.0549,  0.0798,  0.0589,  0.0780,  0.0464,  0.1306,  0.1151,
         0.0887,  0.1713,  0.1065,  0.1059,  0.0533,  0.0836,  0.0750,  0.0843,
         0.0817,  0.0563,  0.0879,  0.0865,  0.0983,  0.0829,  0.0756,  0.0866,
         0.0513,  0.1297,  0.0799,  0.0700,  0.0831,  0.0978,  0.0603,  0.0804,
         0.1322,  0.1346,  0.0870,  0.1388,  0.0674,  0.0998,  0.0837,  0.1072,
         0.0693,  0.0456,  0.1009,  0.1356,  0.0761,  0.0694,  0.1075,  0.0975,
         0.0673,  0.0900,  0.0995,  0.0914,  0.0723,  0.0422,  0.0828,  0.1072,
         0.1339,  0.0983,  0.0835,  0.0646,  0.0857,  0.0996,  0.1340,  0.0673,
         0.1236,  0.0910,  0.0966,  0.0687,  0.0759,  0.1372,  0.0420,  0.1158,
         0.0950,  0.0872,  0.0725,  0.0634,  0.0666,  0.1017,  0.0961,  0.1130,
         0.0962,  0.1241,  0.1502,  0.0416,  0.0574,  0.0700,  0.1124,  0.1227,
         0.1189,  0.0225,  0.1136,  0.1181,  0.1270,  0.1213,  0.1243,  0.1761,
         0.1548,  0.1239,  0.0404,  0.1636,  0.1430,  0.1409,  0.1120,  0.1364,
         0.1279, -0.0793,  0.1149,  0.1198,  0.1229, -0.0306,  0.0464,  0.0490,
         0.0863, -0.0241,  0.0684,  0.1306,  0.0878,  0.0194,  0.0915,  0.1254,
         0.1298,  0.1168,  0.1297,  0.0891, -0.0229,  0.1467,  0.1531,  0.1245,
         0.0271,  0.1118,  0.1171,  0.1114,  0.1144,  0.1195,  0.0285, -0.0078,
         0.1009,  0.1368,  0.0169,  0.1063,  0.1386, -0.0558,  0.1463,  0.1305,
         0.0954,  0.0375,  0.0382,  0.1151,  0.1527,  0.0164,  0.1302,  0.1502,
         0.1513,  0.1544,  0.1190,  0.0352,  0.1269,  0.1605,  0.1153, -0.0571,
         0.0423,  0.1217,  0.1334,  0.0793,  0.0230,  0.0967,  0.1095,  0.1668,
         0.1233,  0.0995,  0.1118, -0.0252,  0.1280,  0.1124,  0.1014,  0.1450,
         0.0551,  0.1310,  0.0903,  0.1619,  0.0508,  0.1518,  0.0945,  0.0294,
         0.0556,  0.0352,  0.0847,  0.1471,  0.1356,  0.1284,  0.0152,  0.0659,
         0.1076,  0.1502,  0.1621,  0.1220,  0.1313,  0.1484,  0.0087,  0.1330,
         0.1260,  0.1364,  0.1367,  0.1454,  0.1593,  0.1638,  0.0300,  0.1126,
         0.1923,  0.1904,  0.2059,  0.1554,  0.1774,  0.1942,  0.1618,  0.1676,
         0.2000,  0.1791,  0.1816,  0.1790,  0.1501,  0.1421,  0.1511,  0.1729,
         0.1783,  0.1697,  0.1572,  0.1656,  0.1835,  0.1774,  0.1606,  0.1531,
         0.1631,  0.2069,  0.1642,  0.1986,  0.2066,  0.1211,  0.1691,  0.1713,
         0.1332,  0.1738,  0.1568,  0.1667,  0.2060,  0.1452,  0.1422,  0.1575,
         0.1636,  0.1690,  0.1155,  0.1505,  0.1958,  0.2051,  0.1979,  0.1914,
         0.1886,  0.2027,  0.1224,  0.1611,  0.1491,  0.1685,  0.1783,  0.1926,
         0.1942,  0.1795,  0.1708,  0.1565,  0.1540,  0.2092,  0.1690,  0.1940,
         0.1705,  0.1895,  0.1656,  0.1736,  0.1872,  0.1584,  0.1959,  0.1815,
         0.1577,  0.1633,  0.1634,  0.1580,  0.1689,  0.1583,  0.1916,  0.1246,
         0.1186,  0.1687,  0.1897,  0.1225,  0.1677,  0.1647,  0.1666,  0.1870,
         0.1542,  0.1121,  0.1347,  0.1861,  0.1665,  0.1741,  0.1586,  0.1716,
         0.1679,  0.1103,  0.1672,  0.1544,  0.1939,  0.1672,  0.1470,  0.1716,
         0.1844,  0.2071,  0.2025,  0.1997,  0.1792,  0.1627,  0.1753,  0.1868,
         0.1516,  0.1711,  0.1717,  0.1893,  0.1371,  0.1485,  0.1999,  0.2069,
         0.1401,  0.0915,  0.1425,  0.1931,  0.1640,  0.1693,  0.1772,  0.1539],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.2739, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:24,568 :: INFO :: Epoch 40: loss tensor(92.7643, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0891,  0.0672,  0.1649,  0.1043,  0.0703,  0.1137,  0.1013,  0.1009,
         0.0711,  0.0820,  0.1143,  0.0880,  0.0772,  0.1103,  0.0995,  0.0647,
         0.0639,  0.0847,  0.0718,  0.1310,  0.0958,  0.0657,  0.1058,  0.1206,
         0.1117,  0.0569,  0.0464,  0.1114,  0.1663,  0.0718,  0.1098,  0.0820,
         0.0965,  0.0936,  0.0815,  0.0669,  0.0599,  0.1301,  0.0752,  0.0979,
         0.0952,  0.0756,  0.0983,  0.0905,  0.0557,  0.0669,  0.0649,  0.1036,
         0.1129,  0.0529,  0.0772,  0.0547,  0.0760,  0.0439,  0.1388,  0.1159,
         0.0894,  0.1902,  0.1101,  0.1079,  0.0492,  0.0830,  0.0746,  0.0823,
         0.0807,  0.0525,  0.0873,  0.0849,  0.0946,  0.0816,  0.0765,  0.0872,
         0.0490,  0.1361,  0.0760,  0.0697,  0.0823,  0.1013,  0.0594,  0.0808,
         0.1363,  0.1441,  0.0822,  0.1465,  0.0669,  0.1004,  0.0785,  0.1097,
         0.0665,  0.0444,  0.0995,  0.1408,  0.0727,  0.0679,  0.1098,  0.0989,
         0.0620,  0.0902,  0.1017,  0.0931,  0.0718,  0.0379,  0.0781,  0.1102,
         0.1407,  0.1017,  0.0794,  0.0617,  0.0858,  0.0960,  0.1459,  0.0665,
         0.1283,  0.0925,  0.0971,  0.0657,  0.0736,  0.1466,  0.0406,  0.1198,
         0.0946,  0.0841,  0.0721,  0.0654,  0.0632,  0.1024,  0.0984,  0.1136,
         0.0947,  0.1232,  0.1538,  0.0354,  0.0518,  0.0668,  0.1093,  0.1250,
         0.1308,  0.0203,  0.1117,  0.1195,  0.1214,  0.1248,  0.1203,  0.1947,
         0.1603,  0.1196,  0.0327,  0.1788,  0.1441,  0.1457,  0.1161,  0.1388,
         0.1231, -0.0842,  0.1118,  0.1316,  0.1272, -0.0289,  0.0418,  0.0585,
         0.0806, -0.0179,  0.0656,  0.1307,  0.0838,  0.0177,  0.0924,  0.1277,
         0.1333,  0.1150,  0.1279,  0.0840, -0.0199,  0.1498,  0.1560,  0.1331,
         0.0240,  0.1046,  0.1133,  0.1146,  0.1103,  0.1252,  0.0244, -0.0048,
         0.0930,  0.1354,  0.0190,  0.0976,  0.1392, -0.0574,  0.1640,  0.1313,
         0.0892,  0.0409,  0.0349,  0.1105,  0.1584,  0.0152,  0.1295,  0.1550,
         0.1543,  0.1590,  0.1157,  0.0299,  0.1210,  0.1692,  0.1151, -0.0577,
         0.0362,  0.1213,  0.1269,  0.0704,  0.0182,  0.0952,  0.1028,  0.1757,
         0.1219,  0.1023,  0.1049, -0.0230,  0.1293,  0.1104,  0.0995,  0.1515,
         0.0494,  0.1304,  0.0851,  0.1697,  0.0413,  0.1531,  0.0841,  0.0239,
         0.0532,  0.0292,  0.0771,  0.1562,  0.1389,  0.1312,  0.0133,  0.0611,
         0.1023,  0.1557,  0.1749,  0.1226,  0.1366,  0.1524,  0.0139,  0.1320,
         0.1263,  0.1420,  0.1508,  0.1472,  0.1677,  0.1739,  0.0393,  0.1064,
         0.2175,  0.2121,  0.2319,  0.1637,  0.1953,  0.2172,  0.1779,  0.1837,
         0.2261,  0.1999,  0.2015,  0.1964,  0.1596,  0.1464,  0.1628,  0.1915,
         0.2006,  0.1862,  0.1669,  0.1838,  0.2053,  0.1967,  0.1727,  0.1623,
         0.1769,  0.2311,  0.1752,  0.2232,  0.2339,  0.1205,  0.1864,  0.1881,
         0.1397,  0.1930,  0.1754,  0.1823,  0.2343,  0.1539,  0.1493,  0.1702,
         0.1754,  0.1883,  0.1113,  0.1582,  0.2207,  0.2294,  0.2254,  0.2174,
         0.2108,  0.2290,  0.1211,  0.1793,  0.1583,  0.1899,  0.1995,  0.2171,
         0.2185,  0.2008,  0.1927,  0.1653,  0.1677,  0.2355,  0.1832,  0.2177,
         0.1832,  0.2157,  0.1760,  0.1912,  0.2100,  0.1684,  0.2221,  0.2049,
         0.1701,  0.1782,  0.1764,  0.1724,  0.1871,  0.1723,  0.2156,  0.1235,
         0.1218,  0.1840,  0.2118,  0.1207,  0.1813,  0.1789,  0.1839,  0.2089,
         0.1673,  0.1184,  0.1423,  0.2074,  0.1844,  0.1923,  0.1703,  0.1896,
         0.1839,  0.1149,  0.1862,  0.1638,  0.2188,  0.1843,  0.1583,  0.1874,
         0.2066,  0.2346,  0.2266,  0.2280,  0.1978,  0.1730,  0.1932,  0.2093,
         0.1602,  0.1883,  0.1897,  0.2146,  0.1407,  0.1579,  0.2250,  0.2305,
         0.1431,  0.0880,  0.1466,  0.2185,  0.1741,  0.1872,  0.1980,  0.1637],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.2731, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:07:24,568 :: INFO :: ----- frontend -----
2023-05-14 12:07:24,568 :: INFO :: Environment 0
2023-05-14 12:07:33,944 :: INFO :: Epoch 5: loss tensor(819.6264, device='cuda:0'), U.norm 13.157378196716309, V.norm 16.900407791137695, MLP.norm 2.1528615951538086
2023-05-14 12:07:34,178 :: INFO :: Epoch 10: loss tensor(793.3817, device='cuda:0'), U.norm 10.346855163574219, V.norm 16.203771591186523, MLP.norm 3.645454168319702
2023-05-14 12:07:34,413 :: INFO :: Epoch 15: loss tensor(757.7604, device='cuda:0'), U.norm 8.645944595336914, V.norm 15.863945960998535, MLP.norm 5.39034366607666
2023-05-14 12:07:34,647 :: INFO :: Epoch 20: loss tensor(719.0093, device='cuda:0'), U.norm 7.485395431518555, V.norm 15.634498596191406, MLP.norm 7.195570468902588
2023-05-14 12:07:34,663 :: INFO :: Environment 1
2023-05-14 12:07:44,195 :: INFO :: Epoch 5: loss tensor(844.2407, device='cuda:0'), U.norm 13.15710735321045, V.norm 16.94295310974121, MLP.norm 2.1504149436950684
2023-05-14 12:07:44,429 :: INFO :: Epoch 10: loss tensor(817.5204, device='cuda:0'), U.norm 10.346294403076172, V.norm 16.272541046142578, MLP.norm 3.672517776489258
2023-05-14 12:07:44,648 :: INFO :: Epoch 15: loss tensor(780.8675, device='cuda:0'), U.norm 8.645423889160156, V.norm 15.949427604675293, MLP.norm 5.46820068359375
2023-05-14 12:07:44,882 :: INFO :: Epoch 20: loss tensor(742.0845, device='cuda:0'), U.norm 7.484918117523193, V.norm 15.73427677154541, MLP.norm 7.334491729736328
2023-05-14 12:07:44,898 :: INFO :: Environment 2
2023-05-14 12:07:54,446 :: INFO :: Epoch 5: loss tensor(820.2214, device='cuda:0'), U.norm 13.153295516967773, V.norm 16.908416748046875, MLP.norm 2.149163246154785
2023-05-14 12:07:54,680 :: INFO :: Epoch 10: loss tensor(795.0360, device='cuda:0'), U.norm 10.339111328125, V.norm 16.225685119628906, MLP.norm 3.6108460426330566
2023-05-14 12:07:54,915 :: INFO :: Epoch 15: loss tensor(761.1776, device='cuda:0'), U.norm 8.634183883666992, V.norm 15.899477005004883, MLP.norm 5.3226470947265625
2023-05-14 12:07:55,149 :: INFO :: Epoch 20: loss tensor(724.5779, device='cuda:0'), U.norm 7.469303131103516, V.norm 15.6834716796875, MLP.norm 7.100047588348389
2023-05-14 12:07:55,165 :: INFO :: Environment 3
2023-05-14 12:08:04,697 :: INFO :: Epoch 5: loss tensor(826.3180, device='cuda:0'), U.norm 13.152653694152832, V.norm 16.902101516723633, MLP.norm 2.1615734100341797
2023-05-14 12:08:04,947 :: INFO :: Epoch 10: loss tensor(800.6137, device='cuda:0'), U.norm 10.337213516235352, V.norm 16.20921516418457, MLP.norm 3.6317238807678223
2023-05-14 12:08:05,181 :: INFO :: Epoch 15: loss tensor(766.5892, device='cuda:0'), U.norm 8.630749702453613, V.norm 15.871315002441406, MLP.norm 5.342294216156006
2023-05-14 12:08:05,415 :: INFO :: Epoch 20: loss tensor(728.8770, device='cuda:0'), U.norm 7.464149475097656, V.norm 15.646202087402344, MLP.norm 7.124120235443115
2023-05-14 12:08:05,415 :: INFO :: Environment 4
2023-05-14 12:08:14,965 :: INFO :: Epoch 5: loss tensor(818.0364, device='cuda:0'), U.norm 13.157976150512695, V.norm 16.87990951538086, MLP.norm 2.139059066772461
2023-05-14 12:08:15,183 :: INFO :: Epoch 10: loss tensor(793.7701, device='cuda:0'), U.norm 10.3471040725708, V.norm 16.179473876953125, MLP.norm 3.60837984085083
2023-05-14 12:08:15,418 :: INFO :: Epoch 15: loss tensor(760.0176, device='cuda:0'), U.norm 8.645429611206055, V.norm 15.841654777526855, MLP.norm 5.3425517082214355
2023-05-14 12:08:15,652 :: INFO :: Epoch 20: loss tensor(723.2702, device='cuda:0'), U.norm 7.483734607696533, V.norm 15.61652660369873, MLP.norm 7.147558689117432
2023-05-14 12:08:15,652 :: INFO :: Environment 5
2023-05-14 12:08:24,981 :: INFO :: Epoch 5: loss tensor(874.1997, device='cuda:0'), U.norm 13.153634071350098, V.norm 16.997554779052734, MLP.norm 2.166262149810791
2023-05-14 12:08:25,215 :: INFO :: Epoch 10: loss tensor(844.0916, device='cuda:0'), U.norm 10.342141151428223, V.norm 16.345706939697266, MLP.norm 3.7132580280303955
2023-05-14 12:08:25,450 :: INFO :: Epoch 15: loss tensor(804.0648, device='cuda:0'), U.norm 8.640814781188965, V.norm 16.02611541748047, MLP.norm 5.50089693069458
2023-05-14 12:08:25,684 :: INFO :: Epoch 20: loss tensor(760.0397, device='cuda:0'), U.norm 7.480120658874512, V.norm 15.811235427856445, MLP.norm 7.343900203704834
2023-05-14 12:08:25,684 :: INFO :: Environment 6
2023-05-14 12:08:35,060 :: INFO :: Epoch 5: loss tensor(868.5411, device='cuda:0'), U.norm 13.15440559387207, V.norm 16.986692428588867, MLP.norm 2.1787397861480713
2023-05-14 12:08:35,279 :: INFO :: Epoch 10: loss tensor(840.4733, device='cuda:0'), U.norm 10.341808319091797, V.norm 16.32988929748535, MLP.norm 3.70973539352417
2023-05-14 12:08:35,513 :: INFO :: Epoch 15: loss tensor(803.5237, device='cuda:0'), U.norm 8.638718605041504, V.norm 16.011354446411133, MLP.norm 5.491358280181885
2023-05-14 12:08:35,732 :: INFO :: Epoch 20: loss tensor(764.4689, device='cuda:0'), U.norm 7.4752960205078125, V.norm 15.797942161560059, MLP.norm 7.297282695770264
2023-05-14 12:08:35,748 :: INFO :: Environment 7
2023-05-14 12:08:45,092 :: INFO :: Epoch 5: loss tensor(888.1865, device='cuda:0'), U.norm 13.158488273620605, V.norm 17.020158767700195, MLP.norm 2.187605619430542
2023-05-14 12:08:45,326 :: INFO :: Epoch 10: loss tensor(858.5439, device='cuda:0'), U.norm 10.349748611450195, V.norm 16.37565040588379, MLP.norm 3.7753281593322754
2023-05-14 12:08:45,561 :: INFO :: Epoch 15: loss tensor(819.1268, device='cuda:0'), U.norm 8.651154518127441, V.norm 16.059999465942383, MLP.norm 5.604085445404053
2023-05-14 12:08:45,795 :: INFO :: Epoch 20: loss tensor(776.7793, device='cuda:0'), U.norm 7.493258476257324, V.norm 15.84745979309082, MLP.norm 7.492385387420654
2023-05-14 12:08:45,795 :: INFO :: Environment 8
2023-05-14 12:08:55,155 :: INFO :: Epoch 5: loss tensor(861.6096, device='cuda:0'), U.norm 13.157584190368652, V.norm 16.971467971801758, MLP.norm 2.150035858154297
2023-05-14 12:08:55,374 :: INFO :: Epoch 10: loss tensor(836.2471, device='cuda:0'), U.norm 10.347319602966309, V.norm 16.31495475769043, MLP.norm 3.6554343700408936
2023-05-14 12:08:55,593 :: INFO :: Epoch 15: loss tensor(801.5457, device='cuda:0'), U.norm 8.646941184997559, V.norm 15.999195098876953, MLP.norm 5.439456462860107
2023-05-14 12:08:55,827 :: INFO :: Epoch 20: loss tensor(763.7692, device='cuda:0'), U.norm 7.4871745109558105, V.norm 15.789463996887207, MLP.norm 7.3015313148498535
2023-05-14 12:08:55,843 :: INFO :: Environment 9
2023-05-14 12:09:05,219 :: INFO :: Epoch 5: loss tensor(849.7642, device='cuda:0'), U.norm 13.157105445861816, V.norm 16.948396682739258, MLP.norm 2.171873092651367
2023-05-14 12:09:05,453 :: INFO :: Epoch 10: loss tensor(823.2672, device='cuda:0'), U.norm 10.34717082977295, V.norm 16.275964736938477, MLP.norm 3.699317693710327
2023-05-14 12:09:05,672 :: INFO :: Epoch 15: loss tensor(788.1033, device='cuda:0'), U.norm 8.647194862365723, V.norm 15.948731422424316, MLP.norm 5.487752437591553
2023-05-14 12:09:05,906 :: INFO :: Epoch 20: loss tensor(749.6849, device='cuda:0'), U.norm 7.487585067749023, V.norm 15.729522705078125, MLP.norm 7.35638952255249
2023-05-14 12:09:05,922 :: INFO :: Ite = 1, Delta = 4047
2023-05-14 12:09:05,922 :: INFO :: ----- backend -----
2023-05-14 12:09:10,485 :: INFO :: Epoch 5: loss tensor(149.1976, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 9.4141e-03,  1.1457e-02,  1.4719e-02,  1.4832e-02,  8.8199e-03,
         1.4962e-03,  9.1483e-03,  1.7170e-02,  5.4448e-03,  1.7491e-02,
         1.4847e-03,  1.3927e-02,  8.0234e-03,  1.7552e-02,  2.3785e-02,
         1.8932e-02,  1.8143e-02,  1.8724e-02,  1.8997e-02,  1.2256e-02,
         1.1492e-02,  1.9468e-02,  2.1202e-02,  3.2711e-03,  1.8625e-02,
         1.0679e-02,  1.1223e-02,  8.8912e-03,  2.4697e-02,  1.4642e-02,
         1.9619e-02,  2.4130e-02,  1.1246e-02,  1.8624e-02,  1.6934e-03,
        -3.2580e-03,  1.9711e-02,  1.8160e-02,  4.7779e-03,  2.0265e-02,
         1.0334e-02,  1.8711e-03,  2.5427e-02,  1.6482e-02,  1.1313e-02,
         1.8662e-02,  1.4319e-02,  2.6537e-03,  1.4260e-02,  1.4260e-02,
         2.3569e-02,  1.0934e-02,  1.3632e-02,  1.4416e-02,  1.1297e-02,
         2.0072e-02,  8.8793e-03,  1.7987e-02,  2.1194e-03,  6.1916e-04,
         1.7566e-02,  1.8971e-02, -2.1495e-04,  1.4259e-02,  1.6093e-02,
         4.4990e-03,  2.5182e-02,  1.1236e-02,  2.5013e-02,  6.9574e-03,
         2.1762e-02,  4.9982e-03,  2.2878e-02,  5.0461e-03,  1.8784e-02,
         6.1173e-03,  2.0885e-02,  6.3988e-04,  1.2977e-02,  1.8600e-02,
         1.7296e-02,  4.0561e-03,  2.0886e-02,  1.1879e-02,  1.7732e-03,
         1.6795e-02,  2.4518e-02,  1.3435e-02,  2.1329e-03, -2.9030e-03,
         1.0489e-02,  1.7233e-02,  1.4147e-02,  1.8074e-02,  4.0508e-03,
         1.8695e-02,  2.4478e-02,  1.4502e-02,  4.0624e-03,  1.1929e-02,
         4.1272e-03,  1.3208e-02,  1.6751e-02, -2.2196e-03,  8.4588e-03,
        -8.2191e-03,  1.7727e-02,  1.6654e-02,  1.7671e-02,  1.8696e-02,
         6.3534e-03,  1.6131e-02,  2.1725e-02,  1.8407e-02,  2.9465e-03,
         4.8271e-03,  1.3017e-02,  1.3105e-02,  1.2691e-02,  7.2742e-03,
         1.6918e-02,  1.4713e-02,  8.5647e-03,  7.6752e-03,  3.1853e-03,
         1.6292e-02,  3.9286e-03,  1.6107e-02,  2.0072e-02,  1.8387e-02,
         2.2817e-02,  2.4755e-02,  2.3112e-02,  1.9659e-02,  7.6120e-03,
        -1.3615e-03, -3.8860e-03,  1.4345e-02,  1.0741e-02, -6.0080e-03,
         2.5098e-02, -1.0386e-04,  1.1937e-02,  1.3149e-02,  2.1365e-02,
         2.5465e-02,  1.4671e-02,  1.3755e-02,  2.2477e-02,  1.3560e-02,
        -1.5527e-03,  1.9573e-02,  2.5257e-02, -2.0362e-02,  3.1794e-03,
        -2.5938e-03, -1.5407e-04, -2.0117e-02,  2.4706e-02, -1.8551e-02,
         2.5557e-02, -1.8832e-02,  2.3175e-02,  1.3810e-02,  2.5143e-02,
         1.5806e-02, -1.1544e-02,  1.8785e-04,  8.2158e-04,  1.2030e-02,
         2.1262e-02,  1.9256e-02, -1.9952e-02,  1.9532e-02,  2.5757e-02,
         4.0188e-03,  1.9033e-02,  2.1040e-02,  5.5044e-03,  1.5621e-03,
         1.4718e-02,  2.9841e-03,  2.3643e-02, -1.9619e-02,  2.2162e-02,
         2.2584e-02, -1.7417e-02,  1.6434e-02,  2.4818e-02, -2.0454e-02,
        -9.0221e-03,  8.3762e-03,  2.5253e-02, -1.6522e-02,  1.6675e-02,
         2.1059e-02,  1.7563e-02,  1.5691e-02,  2.1866e-02,  1.3354e-02,
         2.5398e-02,  1.6033e-02,  2.1853e-02,  2.2297e-02,  1.8770e-02,
         1.5734e-02, -3.4401e-03, -1.9889e-02,  2.2267e-02,  3.1224e-03,
         2.5773e-02,  2.5114e-02,  1.6061e-02, -5.3102e-03,  1.8383e-02,
         2.1213e-02,  5.1700e-03, -1.3058e-02,  2.5050e-02, -1.9563e-02,
         1.6947e-02,  4.0769e-03,  2.5113e-02,  2.1446e-02,  2.0773e-02,
         1.6955e-02,  1.3519e-02,  1.7977e-02,  1.5438e-02,  2.5630e-02,
         2.1956e-02,  2.0450e-02,  2.3228e-02,  2.0002e-02,  2.4803e-02,
         1.0019e-02,  3.3078e-03,  5.9738e-03,  1.1617e-02,  2.0175e-02,
         9.2308e-03,  1.3197e-02,  1.1826e-02,  2.4960e-02,  5.1960e-03,
         2.0757e-02, -1.8729e-02,  1.3794e-02,  9.5078e-03,  6.3338e-03,
         6.4263e-06,  2.5226e-02,  1.2327e-02,  2.0129e-02, -1.6644e-02,
         1.2913e-02,  1.4347e-02,  1.3318e-02,  1.2139e-02,  2.5697e-02,
         2.5349e-02,  1.5271e-02,  9.4170e-03,  1.2406e-02,  1.7718e-02,
         1.4643e-02,  1.8485e-02,  2.0348e-02,  1.5557e-02,  2.5361e-02,
         1.0404e-02,  1.2531e-02,  1.3797e-02,  1.4641e-02,  1.8170e-02,
         1.2273e-02,  1.4271e-02,  2.2017e-02,  2.5454e-02,  1.4874e-02,
         1.7248e-02,  1.7215e-02,  2.3583e-02,  7.3687e-03,  1.4264e-02,
         2.1135e-02,  1.1432e-02,  1.7474e-02,  2.5591e-02,  1.1761e-02,
         1.0781e-02,  1.5395e-02,  1.1859e-02,  2.0004e-02,  9.8960e-03,
         2.6099e-02,  1.9902e-02,  1.0419e-02,  2.2054e-02,  2.5319e-02,
         1.2248e-02,  2.5733e-02,  6.6589e-03, -1.1339e-04,  2.2541e-02,
         1.3247e-02,  2.0461e-02,  1.0719e-02,  1.6315e-02,  3.9830e-03,
         1.1336e-02,  1.4825e-02,  1.8964e-02,  1.7410e-02,  8.7657e-03,
         2.0743e-02,  1.5758e-02,  1.6403e-02,  2.5154e-02,  1.2326e-02,
         2.6289e-02,  1.7517e-03,  2.6298e-02,  1.5522e-02,  1.5793e-02,
         2.5580e-02,  5.3248e-03,  1.0762e-02,  1.3362e-02,  1.5189e-02,
         2.1623e-02,  6.4311e-03,  1.0865e-02,  9.2176e-03,  1.6288e-02,
         1.9560e-02,  1.4942e-02,  1.7397e-02,  2.0607e-02,  1.7105e-02,
         1.9051e-02,  1.4952e-02,  1.1031e-02,  1.6864e-02,  1.2415e-02,
         1.9451e-02,  2.1458e-02,  1.4745e-02,  1.0241e-02,  2.0197e-02,
         1.5480e-02,  1.3082e-02,  1.4526e-02,  8.6613e-03,  1.3404e-02,
         1.7191e-02,  5.2469e-03,  2.2138e-02,  1.7157e-02,  1.8044e-02,
         1.5594e-02,  7.0230e-03,  2.0608e-02,  4.6254e-03,  2.4933e-02,
         2.6198e-02,  2.0048e-02,  1.4385e-02,  2.2338e-02,  1.6063e-02,
         1.6555e-02, -4.1128e-03,  1.7408e-02,  1.5734e-02,  1.3958e-02,
         2.2549e-02,  2.0810e-02,  2.0703e-02,  1.8810e-02,  4.0346e-03,
         2.6367e-02,  1.1809e-02,  1.4630e-02,  2.1239e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.5402, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:15,000 :: INFO :: Epoch 10: loss tensor(146.9503, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0386,  0.0393,  0.0447,  0.0436,  0.0368,  0.0309,  0.0392,  0.0433,
         0.0336,  0.0448,  0.0312,  0.0418,  0.0351,  0.0465,  0.0517,  0.0430,
         0.0439,  0.0465,  0.0440,  0.0421,  0.0386,  0.0463,  0.0499,  0.0337,
         0.0470,  0.0364,  0.0347,  0.0385,  0.0558,  0.0400,  0.0478,  0.0505,
         0.0411,  0.0471,  0.0304,  0.0231,  0.0461,  0.0469,  0.0326,  0.0501,
         0.0387,  0.0290,  0.0545,  0.0431,  0.0381,  0.0459,  0.0403,  0.0317,
         0.0438,  0.0398,  0.0519,  0.0386,  0.0378,  0.0379,  0.0421,  0.0490,
         0.0369,  0.0485,  0.0324,  0.0305,  0.0432,  0.0431,  0.0280,  0.0431,
         0.0444,  0.0306,  0.0535,  0.0403,  0.0513,  0.0353,  0.0471,  0.0345,
         0.0444,  0.0335,  0.0463,  0.0346,  0.0484,  0.0291,  0.0402,  0.0435,
         0.0467,  0.0351,  0.0474,  0.0423,  0.0306,  0.0466,  0.0502,  0.0436,
         0.0297,  0.0219,  0.0388,  0.0458,  0.0414,  0.0458,  0.0349,  0.0484,
         0.0514,  0.0441,  0.0321,  0.0392,  0.0332,  0.0373,  0.0443,  0.0239,
         0.0392,  0.0161,  0.0444,  0.0431,  0.0435,  0.0458,  0.0363,  0.0427,
         0.0526,  0.0473,  0.0315,  0.0315,  0.0412,  0.0421,  0.0338,  0.0363,
         0.0453,  0.0426,  0.0364,  0.0348,  0.0303,  0.0404,  0.0328,  0.0453,
         0.0477,  0.0459,  0.0513,  0.0461,  0.0495,  0.0448,  0.0359,  0.0264,
         0.0249,  0.0184,  0.0399,  0.0190,  0.0530,  0.0279,  0.0414,  0.0436,
         0.0509,  0.0519,  0.0371,  0.0449,  0.0515,  0.0445,  0.0262,  0.0497,
         0.0530, -0.0353,  0.0312,  0.0264,  0.0275, -0.0326,  0.0454, -0.0290,
         0.0519, -0.0329,  0.0471,  0.0427,  0.0503,  0.0280,  0.0051,  0.0282,
         0.0294,  0.0405,  0.0507,  0.0455, -0.0329,  0.0479,  0.0551,  0.0338,
         0.0287,  0.0488,  0.0340,  0.0266,  0.0431,  0.0327,  0.0434, -0.0295,
         0.0487,  0.0521, -0.0196,  0.0460,  0.0554, -0.0349,  0.0133,  0.0374,
         0.0508, -0.0119,  0.0405,  0.0491,  0.0486,  0.0209,  0.0500,  0.0432,
         0.0536,  0.0470,  0.0486,  0.0393,  0.0470,  0.0471,  0.0224, -0.0357,
         0.0454,  0.0318,  0.0549,  0.0499,  0.0308,  0.0199,  0.0466,  0.0518,
         0.0338,  0.0030,  0.0519, -0.0307,  0.0469,  0.0335,  0.0536,  0.0502,
         0.0460,  0.0473,  0.0407,  0.0489,  0.0405,  0.0544,  0.0486,  0.0376,
         0.0422,  0.0387,  0.0492,  0.0411,  0.0326,  0.0343,  0.0191,  0.0445,
         0.0379,  0.0427,  0.0422,  0.0510,  0.0344,  0.0493, -0.0227,  0.0425,
         0.0381,  0.0353,  0.0291,  0.0528,  0.0428,  0.0502, -0.0080,  0.0415,
         0.0459,  0.0450,  0.0443,  0.0562,  0.0559,  0.0466,  0.0409,  0.0438,
         0.0492,  0.0454,  0.0498,  0.0515,  0.0466,  0.0559,  0.0410,  0.0441,
         0.0447,  0.0459,  0.0492,  0.0437,  0.0454,  0.0530,  0.0560,  0.0461,
         0.0484,  0.0494,  0.0543,  0.0392,  0.0463,  0.0514,  0.0429,  0.0486,
         0.0557,  0.0432,  0.0415,  0.0468,  0.0440,  0.0502,  0.0407,  0.0560,
         0.0510,  0.0407,  0.0525,  0.0560,  0.0442,  0.0574,  0.0381,  0.0311,
         0.0533,  0.0450,  0.0506,  0.0416,  0.0474,  0.0341,  0.0430,  0.0463,
         0.0503,  0.0482,  0.0395,  0.0515,  0.0461,  0.0486,  0.0561,  0.0439,
         0.0570,  0.0320,  0.0570,  0.0463,  0.0470,  0.0557,  0.0373,  0.0421,
         0.0444,  0.0464,  0.0527,  0.0372,  0.0424,  0.0407,  0.0477,  0.0500,
         0.0450,  0.0485,  0.0516,  0.0476,  0.0501,  0.0463,  0.0421,  0.0477,
         0.0435,  0.0490,  0.0516,  0.0459,  0.0412,  0.0513,  0.0469,  0.0445,
         0.0460,  0.0384,  0.0444,  0.0482,  0.0373,  0.0523,  0.0481,  0.0491,
         0.0470,  0.0393,  0.0524,  0.0363,  0.0558,  0.0562,  0.0510,  0.0458,
         0.0531,  0.0474,  0.0480,  0.0264,  0.0481,  0.0467,  0.0457,  0.0542,
         0.0511,  0.0496,  0.0497,  0.0355,  0.0568,  0.0433,  0.0462,  0.0519],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0708, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:19,533 :: INFO :: Epoch 15: loss tensor(142.8621, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0633,  0.0600,  0.0732,  0.0678,  0.0598,  0.0593,  0.0645,  0.0654,
         0.0558,  0.0657,  0.0602,  0.0646,  0.0584,  0.0708,  0.0743,  0.0601,
         0.0612,  0.0670,  0.0594,  0.0682,  0.0607,  0.0636,  0.0741,  0.0630,
         0.0708,  0.0530,  0.0498,  0.0644,  0.0854,  0.0583,  0.0722,  0.0706,
         0.0675,  0.0692,  0.0560,  0.0455,  0.0633,  0.0740,  0.0561,  0.0757,
         0.0615,  0.0536,  0.0795,  0.0646,  0.0570,  0.0658,  0.0607,  0.0588,
         0.0697,  0.0558,  0.0744,  0.0581,  0.0561,  0.0510,  0.0710,  0.0737,
         0.0609,  0.0775,  0.0597,  0.0584,  0.0584,  0.0624,  0.0533,  0.0660,
         0.0665,  0.0485,  0.0753,  0.0649,  0.0713,  0.0581,  0.0639,  0.0602,
         0.0578,  0.0616,  0.0674,  0.0556,  0.0706,  0.0533,  0.0593,  0.0637,
         0.0736,  0.0662,  0.0682,  0.0693,  0.0542,  0.0728,  0.0697,  0.0690,
         0.0516,  0.0406,  0.0623,  0.0711,  0.0618,  0.0655,  0.0646,  0.0737,
         0.0701,  0.0686,  0.0558,  0.0602,  0.0572,  0.0503,  0.0671,  0.0505,
         0.0675,  0.0427,  0.0656,  0.0614,  0.0626,  0.0685,  0.0647,  0.0630,
         0.0806,  0.0692,  0.0562,  0.0519,  0.0610,  0.0688,  0.0476,  0.0615,
         0.0686,  0.0653,  0.0601,  0.0534,  0.0532,  0.0625,  0.0581,  0.0694,
         0.0714,  0.0711,  0.0785,  0.0549,  0.0650,  0.0598,  0.0614,  0.0543,
         0.0567,  0.0219,  0.0673,  0.0459,  0.0782,  0.0563,  0.0694,  0.0749,
         0.0794,  0.0753,  0.0474,  0.0768,  0.0794,  0.0749,  0.0555,  0.0782,
         0.0782, -0.0457,  0.0572,  0.0572,  0.0553, -0.0377,  0.0557, -0.0308,
         0.0732, -0.0407,  0.0632,  0.0697,  0.0694,  0.0278,  0.0284,  0.0561,
         0.0579,  0.0668,  0.0783,  0.0660, -0.0395,  0.0748,  0.0834,  0.0647,
         0.0332,  0.0727,  0.0607,  0.0501,  0.0683,  0.0626,  0.0512, -0.0304,
         0.0707,  0.0797, -0.0102,  0.0728,  0.0832, -0.0439,  0.0443,  0.0654,
         0.0712,  0.0052,  0.0512,  0.0742,  0.0790,  0.0225,  0.0758,  0.0728,
         0.0805,  0.0775,  0.0720,  0.0439,  0.0734,  0.0786,  0.0483, -0.0469,
         0.0575,  0.0592,  0.0828,  0.0689,  0.0325,  0.0467,  0.0715,  0.0821,
         0.0610,  0.0272,  0.0759, -0.0340,  0.0756,  0.0610,  0.0774,  0.0772,
         0.0590,  0.0761,  0.0637,  0.0795,  0.0567,  0.0823,  0.0708,  0.0413,
         0.0504,  0.0414,  0.0674,  0.0729,  0.0614,  0.0615,  0.0219,  0.0602,
         0.0640,  0.0715,  0.0729,  0.0741,  0.0637,  0.0766, -0.0139,  0.0692,
         0.0654,  0.0641,  0.0616,  0.0795,  0.0732,  0.0802,  0.0153,  0.0674,
         0.0799,  0.0781,  0.0788,  0.0863,  0.0866,  0.0796,  0.0732,  0.0760,
         0.0828,  0.0768,  0.0814,  0.0826,  0.0768,  0.0847,  0.0711,  0.0763,
         0.0766,  0.0771,  0.0798,  0.0756,  0.0774,  0.0849,  0.0854,  0.0772,
         0.0797,  0.0836,  0.0850,  0.0734,  0.0810,  0.0797,  0.0752,  0.0805,
         0.0834,  0.0757,  0.0725,  0.0789,  0.0786,  0.0793,  0.0705,  0.0848,
         0.0820,  0.0718,  0.0808,  0.0858,  0.0776,  0.0908,  0.0720,  0.0658,
         0.0852,  0.0786,  0.0787,  0.0720,  0.0782,  0.0668,  0.0757,  0.0798,
         0.0834,  0.0798,  0.0710,  0.0819,  0.0759,  0.0828,  0.0873,  0.0768,
         0.0874,  0.0657,  0.0876,  0.0773,  0.0795,  0.0846,  0.0715,  0.0747,
         0.0756,  0.0778,  0.0840,  0.0688,  0.0749,  0.0730,  0.0807,  0.0788,
         0.0723,  0.0801,  0.0835,  0.0765,  0.0817,  0.0778,  0.0745,  0.0798,
         0.0749,  0.0743,  0.0799,  0.0776,  0.0729,  0.0832,  0.0779,  0.0762,
         0.0780,  0.0641,  0.0764,  0.0793,  0.0715,  0.0818,  0.0783,  0.0802,
         0.0788,  0.0741,  0.0856,  0.0708,  0.0870,  0.0851,  0.0828,  0.0781,
         0.0826,  0.0793,  0.0802,  0.0612,  0.0779,  0.0772,  0.0798,  0.0875,
         0.0800,  0.0731,  0.0800,  0.0690,  0.0869,  0.0758,  0.0789,  0.0813],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3328, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:24,064 :: INFO :: Epoch 20: loss tensor(140.3347, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0811,  0.0683,  0.0990,  0.0834,  0.0758,  0.0814,  0.0816,  0.0822,
         0.0654,  0.0785,  0.0846,  0.0816,  0.0736,  0.0891,  0.0905,  0.0686,
         0.0654,  0.0763,  0.0669,  0.0907,  0.0769,  0.0680,  0.0922,  0.0881,
         0.0912,  0.0581,  0.0542,  0.0857,  0.1114,  0.0660,  0.0917,  0.0812,
         0.0860,  0.0826,  0.0736,  0.0575,  0.0686,  0.0967,  0.0710,  0.0941,
         0.0758,  0.0696,  0.0951,  0.0794,  0.0675,  0.0720,  0.0722,  0.0789,
         0.0881,  0.0598,  0.0874,  0.0663,  0.0657,  0.0518,  0.0956,  0.0933,
         0.0752,  0.1055,  0.0807,  0.0797,  0.0615,  0.0736,  0.0677,  0.0821,
         0.0782,  0.0578,  0.0861,  0.0816,  0.0844,  0.0707,  0.0709,  0.0764,
         0.0608,  0.0864,  0.0810,  0.0648,  0.0835,  0.0713,  0.0665,  0.0739,
         0.0973,  0.0949,  0.0816,  0.0919,  0.0668,  0.0920,  0.0817,  0.0875,
         0.0634,  0.0464,  0.0787,  0.0925,  0.0739,  0.0743,  0.0877,  0.0898,
         0.0767,  0.0835,  0.0740,  0.0729,  0.0698,  0.0537,  0.0822,  0.0705,
         0.0912,  0.0643,  0.0793,  0.0685,  0.0722,  0.0851,  0.0898,  0.0726,
         0.1022,  0.0830,  0.0747,  0.0639,  0.0709,  0.0897,  0.0518,  0.0815,
         0.0858,  0.0812,  0.0739,  0.0581,  0.0674,  0.0775,  0.0784,  0.0888,
         0.0882,  0.0918,  0.1031,  0.0558,  0.0684,  0.0696,  0.0822,  0.0790,
         0.0872,  0.0260,  0.0897,  0.0700,  0.0980,  0.0817,  0.0932,  0.1046,
         0.1054,  0.0940,  0.0501,  0.1073,  0.1046,  0.1030,  0.0827,  0.1016,
         0.0993, -0.0534,  0.0788,  0.0872,  0.0805, -0.0374,  0.0593, -0.0271,
         0.0867, -0.0444,  0.0738,  0.0936,  0.0822,  0.0264,  0.0495,  0.0808,
         0.0843,  0.0884,  0.1011,  0.0791, -0.0414,  0.0985,  0.1089,  0.0940,
         0.0342,  0.0912,  0.0835,  0.0683,  0.0893,  0.0906,  0.0550, -0.0255,
         0.0864,  0.1032,  0.0037,  0.0934,  0.1060, -0.0495,  0.0735,  0.0905,
         0.0858,  0.0250,  0.0520,  0.0942,  0.1059,  0.0204,  0.0972,  0.0995,
         0.1045,  0.1049,  0.0908,  0.0454,  0.0964,  0.1074,  0.0712, -0.0549,
         0.0600,  0.0829,  0.1055,  0.0808,  0.0326,  0.0704,  0.0911,  0.1095,
         0.0847,  0.0500,  0.0940, -0.0317,  0.0994,  0.0830,  0.0930,  0.1011,
         0.0639,  0.1001,  0.0795,  0.1072,  0.0613,  0.1076,  0.0877,  0.0400,
         0.0529,  0.0432,  0.0780,  0.1026,  0.0873,  0.0855,  0.0200,  0.0681,
         0.0849,  0.0979,  0.1019,  0.0931,  0.0906,  0.1017,  0.0014,  0.0928,
         0.0893,  0.0906,  0.0920,  0.1028,  0.1009,  0.1081,  0.0418,  0.0886,
         0.1131,  0.1110,  0.1131,  0.1126,  0.1152,  0.1123,  0.1035,  0.1055,
         0.1162,  0.1071,  0.1120,  0.1121,  0.1026,  0.1098,  0.0979,  0.1064,
         0.1073,  0.1072,  0.1074,  0.1050,  0.1084,  0.1146,  0.1118,  0.1054,
         0.1088,  0.1178,  0.1128,  0.1074,  0.1151,  0.1025,  0.1054,  0.1104,
         0.1056,  0.1061,  0.1025,  0.1092,  0.1133,  0.1046,  0.0972,  0.1101,
         0.1102,  0.1011,  0.1041,  0.1121,  0.1111,  0.1239,  0.1060,  0.1004,
         0.1163,  0.1118,  0.1022,  0.0999,  0.1057,  0.0979,  0.1077,  0.1131,
         0.1154,  0.1095,  0.1012,  0.1087,  0.1025,  0.1167,  0.1161,  0.1093,
         0.1149,  0.0993,  0.1156,  0.1062,  0.1107,  0.1108,  0.1054,  0.1063,
         0.1044,  0.1067,  0.1126,  0.0981,  0.1050,  0.1029,  0.1129,  0.1025,
         0.0929,  0.1093,  0.1143,  0.0994,  0.1104,  0.1067,  0.1041,  0.1102,
         0.1030,  0.0934,  0.1035,  0.1080,  0.1030,  0.1133,  0.1064,  0.1066,
         0.1081,  0.0831,  0.1073,  0.1073,  0.1055,  0.1095,  0.1063,  0.1086,
         0.1101,  0.1088,  0.1184,  0.1055,  0.1154,  0.1106,  0.1126,  0.1096,
         0.1098,  0.1089,  0.1107,  0.0958,  0.1027,  0.1032,  0.1139,  0.1204,
         0.1046,  0.0890,  0.1061,  0.1022,  0.1139,  0.1063,  0.1101,  0.1077],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.2867, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:28,564 :: INFO :: Epoch 25: loss tensor(138.7399, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0923,  0.0708,  0.1207,  0.0922,  0.0841,  0.1000,  0.0916,  0.0893,
         0.0702,  0.0848,  0.1046,  0.0923,  0.0813,  0.1003,  0.0998,  0.0723,
         0.0671,  0.0799,  0.0674,  0.1071,  0.0849,  0.0697,  0.1048,  0.1076,
         0.1043,  0.0593,  0.0539,  0.1004,  0.1338,  0.0674,  0.1049,  0.0854,
         0.0971,  0.0892,  0.0844,  0.0628,  0.0703,  0.1127,  0.0803,  0.1060,
         0.0854,  0.0799,  0.1055,  0.0880,  0.0719,  0.0751,  0.0775,  0.0942,
         0.1011,  0.0622,  0.0917,  0.0678,  0.0712,  0.0497,  0.1154,  0.1059,
         0.0845,  0.1304,  0.0951,  0.0959,  0.0619,  0.0785,  0.0777,  0.0916,
         0.0824,  0.0596,  0.0904,  0.0917,  0.0891,  0.0779,  0.0738,  0.0857,
         0.0587,  0.1071,  0.0871,  0.0685,  0.0888,  0.0837,  0.0675,  0.0787,
         0.1153,  0.1193,  0.0871,  0.1090,  0.0728,  0.1045,  0.0859,  0.0997,
         0.0695,  0.0475,  0.0870,  0.1075,  0.0781,  0.0778,  0.1058,  0.0992,
         0.0768,  0.0910,  0.0863,  0.0789,  0.0760,  0.0536,  0.0886,  0.0859,
         0.1101,  0.0821,  0.0867,  0.0709,  0.0756,  0.0927,  0.1110,  0.0760,
         0.1186,  0.0907,  0.0849,  0.0689,  0.0744,  0.1084,  0.0526,  0.0961,
         0.0960,  0.0893,  0.0809,  0.0609,  0.0732,  0.0870,  0.0917,  0.1024,
         0.0970,  0.1065,  0.1228,  0.0515,  0.0656,  0.0735,  0.0960,  0.0982,
         0.1142,  0.0288,  0.1059,  0.0889,  0.1092,  0.1020,  0.1099,  0.1327,
         0.1267,  0.1051,  0.0483,  0.1346,  0.1246,  0.1266,  0.1054,  0.1190,
         0.1135, -0.0593,  0.0937,  0.1143,  0.1004, -0.0346,  0.0565, -0.0188,
         0.0909, -0.0443,  0.0785,  0.1120,  0.0886,  0.0244,  0.0658,  0.0998,
         0.1064,  0.1022,  0.1175,  0.0844, -0.0404,  0.1168,  0.1295,  0.1199,
         0.0335,  0.1016,  0.0998,  0.0813,  0.1029,  0.1145,  0.0515, -0.0188,
         0.0938,  0.1200,  0.0157,  0.1044,  0.1225, -0.0531,  0.1007,  0.1102,
         0.0926,  0.0419,  0.0492,  0.1066,  0.1278,  0.0172,  0.1119,  0.1213,
         0.1233,  0.1267,  0.1027,  0.0420,  0.1124,  0.1319,  0.0890, -0.0604,
         0.0561,  0.1006,  0.1200,  0.0838,  0.0308,  0.0881,  0.1031,  0.1325,
         0.1020,  0.0685,  0.1035, -0.0270,  0.1172,  0.0977,  0.1009,  0.1203,
         0.0640,  0.1173,  0.0868,  0.1304,  0.0568,  0.1279,  0.0964,  0.0345,
         0.0546,  0.0388,  0.0802,  0.1283,  0.1081,  0.1041,  0.0192,  0.0701,
         0.0978,  0.1197,  0.1276,  0.1068,  0.1132,  0.1221,  0.0172,  0.1102,
         0.1075,  0.1127,  0.1195,  0.1202,  0.1238,  0.1320,  0.0659,  0.1021,
         0.1450,  0.1419,  0.1459,  0.1345,  0.1416,  0.1434,  0.1308,  0.1319,
         0.1486,  0.1359,  0.1407,  0.1388,  0.1235,  0.1296,  0.1200,  0.1338,
         0.1369,  0.1346,  0.1311,  0.1315,  0.1373,  0.1421,  0.1347,  0.1300,
         0.1348,  0.1503,  0.1367,  0.1396,  0.1484,  0.1183,  0.1328,  0.1380,
         0.1231,  0.1340,  0.1307,  0.1364,  0.1474,  0.1255,  0.1189,  0.1321,
         0.1349,  0.1286,  0.1200,  0.1343,  0.1428,  0.1552,  0.1392,  0.1337,
         0.1454,  0.1438,  0.1181,  0.1266,  0.1290,  0.1270,  0.1375,  0.1444,
         0.1460,  0.1373,  0.1292,  0.1314,  0.1257,  0.1497,  0.1419,  0.1404,
         0.1388,  0.1317,  0.1399,  0.1325,  0.1404,  0.1327,  0.1378,  0.1362,
         0.1292,  0.1323,  0.1381,  0.1245,  0.1327,  0.1295,  0.1433,  0.1192,
         0.1062,  0.1355,  0.1439,  0.1146,  0.1356,  0.1324,  0.1303,  0.1381,
         0.1279,  0.1067,  0.1222,  0.1365,  0.1298,  0.1408,  0.1310,  0.1341,
         0.1353,  0.0968,  0.1359,  0.1314,  0.1381,  0.1349,  0.1300,  0.1340,
         0.1394,  0.1423,  0.1501,  0.1386,  0.1417,  0.1312,  0.1398,  0.1395,
         0.1326,  0.1356,  0.1386,  0.1288,  0.1217,  0.1249,  0.1467,  0.1518,
         0.1227,  0.0960,  0.1274,  0.1344,  0.1370,  0.1342,  0.1392,  0.1300],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.9413, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:33,049 :: INFO :: Epoch 30: loss tensor(135.5181, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0958,  0.0682,  0.1368,  0.0952,  0.0893,  0.1132,  0.0929,  0.0934,
         0.0725,  0.0882,  0.1171,  0.0977,  0.0854,  0.1054,  0.1027,  0.0724,
         0.0678,  0.0782,  0.0661,  0.1187,  0.0901,  0.0660,  0.1099,  0.1203,
         0.1129,  0.0591,  0.0522,  0.1103,  0.1489,  0.0652,  0.1119,  0.0829,
         0.1018,  0.0897,  0.0875,  0.0625,  0.0669,  0.1270,  0.0861,  0.1095,
         0.0913,  0.0857,  0.1093,  0.0926,  0.0722,  0.0742,  0.0785,  0.1035,
         0.1076,  0.0635,  0.0876,  0.0640,  0.0751,  0.0469,  0.1279,  0.1117,
         0.0878,  0.1510,  0.1014,  0.1031,  0.0601,  0.0811,  0.0814,  0.0942,
         0.0817,  0.0589,  0.0907,  0.0941,  0.0904,  0.0794,  0.0713,  0.0875,
         0.0553,  0.1263,  0.0873,  0.0659,  0.0886,  0.0902,  0.0637,  0.0791,
         0.1271,  0.1380,  0.0884,  0.1211,  0.0726,  0.1100,  0.0857,  0.1034,
         0.0711,  0.0462,  0.0902,  0.1177,  0.0799,  0.0754,  0.1161,  0.1019,
         0.0734,  0.0923,  0.0962,  0.0803,  0.0783,  0.0509,  0.0892,  0.0996,
         0.1213,  0.0921,  0.0867,  0.0681,  0.0766,  0.0964,  0.1257,  0.0774,
         0.1258,  0.0921,  0.0880,  0.0695,  0.0736,  0.1236,  0.0524,  0.1039,
         0.1003,  0.0919,  0.0845,  0.0627,  0.0759,  0.0942,  0.0990,  0.1110,
         0.1014,  0.1157,  0.1368,  0.0464,  0.0595,  0.0733,  0.1033,  0.1113,
         0.1365,  0.0312,  0.1164,  0.1009,  0.1118,  0.1162,  0.1187,  0.1585,
         0.1421,  0.1101,  0.0435,  0.1582,  0.1381,  0.1435,  0.1215,  0.1326,
         0.1200, -0.0647,  0.1006,  0.1365,  0.1142, -0.0316,  0.0551, -0.0094,
         0.0912, -0.0413,  0.0808,  0.1234,  0.0913,  0.0210,  0.0759,  0.1129,
         0.1221,  0.1105,  0.1262,  0.0841, -0.0385,  0.1293,  0.1441,  0.1410,
         0.0326,  0.1039,  0.1088,  0.0893,  0.1096,  0.1324,  0.0447, -0.0128,
         0.0940,  0.1297,  0.0234,  0.1069,  0.1308, -0.0553,  0.1257,  0.1232,
         0.0936,  0.0522,  0.0461,  0.1113,  0.1451,  0.0163,  0.1200,  0.1368,
         0.1361,  0.1426,  0.1085,  0.0369,  0.1207,  0.1523,  0.1000, -0.0639,
         0.0511,  0.1120,  0.1269,  0.0808,  0.0256,  0.0984,  0.1062,  0.1506,
         0.1125,  0.0817,  0.1060, -0.0227,  0.1301,  0.1050,  0.1061,  0.1358,
         0.0594,  0.1285,  0.0874,  0.1498,  0.0549,  0.1417,  0.0965,  0.0290,
         0.0534,  0.0348,  0.0777,  0.1494,  0.1222,  0.1166,  0.0169,  0.0695,
         0.1035,  0.1360,  0.1490,  0.1166,  0.1302,  0.1381,  0.0292,  0.1202,
         0.1194,  0.1291,  0.1443,  0.1325,  0.1415,  0.1510,  0.0837,  0.1080,
         0.1755,  0.1706,  0.1774,  0.1515,  0.1666,  0.1721,  0.1542,  0.1546,
         0.1797,  0.1636,  0.1673,  0.1620,  0.1390,  0.1415,  0.1373,  0.1578,
         0.1648,  0.1590,  0.1493,  0.1555,  0.1654,  0.1668,  0.1525,  0.1494,
         0.1565,  0.1814,  0.1564,  0.1698,  0.1799,  0.1259,  0.1565,  0.1625,
         0.1359,  0.1592,  0.1557,  0.1599,  0.1802,  0.1407,  0.1351,  0.1513,
         0.1554,  0.1548,  0.1263,  0.1515,  0.1727,  0.1849,  0.1713,  0.1656,
         0.1731,  0.1744,  0.1254,  0.1528,  0.1470,  0.1541,  0.1660,  0.1743,
         0.1748,  0.1644,  0.1554,  0.1488,  0.1463,  0.1811,  0.1638,  0.1696,
         0.1578,  0.1626,  0.1597,  0.1553,  0.1683,  0.1505,  0.1689,  0.1646,
         0.1494,  0.1537,  0.1594,  0.1472,  0.1573,  0.1517,  0.1713,  0.1272,
         0.1121,  0.1579,  0.1720,  0.1215,  0.1571,  0.1538,  0.1528,  0.1644,
         0.1499,  0.1154,  0.1360,  0.1626,  0.1545,  0.1647,  0.1501,  0.1590,
         0.1585,  0.1063,  0.1615,  0.1512,  0.1691,  0.1597,  0.1478,  0.1554,
         0.1669,  0.1745,  0.1800,  0.1711,  0.1671,  0.1458,  0.1644,  0.1673,
         0.1500,  0.1584,  0.1630,  0.1600,  0.1339,  0.1430,  0.1781,  0.1817,
         0.1343,  0.0919,  0.1425,  0.1653,  0.1553,  0.1588,  0.1657,  0.1470],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.3492, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:37,534 :: INFO :: Epoch 35: loss tensor(136.2051, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0980,  0.0652,  0.1509,  0.0978,  0.0904,  0.1216,  0.0941,  0.0936,
         0.0732,  0.0872,  0.1268,  0.0984,  0.0829,  0.1094,  0.1051,  0.0697,
         0.0635,  0.0799,  0.0655,  0.1274,  0.0918,  0.0652,  0.1132,  0.1302,
         0.1184,  0.0539,  0.0478,  0.1159,  0.1621,  0.0623,  0.1169,  0.0803,
         0.1066,  0.0909,  0.0889,  0.0612,  0.0633,  0.1333,  0.0860,  0.1131,
         0.0923,  0.0878,  0.1105,  0.0924,  0.0697,  0.0701,  0.0773,  0.1077,
         0.1133,  0.0608,  0.0854,  0.0602,  0.0732,  0.0418,  0.1377,  0.1169,
         0.0889,  0.1702,  0.1083,  0.1056,  0.0563,  0.0823,  0.0829,  0.0957,
         0.0824,  0.0560,  0.0904,  0.0953,  0.0886,  0.0785,  0.0717,  0.0898,
         0.0516,  0.1374,  0.0887,  0.0637,  0.0894,  0.0970,  0.0618,  0.0789,
         0.1372,  0.1530,  0.0874,  0.1307,  0.0713,  0.1127,  0.0837,  0.1062,
         0.0701,  0.0437,  0.0886,  0.1253,  0.0792,  0.0740,  0.1224,  0.1045,
         0.0677,  0.0935,  0.1011,  0.0824,  0.0763,  0.0478,  0.0873,  0.1046,
         0.1310,  0.1005,  0.0859,  0.0667,  0.0770,  0.0975,  0.1399,  0.0741,
         0.1309,  0.0935,  0.0902,  0.0676,  0.0740,  0.1315,  0.0501,  0.1103,
         0.1016,  0.0917,  0.0858,  0.0641,  0.0737,  0.0963,  0.1047,  0.1172,
         0.0998,  0.1191,  0.1452,  0.0419,  0.0543,  0.0715,  0.1048,  0.1177,
         0.1542,  0.0325,  0.1199,  0.1065,  0.1082,  0.1241,  0.1204,  0.1805,
         0.1526,  0.1092,  0.0388,  0.1781,  0.1459,  0.1551,  0.1316,  0.1386,
         0.1201, -0.0696,  0.1015,  0.1543,  0.1225, -0.0297,  0.0503, -0.0010,
         0.0860, -0.0376,  0.0803,  0.1284,  0.0899,  0.0204,  0.0802,  0.1200,
         0.1318,  0.1123,  0.1300,  0.0805, -0.0368,  0.1361,  0.1530,  0.1571,
         0.0281,  0.1001,  0.1113,  0.0935,  0.1112,  0.1451,  0.0407, -0.0088,
         0.0887,  0.1330,  0.0272,  0.1033,  0.1340, -0.0573,  0.1455,  0.1301,
         0.0903,  0.0568,  0.0437,  0.1110,  0.1557,  0.0134,  0.1213,  0.1466,
         0.1429,  0.1518,  0.1092,  0.0328,  0.1227,  0.1663,  0.1047, -0.0665,
         0.0442,  0.1170,  0.1254,  0.0740,  0.0225,  0.1025,  0.1039,  0.1629,
         0.1160,  0.0886,  0.1017, -0.0197,  0.1352,  0.1065,  0.1054,  0.1461,
         0.0558,  0.1327,  0.0836,  0.1637,  0.0492,  0.1503,  0.0917,  0.0253,
         0.0506,  0.0317,  0.0710,  0.1655,  0.1300,  0.1233,  0.0151,  0.0672,
         0.1030,  0.1465,  0.1664,  0.1218,  0.1416,  0.1487,  0.0370,  0.1240,
         0.1252,  0.1398,  0.1642,  0.1389,  0.1533,  0.1653,  0.0957,  0.1078,
         0.2043,  0.1968,  0.2062,  0.1640,  0.1904,  0.1988,  0.1743,  0.1743,
         0.2080,  0.1892,  0.1916,  0.1832,  0.1517,  0.1483,  0.1509,  0.1795,
         0.1906,  0.1805,  0.1631,  0.1759,  0.1917,  0.1883,  0.1676,  0.1650,
         0.1749,  0.2094,  0.1731,  0.1983,  0.2085,  0.1292,  0.1767,  0.1831,
         0.1458,  0.1810,  0.1786,  0.1806,  0.2111,  0.1526,  0.1471,  0.1693,
         0.1730,  0.1790,  0.1279,  0.1650,  0.2007,  0.2113,  0.2008,  0.1936,
         0.1990,  0.2022,  0.1268,  0.1760,  0.1593,  0.1781,  0.1914,  0.2026,
         0.2006,  0.1899,  0.1790,  0.1617,  0.1644,  0.2093,  0.1823,  0.1968,
         0.1731,  0.1910,  0.1755,  0.1761,  0.1943,  0.1655,  0.1976,  0.1909,
         0.1669,  0.1705,  0.1766,  0.1662,  0.1791,  0.1706,  0.1964,  0.1295,
         0.1154,  0.1769,  0.1977,  0.1229,  0.1753,  0.1719,  0.1712,  0.1868,
         0.1698,  0.1228,  0.1459,  0.1870,  0.1760,  0.1859,  0.1653,  0.1813,
         0.1779,  0.1143,  0.1832,  0.1670,  0.1973,  0.1829,  0.1628,  0.1734,
         0.1920,  0.2041,  0.2063,  0.2019,  0.1911,  0.1556,  0.1865,  0.1934,
         0.1642,  0.1779,  0.1845,  0.1890,  0.1394,  0.1579,  0.2078,  0.2084,
         0.1398,  0.0872,  0.1516,  0.1938,  0.1703,  0.1809,  0.1897,  0.1607],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.5441, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:42,034 :: INFO :: Epoch 40: loss tensor(133.8011, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0982,  0.0649,  0.1627,  0.0965,  0.0924,  0.1244,  0.0919,  0.0943,
         0.0755,  0.0879,  0.1313,  0.1011,  0.0851,  0.1106,  0.1064,  0.0706,
         0.0641,  0.0787,  0.0641,  0.1339,  0.0954,  0.0640,  0.1143,  0.1352,
         0.1229,  0.0534,  0.0447,  0.1201,  0.1697,  0.0601,  0.1226,  0.0784,
         0.1053,  0.0886,  0.0900,  0.0613,  0.0638,  0.1398,  0.0876,  0.1112,
         0.0969,  0.0857,  0.1087,  0.0943,  0.0687,  0.0673,  0.0775,  0.1140,
         0.1133,  0.0616,  0.0810,  0.0577,  0.0756,  0.0407,  0.1414,  0.1189,
         0.0854,  0.1882,  0.1093,  0.1059,  0.0558,  0.0814,  0.0808,  0.0965,
         0.0800,  0.0540,  0.0861,  0.0968,  0.0884,  0.0757,  0.0713,  0.0883,
         0.0493,  0.1507,  0.0872,  0.0613,  0.0878,  0.1007,  0.0600,  0.0757,
         0.1443,  0.1615,  0.0869,  0.1369,  0.0687,  0.1104,  0.0818,  0.1063,
         0.0727,  0.0424,  0.0886,  0.1312,  0.0780,  0.0721,  0.1202,  0.1021,
         0.0652,  0.0901,  0.1047,  0.0832,  0.0726,  0.0472,  0.0846,  0.1119,
         0.1353,  0.1056,  0.0856,  0.0632,  0.0752,  0.0975,  0.1495,  0.0755,
         0.1294,  0.0931,  0.0913,  0.0655,  0.0731,  0.1426,  0.0504,  0.1156,
         0.1014,  0.0912,  0.0812,  0.0621,  0.0715,  0.1009,  0.1068,  0.1209,
         0.0972,  0.1193,  0.1486,  0.0365,  0.0480,  0.0672,  0.1036,  0.1198,
         0.1678,  0.0316,  0.1207,  0.1078,  0.0997,  0.1281,  0.1157,  0.1991,
         0.1575,  0.1053,  0.0345,  0.1935,  0.1478,  0.1608,  0.1370,  0.1402,
         0.1150, -0.0742,  0.0985,  0.1692,  0.1268, -0.0276,  0.0484,  0.0076,
         0.0812, -0.0325,  0.0764,  0.1285,  0.0882,  0.0197,  0.0823,  0.1232,
         0.1370,  0.1108,  0.1281,  0.0764, -0.0346,  0.1376,  0.1564,  0.1689,
         0.0283,  0.0936,  0.1091,  0.0967,  0.1089,  0.1530,  0.0361, -0.0054,
         0.0819,  0.1308,  0.0285,  0.0956,  0.1313, -0.0588,  0.1613,  0.1313,
         0.0859,  0.0593,  0.0408,  0.1064,  0.1595,  0.0121,  0.1186,  0.1498,
         0.1449,  0.1541,  0.1073,  0.0265,  0.1183,  0.1739,  0.1056, -0.0680,
         0.0421,  0.1182,  0.1172,  0.0672,  0.0186,  0.1022,  0.0974,  0.1693,
         0.1147,  0.0920,  0.0938, -0.0169,  0.1355,  0.1032,  0.1005,  0.1526,
         0.0511,  0.1297,  0.0784,  0.1717,  0.0432,  0.1523,  0.0837,  0.0208,
         0.0490,  0.0269,  0.0650,  0.1758,  0.1323,  0.1264,  0.0142,  0.0616,
         0.0989,  0.1517,  0.1795,  0.1234,  0.1486,  0.1555,  0.0429,  0.1219,
         0.1269,  0.1464,  0.1794,  0.1417,  0.1588,  0.1743,  0.1049,  0.1031,
         0.2316,  0.2203,  0.2328,  0.1723,  0.2104,  0.2228,  0.1913,  0.1901,
         0.2343,  0.2104,  0.2139,  0.2010,  0.1589,  0.1497,  0.1616,  0.1979,
         0.2133,  0.1998,  0.1717,  0.1933,  0.2149,  0.2078,  0.1795,  0.1761,
         0.1897,  0.2345,  0.1845,  0.2250,  0.2356,  0.1256,  0.1936,  0.2009,
         0.1526,  0.2001,  0.2003,  0.1980,  0.2391,  0.1598,  0.1554,  0.1843,
         0.1854,  0.2012,  0.1232,  0.1737,  0.2265,  0.2353,  0.2280,  0.2198,
         0.2222,  0.2280,  0.1244,  0.1974,  0.1668,  0.2001,  0.2164,  0.2279,
         0.2250,  0.2120,  0.2014,  0.1698,  0.1793,  0.2358,  0.1968,  0.2217,
         0.1844,  0.2178,  0.1873,  0.1936,  0.2175,  0.1765,  0.2240,  0.2152,
         0.1813,  0.1850,  0.1898,  0.1831,  0.1975,  0.1850,  0.2204,  0.1245,
         0.1150,  0.1923,  0.2197,  0.1176,  0.1887,  0.1857,  0.1872,  0.2070,
         0.1861,  0.1317,  0.1544,  0.2083,  0.1950,  0.2041,  0.1767,  0.2025,
         0.1941,  0.1195,  0.2022,  0.1776,  0.2233,  0.2016,  0.1742,  0.1877,
         0.2153,  0.2315,  0.2298,  0.2296,  0.2111,  0.1611,  0.2051,  0.2174,
         0.1746,  0.1939,  0.2029,  0.2155,  0.1396,  0.1680,  0.2344,  0.2324,
         0.1393,  0.0818,  0.1533,  0.2205,  0.1804,  0.1996,  0.2112,  0.1699],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.5448, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:09:42,034 :: INFO :: ----- frontend -----
2023-05-14 12:09:42,034 :: INFO :: Environment 0
2023-05-14 12:09:51,191 :: INFO :: Epoch 5: loss tensor(872.4337, device='cuda:0'), U.norm 13.160419464111328, V.norm 16.989267349243164, MLP.norm 2.1471712589263916
2023-05-14 12:09:51,410 :: INFO :: Epoch 10: loss tensor(844.5738, device='cuda:0'), U.norm 10.353906631469727, V.norm 16.334211349487305, MLP.norm 3.687282085418701
2023-05-14 12:09:51,644 :: INFO :: Epoch 15: loss tensor(806.8676, device='cuda:0'), U.norm 8.658121109008789, V.norm 16.015670776367188, MLP.norm 5.461165904998779
2023-05-14 12:09:51,863 :: INFO :: Epoch 20: loss tensor(764.5051, device='cuda:0'), U.norm 7.503838062286377, V.norm 15.80218505859375, MLP.norm 7.280041217803955
2023-05-14 12:09:51,878 :: INFO :: Environment 1
2023-05-14 12:10:01,239 :: INFO :: Epoch 5: loss tensor(834.2830, device='cuda:0'), U.norm 13.157909393310547, V.norm 16.919570922851562, MLP.norm 2.1535513401031494
2023-05-14 12:10:01,458 :: INFO :: Epoch 10: loss tensor(808.9242, device='cuda:0'), U.norm 10.348106384277344, V.norm 16.237977981567383, MLP.norm 3.6423861980438232
2023-05-14 12:10:01,692 :: INFO :: Epoch 15: loss tensor(774.6664, device='cuda:0'), U.norm 8.64848518371582, V.norm 15.907767295837402, MLP.norm 5.405402183532715
2023-05-14 12:10:01,911 :: INFO :: Epoch 20: loss tensor(735.4246, device='cuda:0'), U.norm 7.490084171295166, V.norm 15.68552017211914, MLP.norm 7.251530647277832
2023-05-14 12:10:01,926 :: INFO :: Environment 2
2023-05-14 12:10:11,287 :: INFO :: Epoch 5: loss tensor(818.6771, device='cuda:0'), U.norm 13.154535293579102, V.norm 16.9044246673584, MLP.norm 2.1640806198120117
2023-05-14 12:10:11,505 :: INFO :: Epoch 10: loss tensor(794.7525, device='cuda:0'), U.norm 10.340197563171387, V.norm 16.214157104492188, MLP.norm 3.621035099029541
2023-05-14 12:10:11,740 :: INFO :: Epoch 15: loss tensor(761.5674, device='cuda:0'), U.norm 8.635114669799805, V.norm 15.88118839263916, MLP.norm 5.329277038574219
2023-05-14 12:10:11,959 :: INFO :: Epoch 20: loss tensor(724.0529, device='cuda:0'), U.norm 7.470247745513916, V.norm 15.659173965454102, MLP.norm 7.1298828125
2023-05-14 12:10:11,974 :: INFO :: Environment 3
2023-05-14 12:10:21,146 :: INFO :: Epoch 5: loss tensor(834.2490, device='cuda:0'), U.norm 13.151701927185059, V.norm 16.908588409423828, MLP.norm 2.169654130935669
2023-05-14 12:10:21,381 :: INFO :: Epoch 10: loss tensor(807.5171, device='cuda:0'), U.norm 10.336000442504883, V.norm 16.218780517578125, MLP.norm 3.672962188720703
2023-05-14 12:10:21,600 :: INFO :: Epoch 15: loss tensor(771.1919, device='cuda:0'), U.norm 8.629554748535156, V.norm 15.884642601013184, MLP.norm 5.452750205993652
2023-05-14 12:10:21,834 :: INFO :: Epoch 20: loss tensor(731.1052, device='cuda:0'), U.norm 7.463144302368164, V.norm 15.660301208496094, MLP.norm 7.299526214599609
2023-05-14 12:10:21,834 :: INFO :: Environment 4
2023-05-14 12:10:31,194 :: INFO :: Epoch 5: loss tensor(795.3126, device='cuda:0'), U.norm 13.1524019241333, V.norm 16.851119995117188, MLP.norm 2.125669240951538
2023-05-14 12:10:31,429 :: INFO :: Epoch 10: loss tensor(770.8865, device='cuda:0'), U.norm 10.336531639099121, V.norm 16.13738441467285, MLP.norm 3.5897958278656006
2023-05-14 12:10:31,647 :: INFO :: Epoch 15: loss tensor(736.8066, device='cuda:0'), U.norm 8.629650115966797, V.norm 15.793164253234863, MLP.norm 5.30629301071167
2023-05-14 12:10:31,866 :: INFO :: Epoch 20: loss tensor(699.5607, device='cuda:0'), U.norm 7.462396621704102, V.norm 15.562251091003418, MLP.norm 7.093813896179199
2023-05-14 12:10:31,882 :: INFO :: Environment 5
2023-05-14 12:10:41,054 :: INFO :: Epoch 5: loss tensor(851.2611, device='cuda:0'), U.norm 13.15776252746582, V.norm 16.954914093017578, MLP.norm 2.140559196472168
2023-05-14 12:10:41,289 :: INFO :: Epoch 10: loss tensor(823.6106, device='cuda:0'), U.norm 10.3482666015625, V.norm 16.286964416503906, MLP.norm 3.638461112976074
2023-05-14 12:10:41,508 :: INFO :: Epoch 15: loss tensor(787.0288, device='cuda:0'), U.norm 8.648719787597656, V.norm 15.962791442871094, MLP.norm 5.377204418182373
2023-05-14 12:10:41,726 :: INFO :: Epoch 20: loss tensor(747.8922, device='cuda:0'), U.norm 7.489583492279053, V.norm 15.746247291564941, MLP.norm 7.158073902130127
2023-05-14 12:10:41,742 :: INFO :: Environment 6
2023-05-14 12:10:51,086 :: INFO :: Epoch 5: loss tensor(914.9210, device='cuda:0'), U.norm 13.159664154052734, V.norm 17.054969787597656, MLP.norm 2.2084407806396484
2023-05-14 12:10:51,321 :: INFO :: Epoch 10: loss tensor(886.6674, device='cuda:0'), U.norm 10.353306770324707, V.norm 16.42768096923828, MLP.norm 3.8345446586608887
2023-05-14 12:10:51,539 :: INFO :: Epoch 15: loss tensor(848.9009, device='cuda:0'), U.norm 8.657620429992676, V.norm 16.123882293701172, MLP.norm 5.747263431549072
2023-05-14 12:10:51,774 :: INFO :: Epoch 20: loss tensor(808.5169, device='cuda:0'), U.norm 7.503299713134766, V.norm 15.920339584350586, MLP.norm 7.714569091796875
2023-05-14 12:10:51,774 :: INFO :: Environment 7
2023-05-14 12:11:00,947 :: INFO :: Epoch 5: loss tensor(832.2985, device='cuda:0'), U.norm 13.155111312866211, V.norm 16.929672241210938, MLP.norm 2.1862130165100098
2023-05-14 12:11:01,181 :: INFO :: Epoch 10: loss tensor(805.4031, device='cuda:0'), U.norm 10.342451095581055, V.norm 16.254108428955078, MLP.norm 3.6986005306243896
2023-05-14 12:11:01,415 :: INFO :: Epoch 15: loss tensor(770.3020, device='cuda:0'), U.norm 8.638396263122559, V.norm 15.928383827209473, MLP.norm 5.427331924438477
2023-05-14 12:11:01,650 :: INFO :: Epoch 20: loss tensor(733.0959, device='cuda:0'), U.norm 7.473891735076904, V.norm 15.71088981628418, MLP.norm 7.196764945983887
2023-05-14 12:11:01,666 :: INFO :: Environment 8
2023-05-14 12:11:11,010 :: INFO :: Epoch 5: loss tensor(850.6559, device='cuda:0'), U.norm 13.157753944396973, V.norm 16.953397750854492, MLP.norm 2.147352695465088
2023-05-14 12:11:11,244 :: INFO :: Epoch 10: loss tensor(824.5618, device='cuda:0'), U.norm 10.348118782043457, V.norm 16.283273696899414, MLP.norm 3.62650990486145
2023-05-14 12:11:11,479 :: INFO :: Epoch 15: loss tensor(788.9240, device='cuda:0'), U.norm 8.648581504821777, V.norm 15.956757545471191, MLP.norm 5.36584997177124
2023-05-14 12:11:11,713 :: INFO :: Epoch 20: loss tensor(749.9104, device='cuda:0'), U.norm 7.490480422973633, V.norm 15.7378511428833, MLP.norm 7.183225631713867
2023-05-14 12:11:11,729 :: INFO :: Environment 9
2023-05-14 12:11:21,073 :: INFO :: Epoch 5: loss tensor(866.0062, device='cuda:0'), U.norm 13.158231735229492, V.norm 16.97605323791504, MLP.norm 2.1858842372894287
2023-05-14 12:11:21,308 :: INFO :: Epoch 10: loss tensor(838.0690, device='cuda:0'), U.norm 10.349800109863281, V.norm 16.315773010253906, MLP.norm 3.7344584465026855
2023-05-14 12:11:21,542 :: INFO :: Epoch 15: loss tensor(800.9081, device='cuda:0'), U.norm 8.651708602905273, V.norm 15.995903968811035, MLP.norm 5.534128189086914
2023-05-14 12:11:21,776 :: INFO :: Epoch 20: loss tensor(760.4777, device='cuda:0'), U.norm 7.4942169189453125, V.norm 15.78191089630127, MLP.norm 7.423047065734863
2023-05-14 12:11:21,776 :: INFO :: Ite = 1, Delta = 4047
2023-05-14 12:11:21,776 :: INFO :: ----- backend -----
2023-05-14 12:11:27,277 :: INFO :: Epoch 5: loss tensor(85.7390, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.5977e-02,  1.5605e-02,  2.5006e-02,  1.9376e-02,  9.1594e-03,
         9.1673e-03,  1.5688e-02,  1.8619e-02,  8.0327e-03,  2.1370e-02,
         7.9873e-03,  1.4645e-02,  3.9518e-03,  2.2050e-02,  2.6249e-02,
         2.0368e-02,  2.0360e-02,  1.9766e-02,  1.7112e-02,  2.0343e-02,
         1.1206e-02,  2.2478e-02,  2.5695e-02,  9.4520e-03,  1.8863e-02,
         1.1151e-02,  1.4389e-02,  1.4527e-02,  3.0822e-02,  1.9151e-02,
         2.2199e-02,  2.5391e-02,  1.6509e-02,  2.1928e-02,  5.8114e-03,
        -4.2060e-04,  2.0364e-02,  1.2491e-02,  9.1650e-03,  2.2041e-02,
         1.0205e-02,  7.2938e-03,  2.7054e-02,  1.3876e-02,  1.2027e-02,
         2.5037e-02,  1.4973e-02,  1.0807e-02,  1.7377e-02,  1.6753e-02,
         2.4812e-02,  1.5212e-02,  1.7207e-02,  1.1481e-02,  2.4085e-02,
         2.4150e-02,  1.4814e-02,  2.6080e-02,  1.0403e-02,  8.9747e-03,
         1.8983e-02,  1.8566e-02,  5.9851e-03,  1.8268e-02,  1.9912e-02,
         6.3330e-03,  2.7097e-02,  1.6089e-02,  2.6225e-02,  9.6565e-03,
         2.4599e-02,  9.6995e-03,  2.2794e-02,  5.3896e-03,  2.2419e-02,
         1.0525e-02,  2.0441e-02,  8.8302e-03,  1.6428e-02,  1.6740e-02,
         1.8200e-02,  1.0983e-02,  2.3420e-02,  1.7449e-02,  8.5304e-04,
         2.4072e-02,  2.4866e-02,  2.0197e-02,  1.0933e-02, -7.6801e-05,
         8.0984e-03,  2.2389e-02,  1.7392e-02,  2.3617e-02,  1.5060e-02,
         2.0595e-02,  2.5858e-02,  2.2233e-02,  1.2989e-02,  1.3727e-02,
         8.0787e-03,  1.2999e-02,  1.8961e-02, -2.2861e-03,  2.0207e-02,
         5.1864e-03,  1.9962e-02,  2.0121e-02,  1.5934e-02,  2.0316e-02,
         1.9126e-02,  1.5620e-02,  2.6877e-02,  2.0823e-02,  8.0485e-03,
         7.3306e-03,  1.9937e-02,  1.5274e-02,  1.1141e-02,  1.6108e-02,
         2.1138e-02,  1.3472e-02,  8.8422e-03,  1.1024e-02,  6.0963e-03,
         1.0242e-02,  1.2992e-02,  2.0392e-02,  2.3125e-02,  1.9889e-02,
         1.9067e-02,  2.5727e-02,  2.9133e-02,  2.4049e-02,  9.4286e-03,
        -5.0596e-03, -1.3564e-02,  1.5593e-02,  1.4730e-02, -4.9986e-03,
         2.5066e-02, -8.3229e-03,  1.4975e-02,  1.5101e-02,  2.1223e-02,
         2.7373e-02,  1.8205e-02,  4.7078e-03,  2.2101e-02,  1.3986e-02,
        -1.1252e-02,  2.3803e-02,  2.8569e-02, -1.9179e-02,  6.8026e-03,
        -9.2999e-03, -9.4914e-03, -2.2440e-02,  2.2337e-02, -2.2114e-02,
         2.8305e-02, -2.3722e-02,  2.3370e-02,  1.4850e-02,  2.6714e-02,
         1.3965e-02, -1.4595e-02, -2.5630e-03,  1.8957e-04,  1.5515e-02,
         2.6052e-02,  2.3228e-02, -2.3129e-02,  2.0397e-02,  2.9615e-02,
        -1.1942e-02,  1.5680e-02,  2.2854e-02,  8.0150e-03,  1.8453e-03,
         1.6449e-02, -5.0038e-03,  2.1011e-02, -2.1879e-02,  2.2619e-02,
         2.5273e-02, -1.9393e-02,  2.2043e-02,  2.5994e-02, -2.2294e-02,
        -1.4261e-02,  2.2356e-03,  2.7295e-02, -1.9166e-02,  2.0587e-02,
         2.4112e-02,  1.9201e-02,  1.2837e-02,  2.9202e-02,  1.1836e-02,
         2.3763e-02,  1.7051e-02,  2.8003e-02,  2.1981e-02,  1.7847e-02,
         1.4625e-02, -2.1242e-03, -2.3679e-02,  2.3511e-02,  1.1404e-03,
         2.8909e-02,  2.5804e-02,  1.5535e-02, -1.0919e-02,  2.2902e-02,
         2.2784e-02,  2.3285e-03, -1.5465e-02,  2.6828e-02, -2.1619e-02,
         2.0250e-02,  1.3707e-02,  2.8519e-02,  2.7236e-02,  2.4270e-02,
         2.0691e-02,  1.7414e-02,  2.0925e-02,  1.9151e-02,  2.9487e-02,
         2.3190e-02,  2.2124e-02,  2.5259e-02,  2.0799e-02,  2.6257e-02,
         2.6430e-03,  3.9452e-04, -2.2523e-03,  1.1445e-02,  2.2009e-02,
         1.1264e-02,  8.9496e-03,  4.7023e-03,  2.7606e-02, -6.7408e-03,
         1.3425e-02, -2.1164e-02,  1.5099e-02,  3.6818e-03, -1.9535e-03,
        -1.1995e-02,  2.6292e-02,  1.1602e-02,  1.7271e-02, -2.0430e-02,
         1.5385e-02,  1.9289e-02,  1.3936e-02,  1.5553e-02,  3.0978e-02,
         2.6966e-02,  2.0957e-02,  1.4422e-02,  1.8762e-02,  2.3563e-02,
         2.0180e-02,  2.7850e-02,  3.0094e-02,  2.2851e-02,  3.0334e-02,
         1.5628e-02,  1.9303e-02,  2.6765e-02,  1.9012e-02,  2.4934e-02,
         2.2265e-02,  1.7215e-02,  3.0435e-02,  3.0987e-02,  2.2274e-02,
         2.2430e-02,  2.5428e-02,  3.1009e-02,  7.4894e-03,  1.9550e-02,
         2.7355e-02,  1.7516e-02,  2.8860e-02,  3.0453e-02,  1.9097e-02,
         2.1257e-02,  2.0326e-02,  1.8855e-02,  2.6776e-02,  1.7657e-02,
         3.0721e-02,  2.7562e-02,  2.1572e-02,  2.6644e-02,  3.0325e-02,
         1.3120e-02,  3.1018e-02,  1.9470e-02,  6.2948e-03,  2.2334e-02,
         1.8162e-02,  2.7092e-02,  2.0221e-02,  2.1655e-02,  1.7149e-02,
         1.5312e-02,  2.2306e-02,  2.7528e-02,  2.4620e-02,  1.5500e-02,
         2.7127e-02,  1.9819e-02,  2.2992e-02,  3.0870e-02,  2.0564e-02,
         3.1108e-02,  1.7616e-02,  3.1124e-02,  2.3488e-02,  2.5190e-02,
         3.1192e-02,  9.9461e-03,  1.8798e-02,  1.8366e-02,  2.1269e-02,
         3.0547e-02,  1.9347e-02,  1.7885e-02,  1.3895e-02,  2.8591e-02,
         2.4622e-02,  2.3198e-02,  2.5362e-02,  2.7612e-02,  2.3424e-02,
         2.5207e-02,  2.0237e-02,  2.0693e-02,  2.2609e-02,  1.9085e-02,
         2.5503e-02,  2.9960e-02,  2.3023e-02,  1.8738e-02,  2.8415e-02,
         2.0002e-02,  1.7004e-02,  2.1705e-02,  1.6822e-02,  1.8540e-02,
         2.8078e-02,  9.4577e-03,  2.7551e-02,  2.0283e-02,  2.7757e-02,
         2.3423e-02,  8.0976e-03,  2.7015e-02,  1.1246e-02,  2.8254e-02,
         3.1172e-02,  2.4838e-02,  2.2465e-02,  3.0820e-02,  2.1720e-02,
         2.2312e-02, -1.0029e-05,  2.4233e-02,  2.1722e-02,  2.4074e-02,
         3.1064e-02,  2.7315e-02,  2.7577e-02,  2.4902e-02,  1.3582e-02,
         3.1065e-02,  1.6634e-02,  2.2889e-02,  2.6223e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.8118, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:11:32,668 :: INFO :: Epoch 10: loss tensor(87.3410, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0463,  0.0425,  0.0588,  0.0500,  0.0353,  0.0419,  0.0484,  0.0453,
         0.0365,  0.0478,  0.0409,  0.0422,  0.0288,  0.0530,  0.0549,  0.0428,
         0.0431,  0.0478,  0.0411,  0.0518,  0.0392,  0.0467,  0.0551,  0.0446,
         0.0481,  0.0349,  0.0354,  0.0464,  0.0656,  0.0443,  0.0520,  0.0521,
         0.0471,  0.0516,  0.0365,  0.0291,  0.0431,  0.0406,  0.0359,  0.0529,
         0.0392,  0.0355,  0.0552,  0.0391,  0.0382,  0.0499,  0.0396,  0.0412,
         0.0490,  0.0398,  0.0556,  0.0427,  0.0372,  0.0333,  0.0587,  0.0556,
         0.0434,  0.0606,  0.0443,  0.0422,  0.0407,  0.0444,  0.0373,  0.0472,
         0.0489,  0.0317,  0.0546,  0.0451,  0.0525,  0.0396,  0.0484,  0.0413,
         0.0424,  0.0311,  0.0507,  0.0407,  0.0463,  0.0407,  0.0412,  0.0411,
         0.0505,  0.0473,  0.0502,  0.0505,  0.0311,  0.0557,  0.0499,  0.0518,
         0.0374,  0.0255,  0.0377,  0.0544,  0.0450,  0.0483,  0.0494,  0.0504,
         0.0502,  0.0528,  0.0429,  0.0418,  0.0380,  0.0329,  0.0480,  0.0242,
         0.0553,  0.0373,  0.0464,  0.0446,  0.0410,  0.0489,  0.0534,  0.0393,
         0.0601,  0.0497,  0.0395,  0.0359,  0.0485,  0.0448,  0.0296,  0.0488,
         0.0518,  0.0433,  0.0367,  0.0369,  0.0330,  0.0337,  0.0445,  0.0514,
         0.0491,  0.0498,  0.0512,  0.0424,  0.0539,  0.0467,  0.0412,  0.0236,
         0.0079,  0.0223,  0.0469,  0.0261,  0.0564,  0.0184,  0.0487,  0.0510,
         0.0550,  0.0539,  0.0359,  0.0410,  0.0557,  0.0509,  0.0102,  0.0571,
         0.0590, -0.0318,  0.0398,  0.0209,  0.0146, -0.0357,  0.0367, -0.0270,
         0.0541, -0.0380,  0.0432,  0.0481,  0.0504,  0.0247,  0.0026,  0.0286,
         0.0335,  0.0471,  0.0588,  0.0498, -0.0363,  0.0519,  0.0626,  0.0084,
         0.0229,  0.0519,  0.0407,  0.0316,  0.0466,  0.0263,  0.0345, -0.0327,
         0.0497,  0.0564, -0.0264,  0.0553,  0.0584, -0.0367,  0.0111,  0.0344,
         0.0520, -0.0217,  0.0431,  0.0521,  0.0536,  0.0172,  0.0606,  0.0468,
         0.0552,  0.0523,  0.0574,  0.0369,  0.0500,  0.0502,  0.0295, -0.0395,
         0.0431,  0.0324,  0.0611,  0.0475,  0.0267,  0.0114,  0.0545,  0.0571,
         0.0337,  0.0017,  0.0543, -0.0336,  0.0531,  0.0480,  0.0560,  0.0598,
         0.0481,  0.0532,  0.0450,  0.0552,  0.0418,  0.0627,  0.0518,  0.0370,
         0.0416,  0.0362,  0.0506,  0.0386,  0.0335,  0.0272,  0.0157,  0.0427,
         0.0427,  0.0428,  0.0398,  0.0553,  0.0206,  0.0450, -0.0300,  0.0481,
         0.0353,  0.0291,  0.0139,  0.0530,  0.0464,  0.0529, -0.0201,  0.0469,
         0.0574,  0.0498,  0.0536,  0.0665,  0.0618,  0.0568,  0.0518,  0.0557,
         0.0605,  0.0552,  0.0637,  0.0660,  0.0588,  0.0657,  0.0507,  0.0564,
         0.0626,  0.0552,  0.0609,  0.0584,  0.0520,  0.0667,  0.0659,  0.0585,
         0.0590,  0.0630,  0.0669,  0.0430,  0.0576,  0.0621,  0.0547,  0.0653,
         0.0642,  0.0562,  0.0574,  0.0572,  0.0576,  0.0616,  0.0535,  0.0644,
         0.0637,  0.0572,  0.0609,  0.0656,  0.0496,  0.0681,  0.0573,  0.0437,
         0.0579,  0.0554,  0.0619,  0.0553,  0.0579,  0.0539,  0.0508,  0.0597,
         0.0645,  0.0605,  0.0520,  0.0629,  0.0542,  0.0604,  0.0671,  0.0570,
         0.0668,  0.0552,  0.0669,  0.0597,  0.0617,  0.0658,  0.0478,  0.0558,
         0.0545,  0.0576,  0.0667,  0.0558,  0.0551,  0.0511,  0.0655,  0.0595,
         0.0572,  0.0619,  0.0633,  0.0588,  0.0617,  0.0569,  0.0574,  0.0581,
         0.0554,  0.0584,  0.0641,  0.0592,  0.0545,  0.0652,  0.0564,  0.0524,
         0.0586,  0.0507,  0.0551,  0.0643,  0.0470,  0.0623,  0.0556,  0.0642,
         0.0595,  0.0460,  0.0640,  0.0495,  0.0643,  0.0663,  0.0610,  0.0586,
         0.0660,  0.0585,  0.0593,  0.0362,  0.0600,  0.0572,  0.0614,  0.0674,
         0.0626,  0.0608,  0.0607,  0.0503,  0.0664,  0.0540,  0.0601,  0.0617],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.5655, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:11:38,184 :: INFO :: Epoch 15: loss tensor(85.1415, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0683,  0.0563,  0.0901,  0.0726,  0.0514,  0.0711,  0.0728,  0.0646,
         0.0550,  0.0661,  0.0697,  0.0626,  0.0455,  0.0779,  0.0767,  0.0553,
         0.0559,  0.0650,  0.0548,  0.0789,  0.0579,  0.0588,  0.0785,  0.0754,
         0.0717,  0.0475,  0.0458,  0.0722,  0.0973,  0.0585,  0.0751,  0.0686,
         0.0673,  0.0714,  0.0591,  0.0501,  0.0546,  0.0637,  0.0543,  0.0776,
         0.0610,  0.0571,  0.0771,  0.0551,  0.0534,  0.0652,  0.0560,  0.0657,
         0.0752,  0.0512,  0.0764,  0.0576,  0.0517,  0.0425,  0.0892,  0.0809,
         0.0637,  0.0937,  0.0722,  0.0682,  0.0484,  0.0647,  0.0601,  0.0674,
         0.0679,  0.0455,  0.0729,  0.0667,  0.0719,  0.0613,  0.0615,  0.0624,
         0.0526,  0.0536,  0.0695,  0.0608,  0.0654,  0.0673,  0.0531,  0.0586,
         0.0779,  0.0815,  0.0691,  0.0793,  0.0528,  0.0793,  0.0667,  0.0767,
         0.0524,  0.0397,  0.0603,  0.0819,  0.0633,  0.0607,  0.0783,  0.0720,
         0.0645,  0.0729,  0.0670,  0.0612,  0.0595,  0.0398,  0.0679,  0.0464,
         0.0870,  0.0659,  0.0653,  0.0593,  0.0571,  0.0703,  0.0849,  0.0549,
         0.0883,  0.0717,  0.0638,  0.0566,  0.0677,  0.0702,  0.0373,  0.0761,
         0.0755,  0.0650,  0.0578,  0.0510,  0.0510,  0.0525,  0.0710,  0.0760,
         0.0660,  0.0736,  0.0804,  0.0449,  0.0630,  0.0633,  0.0670,  0.0531,
         0.0415,  0.0253,  0.0729,  0.0574,  0.0833,  0.0486,  0.0779,  0.0865,
         0.0862,  0.0742,  0.0414,  0.0782,  0.0871,  0.0862,  0.0409,  0.0874,
         0.0846, -0.0409,  0.0680,  0.0571,  0.0435, -0.0412,  0.0402, -0.0187,
         0.0703, -0.0452,  0.0562,  0.0782,  0.0638,  0.0237,  0.0283,  0.0588,
         0.0665,  0.0722,  0.0850,  0.0667, -0.0412,  0.0799,  0.0924,  0.0401,
         0.0266,  0.0738,  0.0690,  0.0576,  0.0705,  0.0602,  0.0379, -0.0342,
         0.0686,  0.0835, -0.0241,  0.0820,  0.0862, -0.0455,  0.0459,  0.0651,
         0.0675, -0.0106,  0.0483,  0.0743,  0.0868,  0.0184,  0.0868,  0.0797,
         0.0836,  0.0861,  0.0806,  0.0425,  0.0782,  0.0851,  0.0598, -0.0497,
         0.0490,  0.0602,  0.0908,  0.0599,  0.0311,  0.0397,  0.0796,  0.0898,
         0.0625,  0.0285,  0.0765, -0.0379,  0.0817,  0.0769,  0.0775,  0.0895,
         0.0576,  0.0821,  0.0623,  0.0876,  0.0528,  0.0937,  0.0730,  0.0395,
         0.0491,  0.0406,  0.0650,  0.0751,  0.0655,  0.0563,  0.0185,  0.0553,
         0.0675,  0.0746,  0.0749,  0.0768,  0.0520,  0.0733, -0.0276,  0.0776,
         0.0644,  0.0610,  0.0488,  0.0761,  0.0801,  0.0876, -0.0012,  0.0728,
         0.0964,  0.0859,  0.0935,  0.0991,  0.0929,  0.0928,  0.0884,  0.0917,
         0.0979,  0.0877,  0.0984,  0.1009,  0.0919,  0.0974,  0.0817,  0.0927,
         0.0968,  0.0894,  0.0947,  0.0929,  0.0838,  0.1020,  0.0978,  0.0929,
         0.0943,  0.1018,  0.1012,  0.0809,  0.0970,  0.0919,  0.0912,  0.1010,
         0.0918,  0.0927,  0.0912,  0.0932,  0.0978,  0.0926,  0.0862,  0.0932,
         0.0982,  0.0905,  0.0901,  0.0981,  0.0872,  0.1060,  0.0962,  0.0828,
         0.0921,  0.0936,  0.0920,  0.0869,  0.0917,  0.0903,  0.0850,  0.0973,
         0.1018,  0.0949,  0.0869,  0.0964,  0.0837,  0.0989,  0.1022,  0.0939,
         0.1004,  0.0931,  0.1009,  0.0948,  0.0975,  0.0976,  0.0867,  0.0924,
         0.0881,  0.0921,  0.1015,  0.0906,  0.0918,  0.0874,  0.1029,  0.0904,
         0.0837,  0.0972,  0.0979,  0.0901,  0.0968,  0.0923,  0.0926,  0.0924,
         0.0895,  0.0830,  0.0921,  0.0949,  0.0872,  0.1015,  0.0908,  0.0858,
         0.0945,  0.0765,  0.0909,  0.0987,  0.0853,  0.0933,  0.0876,  0.0992,
         0.0940,  0.0860,  0.1007,  0.0894,  0.0979,  0.0985,  0.0959,  0.0938,
         0.0985,  0.0945,  0.0958,  0.0764,  0.0922,  0.0895,  0.1000,  0.1042,
         0.0940,  0.0865,  0.0937,  0.0879,  0.0997,  0.0908,  0.0973,  0.0939],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.9383, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:11:43,684 :: INFO :: Epoch 20: loss tensor(84.0069, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0795,  0.0590,  0.1178,  0.0857,  0.0586,  0.0922,  0.0856,  0.0765,
         0.0634,  0.0745,  0.0906,  0.0758,  0.0538,  0.0922,  0.0890,  0.0607,
         0.0588,  0.0715,  0.0577,  0.0992,  0.0678,  0.0614,  0.0939,  0.0982,
         0.0903,  0.0502,  0.0470,  0.0903,  0.1232,  0.0628,  0.0907,  0.0752,
         0.0772,  0.0803,  0.0702,  0.0599,  0.0559,  0.0800,  0.0630,  0.0921,
         0.0727,  0.0683,  0.0884,  0.0635,  0.0575,  0.0676,  0.0623,  0.0822,
         0.0924,  0.0539,  0.0847,  0.0603,  0.0590,  0.0421,  0.1126,  0.0986,
         0.0730,  0.1251,  0.0896,  0.0830,  0.0475,  0.0756,  0.0714,  0.0768,
         0.0755,  0.0494,  0.0807,  0.0781,  0.0831,  0.0717,  0.0645,  0.0726,
         0.0541,  0.0718,  0.0789,  0.0682,  0.0736,  0.0856,  0.0544,  0.0653,
         0.0991,  0.1097,  0.0788,  0.1013,  0.0611,  0.0911,  0.0739,  0.0907,
         0.0572,  0.0429,  0.0739,  0.1042,  0.0731,  0.0628,  0.0971,  0.0816,
         0.0687,  0.0802,  0.0818,  0.0704,  0.0688,  0.0410,  0.0772,  0.0610,
         0.1117,  0.0859,  0.0741,  0.0619,  0.0636,  0.0832,  0.1110,  0.0613,
         0.1083,  0.0833,  0.0775,  0.0672,  0.0752,  0.0887,  0.0375,  0.0959,
         0.0899,  0.0771,  0.0684,  0.0540,  0.0596,  0.0650,  0.0886,  0.0925,
         0.0728,  0.0884,  0.1033,  0.0407,  0.0602,  0.0696,  0.0840,  0.0763,
         0.0733,  0.0259,  0.0891,  0.0808,  0.0999,  0.0728,  0.0974,  0.1193,
         0.1107,  0.0852,  0.0401,  0.1118,  0.1120,  0.1152,  0.0678,  0.1099,
         0.1012, -0.0472,  0.0859,  0.0896,  0.0677, -0.0420,  0.0379, -0.0040,
         0.0751, -0.0459,  0.0618,  0.1007,  0.0682,  0.0226,  0.0493,  0.0820,
         0.0931,  0.0874,  0.1011,  0.0730, -0.0413,  0.1013,  0.1149,  0.0703,
         0.0240,  0.0856,  0.0883,  0.0764,  0.0861,  0.0889,  0.0364, -0.0308,
         0.0770,  0.1020, -0.0171,  0.0961,  0.1046, -0.0508,  0.0795,  0.0890,
         0.0742,  0.0050,  0.0456,  0.0872,  0.1133,  0.0165,  0.1029,  0.1060,
         0.1058,  0.1126,  0.0950,  0.0412,  0.0979,  0.1143,  0.0822, -0.0556,
         0.0470,  0.0800,  0.1105,  0.0630,  0.0281,  0.0622,  0.0936,  0.1172,
         0.0841,  0.0509,  0.0901, -0.0377,  0.1013,  0.0955,  0.0881,  0.1127,
         0.0586,  0.1018,  0.0679,  0.1143,  0.0513,  0.1173,  0.0833,  0.0371,
         0.0506,  0.0378,  0.0689,  0.1069,  0.0905,  0.0788,  0.0165,  0.0594,
         0.0821,  0.1004,  0.1056,  0.0898,  0.0786,  0.0957, -0.0192,  0.0993,
         0.0861,  0.0868,  0.0831,  0.0932,  0.1078,  0.1171,  0.0226,  0.0895,
         0.1343,  0.1196,  0.1326,  0.1254,  0.1201,  0.1275,  0.1208,  0.1233,
         0.1345,  0.1175,  0.1298,  0.1317,  0.1187,  0.1214,  0.1061,  0.1252,
         0.1287,  0.1193,  0.1226,  0.1233,  0.1133,  0.1338,  0.1242,  0.1214,
         0.1249,  0.1404,  0.1300,  0.1172,  0.1359,  0.1122,  0.1239,  0.1329,
         0.1093,  0.1259,  0.1210,  0.1253,  0.1379,  0.1162,  0.1117,  0.1153,
         0.1274,  0.1201,  0.1091,  0.1240,  0.1235,  0.1431,  0.1346,  0.1216,
         0.1243,  0.1305,  0.1130,  0.1145,  0.1196,  0.1243,  0.1174,  0.1341,
         0.1371,  0.1265,  0.1191,  0.1236,  0.1068,  0.1370,  0.1329,  0.1289,
         0.1288,  0.1304,  0.1299,  0.1256,  0.1309,  0.1232,  0.1242,  0.1271,
         0.1160,  0.1218,  0.1319,  0.1211,  0.1247,  0.1192,  0.1385,  0.1123,
         0.0986,  0.1285,  0.1311,  0.1126,  0.1270,  0.1227,  0.1232,  0.1238,
         0.1178,  0.0966,  0.1106,  0.1272,  0.1155,  0.1342,  0.1195,  0.1141,
         0.1267,  0.0912,  0.1237,  0.1281,  0.1224,  0.1206,  0.1127,  0.1300,
         0.1260,  0.1253,  0.1363,  0.1291,  0.1281,  0.1245,  0.1270,  0.1270,
         0.1253,  0.1263,  0.1286,  0.1157,  0.1164,  0.1149,  0.1377,  0.1400,
         0.1177,  0.1002,  0.1201,  0.1241,  0.1279,  0.1238,  0.1320,  0.1202],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.9106, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:11:49,201 :: INFO :: Epoch 25: loss tensor(82.7213, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0820,  0.0583,  0.1404,  0.0917,  0.0604,  0.1072,  0.0903,  0.0807,
         0.0683,  0.0761,  0.1042,  0.0814,  0.0578,  0.0985,  0.0942,  0.0619,
         0.0583,  0.0732,  0.0569,  0.1124,  0.0715,  0.0597,  0.1007,  0.1124,
         0.1006,  0.0494,  0.0443,  0.1002,  0.1425,  0.0618,  0.0988,  0.0756,
         0.0807,  0.0817,  0.0743,  0.0625,  0.0539,  0.0909,  0.0661,  0.0994,
         0.0783,  0.0735,  0.0944,  0.0654,  0.0578,  0.0660,  0.0627,  0.0929,
         0.1025,  0.0535,  0.0846,  0.0576,  0.0616,  0.0403,  0.1284,  0.1076,
         0.0766,  0.1518,  0.0993,  0.0914,  0.0458,  0.0801,  0.0761,  0.0784,
         0.0761,  0.0487,  0.0840,  0.0824,  0.0878,  0.0756,  0.0622,  0.0759,
         0.0505,  0.0854,  0.0808,  0.0679,  0.0748,  0.0962,  0.0517,  0.0667,
         0.1114,  0.1317,  0.0821,  0.1158,  0.0621,  0.0947,  0.0735,  0.0956,
         0.0577,  0.0432,  0.0808,  0.1200,  0.0753,  0.0610,  0.1076,  0.0843,
         0.0677,  0.0798,  0.0908,  0.0734,  0.0731,  0.0379,  0.0777,  0.0705,
         0.1290,  0.0991,  0.0754,  0.0591,  0.0649,  0.0892,  0.1311,  0.0625,
         0.1198,  0.0874,  0.0838,  0.0710,  0.0774,  0.1037,  0.0361,  0.1084,
         0.0953,  0.0808,  0.0733,  0.0549,  0.0615,  0.0723,  0.0985,  0.1017,
         0.0740,  0.0952,  0.1184,  0.0350,  0.0532,  0.0718,  0.0919,  0.0912,
         0.0987,  0.0250,  0.0963,  0.0955,  0.1053,  0.0897,  0.1063,  0.1474,
         0.1273,  0.0884,  0.0361,  0.1387,  0.1279,  0.1364,  0.0875,  0.1231,
         0.1077, -0.0524,  0.0935,  0.1160,  0.0848, -0.0409,  0.0360,  0.0103,
         0.0736, -0.0435,  0.0633,  0.1146,  0.0679,  0.0192,  0.0632,  0.0969,
         0.1119,  0.0935,  0.1078,  0.0720, -0.0393,  0.1149,  0.1293,  0.0939,
         0.0237,  0.0879,  0.0973,  0.0889,  0.0931,  0.1090,  0.0346, -0.0256,
         0.0772,  0.1113, -0.0096,  0.0983,  0.1139, -0.0540,  0.1061,  0.1041,
         0.0745,  0.0184,  0.0397,  0.0922,  0.1312,  0.0152,  0.1095,  0.1237,
         0.1200,  0.1304,  0.1007,  0.0359,  0.1066,  0.1357,  0.0959, -0.0590,
         0.0429,  0.0914,  0.1182,  0.0606,  0.0245,  0.0761,  0.0965,  0.1371,
         0.0967,  0.0670,  0.0948, -0.0355,  0.1120,  0.1035,  0.0912,  0.1289,
         0.0550,  0.1115,  0.0662,  0.1337,  0.0463,  0.1326,  0.0830,  0.0326,
         0.0491,  0.0347,  0.0663,  0.1308,  0.1068,  0.0939,  0.0154,  0.0604,
         0.0866,  0.1185,  0.1300,  0.0961,  0.0978,  0.1113, -0.0098,  0.1110,
         0.0995,  0.1057,  0.1089,  0.1034,  0.1278,  0.1394,  0.0437,  0.0957,
         0.1698,  0.1505,  0.1691,  0.1449,  0.1443,  0.1589,  0.1487,  0.1501,
         0.1690,  0.1452,  0.1590,  0.1585,  0.1390,  0.1378,  0.1242,  0.1535,
         0.1581,  0.1460,  0.1446,  0.1499,  0.1411,  0.1622,  0.1450,  0.1439,
         0.1505,  0.1765,  0.1530,  0.1514,  0.1728,  0.1235,  0.1524,  0.1611,
         0.1186,  0.1555,  0.1485,  0.1528,  0.1760,  0.1335,  0.1308,  0.1325,
         0.1512,  0.1471,  0.1178,  0.1440,  0.1575,  0.1775,  0.1711,  0.1577,
         0.1547,  0.1653,  0.1247,  0.1397,  0.1418,  0.1547,  0.1475,  0.1691,
         0.1706,  0.1557,  0.1487,  0.1446,  0.1253,  0.1728,  0.1590,  0.1616,
         0.1517,  0.1659,  0.1536,  0.1525,  0.1622,  0.1432,  0.1596,  0.1598,
         0.1387,  0.1462,  0.1574,  0.1475,  0.1538,  0.1461,  0.1717,  0.1248,
         0.1052,  0.1552,  0.1619,  0.1260,  0.1522,  0.1482,  0.1491,  0.1515,
         0.1412,  0.1039,  0.1225,  0.1563,  0.1409,  0.1633,  0.1429,  0.1386,
         0.1544,  0.0981,  0.1536,  0.1520,  0.1573,  0.1454,  0.1316,  0.1562,
         0.1555,  0.1629,  0.1690,  0.1667,  0.1555,  0.1448,  0.1541,  0.1575,
         0.1468,  0.1538,  0.1576,  0.1526,  0.1324,  0.1333,  0.1736,  0.1731,
         0.1337,  0.1031,  0.1392,  0.1590,  0.1503,  0.1530,  0.1636,  0.1400],
       device='cuda:0', requires_grad=True) MLP.norm tensor(12.5443, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:11:54,701 :: INFO :: Epoch 30: loss tensor(81.0970, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 8.1467e-02,  5.7159e-02,  1.5887e-01,  9.1907e-02,  5.9753e-02,
         1.1637e-01,  9.0545e-02,  8.0328e-02,  7.1856e-02,  7.4485e-02,
         1.1069e-01,  8.2721e-02,  5.9924e-02,  9.9797e-02,  9.4796e-02,
         6.2475e-02,  5.6167e-02,  7.2266e-02,  5.5186e-02,  1.2071e-01,
         7.1466e-02,  5.6533e-02,  1.0279e-01,  1.2049e-01,  1.0709e-01,
         4.7490e-02,  4.0847e-02,  1.0416e-01,  1.5642e-01,  5.9672e-02,
         1.0327e-01,  7.3204e-02,  8.2155e-02,  8.0731e-02,  7.4735e-02,
         6.2369e-02,  5.0887e-02,  9.7661e-02,  6.6191e-02,  1.0190e-01,
         8.1947e-02,  7.4826e-02,  9.6538e-02,  6.5111e-02,  5.6809e-02,
         6.1454e-02,  6.1691e-02,  1.0038e-01,  1.0565e-01,  5.1802e-02,
         7.9705e-02,  5.3661e-02,  6.2254e-02,  3.8224e-02,  1.3738e-01,
         1.1088e-01,  7.6182e-02,  1.7578e-01,  1.0253e-01,  9.3468e-02,
         4.3046e-02,  8.0761e-02,  7.6892e-02,  7.7558e-02,  7.4457e-02,
         4.7004e-02,  8.2365e-02,  8.2587e-02,  8.8892e-02,  7.5495e-02,
         6.0376e-02,  7.5997e-02,  4.6412e-02,  9.6337e-02,  7.9921e-02,
         6.5620e-02,  7.3531e-02,  1.0247e-01,  4.7147e-02,  6.4855e-02,
         1.1904e-01,  1.4777e-01,  8.2094e-02,  1.2576e-01,  6.0542e-02,
         9.3756e-02,  7.0081e-02,  9.6522e-02,  5.6625e-02,  4.2381e-02,
         8.2275e-02,  1.3014e-01,  7.5729e-02,  5.8002e-02,  1.1092e-01,
         8.3822e-02,  6.4876e-02,  7.6571e-02,  9.6339e-02,  7.3736e-02,
         7.2336e-02,  3.5026e-02,  7.6260e-02,  7.7343e-02,  1.4088e-01,
         1.0504e-01,  7.3574e-02,  5.4779e-02,  6.4693e-02,  9.1065e-02,
         1.4566e-01,  6.2775e-02,  1.2484e-01,  8.9513e-02,  8.4966e-02,
         7.0346e-02,  7.7171e-02,  1.1651e-01,  3.4510e-02,  1.1660e-01,
         9.4924e-02,  8.1061e-02,  7.4761e-02,  5.4367e-02,  5.9851e-02,
         7.6668e-02,  1.0405e-01,  1.0577e-01,  7.2733e-02,  9.7134e-02,
         1.2658e-01,  2.9310e-02,  4.8156e-02,  7.0979e-02,  9.3528e-02,
         9.9750e-02,  1.1960e-01,  2.6558e-02,  9.7638e-02,  1.0262e-01,
         1.0229e-01,  1.0097e-01,  1.0629e-01,  1.7130e-01,  1.3640e-01,
         8.5800e-02,  3.2352e-02,  1.6044e-01,  1.3581e-01,  1.4970e-01,
         1.0174e-01,  1.2886e-01,  1.0594e-01, -5.6238e-02,  9.3765e-02,
         1.3769e-01,  9.5603e-02, -3.8478e-02,  3.3553e-02,  2.2762e-02,
         6.9310e-02, -3.9657e-02,  6.3589e-02,  1.2131e-01,  6.6092e-02,
         1.6905e-02,  7.2031e-02,  1.0500e-01,  1.2406e-01,  9.3382e-02,
         1.0826e-01,  6.8101e-02, -3.6135e-02,  1.2133e-01,  1.3633e-01,
         1.1324e-01,  2.0637e-02,  8.4308e-02,  9.8686e-02,  9.7071e-02,
         9.3102e-02,  1.2354e-01,  2.8932e-02, -1.9921e-02,  7.2431e-02,
         1.1438e-01, -2.3950e-03,  9.4398e-02,  1.1633e-01, -5.5755e-02,
         1.2805e-01,  1.1177e-01,  7.0974e-02,  2.9458e-02,  3.6350e-02,
         9.2580e-02,  1.4168e-01,  1.3849e-02,  1.0893e-01,  1.3371e-01,
         1.2716e-01,  1.3935e-01,  1.0072e-01,  3.1016e-02,  1.0656e-01,
         1.4972e-01,  1.0238e-01, -6.0875e-02,  3.8444e-02,  9.6652e-02,
         1.1618e-01,  5.5679e-02,  2.0113e-02,  8.3846e-02,  9.2122e-02,
         1.5007e-01,  1.0202e-01,  7.7912e-02,  9.2383e-02, -3.2151e-02,
         1.1572e-01,  1.0383e-01,  9.0791e-02,  1.3944e-01,  5.0104e-02,
         1.1343e-01,  6.1660e-02,  1.4727e-01,  4.1046e-02,  1.3979e-01,
         7.6779e-02,  2.8252e-02,  4.8162e-02,  3.0546e-02,  6.0953e-02,
         1.4798e-01,  1.1575e-01,  1.0293e-01,  1.4508e-02,  5.7561e-02,
         8.4357e-02,  1.2973e-01,  1.4907e-01,  9.9083e-02,  1.1177e-01,
         1.2140e-01, -7.6279e-07,  1.1447e-01,  1.0660e-01,  1.1910e-01,
         1.2937e-01,  1.0857e-01,  1.4054e-01,  1.5494e-01,  6.1183e-02,
         9.4807e-02,  2.0183e-01,  1.7833e-01,  2.0306e-01,  1.5753e-01,
         1.6506e-01,  1.8613e-01,  1.7186e-01,  1.7176e-01,  2.0094e-01,
         1.6875e-01,  1.8501e-01,  1.8157e-01,  1.5354e-01,  1.4670e-01,
         1.3800e-01,  1.7762e-01,  1.8403e-01,  1.6899e-01,  1.5967e-01,
         1.7224e-01,  1.6473e-01,  1.8614e-01,  1.6122e-01,  1.5977e-01,
         1.7098e-01,  2.0883e-01,  1.7017e-01,  1.8303e-01,  2.0700e-01,
         1.2673e-01,  1.7610e-01,  1.8436e-01,  1.2292e-01,  1.8086e-01,
         1.7273e-01,  1.7565e-01,  2.1145e-01,  1.4410e-01,  1.4323e-01,
         1.4542e-01,  1.6892e-01,  1.7113e-01,  1.1860e-01,  1.5752e-01,
         1.8938e-01,  2.0897e-01,  2.0538e-01,  1.9093e-01,  1.8151e-01,
         1.9743e-01,  1.2760e-01,  1.6293e-01,  1.5784e-01,  1.8113e-01,
         1.7490e-01,  2.0083e-01,  2.0102e-01,  1.8111e-01,  1.7505e-01,
         1.5867e-01,  1.3917e-01,  2.0571e-01,  1.7981e-01,  1.9220e-01,
         1.6837e-01,  1.9827e-01,  1.7150e-01,  1.7532e-01,  1.9006e-01,
         1.5759e-01,  1.9262e-01,  1.8946e-01,  1.5665e-01,  1.6565e-01,
         1.7769e-01,  1.6962e-01,  1.7856e-01,  1.6761e-01,  2.0163e-01,
         1.2816e-01,  1.0545e-01,  1.7701e-01,  1.8886e-01,  1.2978e-01,
         1.7196e-01,  1.6839e-01,  1.6935e-01,  1.7449e-01,  1.5881e-01,
         1.0791e-01,  1.2954e-01,  1.8267e-01,  1.6234e-01,  1.8806e-01,
         1.6009e-01,  1.5939e-01,  1.7732e-01,  1.0083e-01,  1.7914e-01,
         1.6999e-01,  1.8976e-01,  1.6699e-01,  1.4400e-01,  1.7743e-01,
         1.8258e-01,  1.9786e-01,  1.9821e-01,  2.0092e-01,  1.7916e-01,
         1.5837e-01,  1.7706e-01,  1.8609e-01,  1.6305e-01,  1.7650e-01,
         1.8197e-01,  1.8667e-01,  1.3918e-01,  1.4536e-01,  2.0592e-01,
         2.0299e-01,  1.4188e-01,  1.0132e-01,  1.5026e-01,  1.9168e-01,
         1.6682e-01,  1.7796e-01,  1.9195e-01,  1.5522e-01], device='cuda:0',
       requires_grad=True) MLP.norm tensor(13.8918, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:12:00,185 :: INFO :: Epoch 35: loss tensor(82.2476, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0786,  0.0574,  0.1741,  0.0898,  0.0582,  0.1214,  0.0905,  0.0785,
         0.0754,  0.0726,  0.1147,  0.0826,  0.0611,  0.1002,  0.0947,  0.0623,
         0.0553,  0.0725,  0.0538,  0.1267,  0.0711,  0.0544,  0.1023,  0.1242,
         0.1124,  0.0466,  0.0389,  0.1059,  0.1660,  0.0577,  0.1064,  0.0710,
         0.0829,  0.0791,  0.0749,  0.0627,  0.0489,  0.1009,  0.0653,  0.1032,
         0.0840,  0.0754,  0.0958,  0.0636,  0.0553,  0.0584,  0.0606,  0.1054,
         0.1078,  0.0523,  0.0758,  0.0509,  0.0627,  0.0366,  0.1426,  0.1127,
         0.0757,  0.1979,  0.1036,  0.0936,  0.0419,  0.0805,  0.0772,  0.0767,
         0.0730,  0.0446,  0.0807,  0.0816,  0.0877,  0.0744,  0.0592,  0.0749,
         0.0443,  0.1047,  0.0769,  0.0644,  0.0728,  0.1069,  0.0448,  0.0637,
         0.1222,  0.1592,  0.0803,  0.1338,  0.0592,  0.0924,  0.0667,  0.0945,
         0.0557,  0.0418,  0.0832,  0.1360,  0.0764,  0.0553,  0.1117,  0.0831,
         0.0629,  0.0749,  0.0989,  0.0754,  0.0705,  0.0329,  0.0724,  0.0818,
         0.1485,  0.1100,  0.0705,  0.0516,  0.0637,  0.0890,  0.1579,  0.0634,
         0.1267,  0.0910,  0.0844,  0.0684,  0.0769,  0.1275,  0.0344,  0.1206,
         0.0934,  0.0792,  0.0750,  0.0556,  0.0580,  0.0809,  0.1059,  0.1068,
         0.0699,  0.0948,  0.1282,  0.0247,  0.0424,  0.0672,  0.0907,  0.1023,
         0.1332,  0.0245,  0.0949,  0.1031,  0.0946,  0.1058,  0.1000,  0.1899,
         0.1386,  0.0801,  0.0273,  0.1750,  0.1358,  0.1548,  0.1089,  0.1296,
         0.0990, -0.0595,  0.0887,  0.1532,  0.1000, -0.0364,  0.0299,  0.0320,
         0.0631, -0.0353,  0.0612,  0.1207,  0.0628,  0.0154,  0.0747,  0.1072,
         0.1289,  0.0893,  0.1037,  0.0631, -0.0333,  0.1221,  0.1372,  0.1256,
         0.0188,  0.0775,  0.0940,  0.1011,  0.0893,  0.1303,  0.0237, -0.0156,
         0.0653,  0.1126,  0.0028,  0.0854,  0.1139, -0.0571,  0.1425,  0.1124,
         0.0661,  0.0363,  0.0327,  0.0890,  0.1441,  0.0123,  0.1037,  0.1359,
         0.1283,  0.1396,  0.0967,  0.0263,  0.1002,  0.1560,  0.1024, -0.0621,
         0.0334,  0.0962,  0.1078,  0.0495,  0.0166,  0.0848,  0.0837,  0.1568,
         0.1007,  0.0826,  0.0865, -0.0292,  0.1135,  0.0993,  0.0866,  0.1446,
         0.0446,  0.1098,  0.0557,  0.1544,  0.0353,  0.1397,  0.0678,  0.0249,
         0.0459,  0.0258,  0.0551,  0.1570,  0.1172,  0.1059,  0.0136,  0.0519,
         0.0778,  0.1341,  0.1617,  0.0980,  0.1190,  0.1261,  0.0080,  0.1114,
         0.1075,  0.1255,  0.1419,  0.1101,  0.1462,  0.1634,  0.0727,  0.0891,
         0.2308,  0.2026,  0.2336,  0.1644,  0.1824,  0.2089,  0.1907,  0.1890,
         0.2306,  0.1888,  0.2079,  0.2011,  0.1628,  0.1481,  0.1468,  0.1982,
         0.2068,  0.1881,  0.1687,  0.1908,  0.1836,  0.2066,  0.1724,  0.1706,
         0.1864,  0.2362,  0.1818,  0.2123,  0.2376,  0.1225,  0.1956,  0.2035,
         0.1230,  0.2027,  0.1944,  0.1939,  0.2435,  0.1509,  0.1502,  0.1560,
         0.1812,  0.1928,  0.1128,  0.1657,  0.2182,  0.2365,  0.2369,  0.2204,
         0.2051,  0.2264,  0.1234,  0.1831,  0.1677,  0.2038,  0.1997,  0.2290,
         0.2285,  0.2041,  0.1991,  0.1668,  0.1498,  0.2349,  0.1960,  0.2195,
         0.1796,  0.2284,  0.1836,  0.1946,  0.2152,  0.1673,  0.2222,  0.2165,
         0.1687,  0.1801,  0.1928,  0.1881,  0.1997,  0.1840,  0.2285,  0.1248,
         0.1035,  0.1944,  0.2121,  0.1269,  0.1869,  0.1838,  0.1851,  0.1930,
         0.1724,  0.1126,  0.1340,  0.2060,  0.1801,  0.2090,  0.1717,  0.1770,
         0.1957,  0.1004,  0.2002,  0.1824,  0.2184,  0.1847,  0.1509,  0.1945,
         0.2060,  0.2296,  0.2222,  0.2322,  0.1996,  0.1663,  0.1962,  0.2112,
         0.1736,  0.1950,  0.2023,  0.2174,  0.1397,  0.1527,  0.2350,  0.2287,
         0.1428,  0.0947,  0.1543,  0.2215,  0.1780,  0.1990,  0.2170,  0.1647],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.9902, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:12:05,577 :: INFO :: Epoch 40: loss tensor(78.6838, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0751,  0.0572,  0.1867,  0.0881,  0.0552,  0.1238,  0.0903,  0.0750,
         0.0801,  0.0705,  0.1172,  0.0817,  0.0607,  0.1002,  0.0950,  0.0612,
         0.0539,  0.0736,  0.0525,  0.1303,  0.0694,  0.0534,  0.1010,  0.1267,
         0.1149,  0.0448,  0.0360,  0.1062,  0.1738,  0.0555,  0.1076,  0.0704,
         0.0847,  0.0775,  0.0764,  0.0622,  0.0465,  0.1008,  0.0626,  0.1040,
         0.0844,  0.0758,  0.0947,  0.0603,  0.0530,  0.0556,  0.0598,  0.1095,
         0.1092,  0.0516,  0.0738,  0.0470,  0.0621,  0.0353,  0.1455,  0.1126,
         0.0741,  0.2184,  0.1042,  0.0939,  0.0409,  0.0786,  0.0786,  0.0753,
         0.0726,  0.0422,  0.0812,  0.0819,  0.0847,  0.0726,  0.0595,  0.0762,
         0.0421,  0.1104,  0.0731,  0.0631,  0.0716,  0.1100,  0.0426,  0.0627,
         0.1239,  0.1675,  0.0772,  0.1386,  0.0594,  0.0911,  0.0638,  0.0935,
         0.0539,  0.0418,  0.0832,  0.1410,  0.0740,  0.0541,  0.1101,  0.0831,
         0.0601,  0.0735,  0.0995,  0.0759,  0.0697,  0.0311,  0.0676,  0.0836,
         0.1537,  0.1122,  0.0671,  0.0478,  0.0617,  0.0867,  0.1667,  0.0625,
         0.1282,  0.0925,  0.0831,  0.0656,  0.0769,  0.1362,  0.0354,  0.1239,
         0.0910,  0.0773,  0.0731,  0.0574,  0.0543,  0.0832,  0.1050,  0.1072,
         0.0661,  0.0899,  0.1256,  0.0214,  0.0380,  0.0626,  0.0848,  0.1004,
         0.1421,  0.0250,  0.0894,  0.0999,  0.0855,  0.1062,  0.0914,  0.2032,
         0.1363,  0.0726,  0.0245,  0.1848,  0.1318,  0.1552,  0.1113,  0.1262,
         0.0900, -0.0624,  0.0810,  0.1644,  0.1008, -0.0347,  0.0265,  0.0367,
         0.0558, -0.0316,  0.0592,  0.1163,  0.0586,  0.0142,  0.0731,  0.1052,
         0.1291,  0.0831,  0.0990,  0.0576, -0.0315,  0.1190,  0.1351,  0.1333,
         0.0160,  0.0693,  0.0862,  0.1009,  0.0833,  0.1327,  0.0204, -0.0128,
         0.0571,  0.1085,  0.0062,  0.0773,  0.1107, -0.0584,  0.1502,  0.1086,
         0.0597,  0.0399,  0.0295,  0.0841,  0.1411,  0.0099,  0.0968,  0.1329,
         0.1254,  0.1345,  0.0907,  0.0220,  0.0911,  0.1559,  0.0988, -0.0634,
         0.0276,  0.0920,  0.0966,  0.0432,  0.0136,  0.0818,  0.0750,  0.1594,
         0.0950,  0.0827,  0.0786, -0.0268,  0.1067,  0.0934,  0.0804,  0.1446,
         0.0409,  0.1031,  0.0491,  0.1574,  0.0309,  0.1353,  0.0585,  0.0215,
         0.0411,  0.0240,  0.0486,  0.1602,  0.1144,  0.1046,  0.0120,  0.0484,
         0.0701,  0.1335,  0.1695,  0.0957,  0.1219,  0.1266,  0.0142,  0.1052,
         0.1044,  0.1277,  0.1493,  0.1092,  0.1462,  0.1669,  0.0804,  0.0811,
         0.2570,  0.2228,  0.2609,  0.1687,  0.1974,  0.2273,  0.2068,  0.2028,
         0.2561,  0.2041,  0.2283,  0.2179,  0.1686,  0.1474,  0.1531,  0.2161,
         0.2244,  0.2039,  0.1734,  0.2058,  0.1983,  0.2250,  0.1809,  0.1772,
         0.1989,  0.2585,  0.1896,  0.2384,  0.2651,  0.1155,  0.2118,  0.2192,
         0.1228,  0.2217,  0.2136,  0.2091,  0.2719,  0.1552,  0.1530,  0.1655,
         0.1886,  0.2107,  0.1069,  0.1711,  0.2447,  0.2597,  0.2642,  0.2453,
         0.2258,  0.2514,  0.1193,  0.2002,  0.1735,  0.2225,  0.2201,  0.2534,
         0.2537,  0.2235,  0.2194,  0.1708,  0.1576,  0.2600,  0.2089,  0.2438,
         0.1874,  0.2539,  0.1925,  0.2104,  0.2369,  0.1735,  0.2487,  0.2413,
         0.1796,  0.1919,  0.2049,  0.2036,  0.2179,  0.1967,  0.2535,  0.1183,
         0.1029,  0.2083,  0.2297,  0.1195,  0.1975,  0.1954,  0.1976,  0.2075,
         0.1824,  0.1218,  0.1375,  0.2262,  0.1951,  0.2269,  0.1797,  0.1921,
         0.2105,  0.1010,  0.2179,  0.1904,  0.2435,  0.1986,  0.1570,  0.2081,
         0.2259,  0.2580,  0.2408,  0.2599,  0.2158,  0.1712,  0.2124,  0.2335,
         0.1827,  0.2103,  0.2194,  0.2450,  0.1363,  0.1576,  0.2602,  0.2500,
         0.1407,  0.0916,  0.1533,  0.2481,  0.1860,  0.2170,  0.2393,  0.1712],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.9044, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:12:05,577 :: INFO :: ----- frontend -----
2023-05-14 12:12:05,577 :: INFO :: Environment 0
2023-05-14 12:12:14,843 :: INFO :: Epoch 5: loss tensor(881.4721, device='cuda:0'), U.norm 13.15874195098877, V.norm 17.00807762145996, MLP.norm 2.157327175140381
2023-05-14 12:12:15,062 :: INFO :: Epoch 10: loss tensor(854.0940, device='cuda:0'), U.norm 10.350268363952637, V.norm 16.361806869506836, MLP.norm 3.687227725982666
2023-05-14 12:12:15,296 :: INFO :: Epoch 15: loss tensor(816.1046, device='cuda:0'), U.norm 8.652461051940918, V.norm 16.047555923461914, MLP.norm 5.484522819519043
2023-05-14 12:12:15,515 :: INFO :: Epoch 20: loss tensor(774.3674, device='cuda:0'), U.norm 7.496087551116943, V.norm 15.83730411529541, MLP.norm 7.369174480438232
2023-05-14 12:12:15,531 :: INFO :: Environment 1
2023-05-14 12:12:24,688 :: INFO :: Epoch 5: loss tensor(861.6689, device='cuda:0'), U.norm 13.150662422180176, V.norm 16.98006248474121, MLP.norm 2.1826107501983643
2023-05-14 12:12:24,906 :: INFO :: Epoch 10: loss tensor(833.0711, device='cuda:0'), U.norm 10.334940910339355, V.norm 16.3210391998291, MLP.norm 3.7296195030212402
2023-05-14 12:12:25,141 :: INFO :: Epoch 15: loss tensor(795.8115, device='cuda:0'), U.norm 8.628238677978516, V.norm 15.999728202819824, MLP.norm 5.540801525115967
2023-05-14 12:12:25,360 :: INFO :: Epoch 20: loss tensor(756.0594, device='cuda:0'), U.norm 7.460814952850342, V.norm 15.783926963806152, MLP.norm 7.39212703704834
2023-05-14 12:12:25,375 :: INFO :: Environment 2
2023-05-14 12:12:34,548 :: INFO :: Epoch 5: loss tensor(853.4083, device='cuda:0'), U.norm 13.157573699951172, V.norm 16.963350296020508, MLP.norm 2.150998830795288
2023-05-14 12:12:34,766 :: INFO :: Epoch 10: loss tensor(827.2849, device='cuda:0'), U.norm 10.347888946533203, V.norm 16.300973892211914, MLP.norm 3.6608660221099854
2023-05-14 12:12:34,985 :: INFO :: Epoch 15: loss tensor(790.5398, device='cuda:0'), U.norm 8.648810386657715, V.norm 15.981082916259766, MLP.norm 5.475324630737305
2023-05-14 12:12:35,220 :: INFO :: Epoch 20: loss tensor(751.0878, device='cuda:0'), U.norm 7.490684986114502, V.norm 15.768373489379883, MLP.norm 7.3728227615356445
2023-05-14 12:12:35,235 :: INFO :: Environment 3
2023-05-14 12:12:44,392 :: INFO :: Epoch 5: loss tensor(822.5018, device='cuda:0'), U.norm 13.154585838317871, V.norm 16.9035587310791, MLP.norm 2.1419460773468018
2023-05-14 12:12:44,611 :: INFO :: Epoch 10: loss tensor(798.8077, device='cuda:0'), U.norm 10.34073543548584, V.norm 16.214548110961914, MLP.norm 3.588602304458618
2023-05-14 12:12:44,845 :: INFO :: Epoch 15: loss tensor(765.9825, device='cuda:0'), U.norm 8.63633918762207, V.norm 15.881271362304688, MLP.norm 5.3015971183776855
2023-05-14 12:12:45,064 :: INFO :: Epoch 20: loss tensor(728.9960, device='cuda:0'), U.norm 7.472156524658203, V.norm 15.657316207885742, MLP.norm 7.094146728515625
2023-05-14 12:12:45,080 :: INFO :: Environment 4
2023-05-14 12:12:54,268 :: INFO :: Epoch 5: loss tensor(794.5137, device='cuda:0'), U.norm 13.150727272033691, V.norm 16.848163604736328, MLP.norm 2.113906145095825
2023-05-14 12:12:54,487 :: INFO :: Epoch 10: loss tensor(772.5640, device='cuda:0'), U.norm 10.332527160644531, V.norm 16.13765525817871, MLP.norm 3.522254467010498
2023-05-14 12:12:54,706 :: INFO :: Epoch 15: loss tensor(740.5620, device='cuda:0'), U.norm 8.62260627746582, V.norm 15.799271583557129, MLP.norm 5.196192741394043
2023-05-14 12:12:54,925 :: INFO :: Epoch 20: loss tensor(706.0726, device='cuda:0'), U.norm 7.451902389526367, V.norm 15.5747709274292, MLP.norm 6.946295261383057
2023-05-14 12:12:54,940 :: INFO :: Environment 5
2023-05-14 12:13:04,113 :: INFO :: Epoch 5: loss tensor(828.8560, device='cuda:0'), U.norm 13.157404899597168, V.norm 16.902793884277344, MLP.norm 2.1495213508605957
2023-05-14 12:13:04,347 :: INFO :: Epoch 10: loss tensor(802.7192, device='cuda:0'), U.norm 10.346981048583984, V.norm 16.208951950073242, MLP.norm 3.6520769596099854
2023-05-14 12:13:04,581 :: INFO :: Epoch 15: loss tensor(766.7315, device='cuda:0'), U.norm 8.646684646606445, V.norm 15.870580673217773, MLP.norm 5.425486087799072
2023-05-14 12:13:04,800 :: INFO :: Epoch 20: loss tensor(727.1312, device='cuda:0'), U.norm 7.487207412719727, V.norm 15.644513130187988, MLP.norm 7.256050109863281
2023-05-14 12:13:04,815 :: INFO :: Environment 6
2023-05-14 12:13:14,176 :: INFO :: Epoch 5: loss tensor(890.0808, device='cuda:0'), U.norm 13.159114837646484, V.norm 17.02518653869629, MLP.norm 2.1913552284240723
2023-05-14 12:13:14,395 :: INFO :: Epoch 10: loss tensor(860.5853, device='cuda:0'), U.norm 10.351750373840332, V.norm 16.3865909576416, MLP.norm 3.7444846630096436
2023-05-14 12:13:14,629 :: INFO :: Epoch 15: loss tensor(820.7631, device='cuda:0'), U.norm 8.65546989440918, V.norm 16.07668685913086, MLP.norm 5.522173881530762
2023-05-14 12:13:14,864 :: INFO :: Epoch 20: loss tensor(778.1060, device='cuda:0'), U.norm 7.50040864944458, V.norm 15.868854522705078, MLP.norm 7.3918914794921875
2023-05-14 12:13:14,879 :: INFO :: Environment 7
2023-05-14 12:13:24,021 :: INFO :: Epoch 5: loss tensor(875.6406, device='cuda:0'), U.norm 13.160367012023926, V.norm 17.006669998168945, MLP.norm 2.193547248840332
2023-05-14 12:13:24,255 :: INFO :: Epoch 10: loss tensor(845.6903, device='cuda:0'), U.norm 10.353033065795898, V.norm 16.359783172607422, MLP.norm 3.7455761432647705
2023-05-14 12:13:24,489 :: INFO :: Epoch 15: loss tensor(806.3422, device='cuda:0'), U.norm 8.656027793884277, V.norm 16.043596267700195, MLP.norm 5.527278423309326
2023-05-14 12:13:24,724 :: INFO :: Epoch 20: loss tensor(763.5040, device='cuda:0'), U.norm 7.499754905700684, V.norm 15.830223083496094, MLP.norm 7.364436149597168
2023-05-14 12:13:24,724 :: INFO :: Environment 8
2023-05-14 12:13:34,052 :: INFO :: Epoch 5: loss tensor(858.1889, device='cuda:0'), U.norm 13.160173416137695, V.norm 16.96763038635254, MLP.norm 2.164609909057617
2023-05-14 12:13:34,287 :: INFO :: Epoch 10: loss tensor(831.1993, device='cuda:0'), U.norm 10.352840423583984, V.norm 16.30337142944336, MLP.norm 3.686633586883545
2023-05-14 12:13:34,506 :: INFO :: Epoch 15: loss tensor(795.1243, device='cuda:0'), U.norm 8.655997276306152, V.norm 15.979120254516602, MLP.norm 5.452118396759033
2023-05-14 12:13:34,740 :: INFO :: Epoch 20: loss tensor(754.4544, device='cuda:0'), U.norm 7.500272750854492, V.norm 15.760210990905762, MLP.norm 7.270869731903076
2023-05-14 12:13:34,740 :: INFO :: Environment 9
2023-05-14 12:13:44,100 :: INFO :: Epoch 5: loss tensor(804.3691, device='cuda:0'), U.norm 13.155034065246582, V.norm 16.860334396362305, MLP.norm 2.1395583152770996
2023-05-14 12:13:44,334 :: INFO :: Epoch 10: loss tensor(781.7766, device='cuda:0'), U.norm 10.340930938720703, V.norm 16.151660919189453, MLP.norm 3.5726919174194336
2023-05-14 12:13:44,553 :: INFO :: Epoch 15: loss tensor(750.0955, device='cuda:0'), U.norm 8.636029243469238, V.norm 15.811389923095703, MLP.norm 5.216920852661133
2023-05-14 12:13:44,788 :: INFO :: Epoch 20: loss tensor(715.3887, device='cuda:0'), U.norm 7.471365451812744, V.norm 15.585046768188477, MLP.norm 6.939393997192383
2023-05-14 12:13:44,803 :: INFO :: Ite = 1, Delta = 4028
2023-05-14 12:13:44,803 :: INFO :: ----- backend -----
2023-05-14 12:13:49,335 :: INFO :: Epoch 5: loss tensor(156.4784, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 7.6484e-03,  1.0527e-02,  8.2967e-03,  1.2257e-02,  9.7045e-03,
         1.7838e-03,  6.3731e-03,  1.5496e-02,  2.4631e-03,  1.6420e-02,
         4.1987e-03,  9.9798e-03,  7.3642e-03,  1.3441e-02,  1.8863e-02,
         1.5247e-02,  1.4624e-02,  1.5803e-02,  1.3156e-02,  1.0897e-02,
         6.0600e-03,  1.8708e-02,  1.6234e-02, -3.3425e-03,  1.2249e-02,
         8.5185e-03,  8.4535e-03,  5.7401e-03,  1.7398e-02,  1.1725e-02,
         1.4437e-02,  2.3889e-02,  1.0893e-02,  1.2668e-02, -5.1438e-03,
        -5.7074e-03,  1.6344e-02,  1.9701e-02,  4.4528e-03,  1.3440e-02,
         9.2658e-04,  3.0945e-04,  2.0163e-02,  1.1421e-02,  1.0756e-02,
         1.7877e-02,  1.2008e-02,  3.0218e-03,  1.2643e-02,  1.2096e-02,
         1.6154e-02,  8.6551e-03,  7.8233e-03,  9.9856e-03,  7.2128e-03,
         1.7719e-02,  8.3455e-03,  1.4033e-02,  4.5719e-04, -4.4455e-03,
         1.5971e-02,  1.1511e-02,  3.3774e-04,  1.2363e-02,  1.2736e-02,
         4.1401e-04,  2.2564e-02,  6.5826e-03,  2.4444e-02,  2.8800e-03,
         1.8264e-02,  3.1727e-03,  2.2580e-02,  2.9967e-03,  1.8473e-02,
         1.7764e-03,  1.9759e-02, -2.6548e-03,  1.1982e-02,  1.4686e-02,
         1.1869e-02,  1.2025e-03,  1.9726e-02,  1.1490e-02, -3.3636e-03,
         1.3790e-02,  1.9931e-02,  5.1287e-03,  6.0531e-03, -2.9767e-03,
         6.5900e-03,  1.4289e-02,  1.5713e-02,  1.9488e-02, -4.8407e-04,
         1.8783e-02,  2.2154e-02,  1.1504e-02,  1.4788e-03,  7.9258e-03,
        -1.5250e-03,  1.4069e-02,  1.2153e-02, -4.2288e-03,  5.5465e-03,
        -2.4140e-03,  1.1925e-02,  1.1815e-02,  1.1745e-02,  1.4084e-02,
         3.9458e-03,  1.3027e-02,  1.0371e-02,  1.4236e-02, -2.4193e-03,
        -3.1365e-04,  9.8688e-03,  5.2082e-03,  1.0702e-02,  1.8361e-03,
         1.1206e-02,  9.9811e-03,  7.9911e-04,  4.2668e-03,  4.5615e-03,
         1.1154e-02, -5.2554e-04,  1.5385e-02,  1.8115e-02,  1.9381e-02,
         2.6033e-02,  2.5758e-02,  2.2919e-02,  1.7789e-02,  9.5524e-03,
         3.7770e-03,  5.7916e-03,  1.4201e-02,  9.6552e-03, -4.3066e-03,
         2.5924e-02,  5.3638e-03,  1.3394e-02,  3.4542e-03,  2.2967e-02,
         2.5842e-02,  1.5825e-02,  1.8611e-02,  2.6383e-02,  1.6298e-02,
         1.0044e-02,  1.6711e-02,  2.6280e-02, -2.1685e-02,  3.7350e-03,
         3.6180e-03,  2.0702e-03, -1.8833e-02,  2.2999e-02, -1.7679e-02,
         2.3117e-02, -1.8752e-02,  2.4158e-02,  1.8200e-02,  2.5391e-02,
         1.4082e-02, -9.3911e-03,  5.4607e-04,  4.6389e-03,  1.1097e-02,
         2.1482e-02,  2.0318e-02, -1.8415e-02,  2.2246e-02,  2.6286e-02,
         1.5828e-02,  1.4516e-02,  2.2929e-02,  7.6106e-03, -4.5614e-04,
         1.6511e-02,  1.4248e-02,  2.3211e-02, -1.8049e-02,  2.3204e-02,
         2.3347e-02, -1.5005e-02,  1.8161e-02,  2.1974e-02, -2.0472e-02,
        -1.5818e-02,  1.3543e-02,  2.5633e-02, -1.2388e-02,  1.6609e-02,
         2.1702e-02,  1.1368e-02,  1.4619e-02,  1.8156e-02,  1.6067e-02,
         2.6109e-02,  1.4441e-02,  1.9291e-02,  2.2416e-02,  2.0658e-02,
         1.2360e-02,  3.0201e-04, -1.9318e-02,  2.4462e-02,  5.6466e-03,
         2.6054e-02,  2.5731e-02,  1.4974e-02,  1.7930e-03,  1.8537e-02,
         2.1236e-02,  9.0492e-03, -1.3021e-02,  2.5872e-02, -1.8572e-02,
         1.4953e-02,  4.7179e-03,  2.2708e-02,  1.4941e-02,  2.0691e-02,
         1.6690e-02,  1.4353e-02,  1.8182e-02,  1.6832e-02,  2.6375e-02,
         2.2980e-02,  2.0290e-02,  2.1815e-02,  1.8618e-02,  2.5399e-02,
         1.5015e-02,  7.3947e-03,  8.8523e-03,  8.6675e-03,  2.1187e-02,
         1.0499e-02,  1.8438e-02,  1.5530e-02,  2.2970e-02,  1.3330e-02,
         2.6040e-02, -1.6379e-02,  1.5391e-02,  1.4498e-02,  9.9509e-03,
        -9.4254e-03,  2.6063e-02,  1.3519e-02,  2.3231e-02, -1.1728e-02,
         1.5200e-02,  1.4429e-02,  1.4556e-02,  1.2382e-02,  1.8652e-02,
         1.3728e-02,  1.5777e-02,  5.3197e-04,  3.9516e-03,  1.6215e-02,
         8.7007e-03,  1.9141e-02,  1.1942e-02,  1.1700e-02,  1.9389e-02,
         7.4981e-03,  6.5946e-03,  8.8586e-03,  1.2235e-02,  1.5054e-02,
         9.1239e-03,  8.2152e-03,  1.6327e-02,  1.9126e-02,  1.4178e-02,
         9.3709e-03,  1.0569e-02,  1.8657e-02,  9.4362e-03,  1.7031e-02,
         1.8510e-02,  1.9667e-03,  1.4986e-02,  1.8088e-02,  5.3086e-03,
         1.1001e-02,  8.5072e-03,  7.2597e-03,  1.5672e-02,  7.4299e-03,
         2.2433e-02,  1.6434e-02,  1.1427e-02,  1.9342e-02,  1.7917e-02,
         5.3668e-03,  2.6173e-02,  1.2760e-02,  8.5798e-04,  1.4059e-02,
         1.1901e-02,  1.6002e-02,  4.3368e-03,  1.4294e-02,  8.4665e-03,
         1.2380e-02,  6.9201e-03,  1.6456e-02,  1.1512e-02,  5.6565e-03,
         1.6432e-02,  8.6342e-03,  1.7812e-02,  1.4230e-02,  9.3539e-03,
         1.8418e-02,  9.5926e-03,  2.3320e-02,  1.0674e-02,  1.2918e-02,
         2.5326e-02,  4.2576e-03,  7.6182e-03,  1.0157e-02,  1.2673e-02,
         1.4706e-02,  8.4334e-03,  3.8188e-03,  2.3383e-03,  1.6213e-02,
         1.6659e-02,  1.2204e-02,  1.1898e-02,  1.7721e-02,  1.3988e-02,
         1.3655e-02,  9.1864e-03,  6.8134e-03,  8.8784e-03,  8.5933e-03,
         1.8035e-02,  1.5976e-02,  1.0772e-02,  9.7064e-03,  8.0727e-03,
         9.7607e-03,  1.3787e-02,  7.2291e-03,  3.5551e-03,  1.0991e-02,
         1.1418e-02, -1.4380e-05,  1.4666e-02,  1.4479e-02,  1.4091e-02,
         1.3339e-02,  9.0037e-03,  1.9728e-02, -1.3169e-03,  1.6898e-02,
         2.3392e-02,  1.1730e-02,  8.6131e-03,  1.8460e-02,  8.1535e-03,
         4.6686e-03, -2.5934e-03,  1.5303e-02,  1.2694e-02,  9.8200e-03,
         2.2694e-02,  1.7336e-02,  1.8194e-02,  1.3847e-02, -1.6697e-03,
         2.0270e-02,  2.6621e-03,  8.7842e-03,  1.6842e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(3.5259, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:13:53,788 :: INFO :: Epoch 10: loss tensor(150.8043, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0357,  0.0374,  0.0374,  0.0394,  0.0368,  0.0291,  0.0354,  0.0418,
         0.0293,  0.0430,  0.0333,  0.0348,  0.0341,  0.0415,  0.0450,  0.0392,
         0.0382,  0.0424,  0.0351,  0.0399,  0.0302,  0.0438,  0.0444,  0.0248,
         0.0381,  0.0350,  0.0321,  0.0332,  0.0479,  0.0367,  0.0420,  0.0481,
         0.0394,  0.0403,  0.0200,  0.0174,  0.0403,  0.0481,  0.0318,  0.0425,
         0.0274,  0.0267,  0.0477,  0.0368,  0.0360,  0.0436,  0.0374,  0.0311,
         0.0414,  0.0359,  0.0436,  0.0345,  0.0321,  0.0332,  0.0370,  0.0456,
         0.0341,  0.0431,  0.0281,  0.0223,  0.0397,  0.0352,  0.0269,  0.0396,
         0.0408,  0.0240,  0.0482,  0.0339,  0.0491,  0.0288,  0.0440,  0.0318,
         0.0437,  0.0311,  0.0442,  0.0281,  0.0460,  0.0222,  0.0375,  0.0413,
         0.0400,  0.0305,  0.0465,  0.0405,  0.0229,  0.0429,  0.0451,  0.0334,
         0.0330,  0.0210,  0.0341,  0.0431,  0.0419,  0.0456,  0.0279,  0.0470,
         0.0476,  0.0397,  0.0267,  0.0341,  0.0243,  0.0356,  0.0388,  0.0215,
         0.0349,  0.0232,  0.0378,  0.0365,  0.0374,  0.0399,  0.0320,  0.0378,
         0.0391,  0.0414,  0.0217,  0.0228,  0.0355,  0.0330,  0.0317,  0.0287,
         0.0374,  0.0369,  0.0273,  0.0298,  0.0320,  0.0345,  0.0251,  0.0438,
         0.0453,  0.0482,  0.0557,  0.0457,  0.0486,  0.0362,  0.0389,  0.0340,
         0.0370,  0.0218,  0.0390,  0.0228,  0.0545,  0.0354,  0.0435,  0.0328,
         0.0532,  0.0541,  0.0373,  0.0498,  0.0560,  0.0470,  0.0406,  0.0463,
         0.0552, -0.0371,  0.0329,  0.0340,  0.0316, -0.0279,  0.0421, -0.0291,
         0.0498, -0.0331,  0.0455,  0.0480,  0.0511,  0.0246,  0.0112,  0.0299,
         0.0345,  0.0406,  0.0511,  0.0471, -0.0290,  0.0521,  0.0560,  0.0467,
         0.0259,  0.0516,  0.0372,  0.0251,  0.0457,  0.0449,  0.0382, -0.0236,
         0.0515,  0.0525, -0.0062,  0.0467,  0.0506, -0.0336, -0.0130,  0.0438,
         0.0528,  0.0050,  0.0398,  0.0499,  0.0413,  0.0224,  0.0474,  0.0465,
         0.0556,  0.0447,  0.0473,  0.0346,  0.0501,  0.0428,  0.0288, -0.0343,
         0.0470,  0.0356,  0.0548,  0.0527,  0.0251,  0.0310,  0.0478,  0.0517,
         0.0393,  0.0043,  0.0539, -0.0245,  0.0442,  0.0342,  0.0494,  0.0433,
         0.0433,  0.0460,  0.0424,  0.0486,  0.0401,  0.0556,  0.0510,  0.0367,
         0.0349,  0.0344,  0.0521,  0.0459,  0.0379,  0.0388,  0.0140,  0.0445,
         0.0401,  0.0488,  0.0466,  0.0505,  0.0438,  0.0559, -0.0087,  0.0452,
         0.0445,  0.0401,  0.0121,  0.0552,  0.0441,  0.0536,  0.0118,  0.0448,
         0.0462,  0.0459,  0.0447,  0.0490,  0.0442,  0.0471,  0.0313,  0.0352,
         0.0473,  0.0391,  0.0502,  0.0428,  0.0423,  0.0494,  0.0378,  0.0380,
         0.0394,  0.0432,  0.0460,  0.0400,  0.0389,  0.0475,  0.0496,  0.0452,
         0.0403,  0.0423,  0.0493,  0.0409,  0.0490,  0.0486,  0.0330,  0.0462,
         0.0475,  0.0366,  0.0416,  0.0397,  0.0388,  0.0458,  0.0379,  0.0520,
         0.0473,  0.0418,  0.0488,  0.0480,  0.0369,  0.0574,  0.0442,  0.0326,
         0.0453,  0.0436,  0.0458,  0.0346,  0.0451,  0.0393,  0.0431,  0.0382,
         0.0479,  0.0419,  0.0356,  0.0470,  0.0379,  0.0497,  0.0452,  0.0407,
         0.0492,  0.0407,  0.0541,  0.0416,  0.0440,  0.0556,  0.0362,  0.0388,
         0.0408,  0.0437,  0.0456,  0.0397,  0.0351,  0.0334,  0.0480,  0.0464,
         0.0418,  0.0431,  0.0482,  0.0441,  0.0449,  0.0403,  0.0376,  0.0396,
         0.0395,  0.0467,  0.0458,  0.0417,  0.0405,  0.0391,  0.0407,  0.0442,
         0.0383,  0.0326,  0.0419,  0.0424,  0.0313,  0.0446,  0.0448,  0.0452,
         0.0445,  0.0414,  0.0509,  0.0294,  0.0470,  0.0537,  0.0428,  0.0394,
         0.0486,  0.0394,  0.0357,  0.0286,  0.0458,  0.0434,  0.0411,  0.0540,
         0.0476,  0.0459,  0.0447,  0.0286,  0.0506,  0.0338,  0.0402,  0.0471],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0501, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:13:58,289 :: INFO :: Epoch 15: loss tensor(147.5219, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0594,  0.0554,  0.0655,  0.0621,  0.0569,  0.0549,  0.0597,  0.0629,
         0.0477,  0.0624,  0.0617,  0.0561,  0.0536,  0.0652,  0.0674,  0.0576,
         0.0490,  0.0609,  0.0509,  0.0651,  0.0516,  0.0583,  0.0682,  0.0529,
         0.0617,  0.0513,  0.0444,  0.0585,  0.0776,  0.0531,  0.0658,  0.0632,
         0.0626,  0.0607,  0.0439,  0.0399,  0.0551,  0.0728,  0.0536,  0.0668,
         0.0504,  0.0481,  0.0685,  0.0567,  0.0534,  0.0569,  0.0538,  0.0568,
         0.0661,  0.0486,  0.0655,  0.0516,  0.0515,  0.0435,  0.0656,  0.0706,
         0.0551,  0.0715,  0.0541,  0.0487,  0.0511,  0.0514,  0.0473,  0.0593,
         0.0629,  0.0407,  0.0679,  0.0570,  0.0663,  0.0477,  0.0601,  0.0565,
         0.0522,  0.0585,  0.0624,  0.0485,  0.0652,  0.0451,  0.0527,  0.0596,
         0.0651,  0.0597,  0.0669,  0.0672,  0.0449,  0.0660,  0.0624,  0.0586,
         0.0525,  0.0369,  0.0572,  0.0694,  0.0588,  0.0606,  0.0544,  0.0697,
         0.0639,  0.0627,  0.0480,  0.0553,  0.0455,  0.0420,  0.0578,  0.0454,
         0.0625,  0.0472,  0.0585,  0.0525,  0.0552,  0.0602,  0.0591,  0.0531,
         0.0669,  0.0611,  0.0445,  0.0426,  0.0545,  0.0572,  0.0389,  0.0540,
         0.0588,  0.0576,  0.0472,  0.0461,  0.0522,  0.0525,  0.0486,  0.0674,
         0.0677,  0.0744,  0.0846,  0.0532,  0.0637,  0.0503,  0.0660,  0.0639,
         0.0694,  0.0265,  0.0663,  0.0529,  0.0810,  0.0657,  0.0723,  0.0647,
         0.0828,  0.0795,  0.0482,  0.0819,  0.0852,  0.0780,  0.0715,  0.0748,
         0.0826, -0.0483,  0.0616,  0.0660,  0.0619, -0.0281,  0.0487, -0.0343,
         0.0697, -0.0427,  0.0580,  0.0772,  0.0703,  0.0253,  0.0387,  0.0597,
         0.0652,  0.0677,  0.0786,  0.0680, -0.0326,  0.0806,  0.0845,  0.0782,
         0.0288,  0.0772,  0.0664,  0.0505,  0.0725,  0.0759,  0.0441, -0.0184,
         0.0761,  0.0798,  0.0148,  0.0734,  0.0783, -0.0410,  0.0059,  0.0737,
         0.0748,  0.0316,  0.0502,  0.0739,  0.0719,  0.0231,  0.0746,  0.0769,
         0.0842,  0.0752,  0.0730,  0.0430,  0.0782,  0.0740,  0.0588, -0.0452,
         0.0545,  0.0645,  0.0829,  0.0727,  0.0312,  0.0609,  0.0746,  0.0823,
         0.0686,  0.0314,  0.0786, -0.0203,  0.0719,  0.0626,  0.0708,  0.0702,
         0.0549,  0.0741,  0.0657,  0.0784,  0.0557,  0.0842,  0.0759,  0.0400,
         0.0410,  0.0421,  0.0726,  0.0777,  0.0685,  0.0680,  0.0160,  0.0577,
         0.0673,  0.0788,  0.0778,  0.0747,  0.0743,  0.0845,  0.0134,  0.0742,
         0.0733,  0.0702,  0.0421,  0.0817,  0.0747,  0.0844,  0.0433,  0.0722,
         0.0800,  0.0784,  0.0790,  0.0785,  0.0746,  0.0798,  0.0640,  0.0676,
         0.0796,  0.0703,  0.0815,  0.0741,  0.0724,  0.0784,  0.0678,  0.0705,
         0.0711,  0.0745,  0.0768,  0.0716,  0.0703,  0.0793,  0.0798,  0.0762,
         0.0720,  0.0766,  0.0803,  0.0740,  0.0830,  0.0764,  0.0656,  0.0782,
         0.0744,  0.0693,  0.0730,  0.0720,  0.0731,  0.0746,  0.0683,  0.0797,
         0.0785,  0.0723,  0.0763,  0.0775,  0.0702,  0.0902,  0.0778,  0.0669,
         0.0772,  0.0771,  0.0738,  0.0649,  0.0751,  0.0717,  0.0740,  0.0720,
         0.0808,  0.0725,  0.0663,  0.0773,  0.0661,  0.0837,  0.0767,  0.0734,
         0.0799,  0.0734,  0.0849,  0.0730,  0.0765,  0.0857,  0.0703,  0.0715,
         0.0712,  0.0749,  0.0767,  0.0713,  0.0678,  0.0659,  0.0809,  0.0742,
         0.0699,  0.0749,  0.0799,  0.0729,  0.0767,  0.0721,  0.0690,  0.0702,
         0.0708,  0.0696,  0.0734,  0.0735,  0.0717,  0.0714,  0.0719,  0.0750,
         0.0705,  0.0585,  0.0735,  0.0736,  0.0653,  0.0737,  0.0748,  0.0767,
         0.0764,  0.0760,  0.0834,  0.0640,  0.0771,  0.0833,  0.0746,  0.0711,
         0.0781,  0.0716,  0.0681,  0.0633,  0.0755,  0.0733,  0.0752,  0.0870,
         0.0767,  0.0687,  0.0751,  0.0617,  0.0807,  0.0666,  0.0730,  0.0760],
       device='cuda:0', requires_grad=True) MLP.norm tensor(8.3245, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:02,836 :: INFO :: Epoch 20: loss tensor(145.2805, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0744,  0.0598,  0.0904,  0.0765,  0.0707,  0.0749,  0.0750,  0.0782,
         0.0549,  0.0730,  0.0843,  0.0700,  0.0643,  0.0792,  0.0794,  0.0663,
         0.0501,  0.0684,  0.0591,  0.0860,  0.0665,  0.0601,  0.0831,  0.0751,
         0.0794,  0.0584,  0.0495,  0.0781,  0.1032,  0.0599,  0.0822,  0.0685,
         0.0768,  0.0721,  0.0587,  0.0526,  0.0589,  0.0922,  0.0667,  0.0811,
         0.0655,  0.0618,  0.0796,  0.0687,  0.0597,  0.0590,  0.0622,  0.0754,
         0.0821,  0.0493,  0.0771,  0.0564,  0.0642,  0.0451,  0.0902,  0.0888,
         0.0683,  0.0974,  0.0728,  0.0671,  0.0528,  0.0627,  0.0579,  0.0689,
         0.0753,  0.0502,  0.0767,  0.0687,  0.0740,  0.0579,  0.0657,  0.0702,
         0.0519,  0.0827,  0.0691,  0.0574,  0.0729,  0.0633,  0.0545,  0.0687,
         0.0852,  0.0852,  0.0774,  0.0884,  0.0555,  0.0804,  0.0716,  0.0761,
         0.0612,  0.0404,  0.0736,  0.0904,  0.0681,  0.0637,  0.0729,  0.0816,
         0.0691,  0.0753,  0.0638,  0.0683,  0.0562,  0.0422,  0.0665,  0.0639,
         0.0856,  0.0634,  0.0690,  0.0599,  0.0653,  0.0735,  0.0832,  0.0578,
         0.0886,  0.0714,  0.0604,  0.0561,  0.0660,  0.0745,  0.0418,  0.0742,
         0.0738,  0.0706,  0.0579,  0.0502,  0.0635,  0.0659,  0.0673,  0.0852,
         0.0830,  0.0960,  0.1105,  0.0504,  0.0663,  0.0575,  0.0877,  0.0906,
         0.1002,  0.0315,  0.0881,  0.0804,  0.1019,  0.0936,  0.0960,  0.0960,
         0.1092,  0.0990,  0.0507,  0.1125,  0.1113,  0.1059,  0.1004,  0.0995,
         0.1053, -0.0558,  0.0862,  0.0968,  0.0895, -0.0212,  0.0529, -0.0329,
         0.0808, -0.0473,  0.0692,  0.1036,  0.0818,  0.0230,  0.0648,  0.0860,
         0.0936,  0.0896,  0.0999,  0.0798, -0.0298,  0.1056,  0.1089,  0.1081,
         0.0330,  0.0961,  0.0914,  0.0718,  0.0936,  0.1047,  0.0453, -0.0058,
         0.0934,  0.1023,  0.0379,  0.0922,  0.0993, -0.0431,  0.0314,  0.1003,
         0.0888,  0.0588,  0.0502,  0.0918,  0.0997,  0.0243,  0.0972,  0.1041,
         0.1093,  0.1024,  0.0933,  0.0429,  0.1016,  0.1027,  0.0859, -0.0516,
         0.0541,  0.0895,  0.1057,  0.0834,  0.0317,  0.0876,  0.0947,  0.1105,
         0.0941,  0.0583,  0.0970, -0.0082,  0.0946,  0.0860,  0.0851,  0.0933,
         0.0555,  0.0976,  0.0804,  0.1058,  0.0607,  0.1090,  0.0935,  0.0378,
         0.0437,  0.0416,  0.0840,  0.1074,  0.0961,  0.0943,  0.0168,  0.0659,
         0.0886,  0.1061,  0.1072,  0.0944,  0.1025,  0.1099,  0.0394,  0.0992,
         0.0989,  0.0980,  0.0724,  0.1042,  0.1027,  0.1126,  0.0744,  0.0943,
         0.1134,  0.1107,  0.1132,  0.1033,  0.1022,  0.1116,  0.0941,  0.0976,
         0.1120,  0.0999,  0.1111,  0.1031,  0.0987,  0.1027,  0.0946,  0.1008,
         0.1006,  0.1044,  0.1040,  0.1002,  0.0997,  0.1086,  0.1071,  0.1036,
         0.1008,  0.1106,  0.1084,  0.1070,  0.1167,  0.0981,  0.0961,  0.1078,
         0.0944,  0.1002,  0.1020,  0.1020,  0.1071,  0.0979,  0.0950,  0.1026,
         0.1066,  0.1002,  0.0974,  0.1023,  0.1033,  0.1228,  0.1107,  0.1015,
         0.1082,  0.1106,  0.0972,  0.0934,  0.1012,  0.1017,  0.1041,  0.1048,
         0.1124,  0.1008,  0.0946,  0.1036,  0.0899,  0.1175,  0.1055,  0.1055,
         0.1075,  0.1057,  0.1126,  0.1023,  0.1074,  0.1130,  0.1042,  0.1027,
         0.0982,  0.1029,  0.1048,  0.1002,  0.0985,  0.0956,  0.1127,  0.0946,
         0.0908,  0.1039,  0.1104,  0.0958,  0.1055,  0.1011,  0.0978,  0.0990,
         0.0989,  0.0846,  0.0941,  0.1041,  0.1014,  0.1016,  0.1000,  0.1036,
         0.1005,  0.0767,  0.1034,  0.1013,  0.0993,  0.0999,  0.1010,  0.1055,
         0.1072,  0.1104,  0.1154,  0.0979,  0.1042,  0.1093,  0.1041,  0.1018,
         0.1046,  0.1016,  0.0989,  0.0980,  0.1001,  0.0985,  0.1087,  0.1195,
         0.1012,  0.0833,  0.1011,  0.0947,  0.1074,  0.0973,  0.1044,  0.1022],
       device='cuda:0', requires_grad=True) MLP.norm tensor(10.2840, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:07,414 :: INFO :: Epoch 25: loss tensor(144.1785, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0810,  0.0594,  0.1102,  0.0820,  0.0739,  0.0902,  0.0830,  0.0859,
         0.0588,  0.0778,  0.1026,  0.0769,  0.0655,  0.0863,  0.0853,  0.0690,
         0.0501,  0.0703,  0.0602,  0.1001,  0.0737,  0.0616,  0.0901,  0.0905,
         0.0917,  0.0584,  0.0470,  0.0895,  0.1241,  0.0607,  0.0921,  0.0683,
         0.0865,  0.0770,  0.0659,  0.0568,  0.0581,  0.1036,  0.0716,  0.0909,
         0.0743,  0.0682,  0.0867,  0.0726,  0.0604,  0.0589,  0.0619,  0.0877,
         0.0930,  0.0493,  0.0811,  0.0551,  0.0690,  0.0428,  0.1101,  0.0997,
         0.0768,  0.1197,  0.0856,  0.0796,  0.0505,  0.0669,  0.0646,  0.0718,
         0.0801,  0.0532,  0.0816,  0.0745,  0.0747,  0.0607,  0.0660,  0.0776,
         0.0478,  0.0998,  0.0703,  0.0577,  0.0758,  0.0745,  0.0539,  0.0739,
         0.0987,  0.1069,  0.0780,  0.1023,  0.0606,  0.0868,  0.0745,  0.0858,
         0.0639,  0.0404,  0.0814,  0.1043,  0.0688,  0.0640,  0.0860,  0.0884,
         0.0674,  0.0809,  0.0715,  0.0730,  0.0628,  0.0389,  0.0669,  0.0753,
         0.1014,  0.0747,  0.0730,  0.0592,  0.0685,  0.0791,  0.1020,  0.0575,
         0.1049,  0.0750,  0.0690,  0.0600,  0.0715,  0.0874,  0.0396,  0.0879,
         0.0807,  0.0749,  0.0639,  0.0535,  0.0652,  0.0720,  0.0790,  0.0938,
         0.0914,  0.1108,  0.1309,  0.0448,  0.0641,  0.0622,  0.1014,  0.1112,
         0.1275,  0.0342,  0.1034,  0.1022,  0.1142,  0.1165,  0.1117,  0.1257,
         0.1304,  0.1091,  0.0493,  0.1403,  0.1319,  0.1291,  0.1246,  0.1183,
         0.1210, -0.0610,  0.1040,  0.1239,  0.1116, -0.0124,  0.0533, -0.0272,
         0.0840, -0.0481,  0.0736,  0.1243,  0.0860,  0.0207,  0.0855,  0.1065,
         0.1171,  0.1041,  0.1138,  0.0825, -0.0239,  0.1240,  0.1273,  0.1339,
         0.0328,  0.1054,  0.1101,  0.0865,  0.1059,  0.1293,  0.0425,  0.0077,
         0.1007,  0.1171,  0.0563,  0.1018,  0.1139, -0.0425,  0.0581,  0.1213,
         0.0922,  0.0804,  0.0468,  0.1003,  0.1234,  0.0211,  0.1133,  0.1264,
         0.1282,  0.1248,  0.1062,  0.0402,  0.1175,  0.1279,  0.1071, -0.0552,
         0.0493,  0.1080,  0.1209,  0.0829,  0.0308,  0.1079,  0.1068,  0.1344,
         0.1125,  0.0805,  0.1052,  0.0049,  0.1118,  0.1027,  0.0929,  0.1116,
         0.0534,  0.1144,  0.0860,  0.1291,  0.0603,  0.1289,  0.1014,  0.0314,
         0.0452,  0.0411,  0.0848,  0.1335,  0.1185,  0.1147,  0.0175,  0.0695,
         0.1016,  0.1282,  0.1327,  0.1089,  0.1260,  0.1298,  0.0621,  0.1174,
         0.1182,  0.1208,  0.1010,  0.1179,  0.1257,  0.1366,  0.1011,  0.1077,
         0.1448,  0.1413,  0.1462,  0.1229,  0.1273,  0.1417,  0.1200,  0.1234,
         0.1436,  0.1287,  0.1387,  0.1294,  0.1195,  0.1202,  0.1177,  0.1274,
         0.1284,  0.1315,  0.1260,  0.1257,  0.1278,  0.1355,  0.1304,  0.1258,
         0.1254,  0.1435,  0.1316,  0.1389,  0.1490,  0.1119,  0.1228,  0.1343,
         0.1069,  0.1280,  0.1278,  0.1286,  0.1408,  0.1150,  0.1163,  0.1203,
         0.1307,  0.1262,  0.1079,  0.1214,  0.1350,  0.1542,  0.1427,  0.1349,
         0.1377,  0.1432,  0.1131,  0.1205,  0.1223,  0.1299,  0.1322,  0.1365,
         0.1421,  0.1266,  0.1205,  0.1243,  0.1098,  0.1503,  0.1300,  0.1362,
         0.1302,  0.1371,  0.1361,  0.1286,  0.1364,  0.1358,  0.1368,  0.1320,
         0.1202,  0.1272,  0.1288,  0.1256,  0.1255,  0.1210,  0.1431,  0.1065,
         0.1036,  0.1289,  0.1394,  0.1095,  0.1304,  0.1251,  0.1235,  0.1256,
         0.1223,  0.0928,  0.1084,  0.1328,  0.1277,  0.1280,  0.1236,  0.1290,
         0.1269,  0.0887,  0.1310,  0.1240,  0.1319,  0.1241,  0.1225,  0.1304,
         0.1359,  0.1439,  0.1463,  0.1313,  0.1290,  0.1304,  0.1304,  0.1310,
         0.1260,  0.1279,  0.1265,  0.1313,  0.1188,  0.1172,  0.1413,  0.1506,
         0.1197,  0.0882,  0.1212,  0.1271,  0.1288,  0.1243,  0.1335,  0.1240],
       device='cuda:0', requires_grad=True) MLP.norm tensor(11.9428, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:11,978 :: INFO :: Epoch 30: loss tensor(144.2412, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0845,  0.0588,  0.1269,  0.0850,  0.0790,  0.0989,  0.0844,  0.0913,
         0.0604,  0.0807,  0.1131,  0.0836,  0.0672,  0.0910,  0.0881,  0.0715,
         0.0495,  0.0698,  0.0591,  0.1094,  0.0786,  0.0589,  0.0956,  0.1006,
         0.1011,  0.0591,  0.0471,  0.0988,  0.1398,  0.0595,  0.0981,  0.0686,
         0.0897,  0.0779,  0.0697,  0.0596,  0.0575,  0.1133,  0.0747,  0.0933,
         0.0811,  0.0690,  0.0875,  0.0759,  0.0613,  0.0558,  0.0629,  0.0985,
         0.0962,  0.0503,  0.0800,  0.0532,  0.0723,  0.0426,  0.1228,  0.1062,
         0.0784,  0.1403,  0.0910,  0.0851,  0.0506,  0.0685,  0.0651,  0.0734,
         0.0811,  0.0541,  0.0799,  0.0771,  0.0778,  0.0596,  0.0666,  0.0790,
         0.0451,  0.1141,  0.0704,  0.0570,  0.0746,  0.0807,  0.0518,  0.0700,
         0.1073,  0.1219,  0.0802,  0.1118,  0.0602,  0.0863,  0.0748,  0.0900,
         0.0667,  0.0383,  0.0876,  0.1157,  0.0691,  0.0633,  0.0896,  0.0891,
         0.0654,  0.0801,  0.0787,  0.0751,  0.0624,  0.0383,  0.0671,  0.0839,
         0.1116,  0.0796,  0.0756,  0.0573,  0.0697,  0.0841,  0.1155,  0.0588,
         0.1137,  0.0764,  0.0748,  0.0629,  0.0726,  0.0967,  0.0391,  0.0990,
         0.0848,  0.0778,  0.0635,  0.0523,  0.0666,  0.0750,  0.0875,  0.0994,
         0.0943,  0.1191,  0.1452,  0.0386,  0.0578,  0.0627,  0.1079,  0.1261,
         0.1510,  0.0340,  0.1122,  0.1174,  0.1176,  0.1340,  0.1183,  0.1519,
         0.1458,  0.1121,  0.0461,  0.1643,  0.1463,  0.1462,  0.1440,  0.1308,
         0.1290, -0.0649,  0.1136,  0.1471,  0.1279, -0.0036,  0.0543, -0.0194,
         0.0832, -0.0468,  0.0750,  0.1391,  0.0881,  0.0184,  0.0996,  0.1209,
         0.1350,  0.1113,  0.1187,  0.0811, -0.0175,  0.1362,  0.1387,  0.1557,
         0.0321,  0.1062,  0.1215,  0.0955,  0.1109,  0.1493,  0.0373,  0.0193,
         0.1002,  0.1246,  0.0691,  0.1027,  0.1213, -0.0408,  0.0823,  0.1357,
         0.0900,  0.0964,  0.0411,  0.1029,  0.1406,  0.0211,  0.1215,  0.1427,
         0.1409,  0.1409,  0.1123,  0.0362,  0.1249,  0.1475,  0.1213, -0.0574,
         0.0460,  0.1200,  0.1279,  0.0775,  0.0272,  0.1218,  0.1094,  0.1531,
         0.1239,  0.0962,  0.1065,  0.0164,  0.1220,  0.1111,  0.0954,  0.1240,
         0.0490,  0.1236,  0.0847,  0.1468,  0.0561,  0.1422,  0.1005,  0.0287,
         0.0437,  0.0374,  0.0807,  0.1548,  0.1349,  0.1295,  0.0157,  0.0678,
         0.1065,  0.1448,  0.1541,  0.1186,  0.1448,  0.1446,  0.0800,  0.1280,
         0.1317,  0.1382,  0.1255,  0.1259,  0.1433,  0.1554,  0.1231,  0.1132,
         0.1747,  0.1689,  0.1776,  0.1364,  0.1492,  0.1688,  0.1416,  0.1450,
         0.1737,  0.1555,  0.1636,  0.1518,  0.1344,  0.1299,  0.1361,  0.1507,
         0.1546,  0.1557,  0.1429,  0.1474,  0.1534,  0.1596,  0.1493,  0.1429,
         0.1447,  0.1741,  0.1499,  0.1681,  0.1802,  0.1184,  0.1458,  0.1572,
         0.1136,  0.1526,  0.1509,  0.1512,  0.1727,  0.1261,  0.1324,  0.1345,
         0.1502,  0.1496,  0.1105,  0.1354,  0.1644,  0.1833,  0.1730,  0.1662,
         0.1647,  0.1737,  0.1198,  0.1458,  0.1382,  0.1561,  0.1581,  0.1659,
         0.1697,  0.1507,  0.1447,  0.1387,  0.1247,  0.1815,  0.1505,  0.1645,
         0.1477,  0.1670,  0.1544,  0.1511,  0.1630,  0.1535,  0.1675,  0.1589,
         0.1369,  0.1479,  0.1485,  0.1474,  0.1490,  0.1414,  0.1714,  0.1100,
         0.1094,  0.1498,  0.1663,  0.1135,  0.1508,  0.1445,  0.1447,  0.1484,
         0.1408,  0.0964,  0.1177,  0.1584,  0.1510,  0.1511,  0.1420,  0.1516,
         0.1496,  0.0954,  0.1549,  0.1409,  0.1623,  0.1464,  0.1393,  0.1512,
         0.1626,  0.1758,  0.1753,  0.1629,  0.1508,  0.1461,  0.1534,  0.1577,
         0.1423,  0.1504,  0.1507,  0.1625,  0.1319,  0.1309,  0.1718,  0.1800,
         0.1307,  0.0845,  0.1348,  0.1577,  0.1453,  0.1479,  0.1598,  0.1404],
       device='cuda:0', requires_grad=True) MLP.norm tensor(13.3421, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:16,540 :: INFO :: Epoch 35: loss tensor(139.0305, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0839,  0.0562,  0.1394,  0.0850,  0.0776,  0.1056,  0.0844,  0.0920,
         0.0620,  0.0793,  0.1210,  0.0845,  0.0657,  0.0912,  0.0888,  0.0706,
         0.0477,  0.0711,  0.0579,  0.1146,  0.0806,  0.0564,  0.0952,  0.1066,
         0.1066,  0.0585,  0.0440,  0.1034,  0.1510,  0.0586,  0.1024,  0.0656,
         0.0928,  0.0761,  0.0704,  0.0593,  0.0554,  0.1174,  0.0733,  0.0949,
         0.0839,  0.0689,  0.0879,  0.0745,  0.0599,  0.0539,  0.0604,  0.1033,
         0.0977,  0.0474,  0.0766,  0.0509,  0.0729,  0.0400,  0.1321,  0.1087,
         0.0817,  0.1576,  0.0928,  0.0893,  0.0478,  0.0659,  0.0661,  0.0724,
         0.0812,  0.0526,  0.0803,  0.0766,  0.0768,  0.0583,  0.0642,  0.0792,
         0.0412,  0.1243,  0.0688,  0.0562,  0.0746,  0.0844,  0.0496,  0.0735,
         0.1136,  0.1332,  0.0786,  0.1183,  0.0598,  0.0858,  0.0721,  0.0912,
         0.0667,  0.0375,  0.0887,  0.1226,  0.0670,  0.0618,  0.0913,  0.0875,
         0.0635,  0.0790,  0.0807,  0.0768,  0.0624,  0.0337,  0.0641,  0.0878,
         0.1174,  0.0825,  0.0740,  0.0541,  0.0711,  0.0830,  0.1254,  0.0563,
         0.1186,  0.0736,  0.0771,  0.0612,  0.0717,  0.1042,  0.0388,  0.1050,
         0.0854,  0.0775,  0.0645,  0.0527,  0.0638,  0.0756,  0.0910,  0.1010,
         0.0875,  0.1202,  0.1530,  0.0324,  0.0504,  0.0609,  0.1072,  0.1338,
         0.1707,  0.0356,  0.1125,  0.1249,  0.1133,  0.1452,  0.1173,  0.1729,
         0.1546,  0.1072,  0.0408,  0.1841,  0.1544,  0.1578,  0.1582,  0.1322,
         0.1296, -0.0686,  0.1150,  0.1671,  0.1370,  0.0020,  0.0502, -0.0126,
         0.0756, -0.0453,  0.0746,  0.1473,  0.0846,  0.0155,  0.1057,  0.1271,
         0.1466,  0.1095,  0.1172,  0.0750, -0.0131,  0.1411,  0.1432,  0.1731,
         0.0283,  0.0999,  0.1247,  0.0978,  0.1090,  0.1647,  0.0336,  0.0260,
         0.0933,  0.1256,  0.0738,  0.0974,  0.1229, -0.0399,  0.1010,  0.1432,
         0.0835,  0.1049,  0.0365,  0.1005,  0.1494,  0.0173,  0.1209,  0.1526,
         0.1467,  0.1487,  0.1109,  0.0304,  0.1235,  0.1598,  0.1279, -0.0595,
         0.0412,  0.1243,  0.1253,  0.0697,  0.0241,  0.1280,  0.1045,  0.1655,
         0.1275,  0.1047,  0.1005,  0.0236,  0.1220,  0.1109,  0.0909,  0.1292,
         0.0429,  0.1237,  0.0779,  0.1580,  0.0484,  0.1489,  0.0927,  0.0236,
         0.0409,  0.0330,  0.0728,  0.1710,  0.1447,  0.1371,  0.0147,  0.0647,
         0.1035,  0.1555,  0.1709,  0.1214,  0.1583,  0.1529,  0.0921,  0.1304,
         0.1383,  0.1493,  0.1446,  0.1279,  0.1541,  0.1690,  0.1410,  0.1109,
         0.2032,  0.1945,  0.2064,  0.1453,  0.1692,  0.1925,  0.1604,  0.1634,
         0.2015,  0.1796,  0.1871,  0.1722,  0.1458,  0.1351,  0.1518,  0.1713,
         0.1779,  0.1781,  0.1556,  0.1666,  0.1765,  0.1806,  0.1658,  0.1563,
         0.1607,  0.2010,  0.1647,  0.1953,  0.2088,  0.1197,  0.1657,  0.1772,
         0.1177,  0.1747,  0.1722,  0.1709,  0.2021,  0.1337,  0.1453,  0.1451,
         0.1662,  0.1707,  0.1065,  0.1449,  0.1925,  0.2096,  0.2008,  0.1946,
         0.1897,  0.2013,  0.1224,  0.1689,  0.1496,  0.1800,  0.1816,  0.1933,
         0.1950,  0.1722,  0.1672,  0.1488,  0.1364,  0.2095,  0.1683,  0.1911,
         0.1612,  0.1943,  0.1694,  0.1719,  0.1875,  0.1688,  0.1960,  0.1840,
         0.1508,  0.1652,  0.1649,  0.1665,  0.1701,  0.1585,  0.1973,  0.1080,
         0.1103,  0.1678,  0.1907,  0.1122,  0.1673,  0.1605,  0.1625,  0.1674,
         0.1571,  0.0992,  0.1249,  0.1823,  0.1720,  0.1713,  0.1568,  0.1723,
         0.1691,  0.1011,  0.1753,  0.1543,  0.1903,  0.1657,  0.1528,  0.1694,
         0.1869,  0.2051,  0.2010,  0.1927,  0.1698,  0.1578,  0.1732,  0.1821,
         0.1558,  0.1701,  0.1720,  0.1916,  0.1391,  0.1402,  0.2003,  0.2059,
         0.1374,  0.0798,  0.1426,  0.1868,  0.1575,  0.1687,  0.1835,  0.1536],
       device='cuda:0', requires_grad=True) MLP.norm tensor(14.5079, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:21,118 :: INFO :: Epoch 40: loss tensor(140.6614, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0824,  0.0547,  0.1486,  0.0819,  0.0760,  0.1112,  0.0837,  0.0909,
         0.0636,  0.0778,  0.1252,  0.0840,  0.0683,  0.0899,  0.0868,  0.0698,
         0.0481,  0.0700,  0.0560,  0.1166,  0.0823,  0.0550,  0.0937,  0.1086,
         0.1097,  0.0570,  0.0429,  0.1043,  0.1583,  0.0574,  0.1047,  0.0607,
         0.0936,  0.0739,  0.0692,  0.0597,  0.0538,  0.1203,  0.0739,  0.0954,
         0.0873,  0.0689,  0.0869,  0.0742,  0.0584,  0.0518,  0.0576,  0.1077,
         0.0981,  0.0494,  0.0719,  0.0465,  0.0756,  0.0414,  0.1373,  0.1092,
         0.0833,  0.1713,  0.0925,  0.0903,  0.0461,  0.0633,  0.0674,  0.0698,
         0.0785,  0.0504,  0.0784,  0.0736,  0.0733,  0.0567,  0.0618,  0.0784,
         0.0381,  0.1345,  0.0636,  0.0543,  0.0723,  0.0865,  0.0469,  0.0734,
         0.1172,  0.1413,  0.0768,  0.1212,  0.0599,  0.0826,  0.0687,  0.0913,
         0.0672,  0.0364,  0.0880,  0.1272,  0.0646,  0.0607,  0.0905,  0.0857,
         0.0599,  0.0766,  0.0804,  0.0769,  0.0609,  0.0318,  0.0591,  0.0918,
         0.1194,  0.0836,  0.0704,  0.0513,  0.0680,  0.0801,  0.1310,  0.0565,
         0.1206,  0.0703,  0.0772,  0.0577,  0.0692,  0.1116,  0.0399,  0.1079,
         0.0840,  0.0746,  0.0623,  0.0534,  0.0615,  0.0776,  0.0925,  0.1013,
         0.0864,  0.1187,  0.1558,  0.0272,  0.0448,  0.0572,  0.1034,  0.1368,
         0.1868,  0.0344,  0.1101,  0.1285,  0.1051,  0.1528,  0.1107,  0.1907,
         0.1582,  0.1006,  0.0363,  0.2001,  0.1564,  0.1631,  0.1681,  0.1321,
         0.1244, -0.0708,  0.1123,  0.1835,  0.1429,  0.0087,  0.0462, -0.0049,
         0.0700, -0.0421,  0.0722,  0.1506,  0.0817,  0.0147,  0.1102,  0.1296,
         0.1538,  0.1055,  0.1119,  0.0699, -0.0072,  0.1412,  0.1421,  0.1867,
         0.0271,  0.0911,  0.1229,  0.0999,  0.1022,  0.1758,  0.0266,  0.0333,
         0.0842,  0.1218,  0.0789,  0.0888,  0.1199, -0.0374,  0.1171,  0.1453,
         0.0757,  0.1118,  0.0316,  0.0949,  0.1537,  0.0155,  0.1175,  0.1571,
         0.1472,  0.1515,  0.1077,  0.0259,  0.1163,  0.1670,  0.1306, -0.0600,
         0.0353,  0.1241,  0.1181,  0.0610,  0.0210,  0.1304,  0.0958,  0.1729,
         0.1254,  0.1110,  0.0926,  0.0319,  0.1201,  0.1077,  0.0870,  0.1311,
         0.0390,  0.1204,  0.0717,  0.1637,  0.0433,  0.1495,  0.0816,  0.0200,
         0.0386,  0.0289,  0.0656,  0.1822,  0.1496,  0.1410,  0.0145,  0.0606,
         0.0966,  0.1607,  0.1833,  0.1229,  0.1679,  0.1562,  0.1031,  0.1274,
         0.1402,  0.1570,  0.1590,  0.1265,  0.1595,  0.1773,  0.1552,  0.1036,
         0.2287,  0.2176,  0.2322,  0.1503,  0.1857,  0.2139,  0.1763,  0.1783,
         0.2268,  0.2006,  0.2081,  0.1894,  0.1523,  0.1355,  0.1647,  0.1888,
         0.1974,  0.1973,  0.1633,  0.1835,  0.1972,  0.1992,  0.1788,  0.1665,
         0.1729,  0.2237,  0.1752,  0.2199,  0.2349,  0.1161,  0.1826,  0.1937,
         0.1198,  0.1936,  0.1921,  0.1871,  0.2284,  0.1385,  0.1553,  0.1541,
         0.1776,  0.1888,  0.1002,  0.1492,  0.2181,  0.2326,  0.2266,  0.2200,
         0.2128,  0.2261,  0.1222,  0.1897,  0.1558,  0.2015,  0.2021,  0.2169,
         0.2185,  0.1894,  0.1871,  0.1535,  0.1460,  0.2343,  0.1824,  0.2149,
         0.1707,  0.2190,  0.1803,  0.1895,  0.2094,  0.1807,  0.2219,  0.2068,
         0.1609,  0.1789,  0.1771,  0.1833,  0.1877,  0.1713,  0.2207,  0.1032,
         0.1105,  0.1826,  0.2119,  0.1048,  0.1798,  0.1724,  0.1775,  0.1839,
         0.1698,  0.1058,  0.1312,  0.2035,  0.1900,  0.1887,  0.1676,  0.1908,
         0.1854,  0.1069,  0.1921,  0.1633,  0.2150,  0.1805,  0.1637,  0.1838,
         0.2078,  0.2319,  0.2229,  0.2184,  0.1853,  0.1654,  0.1896,  0.2031,
         0.1658,  0.1864,  0.1904,  0.2178,  0.1406,  0.1451,  0.2252,  0.2288,
         0.1390,  0.0769,  0.1445,  0.2130,  0.1642,  0.1862,  0.2041,  0.1630],
       device='cuda:0', requires_grad=True) MLP.norm tensor(15.4789, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 12:14:21,150 :: INFO :: mask tensor([0.5824, 0.5547, 0.6486, 0.5819, 0.5760, 0.6112, 0.5837, 0.5909, 0.5636,
        0.5778, 0.6252, 0.5840, 0.5683, 0.5899, 0.5868, 0.5698, 0.5481, 0.5700,
        0.5560, 0.6166, 0.5823, 0.5550, 0.5937, 0.6086, 0.6097, 0.5570, 0.5429,
        0.6043, 0.6583, 0.5574, 0.6047, 0.5607, 0.5936, 0.5739, 0.5692, 0.5597,
        0.5538, 0.6203, 0.5739, 0.5954, 0.5873, 0.5689, 0.5869, 0.5742, 0.5584,
        0.5518, 0.5576, 0.6077, 0.5981, 0.5494, 0.5719, 0.5465, 0.5756, 0.5414,
        0.6373, 0.6092, 0.5833, 0.6713, 0.5925, 0.5903, 0.5461, 0.5633, 0.5674,
        0.5698, 0.5785, 0.5504, 0.5784, 0.5736, 0.5733, 0.5567, 0.5618, 0.5784,
        0.5381, 0.6345, 0.5636, 0.5543, 0.5723, 0.5865, 0.5469, 0.5734, 0.6172,
        0.6413, 0.5768, 0.6212, 0.5599, 0.5826, 0.5687, 0.5913, 0.5672, 0.5364,
        0.5880, 0.6272, 0.5646, 0.5607, 0.5905, 0.5857, 0.5599, 0.5766, 0.5804,
        0.5769, 0.5609, 0.5318, 0.5591, 0.5918, 0.6194, 0.5836, 0.5704, 0.5513,
        0.5680, 0.5801, 0.6310, 0.5565, 0.6206, 0.5703, 0.5772, 0.5577, 0.5692,
        0.6116, 0.5399, 0.6079, 0.5840, 0.5746, 0.5623, 0.5534, 0.5615, 0.5776,
        0.5925, 0.6013, 0.5864, 0.6187, 0.6558, 0.5272, 0.5448, 0.5572, 0.6034,
        0.6368, 0.6868, 0.5344, 0.6101, 0.6285, 0.6051, 0.6528, 0.6107, 0.6907,
        0.6582, 0.6006, 0.5363, 0.7001, 0.6564, 0.6631, 0.6681, 0.6321, 0.6244,
        0.4292, 0.6123, 0.6835, 0.6429, 0.5087, 0.5462, 0.4951, 0.5700, 0.4579,
        0.5722, 0.6506, 0.5817, 0.5147, 0.6102, 0.6296, 0.6538, 0.6055, 0.6119,
        0.5699, 0.4928, 0.6412, 0.6421, 0.6867, 0.5271, 0.5911, 0.6229, 0.5999,
        0.6022, 0.6758, 0.5266, 0.5333, 0.5842, 0.6218, 0.5789, 0.5888, 0.6199,
        0.4626, 0.6171, 0.6453, 0.5757, 0.6118, 0.5316, 0.5949, 0.6537, 0.5155,
        0.6175, 0.6571, 0.6472, 0.6515, 0.6077, 0.5259, 0.6163, 0.6670, 0.6306,
        0.4400, 0.5353, 0.6241, 0.6181, 0.5610, 0.5210, 0.6304, 0.5958, 0.6729,
        0.6254, 0.6110, 0.5926, 0.5319, 0.6201, 0.6077, 0.5870, 0.6311, 0.5390,
        0.6204, 0.5717, 0.6637, 0.5433, 0.6495, 0.5816, 0.5200, 0.5386, 0.5289,
        0.5656, 0.6822, 0.6496, 0.6410, 0.5145, 0.5606, 0.5966, 0.6607, 0.6833,
        0.6229, 0.6679, 0.6562, 0.6031, 0.6274, 0.6402, 0.6570, 0.6590, 0.6265,
        0.6595, 0.6773, 0.6552, 0.6036, 0.7287, 0.7176, 0.7322, 0.6503, 0.6857,
        0.7139, 0.6763, 0.6783, 0.7268, 0.7006, 0.7081, 0.6894, 0.6523, 0.6355,
        0.6647, 0.6888, 0.6974, 0.6973, 0.6633, 0.6835, 0.6972, 0.6992, 0.6788,
        0.6665, 0.6729, 0.7237, 0.6752, 0.7199, 0.7349, 0.6161, 0.6826, 0.6937,
        0.6198, 0.6936, 0.6921, 0.6871, 0.7284, 0.6385, 0.6553, 0.6541, 0.6776,
        0.6888, 0.6002, 0.6492, 0.7181, 0.7326, 0.7266, 0.7200, 0.7128, 0.7261,
        0.6222, 0.6897, 0.6558, 0.7015, 0.7021, 0.7169, 0.7185, 0.6894, 0.6871,
        0.6535, 0.6460, 0.7343, 0.6824, 0.7149, 0.6707, 0.7190, 0.6803, 0.6895,
        0.7094, 0.6807, 0.7219, 0.7068, 0.6609, 0.6789, 0.6771, 0.6833, 0.6877,
        0.6713, 0.7207, 0.6032, 0.6105, 0.6826, 0.7119, 0.6048, 0.6798, 0.6724,
        0.6775, 0.6839, 0.6698, 0.6058, 0.6312, 0.7035, 0.6900, 0.6887, 0.6676,
        0.6908, 0.6854, 0.6069, 0.6921, 0.6633, 0.7150, 0.6805, 0.6637, 0.6838,
        0.7078, 0.7319, 0.7229, 0.7184, 0.6853, 0.6654, 0.6896, 0.7031, 0.6658,
        0.6864, 0.6904, 0.7178, 0.6406, 0.6451, 0.7252, 0.7288, 0.6390, 0.5769,
        0.6445, 0.7130, 0.6642, 0.6862, 0.7041, 0.6630], device='cuda:0')
2023-05-14 12:14:37,354 :: INFO :: Epoch 5: loss tensor(26.0684, device='cuda:0', grad_fn=<AddBackward0>), U.norm 16.09935760498047, V.norm 15.82856559753418, MLP.norm 9.92416000366211
2023-05-14 12:15:25,561 :: INFO :: ----- val -----
2023-05-14 12:15:25,561 :: INFO :: Precision [0.009338349192299181, 0.004762115512281521, 0.004235450320867404]
2023-05-14 12:15:25,561 :: INFO :: Recall [0.0023528026355414773, 0.007615130244056754, 0.013738071254925622]
2023-05-14 12:15:25,561 :: INFO :: ndcg [0.009338349192299181, 0.01607884562847746, 0.02156064917001743]
2023-05-14 12:16:12,877 :: INFO :: ----- test -----
2023-05-14 12:16:12,877 :: INFO :: Precision [0.004912931928598723, 0.0034117582837491466, 0.0030351001692231613]
2023-05-14 12:16:12,877 :: INFO :: Recall [0.0018478783090272236, 0.008630326954666208, 0.015375132344262854]
2023-05-14 12:16:12,877 :: INFO :: ndcg [0.004912931928598723, 0.010675110412375175, 0.014724531006023502]
2023-05-14 12:16:18,722 :: INFO :: Epoch 10: loss tensor(28.8832, device='cuda:0', grad_fn=<AddBackward0>), U.norm 26.428245544433594, V.norm 24.777650833129883, MLP.norm 11.38991928100586
2023-05-14 12:17:06,147 :: INFO :: ----- val -----
2023-05-14 12:17:06,147 :: INFO :: Precision [0.01013498561628679, 0.005709227705244576, 0.00554104890462481]
2023-05-14 12:17:06,147 :: INFO :: Recall [0.0025263819145644397, 0.008595455758216127, 0.01787926445958019]
2023-05-14 12:17:06,147 :: INFO :: ndcg [0.01013498561628679, 0.01825132470956296, 0.02610355196252462]
2023-05-14 12:17:54,511 :: INFO :: ----- test -----
2023-05-14 12:17:54,511 :: INFO :: Precision [0.005486107320268573, 0.0037392870789890497, 0.0037392870789889395]
2023-05-14 12:17:54,511 :: INFO :: Recall [0.0019132216033950278, 0.008609374826453494, 0.01879745902504329]
2023-05-14 12:17:54,511 :: INFO :: ndcg [0.005486107320268573, 0.01147620373626114, 0.01719625770248491]
2023-05-14 12:18:00,371 :: INFO :: Epoch 15: loss tensor(24.7536, device='cuda:0', grad_fn=<AddBackward0>), U.norm 35.78653335571289, V.norm 44.227020263671875, MLP.norm 10.901071548461914
2023-05-14 12:18:47,655 :: INFO :: ----- val -----
2023-05-14 12:18:47,655 :: INFO :: Precision [0.011064394777605666, 0.008771852179685664, 0.007448550564283916]
2023-05-14 12:18:47,655 :: INFO :: Recall [0.0030524938862395885, 0.013535047622153193, 0.025240480679279424]
2023-05-14 12:18:47,655 :: INFO :: ndcg [0.011064394777605666, 0.02539322100631593, 0.034439548132371975]
2023-05-14 12:19:34,925 :: INFO :: ----- test -----
2023-05-14 12:19:34,925 :: INFO :: Precision [0.006195753043288389, 0.005628036464872455, 0.004882908455701578]
2023-05-14 12:19:34,925 :: INFO :: Recall [0.0023893135176795283, 0.01297003248507576, 0.023450450976842317]
2023-05-14 12:19:34,925 :: INFO :: ndcg [0.006195753043288389, 0.01593207448887231, 0.02228235535175434]
2023-05-14 12:19:40,816 :: INFO :: Epoch 20: loss tensor(23.8185, device='cuda:0', grad_fn=<AddBackward0>), U.norm 45.982147216796875, V.norm 75.95059967041016, MLP.norm 10.677145004272461
2023-05-14 12:20:27,522 :: INFO :: ----- val -----
2023-05-14 12:20:27,522 :: INFO :: Precision [0.012923213100243417, 0.011949546359813844, 0.008528435494578173]
2023-05-14 12:20:27,522 :: INFO :: Recall [0.0035528694363295256, 0.020008344732496245, 0.029333886146652815]
2023-05-14 12:20:27,522 :: INFO :: ndcg [0.012923213100243417, 0.034804511355892985, 0.04201929198652642]
2023-05-14 12:21:15,558 :: INFO :: ----- test -----
2023-05-14 12:21:15,558 :: INFO :: Precision [0.007287515694088105, 0.007806102953217775, 0.005557071892570368]
2023-05-14 12:21:15,558 :: INFO :: Recall [0.002883723795329342, 0.018305667758616844, 0.02656972367291736]
2023-05-14 12:21:15,558 :: INFO :: ndcg [0.007287515694088105, 0.022328343580205037, 0.027272704692598355]
2023-05-14 12:21:21,496 :: INFO :: Epoch 25: loss tensor(27.9577, device='cuda:0', grad_fn=<AddBackward0>), U.norm 57.2524528503418, V.norm 112.1878662109375, MLP.norm 10.100847244262695
2023-05-14 12:22:10,140 :: INFO :: ----- val -----
2023-05-14 12:22:10,140 :: INFO :: Precision [0.013764107103341448, 0.013383491922991457, 0.010099579552998094]
2023-05-14 12:22:10,140 :: INFO :: Recall [0.00399409144019753, 0.023289633567889028, 0.03306429441574101]
2023-05-14 12:22:10,140 :: INFO :: ndcg [0.013764107103341448, 0.039051776983519394, 0.048105240554803724]
2023-05-14 12:22:59,316 :: INFO :: ----- test -----
2023-05-14 12:22:59,316 :: INFO :: Precision [0.00788798515202795, 0.008815983405207456, 0.0065887875975760455]
2023-05-14 12:22:59,316 :: INFO :: Recall [0.0031724299321303526, 0.020827727751501993, 0.030018018681559202]
2023-05-14 12:22:59,316 :: INFO :: ndcg [0.00788798515202795, 0.02554107777968613, 0.03156275730043807]
2023-05-14 12:23:05,285 :: INFO :: Epoch 30: loss tensor(25.4099, device='cuda:0', grad_fn=<AddBackward0>), U.norm 68.83224487304688, V.norm 148.90089416503906, MLP.norm 9.417819023132324
2023-05-14 12:23:53,679 :: INFO :: ----- val -----
2023-05-14 12:23:53,679 :: INFO :: Precision [0.01664084974551892, 0.014737773843770313, 0.012139853949988656]
2023-05-14 12:23:53,679 :: INFO :: Recall [0.005274010441897234, 0.025891154541640475, 0.03948513098156097]
2023-05-14 12:23:53,679 :: INFO :: ndcg [0.01664084974551892, 0.04298051816130823, 0.05569616453834198]
2023-05-14 12:24:42,668 :: INFO :: ----- test -----
2023-05-14 12:24:42,668 :: INFO :: Precision [0.009825863857197446, 0.009853157923467129, 0.007696926688137875]
2023-05-14 12:24:42,668 :: INFO :: Recall [0.0042498793421330726, 0.02370213907220158, 0.035166448253388466]
2023-05-14 12:24:42,668 :: INFO :: ndcg [0.009825863857197446, 0.02862221351381193, 0.03634720083751975]
2023-05-14 12:24:48,574 :: INFO :: Epoch 35: loss tensor(29.6210, device='cuda:0', grad_fn=<AddBackward0>), U.norm 80.10918426513672, V.norm 181.88893127441406, MLP.norm 8.972161293029785
2023-05-14 12:25:36,922 :: INFO :: ----- val -----
2023-05-14 12:25:36,922 :: INFO :: Precision [0.01779154680238991, 0.01532197388802787, 0.01310909493250709]
2023-05-14 12:25:36,922 :: INFO :: Recall [0.006142276187917498, 0.027092174520705396, 0.04402857479948786]
2023-05-14 12:25:36,922 :: INFO :: ndcg [0.01779154680238991, 0.04497656618726211, 0.05966990056345469]
2023-05-14 12:26:24,098 :: INFO :: ----- test -----
2023-05-14 12:26:24,098 :: INFO :: Precision [0.01165456629728697, 0.010666521098312872, 0.008422948850919832]
2023-05-14 12:26:24,098 :: INFO :: Recall [0.005678182654750487, 0.02607741274945721, 0.03938136891299702]
2023-05-14 12:26:24,098 :: INFO :: ndcg [0.01165456629728697, 0.0313327312319735, 0.03996326342225962]
2023-05-14 12:26:30,020 :: INFO :: Epoch 40: loss tensor(27.1699, device='cuda:0', grad_fn=<AddBackward0>), U.norm 90.60870361328125, V.norm 208.2252960205078, MLP.norm 8.585088729858398
2023-05-14 12:27:17,742 :: INFO :: ----- val -----
2023-05-14 12:27:17,742 :: INFO :: Precision [0.01960610754591724, 0.016038946669616676, 0.013750829829608317]
2023-05-14 12:27:17,742 :: INFO :: Recall [0.007338842310332195, 0.029248061570743996, 0.046669400628110876]
2023-05-14 12:27:17,742 :: INFO :: ndcg [0.01960610754591724, 0.04759589890985179, 0.06270172423897398]
2023-05-14 12:28:04,652 :: INFO :: ----- test -----
2023-05-14 12:28:04,652 :: INFO :: Precision [0.012882799279436651, 0.011299743435776667, 0.008936077296795818]
2023-05-14 12:28:04,652 :: INFO :: Recall [0.007001745480374016, 0.02801160299730322, 0.04246692096431233]
2023-05-14 12:28:04,652 :: INFO :: ndcg [0.012882799279436651, 0.033735016267457195, 0.04298603759636166]
2023-05-14 12:28:10,559 :: INFO :: Epoch 45: loss tensor(29.0873, device='cuda:0', grad_fn=<AddBackward0>), U.norm 100.16468048095703, V.norm 228.77490234375, MLP.norm 8.294471740722656
2023-05-14 12:28:58,501 :: INFO :: ----- val -----
2023-05-14 12:28:58,501 :: INFO :: Precision [0.01880947112192963, 0.016260234565168775, 0.013905731356494831]
2023-05-14 12:28:58,501 :: INFO :: Recall [0.007101869272960837, 0.029704049692285742, 0.04899631082416939]
2023-05-14 12:28:58,501 :: INFO :: ndcg [0.01880947112192963, 0.04795934108451661, 0.06322783573748672]
2023-05-14 12:29:45,365 :: INFO :: ----- test -----
2023-05-14 12:29:45,365 :: INFO :: Precision [0.012964681478246629, 0.011447131393634624, 0.009361864730607787]
2023-05-14 12:29:45,365 :: INFO :: Recall [0.007104475697486229, 0.028823197009668605, 0.045799176090677574]
2023-05-14 12:29:45,365 :: INFO :: ndcg [0.012964681478246629, 0.03408581564629386, 0.044464191991742015]
2023-05-14 12:29:51,210 :: INFO :: Epoch 50: loss tensor(29.7283, device='cuda:0', grad_fn=<AddBackward0>), U.norm 108.78898620605469, V.norm 244.7303009033203, MLP.norm 8.074718475341797
2023-05-14 12:30:38,088 :: INFO :: ----- val -----
2023-05-14 12:30:38,088 :: INFO :: Precision [0.018676698384598362, 0.01633989820756753, 0.013945563177694215]
2023-05-14 12:30:38,088 :: INFO :: Recall [0.007325483443777766, 0.030086308716332062, 0.0490807198082027]
2023-05-14 12:30:38,088 :: INFO :: ndcg [0.018676698384598362, 0.04797888198026673, 0.06330856320052823]
2023-05-14 12:31:24,889 :: INFO :: ----- test -----
2023-05-14 12:31:24,889 :: INFO :: Precision [0.013783503466346417, 0.011823789508160504, 0.009533817348108771]
2023-05-14 12:31:24,889 :: INFO :: Recall [0.007774538183796439, 0.030126887995299417, 0.04698297367050139]
2023-05-14 12:31:24,889 :: INFO :: ndcg [0.013783503466346417, 0.03541200259559191, 0.04562605689301]
2023-05-14 12:31:30,780 :: INFO :: Epoch 55: loss tensor(33.9348, device='cuda:0', grad_fn=<AddBackward0>), U.norm 116.64077758789062, V.norm 257.45025634765625, MLP.norm 8.007966041564941
2023-05-14 12:32:19,124 :: INFO :: ----- val -----
2023-05-14 12:32:19,124 :: INFO :: Precision [0.018897986280150474, 0.016215976986058356, 0.014016375304270906]
2023-05-14 12:32:19,124 :: INFO :: Recall [0.007322208850563562, 0.029975119148651592, 0.05043465044758203]
2023-05-14 12:32:19,124 :: INFO :: ndcg [0.018897986280150474, 0.047888462556160256, 0.06376388414201926]
2023-05-14 12:33:07,378 :: INFO :: ----- test -----
2023-05-14 12:33:07,378 :: INFO :: Precision [0.014083738195316338, 0.011791036628636514, 0.009522899721600782]
2023-05-14 12:33:07,378 :: INFO :: Recall [0.008109627002153997, 0.030263092616870824, 0.04722365210091014]
2023-05-14 12:33:07,378 :: INFO :: ndcg [0.014083738195316338, 0.035656984398205215, 0.04599177395417582]
2023-05-14 12:33:13,300 :: INFO :: Epoch 60: loss tensor(30.4884, device='cuda:0', grad_fn=<AddBackward0>), U.norm 123.73849487304688, V.norm 266.9542541503906, MLP.norm 7.940290927886963
2023-05-14 12:34:01,741 :: INFO :: ----- val -----
2023-05-14 12:34:01,741 :: INFO :: Precision [0.018366895330825403, 0.016472670944898796, 0.014051781367559248]
2023-05-14 12:34:01,741 :: INFO :: Recall [0.007262085349292629, 0.03070690068757791, 0.05096592381963709]
2023-05-14 12:34:01,741 :: INFO :: ndcg [0.018366895330825403, 0.04808497074457215, 0.06364201431998578]
2023-05-14 12:34:49,980 :: INFO :: ----- test -----
2023-05-14 12:34:49,980 :: INFO :: Precision [0.014111032261586332, 0.011971177466018456, 0.009713958185490753]
2023-05-14 12:34:49,980 :: INFO :: Recall [0.008250326830677942, 0.03144868997882613, 0.04880566134039766]
2023-05-14 12:34:49,980 :: INFO :: ndcg [0.014111032261586332, 0.03620761036203946, 0.04674828128236675]
2023-05-14 12:34:55,933 :: INFO :: Epoch 65: loss tensor(30.6464, device='cuda:0', grad_fn=<AddBackward0>), U.norm 130.26206970214844, V.norm 273.68499755859375, MLP.norm 7.739185333251953
2023-05-14 12:35:44,062 :: INFO :: ----- val -----
2023-05-14 12:35:44,062 :: INFO :: Precision [0.01845541048904625, 0.0165257800398313, 0.014224385926089916]
2023-05-14 12:35:44,062 :: INFO :: Recall [0.0073909018084034024, 0.0310885714971381, 0.05178049634520982]
2023-05-14 12:35:44,062 :: INFO :: ndcg [0.01845541048904625, 0.04827019307249074, 0.06411123651713332]
2023-05-14 12:36:32,111 :: INFO :: ----- test -----
2023-05-14 12:36:32,111 :: INFO :: Precision [0.014329384791746275, 0.012107647797368414, 0.009869534363229756]
2023-05-14 12:36:32,111 :: INFO :: Recall [0.00838142789868375, 0.03182126192790356, 0.049928944443001395]
2023-05-14 12:36:32,111 :: INFO :: ndcg [0.014329384791746275, 0.036497517013362527, 0.04740028528014]
2023-05-14 12:36:38,017 :: INFO :: Epoch 70: loss tensor(32.5415, device='cuda:0', grad_fn=<AddBackward0>), U.norm 136.26869201660156, V.norm 279.26251220703125, MLP.norm 7.544641494750977
2023-05-14 12:37:25,800 :: INFO :: ----- val -----
2023-05-14 12:37:25,800 :: INFO :: Precision [0.018543925647267093, 0.01668510732462881, 0.014277495021022438]
2023-05-14 12:37:25,800 :: INFO :: Recall [0.007420592538328831, 0.0314214442703197, 0.05214801102174527]
2023-05-14 12:37:25,800 :: INFO :: ndcg [0.018543925647267093, 0.04857179267011739, 0.06457286302968364]
2023-05-14 12:38:14,036 :: INFO :: ----- test -----
2023-05-14 12:38:14,036 :: INFO :: Precision [0.01465691358698619, 0.012222282875702376, 0.009864075549975749]
2023-05-14 12:38:14,036 :: INFO :: Recall [0.008434981354201733, 0.0323607744217555, 0.050444416532860484]
2023-05-14 12:38:14,036 :: INFO :: ndcg [0.01465691358698619, 0.03691701646272238, 0.04775000393629179]
2023-05-14 12:38:19,926 :: INFO :: Epoch 75: loss tensor(31.7143, device='cuda:0', grad_fn=<AddBackward0>), U.norm 141.89004516601562, V.norm 283.301025390625, MLP.norm 7.461506366729736
2023-05-14 12:39:07,865 :: INFO :: ----- val -----
2023-05-14 12:39:07,865 :: INFO :: Precision [0.018720955963708784, 0.016419561849966288, 0.014219960168178884]
2023-05-14 12:39:07,865 :: INFO :: Recall [0.007477449920803147, 0.03127834689356181, 0.05226560243473311]
2023-05-14 12:39:07,865 :: INFO :: ndcg [0.018720955963708784, 0.04819624640197385, 0.0643878594083383]
2023-05-14 12:39:56,148 :: INFO :: ----- test -----
2023-05-14 12:39:56,148 :: INFO :: Precision [0.014629619520716196, 0.012446094219116305, 0.009918663682515743]
2023-05-14 12:39:56,148 :: INFO :: Recall [0.008664495526448135, 0.033406412805712465, 0.051074726803390307]
2023-05-14 12:39:56,148 :: INFO :: ndcg [0.014629619520716196, 0.03752294393293048, 0.048001205329713095]
2023-05-14 12:40:02,070 :: INFO :: Epoch 80: loss tensor(33.3203, device='cuda:0', grad_fn=<AddBackward0>), U.norm 146.95562744140625, V.norm 285.8818359375, MLP.norm 7.230743408203125
2023-05-14 12:40:50,416 :: INFO :: ----- val -----
2023-05-14 12:40:50,416 :: INFO :: Precision [0.018897986280150474, 0.016446116397432542, 0.014153573799513222]
2023-05-14 12:40:50,416 :: INFO :: Recall [0.007668673164460915, 0.03167170000598994, 0.05223112037159099]
2023-05-14 12:40:50,416 :: INFO :: ndcg [0.018897986280150474, 0.04820151442568478, 0.06409353716101078]
2023-05-14 12:41:38,777 :: INFO :: ----- test -----
2023-05-14 12:41:38,777 :: INFO :: Precision [0.014875266117146133, 0.012402423713084317, 0.009962334188547741]
2023-05-14 12:41:38,777 :: INFO :: Recall [0.008785146012718697, 0.03328664572587418, 0.05165016879771134]
2023-05-14 12:41:38,777 :: INFO :: ndcg [0.014875266117146133, 0.03746574620731486, 0.04820526054180195]
2023-05-14 12:41:44,684 :: INFO :: Epoch 85: loss tensor(32.7734, device='cuda:0', grad_fn=<AddBackward0>), U.norm 151.76527404785156, V.norm 288.0067138671875, MLP.norm 7.184187889099121
2023-05-14 12:42:33,248 :: INFO :: ----- val -----
2023-05-14 12:42:33,248 :: INFO :: Precision [0.01876521354281921, 0.0163576012392117, 0.014153573799513238]
2023-05-14 12:42:33,248 :: INFO :: Recall [0.007584111476648342, 0.031710290344282535, 0.052931594432033664]
2023-05-14 12:42:33,248 :: INFO :: ndcg [0.01876521354281921, 0.047786268783053296, 0.06396327518527853]
2023-05-14 12:43:21,952 :: INFO :: ----- test -----
2023-05-14 12:43:21,952 :: INFO :: Precision [0.015202794912386047, 0.012517058791418283, 0.010090616300016722]
2023-05-14 12:43:21,952 :: INFO :: Recall [0.00898946558487178, 0.034049233954344824, 0.05280739305354222]
2023-05-14 12:43:21,952 :: INFO :: ndcg [0.015202794912386047, 0.037928867485280286, 0.048878823469095986]
2023-05-14 12:43:27,937 :: INFO :: Epoch 90: loss tensor(34.7050, device='cuda:0', grad_fn=<AddBackward0>), U.norm 156.3316192626953, V.norm 289.3402404785156, MLP.norm 7.2395477294921875
2023-05-14 12:44:16,110 :: INFO :: ----- val -----
2023-05-14 12:44:16,110 :: INFO :: Precision [0.018411152909935828, 0.016419561849966288, 0.01404292985173716]
2023-05-14 12:44:16,110 :: INFO :: Recall [0.007429134030486813, 0.03233018727199971, 0.05302147061436454]
2023-05-14 12:44:16,110 :: INFO :: ndcg [0.018411152909935828, 0.047700511447795794, 0.06343409292496069]
2023-05-14 12:45:04,331 :: INFO :: ----- test -----
2023-05-14 12:45:04,331 :: INFO :: Precision [0.015120912713576068, 0.012527976417926278, 0.010033298760849728]
2023-05-14 12:45:04,331 :: INFO :: Recall [0.009019664728134183, 0.03422030504780281, 0.05262457475436962]
2023-05-14 12:45:04,331 :: INFO :: ndcg [0.015120912713576068, 0.03783042488654902, 0.048557143468213104]
2023-05-14 12:45:10,222 :: INFO :: Epoch 95: loss tensor(36.8931, device='cuda:0', grad_fn=<AddBackward0>), U.norm 160.579345703125, V.norm 290.30706787109375, MLP.norm 7.010988712310791
2023-05-14 12:45:58,770 :: INFO :: ----- val -----
2023-05-14 12:45:58,770 :: INFO :: Precision [0.01876521354281921, 0.016295640628457113, 0.014029652578004039]
2023-05-14 12:45:58,770 :: INFO :: Recall [0.007629711055428088, 0.03199153158639819, 0.052829102455234735]
2023-05-14 12:45:58,770 :: INFO :: ndcg [0.01876521354281921, 0.04752787272054257, 0.06353309913066331]
2023-05-14 12:46:47,772 :: INFO :: ----- test -----
2023-05-14 12:46:47,772 :: INFO :: Precision [0.015557617773895955, 0.012577105737212263, 0.010085157486762728]
2023-05-14 12:46:47,772 :: INFO :: Recall [0.009402918908675333, 0.0346351953163447, 0.05331768899760997]
2023-05-14 12:46:47,772 :: INFO :: ndcg [0.015557617773895955, 0.038172036313097804, 0.04900986151251259]
2023-05-14 12:46:53,710 :: INFO :: Epoch 100: loss tensor(33.9674, device='cuda:0', grad_fn=<AddBackward0>), U.norm 164.70639038085938, V.norm 290.9314880371094, MLP.norm 7.0593180656433105
2023-05-14 12:47:42,227 :: INFO :: ----- val -----
2023-05-14 12:47:42,227 :: INFO :: Precision [0.01832263775171498, 0.01661429519805214, 0.01396769196724945]
2023-05-14 12:47:42,227 :: INFO :: Recall [0.0077265574629102884, 0.03259392794328535, 0.052735230333355164]
2023-05-14 12:47:42,227 :: INFO :: ndcg [0.01832263775171498, 0.04801997059557003, 0.06344353730706441]
2023-05-14 12:48:31,557 :: INFO :: ----- test -----
2023-05-14 12:48:31,557 :: INFO :: Precision [0.015967028767945847, 0.012598940990228255, 0.010060592827119725]
2023-05-14 12:48:31,557 :: INFO :: Recall [0.00967098781428972, 0.034732546248211704, 0.053381935961848835]
2023-05-14 12:48:31,557 :: INFO :: ndcg [0.015967028767945847, 0.03839245963473408, 0.04914535628782143]
2023-05-14 12:48:37,541 :: INFO :: Epoch 105: loss tensor(36.0256, device='cuda:0', grad_fn=<AddBackward0>), U.norm 168.6316375732422, V.norm 291.4923400878906, MLP.norm 6.788379192352295
2023-05-14 12:49:27,043 :: INFO :: ----- val -----
2023-05-14 12:49:27,043 :: INFO :: Precision [0.01832263775171498, 0.016454967913254626, 0.013870325293206491]
2023-05-14 12:49:27,043 :: INFO :: Recall [0.007839205056390446, 0.03240964072552543, 0.05261658739778408]
2023-05-14 12:49:27,043 :: INFO :: ndcg [0.01832263775171498, 0.04766100045942771, 0.06318055848885325]
2023-05-14 12:50:16,498 :: INFO :: ----- test -----
2023-05-14 12:50:16,498 :: INFO :: Precision [0.015857852502865876, 0.01262623505649825, 0.01002511054096873]
2023-05-14 12:50:16,498 :: INFO :: Recall [0.009611297306157698, 0.03481403248007461, 0.05329493326515562]
2023-05-14 12:50:16,498 :: INFO :: ndcg [0.015857852502865876, 0.03848102559142141, 0.04913808094772592]
2023-05-14 12:50:22,482 :: INFO :: Epoch 110: loss tensor(34.6921, device='cuda:0', grad_fn=<AddBackward0>), U.norm 172.35653686523438, V.norm 291.6992492675781, MLP.norm 6.85331392288208
2023-05-14 12:51:11,484 :: INFO :: ----- val -----
2023-05-14 12:51:11,500 :: INFO :: Precision [0.01916353175481301, 0.01633989820756753, 0.013985394998893626]
2023-05-14 12:51:11,500 :: INFO :: Recall [0.007886360000625026, 0.03211528442815573, 0.052908849098198894]
2023-05-14 12:51:11,500 :: INFO :: ndcg [0.01916353175481301, 0.047948286523972636, 0.06385989583740191]
2023-05-14 12:52:01,017 :: INFO :: ----- test -----
2023-05-14 12:52:01,017 :: INFO :: Precision [0.015803264370325892, 0.01275178776134021, 0.010098804519897727]
2023-05-14 12:52:01,017 :: INFO :: Recall [0.009510349579147299, 0.035315966692910074, 0.05373515407419199]
2023-05-14 12:52:01,017 :: INFO :: ndcg [0.015803264370325892, 0.03884663072195088, 0.049409548928958144]
2023-05-14 12:52:06,986 :: INFO :: Epoch 115: loss tensor(32.9871, device='cuda:0', grad_fn=<AddBackward0>), U.norm 175.98281860351562, V.norm 291.5450744628906, MLP.norm 6.818873882293701
2023-05-14 12:52:56,175 :: INFO :: ----- val -----
2023-05-14 12:52:56,175 :: INFO :: Precision [0.01929630449214428, 0.01634874972338962, 0.013786235892896676]
2023-05-14 12:52:56,175 :: INFO :: Recall [0.007951440261260672, 0.03234082668307685, 0.052507812225182984]
2023-05-14 12:52:56,175 :: INFO :: ndcg [0.01929630449214428, 0.048050423359272953, 0.06355011424550641]
2023-05-14 12:53:45,317 :: INFO :: ----- test -----
2023-05-14 12:53:45,317 :: INFO :: Precision [0.015830558436595882, 0.012806375893880192, 0.00998144003493673]
2023-05-14 12:53:45,317 :: INFO :: Recall [0.009637166392363285, 0.03554949117869756, 0.05335127484014355]
2023-05-14 12:53:45,317 :: INFO :: ndcg [0.015830558436595882, 0.0390381067769732, 0.04915268978229526]
2023-05-14 12:53:51,286 :: INFO :: Epoch 120: loss tensor(31.2197, device='cuda:0', grad_fn=<AddBackward0>), U.norm 179.47352600097656, V.norm 291.435302734375, MLP.norm 6.945755958557129
2023-05-14 12:54:40,522 :: INFO :: ----- val -----
2023-05-14 12:54:40,522 :: INFO :: Precision [0.019207789333923433, 0.01637530427085587, 0.01393671166187215]
2023-05-14 12:54:40,522 :: INFO :: Recall [0.007765309496013317, 0.03248201575131448, 0.05310643959809557]
2023-05-14 12:54:40,522 :: INFO :: ndcg [0.019207789333923433, 0.04782920540542876, 0.06381363311874576]
2023-05-14 12:55:29,837 :: INFO :: ----- test -----
2023-05-14 12:55:29,837 :: INFO :: Precision [0.016212675364375786, 0.012757246574594207, 0.01003602816747674]
2023-05-14 12:55:29,837 :: INFO :: Recall [0.009751065846308271, 0.03536841587126678, 0.05366894864550531]
2023-05-14 12:55:29,837 :: INFO :: ndcg [0.016212675364375786, 0.03901314665135872, 0.0494934785818006]
2023-05-14 12:55:35,774 :: INFO :: Epoch 125: loss tensor(31.8651, device='cuda:0', grad_fn=<AddBackward0>), U.norm 182.7876434326172, V.norm 291.0489501953125, MLP.norm 6.881154537200928
2023-05-14 12:56:24,916 :: INFO :: ----- val -----
2023-05-14 12:56:24,916 :: INFO :: Precision [0.020004425757911042, 0.016490373976542967, 0.013927860146050066]
2023-05-14 12:56:24,916 :: INFO :: Recall [0.00793960756154838, 0.03267235769016279, 0.05281594285227801]
2023-05-14 12:56:24,916 :: INFO :: ndcg [0.020004425757911042, 0.0487360616181553, 0.0644617246602649]
2023-05-14 12:57:14,153 :: INFO :: ----- test -----
2023-05-14 12:57:14,153 :: INFO :: Precision [0.016431027894535728, 0.012855505213166177, 0.010134286806048729]
2023-05-14 12:57:14,153 :: INFO :: Recall [0.009902017196151483, 0.03580069825671257, 0.054286981797569994]
2023-05-14 12:57:14,153 :: INFO :: ndcg [0.016431027894535728, 0.0394168730989114, 0.04994116762953988]
2023-05-14 12:57:20,137 :: INFO :: Epoch 130: loss tensor(32.6927, device='cuda:0', grad_fn=<AddBackward0>), U.norm 186.12799072265625, V.norm 290.8816223144531, MLP.norm 6.8310627937316895
2023-05-14 12:58:08,873 :: INFO :: ----- val -----
2023-05-14 12:58:08,873 :: INFO :: Precision [0.019517592387696393, 0.016375304270855876, 0.014175702589068471]
2023-05-14 12:58:08,873 :: INFO :: Recall [0.007882096188024875, 0.03248674558733284, 0.05359784652038423]
2023-05-14 12:58:08,873 :: INFO :: ndcg [0.019517592387696393, 0.04831008578463496, 0.06481071179584418]
2023-05-14 12:58:57,766 :: INFO :: ----- test -----
2023-05-14 12:58:57,766 :: INFO :: Precision [0.015967028767945847, 0.012762705387848206, 0.010175227905453731]
2023-05-14 12:58:57,766 :: INFO :: Recall [0.009698026646192588, 0.03561897143969408, 0.05437351326098366]
2023-05-14 12:58:57,766 :: INFO :: ndcg [0.015967028767945847, 0.03905533838299362, 0.04990112782782993]
2023-05-14 12:59:03,734 :: INFO :: Epoch 135: loss tensor(36.4749, device='cuda:0', grad_fn=<AddBackward0>), U.norm 189.343505859375, V.norm 290.4029235839844, MLP.norm 6.846052169799805
2023-05-14 12:59:52,939 :: INFO :: ----- val -----
2023-05-14 12:59:52,939 :: INFO :: Precision [0.019827395441469352, 0.016127461827837516, 0.013834919229918165]
2023-05-14 12:59:52,939 :: INFO :: Recall [0.007868066270600143, 0.0320012985244978, 0.0534308719124416]
2023-05-14 12:59:52,939 :: INFO :: ndcg [0.019827395441469352, 0.047663671817889705, 0.06383653477431359]
2023-05-14 13:00:42,535 :: INFO :: ----- test -----
2023-05-14 13:00:42,535 :: INFO :: Precision [0.016021616900485834, 0.012533435231180277, 0.010382662809105726]
2023-05-14 13:00:42,535 :: INFO :: Recall [0.009750868340226487, 0.0353019933208797, 0.05665865152621325]
2023-05-14 13:00:42,535 :: INFO :: ndcg [0.016021616900485834, 0.038502860074175464, 0.050377190991349526]
2023-05-14 13:00:48,472 :: INFO :: Epoch 140: loss tensor(34.5267, device='cuda:0', grad_fn=<AddBackward0>), U.norm 192.46481323242188, V.norm 289.85626220703125, MLP.norm 6.7647271156311035
2023-05-14 13:01:37,490 :: INFO :: ----- val -----
2023-05-14 13:01:37,490 :: INFO :: Precision [0.01947333480858597, 0.015764549679132073, 0.013626908608099128]
2023-05-14 13:01:37,490 :: INFO :: Recall [0.007647346319102387, 0.03148857387363453, 0.05319685656504095]
2023-05-14 13:01:37,490 :: INFO :: ndcg [0.01947333480858597, 0.046792935829318566, 0.06298044061398146]
2023-05-14 13:02:26,616 :: INFO :: ----- test -----
2023-05-14 13:02:26,616 :: INFO :: Precision [0.015912440635405863, 0.012298706261258348, 0.010338992303073726]
2023-05-14 13:02:26,616 :: INFO :: Recall [0.009606885248211296, 0.03481421144971349, 0.05644852137556219]
2023-05-14 13:02:26,616 :: INFO :: ndcg [0.015912440635405863, 0.038125217392348726, 0.050132003336325746]
2023-05-14 13:02:32,601 :: INFO :: Epoch 145: loss tensor(33.0282, device='cuda:0', grad_fn=<AddBackward0>), U.norm 195.56687927246094, V.norm 289.1226501464844, MLP.norm 6.70641565322876
2023-05-14 13:03:21,603 :: INFO :: ----- val -----
2023-05-14 13:03:21,603 :: INFO :: Precision [0.020668289444567382, 0.016145164859481687, 0.013609205576454968]
2023-05-14 13:03:21,603 :: INFO :: Recall [0.007994898125494646, 0.032362911919805185, 0.052918048412147416]
2023-05-14 13:03:21,603 :: INFO :: ndcg [0.020668289444567382, 0.0480569276358778, 0.0635088673708574]
2023-05-14 13:04:10,932 :: INFO :: ----- test -----
2023-05-14 13:04:10,932 :: INFO :: Precision [0.01662208635842568, 0.012522517604672276, 0.010199792565096726]
2023-05-14 13:04:10,932 :: INFO :: Recall [0.009872741060783312, 0.035128915050301975, 0.05598571187522577]
2023-05-14 13:04:10,932 :: INFO :: ndcg [0.01662208635842568, 0.038811536834544205, 0.049986710754202725]
2023-05-14 13:04:16,886 :: INFO :: Epoch 150: loss tensor(33.4227, device='cuda:0', grad_fn=<AddBackward0>), U.norm 198.5771026611328, V.norm 288.6581726074219, MLP.norm 6.798961639404297
2023-05-14 13:05:06,450 :: INFO :: ----- val -----
2023-05-14 13:05:06,450 :: INFO :: Precision [0.020358486390794423, 0.016295640628457116, 0.013706572250497907]
2023-05-14 13:05:06,450 :: INFO :: Recall [0.00785095765481835, 0.032244822130401814, 0.05316387782674108]
2023-05-14 13:05:06,450 :: INFO :: ndcg [0.020358486390794423, 0.04847370238613977, 0.06393524526687888]
2023-05-14 13:05:56,123 :: INFO :: ----- test -----
2023-05-14 13:05:56,123 :: INFO :: Precision [0.016976909219935585, 0.01266990556253023, 0.010265298324144717]
2023-05-14 13:05:56,123 :: INFO :: Recall [0.010107849114959, 0.035704219791142834, 0.05595001866071478]
2023-05-14 13:05:56,123 :: INFO :: ndcg [0.016976909219935585, 0.039375950981297876, 0.050512245061742345]
2023-05-14 13:06:02,061 :: INFO :: Epoch 155: loss tensor(31.0010, device='cuda:0', grad_fn=<AddBackward0>), U.norm 201.4798126220703, V.norm 288.16192626953125, MLP.norm 6.706243991851807
2023-05-14 13:06:51,328 :: INFO :: ----- val -----
2023-05-14 13:06:51,328 :: INFO :: Precision [0.019915910599690195, 0.01641071033414421, 0.013861473777384427]
2023-05-14 13:06:51,328 :: INFO :: Recall [0.007854968939769499, 0.03274249175616069, 0.053869623049889556]
2023-05-14 13:06:51,328 :: INFO :: ndcg [0.019915910599690195, 0.048802480129227124, 0.06478159980983104]
2023-05-14 13:07:40,471 :: INFO :: ----- test -----
2023-05-14 13:07:40,471 :: INFO :: Precision [0.016649380424695673, 0.012877340466182167, 0.01031169823680372]
2023-05-14 13:07:40,471 :: INFO :: Recall [0.010059507908650976, 0.036070710468119584, 0.056480524075821416]
2023-05-14 13:07:40,471 :: INFO :: ndcg [0.016649380424695673, 0.039930049138988885, 0.0509955446035641]
2023-05-14 13:07:46,440 :: INFO :: Epoch 160: loss tensor(33.5486, device='cuda:0', grad_fn=<AddBackward0>), U.norm 204.31349182128906, V.norm 287.60888671875, MLP.norm 6.76254415512085
2023-05-14 13:08:35,691 :: INFO :: ----- val -----
2023-05-14 13:08:35,691 :: INFO :: Precision [0.020402743969904845, 0.016437264881610465, 0.013919008630227986]
2023-05-14 13:08:35,691 :: INFO :: Recall [0.008130799770552217, 0.032607919172707875, 0.053814313931518226]
2023-05-14 13:08:35,691 :: INFO :: ndcg [0.020402743969904845, 0.04893154781856213, 0.0651864334030249]
2023-05-14 13:09:25,380 :: INFO :: ----- test -----
2023-05-14 13:09:25,380 :: INFO :: Precision [0.01645832196080572, 0.01270811725530822, 0.010317157050057717]
2023-05-14 13:09:25,380 :: INFO :: Recall [0.009761146938992446, 0.03552738426905343, 0.056201763669244745]
2023-05-14 13:09:25,380 :: INFO :: ndcg [0.01645832196080572, 0.03935348556260428, 0.050752512608332294]
2023-05-14 13:09:31,334 :: INFO :: Epoch 165: loss tensor(37.0556, device='cuda:0', grad_fn=<AddBackward0>), U.norm 207.15792846679688, V.norm 287.0096740722656, MLP.norm 6.745105266571045
2023-05-14 13:10:20,726 :: INFO :: ----- val -----
2023-05-14 13:10:20,726 :: INFO :: Precision [0.01978313786235893, 0.016508077008187138, 0.013998672272626765]
2023-05-14 13:10:20,726 :: INFO :: Recall [0.007962474253228958, 0.0326682990238388, 0.053898623814628274]
2023-05-14 13:10:20,726 :: INFO :: ndcg [0.01978313786235893, 0.04873928577282825, 0.06502476205938375]
2023-05-14 13:11:10,445 :: INFO :: ----- test -----
2023-05-14 13:11:10,445 :: INFO :: Precision [0.0161580872318358, 0.012915552158960154, 0.010420874501883717]
2023-05-14 13:11:10,445 :: INFO :: Recall [0.009806360367168919, 0.036445731654645534, 0.05706169511282172]
2023-05-14 13:11:10,445 :: INFO :: ndcg [0.0161580872318358, 0.039706981945920004, 0.05101445263641642]
2023-05-14 13:11:16,414 :: INFO :: Epoch 170: loss tensor(32.7084, device='cuda:0', grad_fn=<AddBackward0>), U.norm 209.951904296875, V.norm 286.3734130859375, MLP.norm 6.718438148498535
2023-05-14 13:12:05,758 :: INFO :: ----- val -----
2023-05-14 13:12:05,758 :: INFO :: Precision [0.019340562071254702, 0.016791325514493822, 0.01396769196724946]
2023-05-14 13:12:05,758 :: INFO :: Recall [0.007788257081951649, 0.033207439585465647, 0.05421396773000765]
2023-05-14 13:12:05,758 :: INFO :: ndcg [0.019340562071254702, 0.04900176818721407, 0.06480942643642044]
2023-05-14 13:12:54,935 :: INFO :: ----- test -----
2023-05-14 13:12:54,935 :: INFO :: Precision [0.01588514656913587, 0.012910093345706156, 0.010480921447677724]
2023-05-14 13:12:54,935 :: INFO :: Recall [0.009677492398216884, 0.03633238161111613, 0.05791044663677712]
2023-05-14 13:12:54,935 :: INFO :: ndcg [0.01588514656913587, 0.0394719939542831, 0.05109054239798057]
2023-05-14 13:13:00,841 :: INFO :: Epoch 175: loss tensor(30.9666, device='cuda:0', grad_fn=<AddBackward0>), U.norm 212.69166564941406, V.norm 285.72564697265625, MLP.norm 6.634982109069824
2023-05-14 13:13:49,654 :: INFO :: ----- val -----
2023-05-14 13:13:49,654 :: INFO :: Precision [0.019030759017481743, 0.01646381942907671, 0.01382164195618504]
2023-05-14 13:13:49,654 :: INFO :: Recall [0.007938923513971383, 0.03314816006575714, 0.05409881013682289]
2023-05-14 13:13:49,654 :: INFO :: ndcg [0.019030759017481743, 0.04827142002045738, 0.06407104445949238]
2023-05-14 13:14:37,952 :: INFO :: ----- test -----
2023-05-14 13:14:37,952 :: INFO :: Precision [0.016185381298105792, 0.012959222664992139, 0.010360827556089719]
2023-05-14 13:14:37,952 :: INFO :: Recall [0.009872140016815238, 0.0366581030748117, 0.057593281639992326]
2023-05-14 13:14:37,952 :: INFO :: ndcg [0.016185381298105792, 0.0395826712520176, 0.05076430574826687]
2023-05-14 13:14:43,859 :: INFO :: Epoch 180: loss tensor(34.3698, device='cuda:0', grad_fn=<AddBackward0>), U.norm 215.41793823242188, V.norm 285.16229248046875, MLP.norm 6.814265251159668
2023-05-14 13:15:32,343 :: INFO :: ----- val -----
2023-05-14 13:15:32,343 :: INFO :: Precision [0.017526001327727372, 0.01655233458729754, 0.013949988935605272]
2023-05-14 13:15:32,343 :: INFO :: Recall [0.007561682006875882, 0.03343758398516006, 0.05404247278120547]
2023-05-14 13:15:32,343 :: INFO :: ndcg [0.017526001327727372, 0.04809236050560077, 0.06404294889078006]
2023-05-14 13:16:21,063 :: INFO :: ----- test -----
2023-05-14 13:16:21,063 :: INFO :: Precision [0.016895027021125608, 0.013068398930072106, 0.010478192041050709]
2023-05-14 13:16:21,063 :: INFO :: Recall [0.010517604336109253, 0.036971935779267016, 0.0573492261161732]
2023-05-14 13:16:21,063 :: INFO :: ndcg [0.016895027021125608, 0.04010682649072581, 0.05144829977801885]
2023-05-14 13:16:26,938 :: INFO :: Epoch 185: loss tensor(31.7751, device='cuda:0', grad_fn=<AddBackward0>), U.norm 218.02134704589844, V.norm 284.6831970214844, MLP.norm 6.694690227508545
2023-05-14 13:17:15,595 :: INFO :: ----- val -----
2023-05-14 13:17:15,595 :: INFO :: Precision [0.019075016596592165, 0.01678247399867173, 0.013976543483071526]
2023-05-14 13:17:15,595 :: INFO :: Recall [0.008031697988329893, 0.033441221149212656, 0.053692553460628636]
2023-05-14 13:17:15,595 :: INFO :: ndcg [0.019075016596592165, 0.04894783669903996, 0.0645215207304342]
2023-05-14 13:18:04,580 :: INFO :: ----- test -----
2023-05-14 13:18:04,580 :: INFO :: Precision [0.01700420328620558, 0.01298651673126213, 0.010464545007915707]
2023-05-14 13:18:04,580 :: INFO :: Recall [0.010284015080534628, 0.036270259774165294, 0.05712324726001543]
2023-05-14 13:18:04,580 :: INFO :: ndcg [0.01700420328620558, 0.039843802337289104, 0.051306900916736016]
2023-05-14 13:18:10,456 :: INFO :: Epoch 190: loss tensor(38.3557, device='cuda:0', grad_fn=<AddBackward0>), U.norm 220.58734130859375, V.norm 283.7722473144531, MLP.norm 6.829655647277832
2023-05-14 13:18:59,065 :: INFO :: ----- val -----
2023-05-14 13:18:59,065 :: INFO :: Precision [0.019827395441469352, 0.016755919451205477, 0.013936711661872134]
2023-05-14 13:18:59,065 :: INFO :: Recall [0.008122247599655434, 0.03309895489350642, 0.05330047293966182]
2023-05-14 13:18:59,065 :: INFO :: ndcg [0.019827395441469352, 0.049451373775508324, 0.06483464399602189]
2023-05-14 13:19:47,957 :: INFO :: ----- test -----
2023-05-14 13:19:47,957 :: INFO :: Precision [0.016649380424695673, 0.013095692996342096, 0.010453627381407711]
2023-05-14 13:19:47,957 :: INFO :: Recall [0.009983009550142325, 0.03649395961189282, 0.05660499277538349]
2023-05-14 13:19:47,957 :: INFO :: ndcg [0.016649380424695673, 0.040110590428179105, 0.051387426923247236]
2023-05-14 13:19:53,863 :: INFO :: Epoch 195: loss tensor(32.6230, device='cuda:0', grad_fn=<AddBackward0>), U.norm 223.10293579101562, V.norm 283.2586975097656, MLP.norm 6.845134735107422
2023-05-14 13:20:42,598 :: INFO :: ----- val -----
2023-05-14 13:20:42,598 :: INFO :: Precision [0.021420668289444566, 0.016906395220180902, 0.013857048019473359]
2023-05-14 13:20:42,598 :: INFO :: Recall [0.008469769589558786, 0.033299292932831584, 0.05331346914525574]
2023-05-14 13:20:42,598 :: INFO :: ndcg [0.021420668289444566, 0.050949216582783695, 0.06599668342224466]
2023-05-14 13:21:31,318 :: INFO :: ----- test -----
2023-05-14 13:21:31,318 :: INFO :: Precision [0.017359026147715485, 0.01325945739396205, 0.010562803646487706]
2023-05-14 13:21:31,318 :: INFO :: Recall [0.010088207737591183, 0.036858931202897814, 0.05744431436232267]
2023-05-14 13:21:31,318 :: INFO :: ndcg [0.017359026147715485, 0.04087436582257557, 0.05224418198008874]
2023-05-14 13:21:37,193 :: INFO :: Epoch 200: loss tensor(34.7963, device='cuda:0', grad_fn=<AddBackward0>), U.norm 225.5626983642578, V.norm 282.5979919433594, MLP.norm 6.851869106292725
2023-05-14 13:22:25,943 :: INFO :: ----- val -----
2023-05-14 13:22:25,943 :: INFO :: Precision [0.020668289444567382, 0.017012613410045917, 0.014197831378623682]
2023-05-14 13:22:25,943 :: INFO :: Recall [0.008337664307435398, 0.03332076724468788, 0.05406109022321586]
2023-05-14 13:22:25,943 :: INFO :: ndcg [0.020668289444567382, 0.05040296432605328, 0.06598716066593349]
2023-05-14 13:23:14,741 :: INFO :: ----- test -----
2023-05-14 13:23:14,741 :: INFO :: Precision [0.017823025274305366, 0.013161198755390076, 0.010614662372400708]
2023-05-14 13:23:14,741 :: INFO :: Recall [0.01048983972819562, 0.036779145921955254, 0.05715909917269824]
2023-05-14 13:23:14,741 :: INFO :: ndcg [0.017823025274305366, 0.040841236069197005, 0.05242663472139603]
2023-05-14 13:23:20,647 :: INFO :: Epoch 205: loss tensor(40.8502, device='cuda:0', grad_fn=<AddBackward0>), U.norm 227.99020385742188, V.norm 282.0909729003906, MLP.norm 6.9534220695495605
2023-05-14 13:24:09,055 :: INFO :: ----- val -----
2023-05-14 13:24:09,055 :: INFO :: Precision [0.020845319761009073, 0.016888692188536746, 0.014255366231467248]
2023-05-14 13:24:09,055 :: INFO :: Recall [0.008041244504726527, 0.03324800840240858, 0.05472200462386895]
2023-05-14 13:24:09,055 :: INFO :: ndcg [0.020845319761009073, 0.05018925087256019, 0.06643529780365366]
2023-05-14 13:24:57,399 :: INFO :: ----- test -----
2023-05-14 13:24:57,399 :: INFO :: Precision [0.016949615153665592, 0.013139363502374083, 0.010756591517004685]
2023-05-14 13:24:57,399 :: INFO :: Recall [0.009947625400551188, 0.036667605133044005, 0.05821841369308885]
2023-05-14 13:24:57,399 :: INFO :: ndcg [0.016949615153665592, 0.04071198760575856, 0.05257817732732067]
2023-05-14 13:25:03,274 :: INFO :: Epoch 210: loss tensor(34.5263, device='cuda:0', grad_fn=<AddBackward0>), U.norm 230.4107208251953, V.norm 281.7979736328125, MLP.norm 6.845514297485352
2023-05-14 13:25:51,650 :: INFO :: ----- val -----
2023-05-14 13:25:51,650 :: INFO :: Precision [0.020845319761009073, 0.016941801283469247, 0.014273069263111421]
2023-05-14 13:25:51,650 :: INFO :: Recall [0.00833421132700098, 0.03329938204141027, 0.05499890066555586]
2023-05-14 13:25:51,650 :: INFO :: ndcg [0.020845319761009073, 0.05022127472345302, 0.06635830083828233]
2023-05-14 13:26:40,182 :: INFO :: ----- test -----
2023-05-14 13:26:40,182 :: INFO :: Precision [0.016976909219935585, 0.013215786887930061, 0.01092854413450569]
2023-05-14 13:26:40,182 :: INFO :: Recall [0.01019021459040824, 0.03716636905025324, 0.05964377638667055]
2023-05-14 13:26:40,182 :: INFO :: ndcg [0.016976909219935585, 0.040783970009014535, 0.053156706865305124]
2023-05-14 13:26:46,072 :: INFO :: Epoch 215: loss tensor(33.3638, device='cuda:0', grad_fn=<AddBackward0>), U.norm 232.84397888183594, V.norm 281.247314453125, MLP.norm 6.996491432189941
2023-05-14 13:27:34,448 :: INFO :: ----- val -----
2023-05-14 13:27:34,448 :: INFO :: Precision [0.020579774286346535, 0.01705687098915634, 0.014193405620712625]
2023-05-14 13:27:34,448 :: INFO :: Recall [0.008099019887629755, 0.03381934645760084, 0.05445514701773178]
2023-05-14 13:27:34,448 :: INFO :: ndcg [0.020579774286346535, 0.050439503247982576, 0.0660199746199186]
2023-05-14 13:28:22,949 :: INFO :: ----- test -----
2023-05-14 13:28:22,949 :: INFO :: Precision [0.017086085485015556, 0.013363174845788015, 0.010953108794148687]
2023-05-14 13:28:22,949 :: INFO :: Recall [0.010432212813327913, 0.03769935055248294, 0.05955982720813065]
2023-05-14 13:28:22,949 :: INFO :: ndcg [0.017086085485015556, 0.04116650721880327, 0.053391927789350174]
2023-05-14 13:28:28,824 :: INFO :: Epoch 220: loss tensor(33.7607, device='cuda:0', grad_fn=<AddBackward0>), U.norm 235.2222900390625, V.norm 280.303466796875, MLP.norm 6.980319976806641
2023-05-14 13:29:17,262 :: INFO :: ----- val -----
2023-05-14 13:29:17,262 :: INFO :: Precision [0.020491259128125692, 0.01705687098915634, 0.014113741978313856]
2023-05-14 13:29:17,277 :: INFO :: Recall [0.00807074898217594, 0.033624845110571136, 0.054745770634727964]
2023-05-14 13:29:17,277 :: INFO :: ndcg [0.020491259128125692, 0.050479229099995936, 0.06593030519370695]
2023-05-14 13:30:06,278 :: INFO :: ----- test -----
2023-05-14 13:30:06,278 :: INFO :: Precision [0.0165402041596157, 0.013253998580708049, 0.010808450242917687]
2023-05-14 13:30:06,278 :: INFO :: Recall [0.009936553751426151, 0.03703342281403246, 0.05917306057553605]
2023-05-14 13:30:06,278 :: INFO :: ndcg [0.0165402041596157, 0.0407054108013775, 0.052654981760532314]
2023-05-14 13:30:12,154 :: INFO :: Epoch 225: loss tensor(33.8227, device='cuda:0', grad_fn=<AddBackward0>), U.norm 237.54551696777344, V.norm 279.8966979980469, MLP.norm 7.113321781158447
2023-05-14 13:31:01,045 :: INFO :: ----- val -----
2023-05-14 13:31:01,045 :: INFO :: Precision [0.02093383491922992, 0.01713653463155509, 0.01417127683115742]
2023-05-14 13:31:01,045 :: INFO :: Recall [0.008294107064205388, 0.03395583498935371, 0.0551008590323108]
2023-05-14 13:31:01,045 :: INFO :: ndcg [0.02093383491922992, 0.05096241021882738, 0.06634288256671153]
2023-05-14 13:31:49,655 :: INFO :: ----- test -----
2023-05-14 13:31:49,655 :: INFO :: Precision [0.01774114307549539, 0.013401386538566002, 0.010879414815219693]
2023-05-14 13:31:49,655 :: INFO :: Recall [0.010693476290940213, 0.03758028051659159, 0.059714682843232246]
2023-05-14 13:31:49,655 :: INFO :: ndcg [0.01774114307549539, 0.0414059602015386, 0.053367127584329654]
2023-05-14 13:31:55,514 :: INFO :: Epoch 230: loss tensor(33.0045, device='cuda:0', grad_fn=<AddBackward0>), U.norm 239.81320190429688, V.norm 279.1078186035156, MLP.norm 6.867602348327637
2023-05-14 13:32:44,109 :: INFO :: ----- val -----
2023-05-14 13:32:44,109 :: INFO :: Precision [0.02062403186545696, 0.016897543704358833, 0.014171276831157433]
2023-05-14 13:32:44,109 :: INFO :: Recall [0.00850357832444174, 0.0335063824940387, 0.05457402000053463]
2023-05-14 13:32:44,109 :: INFO :: ndcg [0.02062403186545696, 0.05027893285370774, 0.0660796294231266]
2023-05-14 13:33:33,110 :: INFO :: ----- test -----
2023-05-14 13:33:33,110 :: INFO :: Precision [0.01722255581636552, 0.013379551285550009, 0.010778426770020694]
2023-05-14 13:33:33,110 :: INFO :: Recall [0.010427620465588351, 0.03771209259578332, 0.05930678703816238]
2023-05-14 13:33:33,110 :: INFO :: ndcg [0.01722255581636552, 0.041182791491767706, 0.05296126426328462]
2023-05-14 13:33:38,953 :: INFO :: Epoch 235: loss tensor(32.9442, device='cuda:0', grad_fn=<AddBackward0>), U.norm 242.09671020507812, V.norm 278.6614074707031, MLP.norm 7.005666255950928
2023-05-14 13:34:27,392 :: INFO :: ----- val -----
2023-05-14 13:34:27,392 :: INFO :: Precision [0.020668289444567382, 0.016755919451205484, 0.014277495021022461]
2023-05-14 13:34:27,392 :: INFO :: Recall [0.008127266273301547, 0.03358392670319851, 0.05515700895460246]
2023-05-14 13:34:27,392 :: INFO :: ndcg [0.020668289444567382, 0.05041221211006531, 0.06648163528847378]
2023-05-14 13:35:16,314 :: INFO :: ----- test -----
2023-05-14 13:35:16,314 :: INFO :: Precision [0.01738632021398548, 0.013445057044597988, 0.010742944483869693]
2023-05-14 13:35:16,314 :: INFO :: Recall [0.010303364611164042, 0.03796744616301515, 0.05908913888918371]
2023-05-14 13:35:16,314 :: INFO :: ndcg [0.01738632021398548, 0.0413261650877476, 0.05283155212005622]
2023-05-14 13:35:22,189 :: INFO :: Epoch 240: loss tensor(35.5949, device='cuda:0', grad_fn=<AddBackward0>), U.norm 244.3155517578125, V.norm 278.3386535644531, MLP.norm 7.037781715393066
2023-05-14 13:36:10,549 :: INFO :: ----- val -----
2023-05-14 13:36:10,549 :: INFO :: Precision [0.02111086523567161, 0.017003761894223833, 0.014388138968798541]
2023-05-14 13:36:10,549 :: INFO :: Recall [0.008684062692546311, 0.03381284096125736, 0.054778611541134205]
2023-05-14 13:36:10,549 :: INFO :: ndcg [0.02111086523567161, 0.050966133331528475, 0.06686844686707721]
2023-05-14 13:36:58,753 :: INFO :: ----- test -----
2023-05-14 13:36:58,753 :: INFO :: Precision [0.01722255581636552, 0.013439598231343986, 0.010879414815219698]
2023-05-14 13:36:58,753 :: INFO :: Recall [0.010412948022578887, 0.037834802675147994, 0.05937161330413563]
2023-05-14 13:36:58,753 :: INFO :: ndcg [0.01722255581636552, 0.041347433166127, 0.053213961368332736]
2023-05-14 13:37:04,644 :: INFO :: Epoch 245: loss tensor(36.3281, device='cuda:0', grad_fn=<AddBackward0>), U.norm 246.51942443847656, V.norm 277.6881103515625, MLP.norm 7.039813041687012
2023-05-14 13:37:52,989 :: INFO :: ----- val -----
2023-05-14 13:37:52,989 :: INFO :: Precision [0.020756804602788226, 0.01706572250497843, 0.014609426864350687]
2023-05-14 13:37:52,989 :: INFO :: Recall [0.008369122258093957, 0.03332356255658079, 0.05547335889080529]
2023-05-14 13:37:52,989 :: INFO :: ndcg [0.020756804602788226, 0.050727695916902654, 0.06742599178200377]
2023-05-14 13:38:41,818 :: INFO :: ----- test -----
2023-05-14 13:38:41,818 :: INFO :: Precision [0.017631966810415414, 0.013548774496423954, 0.010898520661608686]
2023-05-14 13:38:41,818 :: INFO :: Recall [0.010518640954026501, 0.037879917095585525, 0.058921632636834774]
2023-05-14 13:38:41,818 :: INFO :: ndcg [0.017631966810415414, 0.04184281147798129, 0.05356096577631287]
2023-05-14 13:38:47,803 :: INFO :: Epoch 250: loss tensor(34.5882, device='cuda:0', grad_fn=<AddBackward0>), U.norm 248.68377685546875, V.norm 277.173095703125, MLP.norm 7.004395008087158
2023-05-14 13:39:36,695 :: INFO :: ----- val -----
2023-05-14 13:39:36,695 :: INFO :: Precision [0.01916353175481301, 0.017092277052444677, 0.014476654127019411]
2023-05-14 13:39:36,695 :: INFO :: Recall [0.008092304443253361, 0.03365328748154481, 0.05549145760433835]
2023-05-14 13:39:36,695 :: INFO :: ndcg [0.01916353175481301, 0.0500889218125806, 0.06638767170609249]
2023-05-14 13:40:25,462 :: INFO :: ----- test -----
2023-05-14 13:40:25,462 :: INFO :: Precision [0.016185381298105792, 0.013619739068725932, 0.010969485233910684]
2023-05-14 13:40:25,462 :: INFO :: Recall [0.010131103310770755, 0.0380498863296573, 0.05956390249664634]
2023-05-14 13:40:25,462 :: INFO :: ndcg [0.016185381298105792, 0.041428694095067164, 0.05325024727191388]
2023-05-14 13:40:31,368 :: INFO :: Epoch 255: loss tensor(33.6739, device='cuda:0', grad_fn=<AddBackward0>), U.norm 250.84201049804688, V.norm 276.76605224609375, MLP.norm 6.99050235748291
2023-05-14 13:41:19,931 :: INFO :: ----- val -----
2023-05-14 13:41:19,931 :: INFO :: Precision [0.01876521354281921, 0.01725160433724219, 0.014715645054215729]
2023-05-14 13:41:19,931 :: INFO :: Recall [0.008105809819816468, 0.03456106162163417, 0.05612940894689644]
2023-05-14 13:41:19,931 :: INFO :: ndcg [0.01876521354281921, 0.050234674037582916, 0.06690045169184805]
2023-05-14 13:42:08,448 :: INFO :: ----- test -----
2023-05-14 13:42:08,448 :: INFO :: Precision [0.0157759703040559, 0.013510562803645968, 0.011054096839347674]
2023-05-14 13:42:08,448 :: INFO :: Recall [0.009862100114258194, 0.03801474456158294, 0.05969462703534549]
2023-05-14 13:42:08,448 :: INFO :: ndcg [0.0157759703040559, 0.04111046416278979, 0.05337765658341916]
2023-05-14 13:42:14,354 :: INFO :: Epoch 260: loss tensor(35.0535, device='cuda:0', grad_fn=<AddBackward0>), U.norm 252.8916473388672, V.norm 276.3582763671875, MLP.norm 7.032251358032227
2023-05-14 13:43:02,933 :: INFO :: ----- val -----
2023-05-14 13:43:02,933 :: INFO :: Precision [0.019207789333923433, 0.01705687098915634, 0.014578446558973371]
2023-05-14 13:43:02,933 :: INFO :: Recall [0.008367553594585044, 0.03381720458968759, 0.055959466480649506]
2023-05-14 13:43:02,933 :: INFO :: ndcg [0.019207789333923433, 0.049998622510743926, 0.06678587239366775]
2023-05-14 13:43:51,949 :: INFO :: ----- test -----
2023-05-14 13:43:51,949 :: INFO :: Precision [0.016130793165565805, 0.013483268737375974, 0.011092308532125669]
2023-05-14 13:43:51,949 :: INFO :: Recall [0.009895134799718996, 0.037757210348986006, 0.06054801683395115]
2023-05-14 13:43:51,949 :: INFO :: ndcg [0.016130793165565805, 0.040835781007811774, 0.05328024984343638]
2023-05-14 13:43:57,824 :: INFO :: Epoch 265: loss tensor(35.2457, device='cuda:0', grad_fn=<AddBackward0>), U.norm 254.975830078125, V.norm 275.9994201660156, MLP.norm 6.996372222900391
2023-05-14 13:44:46,434 :: INFO :: ----- val -----
2023-05-14 13:44:46,434 :: INFO :: Precision [0.018676698384598362, 0.017242752821420106, 0.014600575348528598]
2023-05-14 13:44:46,434 :: INFO :: Recall [0.007993217842457503, 0.033950443134728545, 0.055799286896572514]
2023-05-14 13:44:46,434 :: INFO :: ndcg [0.018676698384598362, 0.05049464261282987, 0.0668775058650778]
2023-05-14 13:45:35,200 :: INFO :: ----- test -----
2023-05-14 13:45:35,200 :: INFO :: Precision [0.016703968557235657, 0.013734374147059896, 0.011024073366450669]
2023-05-14 13:45:35,200 :: INFO :: Recall [0.010110879931041176, 0.0384311046286833, 0.05977077415387526]
2023-05-14 13:45:35,200 :: INFO :: ndcg [0.016703968557235657, 0.041948287902718186, 0.053860055436644905]
2023-05-14 13:45:41,091 :: INFO :: Epoch 270: loss tensor(35.0433, device='cuda:0', grad_fn=<AddBackward0>), U.norm 256.9658508300781, V.norm 275.5726013183594, MLP.norm 6.848440170288086
2023-05-14 13:46:29,763 :: INFO :: ----- val -----
2023-05-14 13:46:29,763 :: INFO :: Precision [0.01876521354281921, 0.017384377074573444, 0.014786457180792403]
2023-05-14 13:46:29,763 :: INFO :: Recall [0.008151576975031716, 0.03388439324282279, 0.0556389253626123]
2023-05-14 13:46:29,763 :: INFO :: ndcg [0.01876521354281921, 0.05079503831351948, 0.06726069530613012]
2023-05-14 13:47:18,341 :: INFO :: ----- test -----
2023-05-14 13:47:18,341 :: INFO :: Precision [0.015912440635405863, 0.013608821442217934, 0.011149626071292666]
2023-05-14 13:47:18,341 :: INFO :: Recall [0.009773593192561367, 0.037851848651274306, 0.06021479129257667]
2023-05-14 13:47:18,341 :: INFO :: ndcg [0.015912440635405863, 0.04129077915582755, 0.053621909396185795]
2023-05-14 13:47:24,201 :: INFO :: Epoch 275: loss tensor(33.0874, device='cuda:0', grad_fn=<AddBackward0>), U.norm 259.03955078125, V.norm 275.1670837402344, MLP.norm 6.8774213790893555
2023-05-14 13:48:12,826 :: INFO :: ----- val -----
2023-05-14 13:48:12,826 :: INFO :: Precision [0.02080106218189865, 0.017207346758131768, 0.014560743527329209]
2023-05-14 13:48:12,826 :: INFO :: Recall [0.00844557033435741, 0.03377778906422913, 0.054993946866644695]
2023-05-14 13:48:12,826 :: INFO :: ndcg [0.02080106218189865, 0.051158727296661724, 0.06738397618055814]
2023-05-14 13:49:01,858 :: INFO :: ----- test -----
2023-05-14 13:49:01,858 :: INFO :: Precision [0.01684043888858562, 0.013368633659042011, 0.01102134395982368]
2023-05-14 13:49:01,858 :: INFO :: Recall [0.010089498999932958, 0.037331439176532755, 0.05957079300138253]
2023-05-14 13:49:01,858 :: INFO :: ndcg [0.01684043888858562, 0.041105968484168004, 0.05348219137244462]
2023-05-14 13:49:07,733 :: INFO :: Epoch 280: loss tensor(32.8737, device='cuda:0', grad_fn=<AddBackward0>), U.norm 260.9828796386719, V.norm 274.9029235839844, MLP.norm 6.867739677429199
2023-05-14 13:49:56,280 :: INFO :: ----- val -----
2023-05-14 13:49:56,280 :: INFO :: Precision [0.0212878955521133, 0.01708342553662259, 0.014582872316884427]
2023-05-14 13:49:56,280 :: INFO :: Recall [0.008549578613293772, 0.033437733224681326, 0.05489619994122792]
2023-05-14 13:49:56,280 :: INFO :: ndcg [0.0212878955521133, 0.05100010861325077, 0.06745005270846637]
2023-05-14 13:50:45,061 :: INFO :: ----- test -----
2023-05-14 13:50:45,061 :: INFO :: Precision [0.017795731208035372, 0.013406845351819999, 0.011032261586331683]
2023-05-14 13:50:45,061 :: INFO :: Recall [0.010480694343993853, 0.037638816957592316, 0.05952452796133071]
2023-05-14 13:50:45,061 :: INFO :: ndcg [0.017795731208035372, 0.04153218709672524, 0.05392154566243657]
2023-05-14 13:50:50,952 :: INFO :: Epoch 285: loss tensor(32.1215, device='cuda:0', grad_fn=<AddBackward0>), U.norm 263.00848388671875, V.norm 274.6963195800781, MLP.norm 6.940978527069092
2023-05-14 13:51:39,312 :: INFO :: ----- val -----
2023-05-14 13:51:39,312 :: INFO :: Precision [0.021420668289444566, 0.01744633768532803, 0.014883823854835354]
2023-05-14 13:51:39,312 :: INFO :: Recall [0.00850951186980522, 0.034056492286272916, 0.05610011223031382]
2023-05-14 13:51:39,312 :: INFO :: ndcg [0.021420668289444566, 0.05193776912629483, 0.0683582572808328]
2023-05-14 13:52:28,421 :: INFO :: ----- test -----
2023-05-14 13:52:28,421 :: INFO :: Precision [0.018150554069545282, 0.013488727550629973, 0.011226049456848661]
2023-05-14 13:52:28,421 :: INFO :: Recall [0.010623165037531745, 0.0374746453648617, 0.060740519693658644]
2023-05-14 13:52:28,421 :: INFO :: ndcg [0.018150554069545282, 0.04201300209913843, 0.054701477646885634]
2023-05-14 13:52:34,281 :: INFO :: Epoch 290: loss tensor(37.3253, device='cuda:0', grad_fn=<AddBackward0>), U.norm 264.9166564941406, V.norm 274.4136962890625, MLP.norm 6.963541507720947
2023-05-14 13:53:22,718 :: INFO :: ----- val -----
2023-05-14 13:53:22,718 :: INFO :: Precision [0.021730471343217525, 0.01732241646381886, 0.014711219296304685]
2023-05-14 13:53:22,718 :: INFO :: Recall [0.00886054607517958, 0.033723768734877715, 0.05516040826599249]
2023-05-14 13:53:22,718 :: INFO :: ndcg [0.021730471343217525, 0.05195175076107425, 0.06828376018433134]
2023-05-14 13:54:11,329 :: INFO :: ----- test -----
2023-05-14 13:54:11,329 :: INFO :: Precision [0.017659260876685408, 0.013570609749439952, 0.01113870844478468]
2023-05-14 13:54:11,329 :: INFO :: Recall [0.010292889388477458, 0.03776023764711548, 0.06040891038639305]
2023-05-14 13:54:11,329 :: INFO :: ndcg [0.017659260876685408, 0.041915967288820445, 0.054428828190656975]
2023-05-14 13:54:17,251 :: INFO :: Epoch 295: loss tensor(38.2970, device='cuda:0', grad_fn=<AddBackward0>), U.norm 266.80303955078125, V.norm 274.0195007324219, MLP.norm 6.946184158325195
2023-05-14 13:55:05,595 :: INFO :: ----- val -----
2023-05-14 13:55:05,595 :: INFO :: Precision [0.021996016817880062, 0.01722504978977593, 0.014675813233016333]
2023-05-14 13:55:05,595 :: INFO :: Recall [0.009017821278951618, 0.034102897559176525, 0.055777834680832465]
2023-05-14 13:55:05,595 :: INFO :: ndcg [0.021996016817880062, 0.05184729655661541, 0.06825668752137223]
2023-05-14 13:55:54,377 :: INFO :: ----- test -----
2023-05-14 13:55:54,377 :: INFO :: Precision [0.01861455319613516, 0.013663409574757918, 0.011095037938752678]
2023-05-14 13:55:54,377 :: INFO :: Recall [0.011057310098996187, 0.03863212688572786, 0.06050000552255104]
2023-05-14 13:55:54,377 :: INFO :: ndcg [0.01861455319613516, 0.04253552037751559, 0.054786913934555106]
2023-05-14 13:56:00,268 :: INFO :: Epoch 300: loss tensor(36.0403, device='cuda:0', grad_fn=<AddBackward0>), U.norm 268.62469482421875, V.norm 273.4582824707031, MLP.norm 6.9430036544799805
2023-05-14 13:56:48,581 :: INFO :: ----- val -----
2023-05-14 13:56:48,581 :: INFO :: Precision [0.021420668289444566, 0.01749944678026054, 0.014866120823191206]
2023-05-14 13:56:48,581 :: INFO :: Recall [0.008658409311687838, 0.034267681177921644, 0.056127905655308605]
2023-05-14 13:56:48,581 :: INFO :: ndcg [0.021420668289444566, 0.05206847850276154, 0.06888747905542089]
2023-05-14 13:57:37,112 :: INFO :: ----- test -----
2023-05-14 13:57:37,112 :: INFO :: Precision [0.01823243626835526, 0.01381625634586987, 0.011297014029150666]
2023-05-14 13:57:37,112 :: INFO :: Recall [0.010886609582869407, 0.03911599289712874, 0.061446709393736455]
2023-05-14 13:57:37,112 :: INFO :: ndcg [0.01823243626835526, 0.04289135379220079, 0.05533080606264884]
2023-05-14 13:57:42,987 :: INFO :: Epoch 305: loss tensor(34.9839, device='cuda:0', grad_fn=<AddBackward0>), U.norm 270.4356994628906, V.norm 273.3254699707031, MLP.norm 6.931398868560791
2023-05-14 13:58:31,362 :: INFO :: ----- val -----
2023-05-14 13:58:31,362 :: INFO :: Precision [0.021420668289444566, 0.017490595264438455, 0.015105111750387524]
2023-05-14 13:58:31,362 :: INFO :: Recall [0.008717231878808702, 0.034043028505006116, 0.05733096903451991]
2023-05-14 13:58:31,362 :: INFO :: ndcg [0.021420668289444566, 0.05194759838432762, 0.06931905340786808]
2023-05-14 13:59:20,066 :: INFO :: ----- test -----
2023-05-14 13:59:20,066 :: INFO :: Precision [0.018205142202085266, 0.013838091598885863, 0.011357060974944667]
2023-05-14 13:59:20,066 :: INFO :: Recall [0.010893341036118137, 0.039040984356049656, 0.06153116113600334]
2023-05-14 13:59:20,066 :: INFO :: ndcg [0.018205142202085266, 0.0428653547248351, 0.05541209110600035]
2023-05-14 13:59:25,941 :: INFO :: Epoch 310: loss tensor(33.0794, device='cuda:0', grad_fn=<AddBackward0>), U.norm 272.1553649902344, V.norm 273.07989501953125, MLP.norm 6.845799446105957
2023-05-14 14:00:14,128 :: INFO :: ----- val -----
2023-05-14 14:00:14,128 :: INFO :: Precision [0.021066607656561185, 0.017472892232794288, 0.015180349634875256]
2023-05-14 14:00:14,128 :: INFO :: Recall [0.008219481591893374, 0.03397561273753789, 0.056950274283371516]
2023-05-14 14:00:14,128 :: INFO :: ndcg [0.021066607656561185, 0.0521647047243316, 0.06939743144077441]
2023-05-14 14:01:03,285 :: INFO :: ----- test -----
2023-05-14 14:01:03,285 :: INFO :: Precision [0.017932201539385337, 0.013947267863965827, 0.01121786123696768]
2023-05-14 14:01:03,285 :: INFO :: Recall [0.010507682872661525, 0.03930995210572291, 0.06045201455234561]
2023-05-14 14:01:03,285 :: INFO :: ndcg [0.017932201539385337, 0.04301017336524803, 0.05502714124158112]
2023-05-14 14:01:09,207 :: INFO :: Epoch 315: loss tensor(38.4232, device='cuda:0', grad_fn=<AddBackward0>), U.norm 273.8815002441406, V.norm 272.86419677734375, MLP.norm 6.945376396179199
2023-05-14 14:01:57,660 :: INFO :: ----- val -----
2023-05-14 14:01:57,660 :: INFO :: Precision [0.021022350077450763, 0.017472892232794284, 0.015038725381721883]
2023-05-14 14:01:57,660 :: INFO :: Recall [0.008379124037797744, 0.03442650357491359, 0.05655018105322254]
2023-05-14 14:01:57,660 :: INFO :: ndcg [0.021022350077450763, 0.051851044387905255, 0.06875768859611667]
2023-05-14 14:02:46,130 :: INFO :: ----- test -----
2023-05-14 14:02:46,130 :: INFO :: Precision [0.01746820241279546, 0.01398547955674382, 0.011201484797205676]
2023-05-14 14:02:46,130 :: INFO :: Recall [0.010569173946247621, 0.03962276008760158, 0.06080760797956182]
2023-05-14 14:02:46,130 :: INFO :: ndcg [0.01746820241279546, 0.04276542935297197, 0.0547488142888587]
2023-05-14 14:02:52,036 :: INFO :: Epoch 320: loss tensor(37.0056, device='cuda:0', grad_fn=<AddBackward0>), U.norm 275.50830078125, V.norm 272.544677734375, MLP.norm 6.948539733886719
2023-05-14 14:03:40,098 :: INFO :: ----- val -----
2023-05-14 14:03:40,098 :: INFO :: Precision [0.021818986501438372, 0.01740208010621762, 0.015025448107988762]
2023-05-14 14:03:40,098 :: INFO :: Recall [0.008795103757716388, 0.03390714459140283, 0.05705900244953204]
2023-05-14 14:03:40,098 :: INFO :: ndcg [0.021818986501438372, 0.05213727330894308, 0.06941843719641397]
2023-05-14 14:04:29,162 :: INFO :: ----- test -----
2023-05-14 14:04:29,162 :: INFO :: Precision [0.017604672744145424, 0.013898138544679849, 0.01124515530323766]
2023-05-14 14:04:29,162 :: INFO :: Recall [0.010448570316991774, 0.039291192819856297, 0.06131578310260554]
2023-05-14 14:04:29,162 :: INFO :: ndcg [0.017604672744145424, 0.042722021427806345, 0.05490685668416668]
2023-05-14 14:04:35,083 :: INFO :: Epoch 325: loss tensor(34.1431, device='cuda:0', grad_fn=<AddBackward0>), U.norm 277.1311950683594, V.norm 272.2164001464844, MLP.norm 6.901866912841797
2023-05-14 14:05:24,084 :: INFO :: ----- val -----
2023-05-14 14:05:24,084 :: INFO :: Precision [0.021553441026775835, 0.017437486169505953, 0.014795308696614513]
2023-05-14 14:05:24,084 :: INFO :: Recall [0.008577390291717902, 0.034239433699182704, 0.05647825602745551]
2023-05-14 14:05:24,084 :: INFO :: ndcg [0.021553441026775835, 0.052125173095487407, 0.068711571638469]
2023-05-14 14:06:13,272 :: INFO :: ----- test -----
2023-05-14 14:06:13,272 :: INFO :: Precision [0.018041377804465308, 0.013969103116981825, 0.011299743435777665]
2023-05-14 14:06:13,272 :: INFO :: Recall [0.010746709933257378, 0.039575344648659305, 0.06199614356365722]
2023-05-14 14:06:13,272 :: INFO :: ndcg [0.018041377804465308, 0.043014663106895064, 0.055439532144166484]
2023-05-14 14:06:19,209 :: INFO :: Epoch 330: loss tensor(32.7989, device='cuda:0', grad_fn=<AddBackward0>), U.norm 278.74835205078125, V.norm 271.89349365234375, MLP.norm 6.860244274139404
2023-05-14 14:07:08,068 :: INFO :: ----- val -----
2023-05-14 14:07:08,068 :: INFO :: Precision [0.022040274396990484, 0.01757025890683721, 0.015016596592166665]
2023-05-14 14:07:08,068 :: INFO :: Recall [0.0088771685872442, 0.03474796188364922, 0.056964670896621675]
2023-05-14 14:07:08,068 :: INFO :: ndcg [0.022040274396990484, 0.052646028189744974, 0.06953492886516079]
2023-05-14 14:07:57,085 :: INFO :: ----- test -----
2023-05-14 14:07:57,085 :: INFO :: Precision [0.018259730334625253, 0.014105573448331812, 0.011373437414706656]
2023-05-14 14:07:57,085 :: INFO :: Recall [0.010778735902254628, 0.039673482414617336, 0.062245136726021985]
2023-05-14 14:07:57,085 :: INFO :: ndcg [0.018259730334625253, 0.04342891133202164, 0.05576364235950402]
2023-05-14 14:08:02,991 :: INFO :: Epoch 335: loss tensor(34.7790, device='cuda:0', grad_fn=<AddBackward0>), U.norm 280.35992431640625, V.norm 271.7665710449219, MLP.norm 6.882705211639404
2023-05-14 14:08:52,022 :: INFO :: ----- val -----
2023-05-14 14:08:52,022 :: INFO :: Precision [0.02177472892232795, 0.017437486169505953, 0.01498561628678937]
2023-05-14 14:08:52,022 :: INFO :: Recall [0.008890764127441811, 0.034413241065830553, 0.05699379801524416]
2023-05-14 14:08:52,022 :: INFO :: ndcg [0.02177472892232795, 0.052227659238320824, 0.06926418056263131]
2023-05-14 14:09:42,718 :: INFO :: ----- test -----
2023-05-14 14:09:42,718 :: INFO :: Precision [0.01880561166002511, 0.013898138544679844, 0.011367978601452658]
2023-05-14 14:09:42,718 :: INFO :: Recall [0.011139744134751163, 0.03933495385382631, 0.06207743895825266]
2023-05-14 14:09:42,718 :: INFO :: ndcg [0.01880561166002511, 0.04319163991935567, 0.05582879242729138]
2023-05-14 14:26:13,832 :: INFO :: Epoch 340: loss tensor(34.3620, device='cuda:0', grad_fn=<AddBackward0>), U.norm 282.054931640625, V.norm 271.4046325683594, MLP.norm 6.872948169708252
2023-05-14 14:27:06,379 :: INFO :: ----- val -----
2023-05-14 14:27:06,379 :: INFO :: Precision [0.022704138083646824, 0.01734011949546303, 0.015047576897543984]
2023-05-14 14:27:06,379 :: INFO :: Recall [0.009271877374359472, 0.03384403237112567, 0.05666939209229969]
2023-05-14 14:27:06,379 :: INFO :: ndcg [0.022704138083646824, 0.05273012716779216, 0.07014755037879684]
2023-05-14 14:27:59,692 :: INFO :: ----- test -----
2023-05-14 14:27:59,692 :: INFO :: Precision [0.019023964190185053, 0.013794421092853877, 0.011370708008079657]
2023-05-14 14:27:59,692 :: INFO :: Recall [0.011082028413432398, 0.03889844764104391, 0.06145824640896379]
2023-05-14 14:27:59,692 :: INFO :: ndcg [0.019023964190185053, 0.043199386896431886, 0.05585932051674313]
2023-05-14 14:28:06,254 :: INFO :: Epoch 345: loss tensor(36.1517, device='cuda:0', grad_fn=<AddBackward0>), U.norm 283.628662109375, V.norm 271.2487487792969, MLP.norm 6.893748760223389
2023-05-14 14:28:58,567 :: INFO :: ----- val -----
2023-05-14 14:28:58,567 :: INFO :: Precision [0.022438592608984287, 0.017499446780260546, 0.015056428413366057]
2023-05-14 14:28:58,567 :: INFO :: Recall [0.009247957471811168, 0.03458299058192453, 0.05714548206272469]
2023-05-14 14:28:58,567 :: INFO :: ndcg [0.022438592608984287, 0.0526677365679589, 0.06980015989161706]
2023-05-14 14:29:51,911 :: INFO :: ----- test -----
2023-05-14 14:29:51,911 :: INFO :: Precision [0.018532670997325182, 0.013996397183251819, 0.011307931655658663]
2023-05-14 14:29:51,911 :: INFO :: Recall [0.011033076794748747, 0.039297973067691644, 0.061499614587354576]
2023-05-14 14:29:51,911 :: INFO :: ndcg [0.018532670997325182, 0.04340900394618068, 0.05577097040210028]
2023-05-14 14:29:58,364 :: INFO :: Epoch 350: loss tensor(36.7814, device='cuda:0', grad_fn=<AddBackward0>), U.norm 285.2054443359375, V.norm 271.2038269042969, MLP.norm 6.874579906463623
2023-05-14 14:30:51,457 :: INFO :: ----- val -----
2023-05-14 14:30:51,457 :: INFO :: Precision [0.023190971453861474, 0.017481743748616385, 0.014990042044700424]
2023-05-14 14:30:51,457 :: INFO :: Recall [0.00929877228003621, 0.03443531972860101, 0.05706467154860912]
2023-05-14 14:30:51,457 :: INFO :: ndcg [0.023190971453861474, 0.053296964979685404, 0.07032752179611325]
2023-05-14 14:31:45,317 :: INFO :: ----- test -----
2023-05-14 14:31:45,317 :: INFO :: Precision [0.019378787051694962, 0.014050985315791818, 0.01143348436050065]
2023-05-14 14:31:45,317 :: INFO :: Recall [0.011289935727557538, 0.039625299452407475, 0.061974900012181186]
2023-05-14 14:31:45,317 :: INFO :: ndcg [0.019378787051694962, 0.04397076227390113, 0.05650552495733849]
2023-05-14 14:31:51,661 :: INFO :: Epoch 355: loss tensor(35.2390, device='cuda:0', grad_fn=<AddBackward0>), U.norm 286.774169921875, V.norm 270.8939208984375, MLP.norm 6.898749351501465
2023-05-14 14:32:44,145 :: INFO :: ----- val -----
2023-05-14 14:32:44,145 :: INFO :: Precision [0.024031865456959504, 0.017481743748616378, 0.015118389024120667]
2023-05-14 14:32:44,145 :: INFO :: Recall [0.009588458202318798, 0.034198177951797935, 0.05724602352341123]
2023-05-14 14:32:44,145 :: INFO :: ndcg [0.024031865456959504, 0.05375983157197659, 0.0708943973856214]
2023-05-14 14:33:37,005 :: INFO :: ----- test -----
2023-05-14 14:33:37,005 :: INFO :: Precision [0.019051258256455046, 0.014078279382061817, 0.011482613679786646]
2023-05-14 14:33:37,005 :: INFO :: Recall [0.011091728817739734, 0.03958082196626978, 0.062317812402029084]
2023-05-14 14:33:37,005 :: INFO :: ndcg [0.019051258256455046, 0.04406714120468548, 0.05668405438531848]
2023-05-14 14:33:43,271 :: INFO :: Epoch 360: loss tensor(33.5450, device='cuda:0', grad_fn=<AddBackward0>), U.norm 288.32049560546875, V.norm 270.6895751953125, MLP.norm 6.859889030456543
2023-05-14 14:34:35,115 :: INFO :: ----- val -----
2023-05-14 14:34:35,115 :: INFO :: Precision [0.023190971453861474, 0.017809249834033487, 0.015321973888028657]
2023-05-14 14:34:35,115 :: INFO :: Recall [0.009239144092425173, 0.03449597719277476, 0.057440867116002235]
2023-05-14 14:34:35,115 :: INFO :: ndcg [0.023190971453861474, 0.05390013942817783, 0.07112701819803902]
2023-05-14 14:35:25,601 :: INFO :: ----- test -----
2023-05-14 14:35:25,601 :: INFO :: Precision [0.01918772858780501, 0.014203832086903814, 0.011512637152683651]
2023-05-14 14:35:25,601 :: INFO :: Recall [0.011207915212308989, 0.03978652074655138, 0.06211107325863669]
2023-05-14 14:35:25,601 :: INFO :: ndcg [0.01918772858780501, 0.04422313980497461, 0.05687143589087283]
2023-05-14 14:35:31,773 :: INFO :: Epoch 365: loss tensor(34.3528, device='cuda:0', grad_fn=<AddBackward0>), U.norm 289.8031005859375, V.norm 270.4431457519531, MLP.norm 6.905776023864746
2023-05-14 14:36:24,241 :: INFO :: ----- val -----
2023-05-14 14:36:24,241 :: INFO :: Precision [0.02279265324186767, 0.017658774065058055, 0.01558751936269122]
2023-05-14 14:36:24,241 :: INFO :: Recall [0.009125056476124677, 0.034492630099240196, 0.05800282619700786]
2023-05-14 14:36:24,257 :: INFO :: ndcg [0.02279265324186767, 0.05348235688229718, 0.07140650725134924]
2023-05-14 14:37:16,086 :: INFO :: ----- test -----
2023-05-14 14:37:16,086 :: INFO :: Precision [0.019242316720344998, 0.01397456193023582, 0.011507178339429642]
2023-05-14 14:37:16,086 :: INFO :: Recall [0.011222472047652981, 0.039177191988546814, 0.061522794333073366]
2023-05-14 14:37:16,101 :: INFO :: ndcg [0.019242316720344998, 0.043795334476467086, 0.05658091889921346]
2023-05-14 14:37:22,132 :: INFO :: Epoch 370: loss tensor(32.8404, device='cuda:0', grad_fn=<AddBackward0>), U.norm 291.29595947265625, V.norm 270.3914489746094, MLP.norm 6.798543930053711
2023-05-14 14:38:12,429 :: INFO :: ----- val -----
2023-05-14 14:38:12,429 :: INFO :: Precision [0.023810577561407392, 0.017658774065058058, 0.015401637530427425]
2023-05-14 14:38:12,429 :: INFO :: Recall [0.009557756217189099, 0.034293421986142554, 0.05774607197171584]
2023-05-14 14:38:12,429 :: INFO :: ndcg [0.023810577561407392, 0.0539351123255023, 0.07147174150060923]
2023-05-14 14:39:02,601 :: INFO :: ----- test -----
2023-05-14 14:39:02,601 :: INFO :: Precision [0.020143020907254762, 0.014121949888093819, 0.011501719526175631]
2023-05-14 14:39:02,601 :: INFO :: Recall [0.01187407554056877, 0.03967344689329091, 0.06138093095864893]
2023-05-14 14:39:02,601 :: INFO :: ndcg [0.020143020907254762, 0.044378067965512295, 0.0569274373254864]
2023-05-14 14:39:08,492 :: INFO :: Epoch 375: loss tensor(37.6653, device='cuda:0', grad_fn=<AddBackward0>), U.norm 292.802734375, V.norm 270.17681884765625, MLP.norm 6.8819780349731445
2023-05-14 14:39:59,819 :: INFO :: ----- val -----
2023-05-14 14:39:59,819 :: INFO :: Precision [0.023987607877849083, 0.017703031644168473, 0.015366231467139092]
2023-05-14 14:39:59,819 :: INFO :: Recall [0.009682694132720587, 0.034519952530371434, 0.057818151883709375]
2023-05-14 14:39:59,819 :: INFO :: ndcg [0.023987607877849083, 0.054127964292723936, 0.07176221626006304]
2023-05-14 14:40:50,850 :: INFO :: ----- test -----
2023-05-14 14:40:50,850 :: INFO :: Precision [0.019979256509634804, 0.014181996833887814, 0.011498990119548635]
2023-05-14 14:40:50,850 :: INFO :: Recall [0.011578107888322337, 0.0395871551221544, 0.06164591589935602]
2023-05-14 14:40:50,850 :: INFO :: ndcg [0.019979256509634804, 0.04444700419395604, 0.056949001784100915]
2023-05-14 14:40:57,006 :: INFO :: Epoch 380: loss tensor(33.2862, device='cuda:0', grad_fn=<AddBackward0>), U.norm 294.1937255859375, V.norm 270.3717346191406, MLP.norm 6.881117820739746
2023-05-14 14:41:49,194 :: INFO :: ----- val -----
2023-05-14 14:41:49,194 :: INFO :: Precision [0.023545032086744855, 0.017649922549235975, 0.015525558751936631]
2023-05-14 14:41:49,194 :: INFO :: Recall [0.009689889552939151, 0.03454111111733477, 0.05794342132385363]
2023-05-14 14:41:49,194 :: INFO :: ndcg [0.023545032086744855, 0.05361884761804613, 0.0716460838002731]
2023-05-14 14:42:42,522 :: INFO :: ----- test -----
2023-05-14 14:42:42,522 :: INFO :: Precision [0.01918772858780501, 0.014225667339919808, 0.011507178339429635]
2023-05-14 14:42:42,522 :: INFO :: Recall [0.011183315295179456, 0.039827407046871016, 0.06154184363396386]
2023-05-14 14:42:42,522 :: INFO :: ndcg [0.01918772858780501, 0.04424369116815373, 0.05670773451140344]
2023-05-14 14:42:48,866 :: INFO :: Epoch 385: loss tensor(42.3156, device='cuda:0', grad_fn=<AddBackward0>), U.norm 295.6307067871094, V.norm 270.289794921875, MLP.norm 6.748640060424805
2023-05-14 14:43:40,959 :: INFO :: ----- val -----
2023-05-14 14:43:40,959 :: INFO :: Precision [0.022438592608984287, 0.01791546802389849, 0.015587519362691237]
2023-05-14 14:43:40,959 :: INFO :: Recall [0.009247241807112809, 0.03456342689579454, 0.05809344342043098]
2023-05-14 14:43:40,959 :: INFO :: ndcg [0.022438592608984287, 0.05367701848635206, 0.07155454671530288]
2023-05-14 14:44:33,632 :: INFO :: ----- test -----
2023-05-14 14:44:33,632 :: INFO :: Precision [0.01823243626835526, 0.01418745564714181, 0.011537201812326642]
2023-05-14 14:44:33,632 :: INFO :: Recall [0.010781239612334025, 0.040082746141689954, 0.061642111218395836]
2023-05-14 14:44:33,632 :: INFO :: ndcg [0.01823243626835526, 0.04393425370070454, 0.056480700442317545]
2023-05-14 14:44:40,022 :: INFO :: Epoch 390: loss tensor(37.1073, device='cuda:0', grad_fn=<AddBackward0>), U.norm 297.0174255371094, V.norm 270.1006164550781, MLP.norm 6.877326488494873
2023-05-14 14:45:32,366 :: INFO :: ----- val -----
2023-05-14 14:45:32,366 :: INFO :: Precision [0.023102456295640627, 0.01797742863465308, 0.015653905731356863]
2023-05-14 14:45:32,366 :: INFO :: Recall [0.009635301473062472, 0.03484710378452698, 0.05811305471566771]
2023-05-14 14:45:32,366 :: INFO :: ndcg [0.023102456295640627, 0.054061468865981954, 0.07195992143754176]
2023-05-14 14:46:25,929 :: INFO :: ----- test -----
2023-05-14 14:46:25,929 :: INFO :: Precision [0.018314318467165237, 0.014252961406189806, 0.011616354604509629]
2023-05-14 14:46:25,929 :: INFO :: Recall [0.010521935833460401, 0.040180167060609395, 0.06238688805673356]
2023-05-14 14:46:25,929 :: INFO :: ndcg [0.018314318467165237, 0.04404902197581352, 0.056764576067038934]
2023-05-14 14:46:32,257 :: INFO :: Epoch 395: loss tensor(34.1109, device='cuda:0', grad_fn=<AddBackward0>), U.norm 298.3426818847656, V.norm 270.22076416015625, MLP.norm 6.836911201477051
2023-05-14 14:47:24,304 :: INFO :: ----- val -----
2023-05-14 14:47:24,304 :: INFO :: Precision [0.022217304713432175, 0.018198716530205182, 0.015768975437044]
2023-05-14 14:47:24,304 :: INFO :: Recall [0.008987552933024784, 0.035062013028657586, 0.058422984087420186]
2023-05-14 14:47:24,304 :: INFO :: ndcg [0.022217304713432175, 0.0540315642882613, 0.07183925312477199]
2023-05-14 14:48:15,617 :: INFO :: ----- test -----
2023-05-14 14:48:15,617 :: INFO :: Precision [0.018887493858835088, 0.014111032261585816, 0.011668213330422632]
2023-05-14 14:48:15,617 :: INFO :: Recall [0.011109280114510666, 0.039583188128503914, 0.0627072175127801]
2023-05-14 14:48:15,617 :: INFO :: ndcg [0.018887493858835088, 0.043704563915033474, 0.05684400597309396]
2023-05-14 14:48:21,804 :: INFO :: Epoch 400: loss tensor(36.8268, device='cuda:0', grad_fn=<AddBackward0>), U.norm 299.6746826171875, V.norm 270.30499267578125, MLP.norm 6.936075687408447
2023-05-14 14:49:13,273 :: INFO :: ----- val -----
2023-05-14 14:49:13,273 :: INFO :: Precision [0.022881168400088515, 0.017942022571364746, 0.015676034520912093]
2023-05-14 14:49:13,273 :: INFO :: Recall [0.00949151987375974, 0.03484804378036395, 0.05749367065817562]
2023-05-14 14:49:13,273 :: INFO :: ndcg [0.022881168400088515, 0.05393181518540826, 0.07177386930998213]
2023-05-14 14:50:04,211 :: INFO :: ----- test -----
2023-05-14 14:50:04,211 :: INFO :: Precision [0.01896937605764507, 0.014302090725475806, 0.011635460450898623]
2023-05-14 14:50:04,211 :: INFO :: Recall [0.011287133923839994, 0.03977381548647112, 0.061661443982730664]
2023-05-14 14:50:04,211 :: INFO :: ndcg [0.01896937605764507, 0.044352165120417016, 0.057041728055887364]
2023-05-14 14:50:10,242 :: INFO :: Epoch 405: loss tensor(35.0879, device='cuda:0', grad_fn=<AddBackward0>), U.norm 300.9613342285156, V.norm 270.1419982910156, MLP.norm 6.916111946105957
2023-05-14 14:51:00,867 :: INFO :: ----- val -----
2023-05-14 14:51:00,867 :: INFO :: Precision [0.023279486612082317, 0.01817216198273893, 0.015600796636424332]
2023-05-14 14:51:00,867 :: INFO :: Recall [0.009694805990411646, 0.03476514407554791, 0.057518857947807485]
2023-05-14 14:51:00,867 :: INFO :: ndcg [0.023279486612082317, 0.05459161869405554, 0.07195586665572595]
2023-05-14 14:51:51,336 :: INFO :: ----- test -----
2023-05-14 14:51:51,336 :: INFO :: Precision [0.020061138708444785, 0.014225667339919808, 0.011681860363557619]
2023-05-14 14:51:51,336 :: INFO :: Recall [0.011763770176855958, 0.039947306600889486, 0.06241659203967778]
2023-05-14 14:51:51,336 :: INFO :: ndcg [0.020061138708444785, 0.044705232296656155, 0.05752707467839178]
2023-05-14 14:51:57,461 :: INFO :: Epoch 410: loss tensor(34.5995, device='cuda:0', grad_fn=<AddBackward0>), U.norm 302.23150634765625, V.norm 270.1606750488281, MLP.norm 6.902668476104736
2023-05-14 14:52:48,836 :: INFO :: ----- val -----
2023-05-14 14:52:48,836 :: INFO :: Precision [0.024739986722726266, 0.018172161982738935, 0.015565390573135999]
2023-05-14 14:52:48,836 :: INFO :: Recall [0.01015778604391146, 0.03480453189554501, 0.05788162158657116]
2023-05-14 14:52:48,836 :: INFO :: ndcg [0.024739986722726266, 0.05515986092392855, 0.07271911615102102]
2023-05-14 14:53:40,618 :: INFO :: ----- test -----
2023-05-14 14:53:40,618 :: INFO :: Precision [0.01973360991320487, 0.014192914460395813, 0.01159451935149362]
2023-05-14 14:53:40,618 :: INFO :: Recall [0.011318678189853935, 0.03954017099228776, 0.06177004081378947]
2023-05-14 14:53:40,618 :: INFO :: ndcg [0.01973360991320487, 0.04433485175589998, 0.05711931025004379]
2023-05-14 14:53:46,915 :: INFO :: Epoch 415: loss tensor(35.2925, device='cuda:0', grad_fn=<AddBackward0>), U.norm 303.6517333984375, V.norm 270.2191467285156, MLP.norm 6.880390644073486
2023-05-14 14:54:39,227 :: INFO :: ----- val -----
2023-05-14 14:54:39,227 :: INFO :: Precision [0.024164638194290773, 0.018455410489045622, 0.01607435273290594]
2023-05-14 14:54:39,227 :: INFO :: Recall [0.009348671679888586, 0.034955999597487825, 0.05925037953472713]
2023-05-14 14:54:39,227 :: INFO :: ndcg [0.024164638194290773, 0.055641012926739314, 0.07362036222982268]
2023-05-14 14:55:31,696 :: INFO :: ----- test -----
2023-05-14 14:55:31,696 :: INFO :: Precision [0.019324198919154975, 0.014498608002619796, 0.011766471968994626]
2023-05-14 14:55:31,696 :: INFO :: Recall [0.011208556978690094, 0.040327844213429775, 0.0626072063822052]
2023-05-14 14:55:31,696 :: INFO :: ndcg [0.019324198919154975, 0.04491679459238963, 0.05770227324661676]
2023-05-14 14:55:37,821 :: INFO :: Epoch 420: loss tensor(36.5639, device='cuda:0', grad_fn=<AddBackward0>), U.norm 305.0245361328125, V.norm 270.3204650878906, MLP.norm 6.896803379058838
2023-05-14 14:56:29,430 :: INFO :: ----- val -----
2023-05-14 14:56:29,430 :: INFO :: Precision [0.023058198716530205, 0.017977428634653077, 0.015764549679132923]
2023-05-14 14:56:29,430 :: INFO :: Recall [0.00897381819920126, 0.03454486785625069, 0.05760730883282203]
2023-05-14 14:56:29,430 :: INFO :: ndcg [0.023058198716530205, 0.05415754817972316, 0.07207933386497598]
2023-05-14 14:57:21,212 :: INFO :: ----- test -----
2023-05-14 14:57:21,212 :: INFO :: Precision [0.019160434521535017, 0.014482231562857798, 0.011739177902724617]
2023-05-14 14:57:21,212 :: INFO :: Recall [0.011148739149752772, 0.04018650722634204, 0.06244779663077056]
2023-05-14 14:57:21,212 :: INFO :: ndcg [0.019160434521535017, 0.044690756491136865, 0.057445623026773175]
2023-05-14 14:57:27,337 :: INFO :: Epoch 425: loss tensor(38.6528, device='cuda:0', grad_fn=<AddBackward0>), U.norm 306.2646789550781, V.norm 270.1357727050781, MLP.norm 6.9010772705078125
2023-05-14 14:58:18,587 :: INFO :: ----- val -----
2023-05-14 14:58:18,587 :: INFO :: Precision [0.023279486612082317, 0.017977428634653084, 0.01572914361584456]
2023-05-14 14:58:18,587 :: INFO :: Recall [0.009305651650680213, 0.03437958089931128, 0.05756365635549177]
2023-05-14 14:58:18,587 :: INFO :: ndcg [0.023279486612082317, 0.05433602052833977, 0.07229420204220088]
2023-05-14 14:59:09,431 :: INFO :: ----- test -----
2023-05-14 14:59:09,431 :: INFO :: Precision [0.019515257383044927, 0.0144167258038098, 0.011621813417763624]
2023-05-14 14:59:09,431 :: INFO :: Recall [0.011349776997155047, 0.0405975371436137, 0.06211876909965179]
2023-05-14 14:59:09,431 :: INFO :: ndcg [0.019515257383044927, 0.04481323205573354, 0.05729592076356241]
2023-05-14 14:59:15,587 :: INFO :: Epoch 430: loss tensor(33.9362, device='cuda:0', grad_fn=<AddBackward0>), U.norm 307.5055236816406, V.norm 269.9956359863281, MLP.norm 6.86949348449707
2023-05-14 15:00:07,337 :: INFO :: ----- val -----
2023-05-14 15:00:07,337 :: INFO :: Precision [0.023412259349413586, 0.018189865014383105, 0.015689311794645177]
2023-05-14 15:00:07,337 :: INFO :: Recall [0.00981391859000526, 0.03477736531039027, 0.05749916548741336]
2023-05-14 15:00:07,337 :: INFO :: ndcg [0.023412259349413586, 0.05462196077083158, 0.07237796658870604]
2023-05-14 15:00:58,385 :: INFO :: ----- test -----
2023-05-14 15:00:58,385 :: INFO :: Precision [0.020443255636224685, 0.01432392597849181, 0.011594519351493622]
2023-05-14 15:00:58,385 :: INFO :: Recall [0.011912012115725118, 0.03971189715500929, 0.06157016929311535]
2023-05-14 15:00:58,385 :: INFO :: ndcg [0.020443255636224685, 0.04493406229181916, 0.0574423669418039]
2023-05-14 15:01:14,056 :: INFO :: Epoch 435: loss tensor(36.9012, device='cuda:0', grad_fn=<AddBackward0>), U.norm 308.80657958984375, V.norm 270.2071533203125, MLP.norm 6.856453895568848
2023-05-14 15:02:04,775 :: INFO :: ----- val -----
2023-05-14 15:02:04,775 :: INFO :: Precision [0.022571365346315556, 0.017942022571364753, 0.01575127240539979]
2023-05-14 15:02:04,775 :: INFO :: Recall [0.008957593615663022, 0.03435924613794942, 0.057673445167002534]
2023-05-14 15:02:04,775 :: INFO :: ndcg [0.022571365346315556, 0.05395552626997193, 0.07218558182906583]
2023-05-14 15:02:56,072 :: INFO :: ----- test -----
2023-05-14 15:02:56,072 :: INFO :: Precision [0.019706315846934875, 0.014394890550793798, 0.01158906053823962]
2023-05-14 15:02:56,072 :: INFO :: Recall [0.011241057274785802, 0.040267475488597514, 0.061376391130439065]
2023-05-14 15:02:56,072 :: INFO :: ndcg [0.019706315846934875, 0.044781356969732795, 0.05698740568403289]
2023-05-14 15:03:02,088 :: INFO :: Epoch 440: loss tensor(44.3610, device='cuda:0', grad_fn=<AddBackward0>), U.norm 310.0592041015625, V.norm 270.3209533691406, MLP.norm 6.913342475891113
2023-05-14 15:03:54,510 :: INFO :: ----- val -----
2023-05-14 15:03:54,510 :: INFO :: Precision [0.023058198716530205, 0.01809249834034017, 0.01566718300508999]
2023-05-14 15:03:54,510 :: INFO :: Recall [0.009365696455827736, 0.03441001505463098, 0.05728027559865993]
2023-05-14 15:03:54,510 :: INFO :: ndcg [0.023058198716530205, 0.054410553544217366, 0.07213991299925993]
2023-05-14 15:04:49,026 :: INFO :: ----- test -----
2023-05-14 15:04:49,026 :: INFO :: Precision [0.019242316720344998, 0.014214749713411816, 0.011507178339429618]
2023-05-14 15:04:49,026 :: INFO :: Recall [0.011196332059300915, 0.03959165961150396, 0.06121839695439141]
2023-05-14 15:04:49,026 :: INFO :: ndcg [0.019242316720344998, 0.044343749879975795, 0.0567362660792918]
2023-05-14 15:04:55,510 :: INFO :: Epoch 445: loss tensor(36.6205, device='cuda:0', grad_fn=<AddBackward0>), U.norm 311.28741455078125, V.norm 270.3796081542969, MLP.norm 6.830995082855225
2023-05-14 15:05:50,120 :: INFO :: ----- val -----
2023-05-14 15:05:50,135 :: INFO :: Precision [0.023058198716530205, 0.017968577118831, 0.015459172383270957]
2023-05-14 15:05:50,135 :: INFO :: Recall [0.009490392354288614, 0.034293518127557536, 0.05663736308181747]
2023-05-14 15:05:50,135 :: INFO :: ndcg [0.023058198716530205, 0.05413941089822643, 0.07150045648555216]
2023-05-14 15:06:41,089 :: INFO :: ----- test -----
2023-05-14 15:06:41,089 :: INFO :: Precision [0.02003384464217479, 0.014280255472459804, 0.01150717833942962]
2023-05-14 15:06:41,089 :: INFO :: Recall [0.011908728686799343, 0.04008031980792174, 0.06159562428847044]
2023-05-14 15:06:41,089 :: INFO :: ndcg [0.02003384464217479, 0.04469896032796939, 0.057050082051027164]
2023-05-14 15:06:47,198 :: INFO :: Epoch 450: loss tensor(35.0010, device='cuda:0', grad_fn=<AddBackward0>), U.norm 312.5133056640625, V.norm 270.4683837890625, MLP.norm 6.801501274108887
2023-05-14 15:07:37,104 :: INFO :: ----- val -----
2023-05-14 15:07:37,104 :: INFO :: Precision [0.024518698827174154, 0.018287231688426022, 0.015525558751936603]
2023-05-14 15:07:37,104 :: INFO :: Recall [0.010287763974279144, 0.03446795573833432, 0.056388793293309356]
2023-05-14 15:07:37,104 :: INFO :: ndcg [0.024518698827174154, 0.05558421160042041, 0.07252792648717671]
2023-05-14 15:08:29,104 :: INFO :: ----- test -----
2023-05-14 15:08:29,104 :: INFO :: Precision [0.019979256509634804, 0.0143184671652378, 0.011553578252088621]
2023-05-14 15:08:29,104 :: INFO :: Recall [0.011770580780228664, 0.039464703916217564, 0.06104702534735848]
2023-05-14 15:08:29,104 :: INFO :: ndcg [0.019979256509634804, 0.04505990835541212, 0.05734071892852577]
2023-05-14 15:08:35,229 :: INFO :: Epoch 455: loss tensor(37.3267, device='cuda:0', grad_fn=<AddBackward0>), U.norm 313.70867919921875, V.norm 270.6678466796875, MLP.norm 6.70830774307251
2023-05-14 15:09:27,214 :: INFO :: ----- val -----
2023-05-14 15:09:27,214 :: INFO :: Precision [0.024784244301836688, 0.0185793317105548, 0.015715866342111435]
2023-05-14 15:09:27,214 :: INFO :: Recall [0.010051699955346083, 0.03542452783825994, 0.057015495853962254]
2023-05-14 15:09:27,214 :: INFO :: ndcg [0.024784244301836688, 0.05608939322371401, 0.07323104140952746]
2023-05-14 15:10:18,980 :: INFO :: ----- test -----
2023-05-14 15:10:18,980 :: INFO :: Precision [0.020470549702494678, 0.01432392597849181, 0.011621813417763617]
2023-05-14 15:10:18,980 :: INFO :: Recall [0.011835538626645164, 0.03950403387747045, 0.06181839170097491]
2023-05-14 15:10:18,980 :: INFO :: ndcg [0.020470549702494678, 0.04510991082688246, 0.05768776530578604]
2023-05-14 15:10:25,152 :: INFO :: Epoch 460: loss tensor(37.6355, device='cuda:0', grad_fn=<AddBackward0>), U.norm 314.8777160644531, V.norm 270.6369934082031, MLP.norm 6.768857479095459
2023-05-14 15:11:16,480 :: INFO :: ----- val -----
2023-05-14 15:11:16,480 :: INFO :: Precision [0.025138304934720072, 0.01849081655233396, 0.015658331489267874]
2023-05-14 15:11:16,480 :: INFO :: Recall [0.01002763558091677, 0.0353093446431293, 0.05741659715759349]
2023-05-14 15:11:16,480 :: INFO :: ndcg [0.025138304934720072, 0.05635596924852024, 0.07338066080370287]
2023-05-14 15:12:07,792 :: INFO :: ----- test -----
2023-05-14 15:12:07,792 :: INFO :: Precision [0.020443255636224685, 0.014422184617063802, 0.011703695616573619]
2023-05-14 15:12:07,792 :: INFO :: Recall [0.011661733167118148, 0.04023485178152618, 0.06275552494314926]
2023-05-14 15:12:07,792 :: INFO :: ndcg [0.020443255636224685, 0.04540965491993506, 0.058151482365924385]
2023-05-14 15:12:13,964 :: INFO :: Epoch 465: loss tensor(41.2098, device='cuda:0', grad_fn=<AddBackward0>), U.norm 316.0052490234375, V.norm 270.5740661621094, MLP.norm 6.826889514923096
2023-05-14 15:13:06,652 :: INFO :: ----- val -----
2023-05-14 15:13:06,652 :: INFO :: Precision [0.024474441248063732, 0.018650143837131476, 0.015906173932286286]
2023-05-14 15:13:06,652 :: INFO :: Recall [0.010155417368553025, 0.03553547177106005, 0.057777000305478055]
2023-05-14 15:13:06,652 :: INFO :: ndcg [0.024474441248063732, 0.05636826045900193, 0.07391383815855752]
2023-05-14 15:13:59,152 :: INFO :: ----- test -----
2023-05-14 15:13:59,152 :: INFO :: Precision [0.021016431027894536, 0.014564113761667795, 0.011793766035264608]
2023-05-14 15:13:59,152 :: INFO :: Recall [0.011967959042443257, 0.040432216143676085, 0.06280102048245471]
2023-05-14 15:13:59,152 :: INFO :: ndcg [0.021016431027894536, 0.04600545941514815, 0.058621078720448964]
2023-05-14 15:14:05,308 :: INFO :: Epoch 470: loss tensor(35.9532, device='cuda:0', grad_fn=<AddBackward0>), U.norm 317.22491455078125, V.norm 270.5791320800781, MLP.norm 6.686929702758789
2023-05-14 15:14:57,918 :: INFO :: ----- val -----
2023-05-14 15:14:57,918 :: INFO :: Precision [0.023987607877849083, 0.018570480194732713, 0.016083204248728004]
2023-05-14 15:14:57,918 :: INFO :: Recall [0.009749725888190093, 0.03557868121152154, 0.05808251037085316]
2023-05-14 15:14:57,918 :: INFO :: ndcg [0.023987607877849083, 0.056014266616316095, 0.07401386302238781]
2023-05-14 15:15:50,183 :: INFO :: ----- test -----
2023-05-14 15:15:50,183 :: INFO :: Precision [0.019951962443364814, 0.01454773732190579, 0.011654566297287618]
2023-05-14 15:15:50,183 :: INFO :: Recall [0.011283685217067109, 0.04032081338164239, 0.06187368142869398]
2023-05-14 15:15:50,183 :: INFO :: ndcg [0.019951962443364814, 0.045537996989317826, 0.05780800413471956]
2023-05-14 15:15:56,324 :: INFO :: Epoch 475: loss tensor(35.4494, device='cuda:0', grad_fn=<AddBackward0>), U.norm 318.3228454589844, V.norm 270.7464294433594, MLP.norm 6.783393859863281
2023-05-14 15:16:48,262 :: INFO :: ----- val -----
2023-05-14 15:16:48,262 :: INFO :: Precision [0.024562956406284576, 0.018508519583978124, 0.015915025448108362]
2023-05-14 15:16:48,262 :: INFO :: Recall [0.010183013663327915, 0.0354094626797502, 0.05768339521302894]
2023-05-14 15:16:48,262 :: INFO :: ndcg [0.024562956406284576, 0.056229987655529744, 0.07388817565854898]
2023-05-14 15:17:39,778 :: INFO :: ----- test -----
2023-05-14 15:17:39,778 :: INFO :: Precision [0.020252197172334733, 0.014411266990555796, 0.011714613243081609]
2023-05-14 15:17:39,778 :: INFO :: Recall [0.011842032213610283, 0.040011230306371416, 0.06219890327566154]
2023-05-14 15:17:39,778 :: INFO :: ndcg [0.020252197172334733, 0.04522490930462184, 0.057816996195593606]
2023-05-14 15:17:45,934 :: INFO :: Epoch 480: loss tensor(37.8586, device='cuda:0', grad_fn=<AddBackward0>), U.norm 319.4659118652344, V.norm 270.947265625, MLP.norm 6.827201843261719
2023-05-14 15:18:37,309 :: INFO :: ----- val -----
2023-05-14 15:18:37,309 :: INFO :: Precision [0.024607213985394998, 0.018579331710554804, 0.015782252710777083]
2023-05-14 15:18:37,309 :: INFO :: Recall [0.010392017898491642, 0.035366689295461544, 0.05726964526352378]
2023-05-14 15:18:37,309 :: INFO :: ndcg [0.024607213985394998, 0.05618240705514828, 0.07342972163704407]
2023-05-14 15:19:28,419 :: INFO :: ----- test -----
2023-05-14 15:19:28,419 :: INFO :: Precision [0.020470549702494678, 0.0144221846170638, 0.011728260276216613]
2023-05-14 15:19:28,419 :: INFO :: Recall [0.011932739906038648, 0.039904358936515505, 0.062010730646841615]
2023-05-14 15:19:28,419 :: INFO :: ndcg [0.020470549702494678, 0.04530904951622191, 0.057984124605043666]
2023-05-14 15:19:34,575 :: INFO :: Epoch 485: loss tensor(36.5108, device='cuda:0', grad_fn=<AddBackward0>), U.norm 320.6748046875, V.norm 271.0616760253906, MLP.norm 6.749453067779541
2023-05-14 15:20:25,762 :: INFO :: ----- val -----
2023-05-14 15:20:25,762 :: INFO :: Precision [0.025757911042265987, 0.018880283248505658, 0.015919451206019387]
2023-05-14 15:20:25,762 :: INFO :: Recall [0.01105642633742656, 0.03598081076380289, 0.05770100750178076]
2023-05-14 15:20:25,762 :: INFO :: ndcg [0.025757911042265987, 0.05699026383584337, 0.07415586937509915]
2023-05-14 15:21:17,356 :: INFO :: ----- test -----
2023-05-14 15:21:17,356 :: INFO :: Precision [0.020907254762814565, 0.01462961952071579, 0.01181833069490761]
2023-05-14 15:21:17,356 :: INFO :: Recall [0.012068583794629897, 0.040690834169701144, 0.06257081039084507]
2023-05-14 15:21:17,356 :: INFO :: ndcg [0.020907254762814565, 0.04597195008600468, 0.05867507911447415]
2023-05-14 15:21:23,340 :: INFO :: Epoch 490: loss tensor(36.7626, device='cuda:0', grad_fn=<AddBackward0>), U.norm 321.8374328613281, V.norm 271.34521484375, MLP.norm 6.72453498840332
2023-05-14 15:22:13,997 :: INFO :: ----- val -----
2023-05-14 15:22:13,997 :: INFO :: Precision [0.02496127461827838, 0.018774065058640637, 0.016003540606329223]
2023-05-14 15:22:13,997 :: INFO :: Recall [0.010634767296224527, 0.035889191659675225, 0.058718318782793176]
2023-05-14 15:22:13,997 :: INFO :: ndcg [0.02496127461827838, 0.05685394110028027, 0.07431238862183069]
2023-05-14 15:23:04,403 :: INFO :: ----- test -----
2023-05-14 15:23:04,403 :: INFO :: Precision [0.020061138708444785, 0.014536819695397796, 0.01181833069490761]
2023-05-14 15:23:04,403 :: INFO :: Recall [0.011656726830473198, 0.040616884458295845, 0.06277810114098095]
2023-05-14 15:23:04,403 :: INFO :: ndcg [0.020061138708444785, 0.04550726220137946, 0.058319614070604986]
2023-05-14 15:23:10,512 :: INFO :: Epoch 495: loss tensor(35.3326, device='cuda:0', grad_fn=<AddBackward0>), U.norm 322.9732360839844, V.norm 271.52191162109375, MLP.norm 6.674963474273682
2023-05-14 15:24:01,950 :: INFO :: ----- val -----
2023-05-14 15:24:01,950 :: INFO :: Precision [0.025359592830272185, 0.018836025669395233, 0.016118610312016328]
2023-05-14 15:24:01,950 :: INFO :: Recall [0.010660727078324977, 0.03610667488295915, 0.05935304700149384]
2023-05-14 15:24:01,950 :: INFO :: ndcg [0.025359592830272185, 0.056827231090907655, 0.07475721643462147]
2023-05-14 15:24:52,465 :: INFO :: ----- test -----
2023-05-14 15:24:52,465 :: INFO :: Precision [0.02068890223265462, 0.014525902068889797, 0.01181833069490762]
2023-05-14 15:24:52,465 :: INFO :: Recall [0.012049437440035164, 0.04050381392486439, 0.06302838128824442]
2023-05-14 15:24:52,465 :: INFO :: ndcg [0.02068890223265462, 0.04571860459935527, 0.05867445681735439]
2023-05-14 15:24:57,465 :: INFO :: Epoch 500:
2023-05-14 15:25:48,340 :: INFO :: ----- val -----
2023-05-14 15:25:48,340 :: INFO :: Precision [0.024651471564505423, 0.019030759017481084, 0.01585306483735379]
2023-05-14 15:25:48,340 :: INFO :: Recall [0.010198043374262765, 0.03636602602228438, 0.058328574997972914]
2023-05-14 15:25:48,340 :: INFO :: ndcg [0.024651471564505423, 0.057094045002911174, 0.0740166935998915]
2023-05-14 15:26:40,747 :: INFO :: ----- test -----
2023-05-14 15:26:40,747 :: INFO :: Precision [0.02093454882908456, 0.014711501719525781, 0.011788307222010623]
2023-05-14 15:26:40,747 :: INFO :: Recall [0.012067736352601171, 0.04082842373140853, 0.06300113625007792]
2023-05-14 15:26:40,747 :: INFO :: ndcg [0.02093454882908456, 0.04619078233728112, 0.0587604688040702]
2023-05-14 15:26:40,747 :: INFO :: final:
2023-05-14 15:26:40,747 :: INFO :: ----- test -----
2023-05-14 15:26:40,747 :: INFO :: Precision [0.02068890223265462, 0.014525902068889797, 0.01181833069490762]
2023-05-14 15:26:40,747 :: INFO :: Recall [0.012049437440035164, 0.04050381392486439, 0.06302838128824442]
2023-05-14 15:26:40,747 :: INFO :: ndcg [0.02068890223265462, 0.04571860459935527, 0.05867445681735439]
2023-05-14 15:26:40,747 :: INFO :: max_epoch 495:
