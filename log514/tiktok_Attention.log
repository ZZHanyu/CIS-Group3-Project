2023-05-09 10:53:25,429 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 10:53:25,429 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:02:20,820 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:02:20,820 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:03:01,289 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:03:01,289 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:03:02,867 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:04:10,132 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:04:10,132 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:04:11,726 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:07:35,351 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:07:35,351 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:07:36,835 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:15:08,679 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:15:08,679 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:15:10,163 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:16:08,038 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:16:08,038 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:16:09,570 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:17:28,945 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:17:28,945 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:17:30,429 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:18:10,319 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:18:10,319 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:18:11,820 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:31:20,710 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:31:20,710 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:31:22,444 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:41:30,054 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:41:30,054 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:41:31,663 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:42:10,023 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:42:10,023 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:42:11,507 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:44:26,992 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:44:26,992 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:44:28,585 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:46:08,226 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:46:08,226 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:46:09,804 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:49:46,538 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 11:49:46,538 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 11:49:48,101 :: INFO :: torch.Size([76083, 384])
2023-05-09 11:50:08,320 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:05:09,648 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:05:09,648 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:05:11,273 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:05:31,867 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:14:47,726 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:14:47,726 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:14:49,288 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:15:10,054 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:17:04,726 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:17:04,726 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:17:06,273 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:17:26,617 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:30:39,867 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:30:39,867 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=512, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=512, tolog=1, wdi=2)
2023-05-09 12:30:41,507 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:31:01,804 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:31:21,913 :: INFO :: Epoch 5: loss tensor(0.9275, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.3382109999656677, V.norm 0.34504234790802, MLP.norm 0.024449408054351807
2023-05-09 12:31:44,288 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:31:44,288 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:31:45,819 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:32:05,429 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:32:23,804 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.011279151774942875, V.norm 0.011505632661283016, MLP.norm 0.0008211754029616714
2023-05-09 12:32:44,882 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:32:44,882 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-09 12:32:46,445 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:33:06,913 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:35:14,538 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:35:14,538 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:35:16,148 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:35:36,069 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:35:53,804 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.009770270437002182, V.norm 0.011513358913362026, MLP.norm 0.0005819871439598501
2023-05-09 12:36:12,819 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:36:12,819 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=128, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=256, tolog=1, wdi=2)
2023-05-09 12:36:14,398 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:36:34,398 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:42:53,976 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-09 12:42:53,976 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=32, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-09 12:42:55,570 :: INFO :: torch.Size([76083, 384])
2023-05-09 12:43:15,835 :: INFO :: ERM mask: from pre-train tensor([0.5523, 0.5259, 0.5222, 0.5572, 0.5327, 0.5070, 0.5500, 0.5492, 0.5193,
        0.5253, 0.5332, 0.4896, 0.5381, 0.5486, 0.5384, 0.5155, 0.5169, 0.5520,
        0.5182, 0.5483, 0.5210, 0.5311, 0.5536, 0.5246, 0.5053, 0.5231, 0.5224,
        0.5452, 0.5878, 0.5132, 0.5371, 0.5449, 0.5574, 0.5425, 0.4957, 0.4715,
        0.5335, 0.5379, 0.5006, 0.5452, 0.4860, 0.5240, 0.5384, 0.5114, 0.5147,
        0.5179, 0.5337, 0.5109, 0.5339, 0.5208, 0.5157, 0.5156, 0.5030, 0.5171,
        0.5412, 0.5667, 0.5418, 0.5779, 0.5301, 0.5018, 0.5237, 0.5331, 0.5018,
        0.5221, 0.5412, 0.5186, 0.5697, 0.5200, 0.5374, 0.4886, 0.5462, 0.5385,
        0.5334, 0.5632, 0.5535, 0.5001, 0.5610, 0.5176, 0.5444, 0.5573, 0.5491,
        0.5457, 0.5399, 0.5795, 0.4930, 0.5522, 0.5268, 0.5256, 0.5224, 0.4933,
        0.5170, 0.5647, 0.5282, 0.5471, 0.5299, 0.5576, 0.5260, 0.5116, 0.5053,
        0.5105, 0.5310, 0.5316, 0.5313, 0.5211, 0.5433, 0.4741, 0.5302, 0.4994,
        0.5328, 0.5490, 0.5356, 0.5382, 0.5416, 0.5427, 0.4899, 0.4788, 0.5292,
        0.5111, 0.5119, 0.5341, 0.5110, 0.5220, 0.5238, 0.5114, 0.5268, 0.5035,
        0.5178, 0.5598, 0.6188, 0.6112, 0.6297, 0.5914, 0.5822, 0.6098, 0.5972,
        0.5793, 0.5546, 0.5439, 0.5836, 0.5087, 0.6384, 0.5124, 0.6331, 0.6191,
        0.6379, 0.6253, 0.5755, 0.6454, 0.6367, 0.6163, 0.5238, 0.6290, 0.6323,
        0.4405, 0.5312, 0.5224, 0.5356, 0.4422, 0.5724, 0.4324, 0.6181, 0.4321,
        0.6054, 0.5531, 0.5971, 0.5355, 0.4743, 0.5560, 0.5408, 0.6033, 0.6434,
        0.5992, 0.4383, 0.6331, 0.6397, 0.5495, 0.5566, 0.6159, 0.5573, 0.5036,
        0.6179, 0.5539, 0.5696, 0.4443, 0.6122, 0.6447, 0.4550, 0.6233, 0.6560,
        0.4388, 0.4753, 0.5810, 0.6089, 0.4624, 0.5718, 0.6314, 0.6283, 0.5277,
        0.6181, 0.6399, 0.6389, 0.6474, 0.5974, 0.5941, 0.6270, 0.6447, 0.5028,
        0.4349, 0.5705, 0.5839, 0.6426, 0.6031, 0.5620, 0.5096, 0.6088, 0.6536,
        0.6060, 0.4722, 0.6198, 0.4432, 0.6411, 0.5622, 0.6283, 0.6331, 0.5882,
        0.6428, 0.6067, 0.6445, 0.5770, 0.6447, 0.6117, 0.5690, 0.5810, 0.5622,
        0.6089, 0.6416, 0.5664, 0.5543, 0.5334, 0.6065, 0.6115, 0.6256, 0.6344,
        0.6195, 0.5579, 0.6275, 0.4503, 0.6084, 0.5772, 0.5515, 0.5226, 0.6330,
        0.6331, 0.6493, 0.4748, 0.6175, 0.6572, 0.5928, 0.6355, 0.6349, 0.6353,
        0.6695, 0.6037, 0.6250, 0.5876, 0.6446, 0.6170, 0.5594, 0.6146, 0.6268,
        0.5421, 0.6314, 0.5941, 0.5577, 0.6264, 0.5975, 0.6488, 0.6527, 0.5910,
        0.6111, 0.6245, 0.6489, 0.6085, 0.6041, 0.6553, 0.6085, 0.6204, 0.6403,
        0.5950, 0.6281, 0.6303, 0.6395, 0.6359, 0.6108, 0.5381, 0.6115, 0.6221,
        0.5269, 0.6162, 0.6224, 0.5490, 0.6288, 0.6160, 0.5294, 0.6583, 0.5966,
        0.6049, 0.5663, 0.6351, 0.6151, 0.5785, 0.6551, 0.6571, 0.6495, 0.6082,
        0.6212, 0.6220, 0.6025, 0.6219, 0.5652, 0.6332, 0.5718, 0.6434, 0.5699,
        0.6529, 0.5734, 0.6102, 0.6301, 0.5981, 0.6002, 0.6296, 0.5708, 0.6193,
        0.6039, 0.6597, 0.6034, 0.5672, 0.6290, 0.6162, 0.5913, 0.6254, 0.6155,
        0.6071, 0.6643, 0.6338, 0.6073, 0.5505, 0.5813, 0.5580, 0.6123, 0.6094,
        0.5500, 0.6313, 0.5396, 0.6448, 0.5937, 0.5418, 0.6394, 0.5797, 0.6368,
        0.6088, 0.5735, 0.6118, 0.6520, 0.6552, 0.6271, 0.6436, 0.5578, 0.5755,
        0.6347, 0.6170, 0.6345, 0.6267, 0.6242, 0.6599, 0.6558, 0.6137, 0.6050,
        0.6174, 0.5227, 0.6234, 0.6302, 0.6345, 0.6083], dtype=torch.float64)
2023-05-09 12:43:33,585 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.009770270437002182, V.norm 0.011513358913362026, MLP.norm 0.0005819871439598501
2023-05-09 12:44:28,445 :: INFO :: ----- val -----
2023-05-09 12:44:28,445 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:44:28,445 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:44:28,445 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 12:45:23,694 :: INFO :: ----- test -----
2023-05-09 12:45:23,694 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 5.458813253998581e-06]
2023-05-09 12:45:23,694 :: INFO :: Recall [0.0, 5.458813253998581e-05, 5.458813253998581e-05]
2023-05-09 12:45:23,694 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 2.1117572314784737e-05]
2023-05-09 12:45:29,585 :: INFO :: Epoch 10: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.462642441969365e-05, V.norm 2.9048382202745415e-05, MLP.norm 1.4902383327353164e-06
2023-05-09 12:46:25,070 :: INFO :: ----- val -----
2023-05-09 12:46:25,070 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:46:25,070 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:46:25,070 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 12:47:19,679 :: INFO :: ----- test -----
2023-05-09 12:47:19,679 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 12:47:19,679 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 12:47:19,679 :: INFO :: ndcg [0.0, 0.0, 9.098022089997634e-06]
2023-05-09 12:47:25,445 :: INFO :: Epoch 15: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 6.051975987020342e-08, V.norm 7.146434910509925e-08, MLP.norm 3.6609961728828466e-09
2023-05-09 12:48:19,961 :: INFO :: ----- val -----
2023-05-09 12:48:19,961 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:48:19,961 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:48:19,961 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:49:14,194 :: INFO :: ----- test -----
2023-05-09 12:49:14,194 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 12:49:14,194 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 12:49:14,194 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.08676716406861e-05]
2023-05-09 12:49:19,898 :: INFO :: Epoch 20: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.4722492669427822e-10, V.norm 1.73727879304586e-10, MLP.norm 8.995852673887583e-12
2023-05-09 12:50:14,163 :: INFO :: ----- val -----
2023-05-09 12:50:14,163 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:50:14,163 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:50:14,163 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 12:51:10,366 :: INFO :: ----- test -----
2023-05-09 12:51:10,366 :: INFO :: Precision [0.0, 5.458813253998581e-06, 2.7294066269992907e-06]
2023-05-09 12:51:10,366 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 2.7294066269992904e-05]
2023-05-09 12:51:10,366 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.0558786157392369e-05]
2023-05-09 12:51:16,210 :: INFO :: Epoch 25: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 3.550328976122119e-13, V.norm 4.2008388954906106e-13, MLP.norm 2.159724798360494e-14
2023-05-09 12:52:08,445 :: INFO :: ----- val -----
2023-05-09 12:52:08,445 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:52:08,445 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:52:08,445 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 12:53:01,085 :: INFO :: ----- test -----
2023-05-09 12:53:01,085 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-09 12:53:01,085 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-09 12:53:01,085 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 4.9397789354581314e-05]
2023-05-09 12:53:06,945 :: INFO :: Epoch 30: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 8.551658929160401e-16, V.norm 1.0109702449647394e-15, MLP.norm 5.1804488731958706e-17
2023-05-09 12:54:00,569 :: INFO :: ----- val -----
2023-05-09 12:54:00,569 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 12:54:00,569 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 12:54:00,569 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 12:54:54,835 :: INFO :: ----- test -----
2023-05-09 12:54:54,835 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.3647033134996452e-05]
2023-05-09 12:54:54,835 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00013647033134996453]
2023-05-09 12:54:54,835 :: INFO :: ndcg [0.0, 3.596073380937646e-05, 5.3899409031058956e-05]
2023-05-09 12:55:00,744 :: INFO :: Epoch 35: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.049087263922991e-18, V.norm 2.4256051963012565e-18, MLP.norm 1.236366440561638e-19
2023-05-09 12:55:57,085 :: INFO :: ----- val -----
2023-05-09 12:55:57,085 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:55:57,085 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:55:57,085 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:56:51,744 :: INFO :: ----- test -----
2023-05-09 12:56:51,744 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 12:56:51,744 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 12:56:51,744 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 3.6392088359990535e-05]
2023-05-09 12:56:57,572 :: INFO :: Epoch 40: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.6740958743820435e-22, V.norm 1.9087629348058244e-22, MLP.norm 3.743392066509216e-23
2023-05-09 12:57:53,337 :: INFO :: ----- val -----
2023-05-09 12:57:53,337 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:57:53,337 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:57:53,337 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 12:58:48,228 :: INFO :: ----- test -----
2023-05-09 12:58:48,228 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 12:58:48,228 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 12:58:48,228 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 12:58:54,181 :: INFO :: Epoch 45: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-09 12:59:48,541 :: INFO :: ----- val -----
2023-05-09 12:59:48,541 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 12:59:48,541 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 12:59:48,541 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:00:42,384 :: INFO :: ----- test -----
2023-05-09 13:00:42,384 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:00:42,384 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:00:42,384 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:00:48,058 :: INFO :: Epoch 50: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-09 13:01:42,167 :: INFO :: ----- val -----
2023-05-09 13:01:42,167 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:01:42,167 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:01:42,167 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:02:37,761 :: INFO :: ----- test -----
2023-05-09 13:02:37,761 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:02:37,761 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:02:37,761 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:02:43,636 :: INFO :: Epoch 55: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 9.061725165810525e-14
2023-05-09 13:03:41,591 :: INFO :: ----- val -----
2023-05-09 13:03:41,591 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:03:41,591 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:03:41,591 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:04:38,591 :: INFO :: ----- test -----
2023-05-09 13:04:38,591 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:04:38,591 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 13:04:38,591 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-09 13:04:44,435 :: INFO :: Epoch 60: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011874802876263857
2023-05-09 13:05:38,748 :: INFO :: ----- val -----
2023-05-09 13:05:38,748 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:05:38,748 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:05:38,748 :: INFO :: ndcg [0.0, 0.0, 2.5586618837609017e-05]
2023-05-09 13:06:32,484 :: INFO :: ----- test -----
2023-05-09 13:06:32,484 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.637643976199574e-05]
2023-05-09 13:06:32,484 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.0001637643976199574]
2023-05-09 13:06:32,484 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 5.458326008491455e-05]
2023-05-09 13:06:38,343 :: INFO :: Epoch 65: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011640415759757161
2023-05-09 13:07:32,078 :: INFO :: ----- val -----
2023-05-09 13:07:32,078 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:07:32,078 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:07:32,078 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-09 13:08:27,156 :: INFO :: ----- test -----
2023-05-09 13:08:27,156 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:08:27,156 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:08:27,156 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:08:33,109 :: INFO :: Epoch 70: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011022815015166998
2023-05-09 13:09:27,531 :: INFO :: ----- val -----
2023-05-09 13:09:27,531 :: INFO :: Precision [0.0, 8.851515822084533e-06, 1.32772737331268e-05]
2023-05-09 13:09:27,531 :: INFO :: Recall [0.0, 4.425757911042266e-05, 0.00013277273733126798]
2023-05-09 13:09:27,531 :: INFO :: ndcg [0.0, 1.7121168720271814e-05, 4.323733698678516e-05]
2023-05-09 13:10:23,000 :: INFO :: ----- test -----
2023-05-09 13:10:23,000 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.637643976199574e-05]
2023-05-09 13:10:23,000 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.0001637643976199574]
2023-05-09 13:10:23,000 :: INFO :: ndcg [0.0, 4.5000063168771676e-05, 7.291844991954106e-05]
2023-05-09 13:10:28,968 :: INFO :: Epoch 75: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012304242700338364
2023-05-09 13:11:23,625 :: INFO :: ----- val -----
2023-05-09 13:11:23,625 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:11:23,625 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:11:23,625 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 13:12:18,172 :: INFO :: ----- test -----
2023-05-09 13:12:18,172 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 2.456465964299361e-05]
2023-05-09 13:12:18,172 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.0002456465964299361]
2023-05-09 13:12:18,172 :: INFO :: ndcg [0.0, 3.596073380937646e-05, 9.174096009636299e-05]
2023-05-09 13:12:23,999 :: INFO :: Epoch 80: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001238734694197774
2023-05-09 13:13:19,093 :: INFO :: ----- val -----
2023-05-09 13:13:19,093 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:13:19,093 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:13:19,093 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:14:12,719 :: INFO :: ----- test -----
2023-05-09 13:14:12,719 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 13:14:12,719 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 13:14:12,719 :: INFO :: ndcg [0.0, 0.0, 1.833266182361895e-05]
2023-05-09 13:14:18,360 :: INFO :: Epoch 85: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012542895274236798
2023-05-09 13:15:10,594 :: INFO :: ----- val -----
2023-05-09 13:15:10,594 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:15:10,594 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:15:10,594 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:16:03,213 :: INFO :: ----- test -----
2023-05-09 13:16:03,213 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:16:03,213 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:16:03,213 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.5110393031534075e-05]
2023-05-09 13:16:08,838 :: INFO :: Epoch 90: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0016105335671454668
2023-05-09 13:17:03,260 :: INFO :: ----- val -----
2023-05-09 13:17:03,260 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:17:03,260 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:17:03,260 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 13:17:57,526 :: INFO :: ----- test -----
2023-05-09 13:17:57,526 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:17:57,526 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 13:17:57,526 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 3.9089354184380295e-05]
2023-05-09 13:18:03,401 :: INFO :: Epoch 95: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011949132895097136
2023-05-09 13:18:58,807 :: INFO :: ----- val -----
2023-05-09 13:18:58,807 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:18:58,807 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:18:58,807 :: INFO :: ndcg [0.0, 0.0, 2.5586618837609017e-05]
2023-05-09 13:19:54,741 :: INFO :: ----- test -----
2023-05-09 13:19:54,741 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.3647033134996452e-05]
2023-05-09 13:19:54,741 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00013647033134996453]
2023-05-09 13:19:54,741 :: INFO :: ndcg [0.0, 4.619619152836695e-05, 5.15912573535344e-05]
2023-05-09 13:20:00,756 :: INFO :: Epoch 100: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001250851433724165
2023-05-09 13:20:55,943 :: INFO :: ----- val -----
2023-05-09 13:20:55,943 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:20:55,943 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:20:55,943 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:21:50,444 :: INFO :: ----- test -----
2023-05-09 13:21:50,444 :: INFO :: Precision [0.0, 2.1835253015994326e-05, 2.1835253015994322e-05]
2023-05-09 13:21:50,444 :: INFO :: Recall [0.0, 0.00010917626507997162, 0.00021835253015994323]
2023-05-09 13:21:50,444 :: INFO :: ndcg [0.0, 5.507349093307493e-05, 9.050838439175812e-05]
2023-05-09 13:21:56,146 :: INFO :: Epoch 105: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0011035866336897016
2023-05-09 13:22:54,647 :: INFO :: ----- val -----
2023-05-09 13:22:54,647 :: INFO :: Precision [0.0, 0.0, 1.32772737331268e-05]
2023-05-09 13:22:54,647 :: INFO :: Recall [0.0, 0.0, 0.00013277273733126798]
2023-05-09 13:22:54,647 :: INFO :: ndcg [0.0, 0.0, 4.526992050341619e-05]
2023-05-09 13:23:50,053 :: INFO :: ----- test -----
2023-05-09 13:23:50,053 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:23:50,053 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:23:50,053 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.844854068323679e-05]
2023-05-09 13:23:56,006 :: INFO :: Epoch 110: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0012372490018606186
2023-05-09 13:24:53,162 :: INFO :: ----- val -----
2023-05-09 13:24:53,162 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:24:53,162 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:24:53,162 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:25:47,852 :: INFO :: ----- test -----
2023-05-09 13:25:47,852 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:25:47,852 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:25:47,852 :: INFO :: ndcg [0.0, 0.0, 2.5042984554661586e-05]
2023-05-09 13:25:53,633 :: INFO :: Epoch 115: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014067536685615778
2023-05-09 13:26:47,602 :: INFO :: ----- val -----
2023-05-09 13:26:47,602 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:26:47,602 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:26:47,602 :: INFO :: ndcg [0.0, 0.0, 2.754583578894539e-05]
2023-05-09 13:27:41,586 :: INFO :: ----- test -----
2023-05-09 13:27:41,586 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 13:27:41,586 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 13:27:41,586 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.596073380937646e-05]
2023-05-09 13:27:47,461 :: INFO :: Epoch 120: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014542959397658706
2023-05-09 13:28:41,867 :: INFO :: ----- val -----
2023-05-09 13:28:41,867 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:28:41,867 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:28:41,867 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:29:37,820 :: INFO :: ----- test -----
2023-05-09 13:29:37,820 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:29:37,820 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:29:37,820 :: INFO :: ndcg [0.0, 0.0, 2.7385438061145575e-05]
2023-05-09 13:29:43,570 :: INFO :: Epoch 125: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001421201042830944
2023-05-09 13:30:37,476 :: INFO :: ----- val -----
2023-05-09 13:30:37,476 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:30:37,476 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:30:37,476 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:31:32,617 :: INFO :: ----- test -----
2023-05-09 13:31:32,617 :: INFO :: Precision [2.7294066269992904e-05, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 13:31:32,617 :: INFO :: Recall [2.7294066269992904e-05, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 13:31:32,617 :: INFO :: ndcg [2.7294066269992904e-05, 2.7294066269992904e-05, 5.652211579591172e-05]
2023-05-09 13:31:38,320 :: INFO :: Epoch 130: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.00179652264341712
2023-05-09 13:32:31,664 :: INFO :: ----- val -----
2023-05-09 13:32:31,664 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:32:31,664 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:32:31,664 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:33:27,370 :: INFO :: ----- test -----
2023-05-09 13:33:27,370 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 13:33:27,370 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 13:33:27,370 :: INFO :: ndcg [0.0, 0.0, 1.0558786157392369e-05]
2023-05-09 13:33:33,057 :: INFO :: Epoch 135: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0019852654077112675
2023-05-09 13:34:26,245 :: INFO :: ----- val -----
2023-05-09 13:34:26,245 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:34:26,245 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:34:26,245 :: INFO :: ndcg [0.0, 0.0, 2.8714238111045017e-05]
2023-05-09 13:35:21,463 :: INFO :: ----- test -----
2023-05-09 13:35:21,463 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.637643976199574e-05]
2023-05-09 13:35:21,463 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00015011736448496097]
2023-05-09 13:35:21,463 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 6.0757409596059037e-05]
2023-05-09 13:35:27,401 :: INFO :: Epoch 140: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017755500739440322
2023-05-09 13:36:24,231 :: INFO :: ----- val -----
2023-05-09 13:36:24,231 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:36:24,231 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:36:24,231 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 13:37:19,356 :: INFO :: ----- test -----
2023-05-09 13:37:19,356 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:37:19,356 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:37:19,356 :: INFO :: ndcg [0.0, 0.0, 2.6222416349463367e-05]
2023-05-09 13:37:25,262 :: INFO :: Epoch 145: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017130282940343022
2023-05-09 13:38:18,341 :: INFO :: ----- val -----
2023-05-09 13:38:18,341 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:38:18,341 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:38:18,341 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:39:13,075 :: INFO :: ----- test -----
2023-05-09 13:39:13,075 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 13:39:13,075 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 13:39:13,075 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 3.158570835667895e-05]
2023-05-09 13:39:19,059 :: INFO :: Epoch 150: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017711068503558636
2023-05-09 13:40:13,200 :: INFO :: ----- val -----
2023-05-09 13:40:13,216 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:40:13,216 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:40:13,216 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 13:41:06,622 :: INFO :: ----- test -----
2023-05-09 13:41:06,622 :: INFO :: Precision [0.0, 2.1835253015994326e-05, 1.3647033134996452e-05]
2023-05-09 13:41:06,622 :: INFO :: Recall [0.0, 0.00010917626507997162, 0.00013647033134996453]
2023-05-09 13:41:06,622 :: INFO :: ndcg [0.0, 4.960776694437291e-05, 5.8693649829812595e-05]
2023-05-09 13:41:12,356 :: INFO :: Epoch 155: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0018197373719885945
2023-05-09 13:42:06,326 :: INFO :: ----- val -----
2023-05-09 13:42:06,326 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:42:06,326 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:42:06,326 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:43:00,967 :: INFO :: ----- test -----
2023-05-09 13:43:00,967 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:43:00,967 :: INFO :: Recall [0.0, 5.458813253998581e-05, 9.552923194497516e-05]
2023-05-09 13:43:00,967 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 3.678120250685574e-05]
2023-05-09 13:43:06,702 :: INFO :: Epoch 160: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0016893809661269188
2023-05-09 13:44:00,389 :: INFO :: ----- val -----
2023-05-09 13:44:00,389 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:44:00,389 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:44:00,389 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:44:55,486 :: INFO :: ----- test -----
2023-05-09 13:44:55,486 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:44:55,486 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:44:55,486 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.9656808247390002e-05]
2023-05-09 13:45:01,235 :: INFO :: Epoch 165: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0021255980245769024
2023-05-09 13:45:57,079 :: INFO :: ----- val -----
2023-05-09 13:45:57,079 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:45:57,079 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:45:57,079 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 13:46:51,329 :: INFO :: ----- test -----
2023-05-09 13:46:51,329 :: INFO :: Precision [5.458813253998581e-05, 2.1835253015994326e-05, 3.275287952399148e-05]
2023-05-09 13:46:51,329 :: INFO :: Recall [5.458813253998581e-05, 0.00010917626507997162, 0.0003275287952399148]
2023-05-09 13:46:51,329 :: INFO :: ndcg [5.458813253998581e-05, 7.99900801919699e-05, 0.0001525794239754394]
2023-05-09 13:46:57,236 :: INFO :: Epoch 170: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001959695480763912
2023-05-09 13:47:50,566 :: INFO :: ----- val -----
2023-05-09 13:47:50,566 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 13:47:50,566 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 13:47:50,566 :: INFO :: ndcg [0.0, 0.0, 2.9087726610843255e-05]
2023-05-09 13:48:45,128 :: INFO :: ----- test -----
2023-05-09 13:48:45,128 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:48:45,128 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:48:45,128 :: INFO :: ndcg [0.0, 0.0, 2.399584170259722e-05]
2023-05-09 13:48:50,972 :: INFO :: Epoch 175: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0020478025544434786
2023-05-09 13:49:44,128 :: INFO :: ----- val -----
2023-05-09 13:49:44,144 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 13:49:44,144 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 13:49:44,144 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 13:50:37,550 :: INFO :: ----- test -----
2023-05-09 13:50:37,550 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 13:50:37,550 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 13:50:37,550 :: INFO :: ndcg [0.0, 0.0, 1.9169105410237195e-05]
2023-05-09 13:50:43,316 :: INFO :: Epoch 180: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0018744878470897675
2023-05-09 13:51:37,441 :: INFO :: ----- val -----
2023-05-09 13:51:37,441 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:51:37,441 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:51:37,441 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:52:32,222 :: INFO :: ----- test -----
2023-05-09 13:52:32,222 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 13:52:32,222 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 13:52:32,222 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.6942981076463772e-05]
2023-05-09 13:52:38,035 :: INFO :: Epoch 185: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0020866640843451023
2023-05-09 13:53:31,175 :: INFO :: ----- val -----
2023-05-09 13:53:31,175 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:53:31,175 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:53:31,175 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:54:23,847 :: INFO :: ----- test -----
2023-05-09 13:54:23,847 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 13:54:23,847 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 13:54:23,847 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.996569373068374e-05]
2023-05-09 13:54:29,456 :: INFO :: Epoch 190: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001873630448244512
2023-05-09 13:55:23,956 :: INFO :: ----- val -----
2023-05-09 13:55:23,972 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:55:23,972 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:55:23,972 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:56:19,316 :: INFO :: ----- test -----
2023-05-09 13:56:19,316 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 13:56:19,316 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 13:56:19,316 :: INFO :: ndcg [0.0, 2.231370067438001e-05, 4.002204201722247e-05]
2023-05-09 13:56:25,206 :: INFO :: Epoch 195: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001954291481524706
2023-05-09 13:57:18,800 :: INFO :: ----- val -----
2023-05-09 13:57:18,800 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:57:18,800 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:57:18,800 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 13:58:12,725 :: INFO :: ----- test -----
2023-05-09 13:58:12,725 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 13:58:12,725 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 13:58:12,725 :: INFO :: ndcg [0.0, 0.0, 2.4322419827661183e-05]
2023-05-09 13:58:18,491 :: INFO :: Epoch 200: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002102443715557456
2023-05-09 13:59:13,522 :: INFO :: ----- val -----
2023-05-09 13:59:13,522 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 13:59:13,522 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 13:59:13,522 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:00:06,616 :: INFO :: ----- test -----
2023-05-09 14:00:06,616 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:00:06,616 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 14:00:06,616 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 4.828362052060185e-05]
2023-05-09 14:00:12,319 :: INFO :: Epoch 205: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0021281905937939882
2023-05-09 14:01:04,025 :: INFO :: ----- val -----
2023-05-09 14:01:04,025 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:01:04,025 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:01:04,025 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 14:01:57,494 :: INFO :: ----- test -----
2023-05-09 14:01:57,494 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 14:01:57,494 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 14:01:57,494 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 1.9644669042832058e-05]
2023-05-09 14:02:03,369 :: INFO :: Epoch 210: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002356016542762518
2023-05-09 14:03:00,275 :: INFO :: ----- val -----
2023-05-09 14:03:00,275 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:03:00,275 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:03:00,291 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:03:54,434 :: INFO :: ----- test -----
2023-05-09 14:03:54,434 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.910584638899503e-05]
2023-05-09 14:03:54,434 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00019105846388995032]
2023-05-09 14:03:54,434 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 5.943215900167607e-05]
2023-05-09 14:04:00,199 :: INFO :: Epoch 215: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002370045520365238
2023-05-09 14:04:54,903 :: INFO :: ----- val -----
2023-05-09 14:04:54,903 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:04:54,903 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:04:54,903 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:05:49,668 :: INFO :: ----- test -----
2023-05-09 14:05:49,668 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:05:49,668 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-09 14:05:49,668 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 4.581181611998088e-05]
2023-05-09 14:05:55,324 :: INFO :: Epoch 220: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0022792594972997904
2023-05-09 14:06:50,871 :: INFO :: ----- val -----
2023-05-09 14:06:50,871 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:06:50,871 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:06:50,871 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:07:45,543 :: INFO :: ----- test -----
2023-05-09 14:07:45,543 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.910584638899503e-05]
2023-05-09 14:07:45,543 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00019105846388995032]
2023-05-09 14:07:45,543 :: INFO :: ndcg [2.7294066269992904e-05, 5.458813253998581e-05, 8.888791945267019e-05]
2023-05-09 14:07:51,293 :: INFO :: Epoch 225: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.6362226100329836e-20, MLP.norm 0.002370502334088087
2023-05-09 14:08:48,090 :: INFO :: ----- val -----
2023-05-09 14:08:48,090 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:08:48,090 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:08:48,090 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 14:09:42,968 :: INFO :: ----- test -----
2023-05-09 14:09:42,968 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:09:42,968 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:09:42,968 :: INFO :: ndcg [0.0, 0.0, 1.61060871767528e-05]
2023-05-09 14:09:48,749 :: INFO :: Epoch 230: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.0001335697118474e-11, MLP.norm 0.0023682070896029472
2023-05-09 14:10:45,593 :: INFO :: ----- val -----
2023-05-09 14:10:45,593 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:10:45,593 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:10:45,593 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:11:40,955 :: INFO :: ----- test -----
2023-05-09 14:11:40,955 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:11:40,955 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:11:40,955 :: INFO :: ndcg [0.0, 0.0, 1.6500073778689248e-05]
2023-05-09 14:11:46,830 :: INFO :: Epoch 235: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.9110630091745406e-06, MLP.norm 0.002698881784453988
2023-05-09 14:12:40,596 :: INFO :: ----- val -----
2023-05-09 14:12:40,596 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:12:40,596 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:12:40,596 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-09 14:13:35,148 :: INFO :: ----- test -----
2023-05-09 14:13:35,148 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-09 14:13:35,148 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00012282329821496806]
2023-05-09 14:13:35,148 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 4.6414318802527346e-05]
2023-05-09 14:13:41,117 :: INFO :: Epoch 240: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.3046620450651858e-09, MLP.norm 0.002717302879318595
2023-05-09 14:14:37,742 :: INFO :: ----- val -----
2023-05-09 14:14:37,742 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:14:37,742 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:14:37,742 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 14:15:33,304 :: INFO :: ----- test -----
2023-05-09 14:15:33,304 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 14:15:33,304 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-09 14:15:33,304 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 3.5995757313990405e-05]
2023-05-09 14:15:39,335 :: INFO :: Epoch 245: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.1357173207215965e-05, MLP.norm 0.0025642472319304943
2023-05-09 14:16:35,679 :: INFO :: ----- val -----
2023-05-09 14:16:35,679 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:16:35,679 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:16:35,679 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:17:33,007 :: INFO :: ----- test -----
2023-05-09 14:17:33,007 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-09 14:17:33,007 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-09 14:17:33,007 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 4.3734609475603034e-05]
2023-05-09 14:17:38,976 :: INFO :: Epoch 250: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.1096550376096275e-06, MLP.norm 0.0026513184420764446
2023-05-09 14:18:36,382 :: INFO :: ----- val -----
2023-05-09 14:18:36,382 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-09 14:18:36,382 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-09 14:18:36,382 :: INFO :: ndcg [0.0, 0.0, 2.611616826651335e-05]
2023-05-09 14:19:32,273 :: INFO :: ----- test -----
2023-05-09 14:19:32,273 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:19:32,273 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:19:32,273 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.847717290816175e-05]
2023-05-09 14:19:38,069 :: INFO :: Epoch 255: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.639742731640581e-05, MLP.norm 0.0025122377555817366
2023-05-09 14:20:34,492 :: INFO :: ----- val -----
2023-05-09 14:20:34,492 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:20:34,492 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:20:34,492 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:21:30,648 :: INFO :: ----- test -----
2023-05-09 14:21:30,648 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 14:21:30,648 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-09 14:21:30,648 :: INFO :: ndcg [0.0, 0.0, 2.582842974752692e-05]
2023-05-09 14:21:36,601 :: INFO :: Epoch 260: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 1.3206288713263348e-05, MLP.norm 0.002689681714400649
2023-05-09 14:22:33,695 :: INFO :: ----- val -----
2023-05-09 14:22:33,695 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:22:33,695 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:22:33,695 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:23:28,257 :: INFO :: ----- test -----
2023-05-09 14:23:28,257 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-09 14:23:28,257 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-09 14:23:28,257 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 3.0147106913685697e-05]
2023-05-09 14:23:34,023 :: INFO :: Epoch 265: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.83527073811274e-05, MLP.norm 0.002746578538790345
2023-05-09 14:24:27,288 :: INFO :: ----- val -----
2023-05-09 14:24:27,288 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:24:27,288 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:24:27,288 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-09 14:25:23,742 :: INFO :: ----- test -----
2023-05-09 14:25:23,742 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 14:25:23,742 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 14:25:23,742 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-09 14:25:29,554 :: INFO :: Epoch 270: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.501564762904309e-05, MLP.norm 0.0026715854182839394
2023-05-09 14:26:26,992 :: INFO :: ----- val -----
2023-05-09 14:26:26,992 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:26:26,992 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:26:26,992 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:27:25,429 :: INFO :: ----- test -----
2023-05-09 14:27:25,429 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-09 14:27:25,429 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-09 14:27:25,429 :: INFO :: ndcg [0.0, 0.0, 3.4208415121531715e-05]
2023-05-09 14:27:31,444 :: INFO :: Epoch 275: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.716950570582412e-05, MLP.norm 0.0030431768391281366
2023-05-09 14:28:28,804 :: INFO :: ----- val -----
2023-05-09 14:28:28,804 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:28:28,804 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:28:28,804 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-09 14:29:27,179 :: INFO :: ----- test -----
2023-05-09 14:29:27,179 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:29:27,179 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:29:27,179 :: INFO :: ndcg [0.0, 0.0, 1.7708341342842464e-05]
2023-05-09 14:29:33,211 :: INFO :: Epoch 280: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.874914884567261e-05, MLP.norm 0.002980073681101203
2023-05-09 14:30:29,773 :: INFO :: ----- val -----
2023-05-09 14:30:29,773 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:30:29,773 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:30:29,773 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:31:27,867 :: INFO :: ----- test -----
2023-05-09 14:31:27,867 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 14:31:27,867 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 14:31:27,867 :: INFO :: ndcg [0.0, 2.8975553022677294e-05, 3.7585872275522124e-05]
2023-05-09 14:31:33,773 :: INFO :: Epoch 285: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0002696090959943831, MLP.norm 0.0028922739438712597
2023-05-09 14:32:31,866 :: INFO :: ----- val -----
2023-05-09 14:32:31,866 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:32:31,866 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:32:31,866 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:33:30,367 :: INFO :: ----- test -----
2023-05-09 14:33:30,367 :: INFO :: Precision [0.0, 0.0, 1.3647033134996452e-05]
2023-05-09 14:33:30,367 :: INFO :: Recall [0.0, 0.0, 0.00012282329821496806]
2023-05-09 14:33:30,367 :: INFO :: ndcg [0.0, 0.0, 4.2191885882592485e-05]
2023-05-09 14:33:36,273 :: INFO :: Epoch 290: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00023252741084434092, MLP.norm 0.0027972508687525988
2023-05-09 14:34:34,085 :: INFO :: ----- val -----
2023-05-09 14:34:34,085 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-09 14:34:34,085 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-09 14:34:34,085 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-09 14:35:32,648 :: INFO :: ----- test -----
2023-05-09 14:35:32,648 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-09 14:35:32,648 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-09 14:35:32,648 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.329170217782851e-05]
2023-05-09 14:35:38,523 :: INFO :: Epoch 295: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00022146580158732831, MLP.norm 0.003257307456806302
2023-05-09 14:36:35,367 :: INFO :: ----- val -----
2023-05-09 14:36:35,367 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:36:35,367 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:36:35,367 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:37:32,320 :: INFO :: ----- test -----
2023-05-09 14:37:32,320 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-09 14:37:32,320 :: INFO :: Recall [0.0, 0.0, 6.823516567498226e-05]
2023-05-09 14:37:32,320 :: INFO :: ndcg [0.0, 0.0, 2.5204109266750434e-05]
2023-05-09 14:37:38,086 :: INFO :: Epoch 300: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0001995335624087602, MLP.norm 0.0031524167861789465
2023-05-09 14:38:36,992 :: INFO :: ----- val -----
2023-05-09 14:38:36,992 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:38:36,992 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:38:36,992 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:39:34,429 :: INFO :: ----- test -----
2023-05-09 14:39:34,429 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-09 14:39:34,429 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-09 14:39:34,429 :: INFO :: ndcg [0.0, 0.0, 9.098022089997634e-06]
2023-05-09 14:39:40,257 :: INFO :: Epoch 305: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0001882243959698826, MLP.norm 0.003152557648718357
2023-05-09 14:40:36,382 :: INFO :: ----- val -----
2023-05-09 14:40:36,382 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:40:36,382 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:40:36,382 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: ----- test -----
2023-05-09 14:41:32,711 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:41:32,711 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:41:38,523 :: INFO :: Epoch 310: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00021251854195725173, MLP.norm 0.00324290432035923
2023-05-09 14:42:35,101 :: INFO :: ----- val -----
2023-05-09 14:42:35,101 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:42:35,101 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:42:35,101 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:43:33,179 :: INFO :: ----- test -----
2023-05-09 14:43:33,179 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-09 14:43:33,179 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-09 14:43:33,179 :: INFO :: ndcg [0.0, 0.0, 1.8196044179995267e-05]
2023-05-09 14:43:39,117 :: INFO :: Epoch 315: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0002257401356473565, MLP.norm 0.0031606138218194246
2023-05-09 14:44:35,476 :: INFO :: ----- val -----
2023-05-09 14:44:35,476 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:44:35,476 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:44:35,476 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:45:33,163 :: INFO :: ----- test -----
2023-05-09 14:45:33,163 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:45:33,163 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:45:33,163 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.615688202607925e-05]
2023-05-09 14:45:33,163 :: INFO :: Epoch 315:
2023-05-09 14:46:27,632 :: INFO :: ----- val -----
2023-05-09 14:46:27,632 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-09 14:46:27,632 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-09 14:46:27,632 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-09 14:47:20,523 :: INFO :: ----- test -----
2023-05-09 14:47:20,523 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-09 14:47:20,523 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-09 14:47:20,523 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.615688202607925e-05]
2023-05-09 14:47:20,523 :: INFO :: final:
2023-05-09 14:47:20,523 :: INFO :: ----- test -----
2023-05-09 14:47:20,523 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-09 14:47:20,523 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-09 14:47:20,523 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.844854068323679e-05]
2023-05-09 14:47:20,523 :: INFO :: max_epoch 105:
2023-05-14 15:32:56,028 :: INFO :: log info to logs/tiktok_Attention.log
2023-05-14 15:32:56,028 :: INFO :: Namespace(alpha=1.0, b_epoch=40, bsz=256, dataset='tiktok', device='cuda:0', epoch=5, f_epoch=20, f_max=10, feat_dim=64, ite=5, lam=0.1, lr=0.001, model='Attention', neg_num=50, neighbor_num=10, num_domains=10, num_epoch=500, p_ctx=[0.001, 0.1], p_emb=[0.001, 0.0001], p_embp=[0.001, 0.0001], p_proj=[0.001, 0.01], p_w=[1, 1, 1, 1], pretrained=0, regi=0.0, reuse=0, sift=0, sigma=0.01, ssz=128, tolog=1, wdi=2)
2023-05-14 15:32:57,731 :: INFO :: torch.Size([76083, 384])
2023-05-14 15:33:18,903 :: INFO :: ERM mask: from pre-train tensor([0.5824, 0.5547, 0.6486, 0.5819, 0.5760, 0.6112, 0.5837, 0.5909, 0.5636,
        0.5778, 0.6252, 0.5840, 0.5683, 0.5899, 0.5868, 0.5698, 0.5481, 0.5700,
        0.5560, 0.6166, 0.5823, 0.5550, 0.5937, 0.6086, 0.6097, 0.5570, 0.5429,
        0.6043, 0.6583, 0.5574, 0.6047, 0.5607, 0.5936, 0.5739, 0.5692, 0.5597,
        0.5538, 0.6203, 0.5739, 0.5954, 0.5873, 0.5689, 0.5869, 0.5742, 0.5584,
        0.5518, 0.5576, 0.6077, 0.5981, 0.5494, 0.5719, 0.5465, 0.5756, 0.5414,
        0.6373, 0.6092, 0.5833, 0.6713, 0.5925, 0.5903, 0.5461, 0.5633, 0.5674,
        0.5698, 0.5785, 0.5504, 0.5784, 0.5736, 0.5733, 0.5567, 0.5618, 0.5784,
        0.5381, 0.6345, 0.5636, 0.5543, 0.5723, 0.5865, 0.5469, 0.5734, 0.6172,
        0.6413, 0.5768, 0.6212, 0.5599, 0.5826, 0.5687, 0.5913, 0.5672, 0.5364,
        0.5880, 0.6272, 0.5646, 0.5607, 0.5905, 0.5857, 0.5599, 0.5766, 0.5804,
        0.5769, 0.5609, 0.5318, 0.5591, 0.5918, 0.6194, 0.5836, 0.5704, 0.5513,
        0.5680, 0.5801, 0.6310, 0.5565, 0.6206, 0.5703, 0.5772, 0.5577, 0.5692,
        0.6116, 0.5399, 0.6079, 0.5840, 0.5746, 0.5623, 0.5534, 0.5615, 0.5776,
        0.5925, 0.6013, 0.5864, 0.6187, 0.6558, 0.5272, 0.5448, 0.5572, 0.6034,
        0.6368, 0.6868, 0.5344, 0.6101, 0.6285, 0.6051, 0.6528, 0.6107, 0.6907,
        0.6582, 0.6006, 0.5363, 0.7001, 0.6564, 0.6631, 0.6681, 0.6321, 0.6244,
        0.4292, 0.6123, 0.6835, 0.6429, 0.5087, 0.5462, 0.4951, 0.5700, 0.4579,
        0.5722, 0.6506, 0.5817, 0.5147, 0.6102, 0.6296, 0.6538, 0.6055, 0.6119,
        0.5699, 0.4928, 0.6412, 0.6421, 0.6867, 0.5271, 0.5911, 0.6229, 0.5999,
        0.6022, 0.6758, 0.5266, 0.5333, 0.5842, 0.6218, 0.5789, 0.5888, 0.6199,
        0.4626, 0.6171, 0.6453, 0.5757, 0.6118, 0.5316, 0.5949, 0.6537, 0.5155,
        0.6175, 0.6571, 0.6472, 0.6515, 0.6077, 0.5259, 0.6163, 0.6670, 0.6306,
        0.4400, 0.5353, 0.6241, 0.6181, 0.5610, 0.5210, 0.6304, 0.5958, 0.6729,
        0.6254, 0.6110, 0.5926, 0.5319, 0.6201, 0.6077, 0.5870, 0.6311, 0.5390,
        0.6204, 0.5717, 0.6637, 0.5433, 0.6495, 0.5816, 0.5200, 0.5386, 0.5289,
        0.5656, 0.6822, 0.6496, 0.6410, 0.5145, 0.5606, 0.5966, 0.6607, 0.6833,
        0.6229, 0.6679, 0.6562, 0.6031, 0.6274, 0.6402, 0.6570, 0.6590, 0.6265,
        0.6595, 0.6773, 0.6552, 0.6036, 0.7287, 0.7176, 0.7322, 0.6503, 0.6857,
        0.7139, 0.6763, 0.6783, 0.7268, 0.7006, 0.7081, 0.6894, 0.6523, 0.6355,
        0.6647, 0.6888, 0.6974, 0.6973, 0.6633, 0.6835, 0.6972, 0.6992, 0.6788,
        0.6665, 0.6729, 0.7237, 0.6752, 0.7199, 0.7349, 0.6161, 0.6826, 0.6937,
        0.6198, 0.6936, 0.6921, 0.6871, 0.7284, 0.6385, 0.6553, 0.6541, 0.6776,
        0.6888, 0.6002, 0.6492, 0.7181, 0.7326, 0.7266, 0.7200, 0.7128, 0.7261,
        0.6222, 0.6897, 0.6558, 0.7015, 0.7021, 0.7169, 0.7185, 0.6894, 0.6871,
        0.6535, 0.6460, 0.7343, 0.6824, 0.7149, 0.6707, 0.7190, 0.6803, 0.6895,
        0.7094, 0.6807, 0.7219, 0.7068, 0.6609, 0.6789, 0.6771, 0.6833, 0.6877,
        0.6713, 0.7207, 0.6032, 0.6105, 0.6826, 0.7119, 0.6048, 0.6798, 0.6724,
        0.6775, 0.6839, 0.6698, 0.6058, 0.6312, 0.7035, 0.6900, 0.6887, 0.6676,
        0.6908, 0.6854, 0.6069, 0.6921, 0.6633, 0.7150, 0.6805, 0.6637, 0.6838,
        0.7078, 0.7319, 0.7229, 0.7184, 0.6853, 0.6654, 0.6896, 0.7031, 0.6658,
        0.6864, 0.6904, 0.7178, 0.6406, 0.6451, 0.7252, 0.7288, 0.6390, 0.5769,
        0.6445, 0.7130, 0.6642, 0.6862, 0.7041, 0.6630])
2023-05-14 15:33:37,278 :: INFO :: Epoch 5: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.011279151774942875, V.norm 0.011505632661283016, MLP.norm 0.0008211754029616714
2023-05-14 15:34:30,481 :: INFO :: ----- val -----
2023-05-14 15:34:30,481 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 15:34:30,481 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 15:34:30,481 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 15:35:24,637 :: INFO :: ----- test -----
2023-05-14 15:35:24,637 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:35:24,637 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 15:35:24,637 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.8581566420740843e-05]
2023-05-14 15:35:30,606 :: INFO :: Epoch 10: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.8411905077518895e-05, V.norm 2.90247771772556e-05, MLP.norm 2.07305993171758e-06
2023-05-14 15:36:23,731 :: INFO :: ----- val -----
2023-05-14 15:36:23,731 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:36:23,731 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:36:23,731 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:37:17,122 :: INFO :: ----- test -----
2023-05-14 15:37:17,122 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:37:17,122 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 15:37:17,122 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 3.515931372737215e-05]
2023-05-14 15:37:23,012 :: INFO :: Epoch 15: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 6.98745310501181e-08, V.norm 7.143380997831628e-08, MLP.norm 5.115038170799835e-09
2023-05-14 15:38:15,903 :: INFO :: ----- val -----
2023-05-14 15:38:15,903 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:38:15,903 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:38:15,903 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:39:09,247 :: INFO :: ----- test -----
2023-05-14 15:39:09,247 :: INFO :: Precision [2.7294066269992904e-05, 1.6376439761995743e-05, 1.910584638899503e-05]
2023-05-14 15:39:09,247 :: INFO :: Recall [2.7294066269992904e-05, 8.18821988099787e-05, 0.00019105846388995032]
2023-05-14 15:39:09,247 :: INFO :: ndcg [2.7294066269992904e-05, 5.62696192926702e-05, 9.191663585719516e-05]
2023-05-14 15:39:15,184 :: INFO :: Epoch 20: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.6980308825687018e-10, V.norm 1.737486821085099e-10, MLP.norm 1.2425021081452492e-11
2023-05-14 15:40:07,981 :: INFO :: ----- val -----
2023-05-14 15:40:07,981 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:40:07,981 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:40:07,981 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:41:01,419 :: INFO :: ----- test -----
2023-05-14 15:41:01,419 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 15:41:01,419 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 15:41:01,419 :: INFO :: ndcg [0.0, 0.0, 3.613471940167777e-05]
2023-05-14 15:41:07,356 :: INFO :: Epoch 25: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 4.101293144074397e-13, V.norm 4.1990299041658186e-13, MLP.norm 2.9991295362984116e-14
2023-05-14 15:42:00,215 :: INFO :: ----- val -----
2023-05-14 15:42:00,215 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 15:42:00,215 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 15:42:00,215 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 15:42:53,481 :: INFO :: ----- test -----
2023-05-14 15:42:53,481 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.637643976199574e-05]
2023-05-14 15:42:53,481 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.0001637643976199574]
2023-05-14 15:42:53,481 :: INFO :: ndcg [0.0, 2.7779424663082022e-05, 6.159385318267728e-05]
2023-05-14 15:45:09,044 :: INFO :: Epoch 30: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 9.862654702668837e-16, V.norm 1.010714017498195e-15, MLP.norm 7.147655099925401e-17
2023-05-14 15:46:01,247 :: INFO :: ----- val -----
2023-05-14 15:46:01,247 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:46:01,247 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:46:01,247 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:46:53,871 :: INFO :: ----- test -----
2023-05-14 15:46:53,871 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.637643976199574e-05]
2023-05-14 15:46:53,871 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.0001637643976199574]
2023-05-14 15:46:53,871 :: INFO :: ndcg [0.0, 4.5000063168771676e-05, 7.171018235538785e-05]
2023-05-14 15:46:59,841 :: INFO :: Epoch 35: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 2.3642896062151508e-18, V.norm 2.424596449544248e-18, MLP.norm 1.7437265873120167e-19
2023-05-14 15:47:52,762 :: INFO :: ----- val -----
2023-05-14 15:47:52,762 :: INFO :: Precision [0.0, 0.0, 1.32772737331268e-05]
2023-05-14 15:47:52,762 :: INFO :: Recall [0.0, 0.0, 0.00013277273733126798]
2023-05-14 15:47:52,762 :: INFO :: ndcg [0.0, 0.0, 3.837992825641353e-05]
2023-05-14 15:48:45,840 :: INFO :: ----- test -----
2023-05-14 15:48:45,840 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:48:45,840 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 15:48:45,840 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.9367011613606176e-05]
2023-05-14 15:48:51,825 :: INFO :: Epoch 40: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 1.6740958743820435e-22, V.norm 1.9087629348058244e-22, MLP.norm 0.0
2023-05-14 15:49:44,513 :: INFO :: ----- val -----
2023-05-14 15:49:44,513 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:49:44,513 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:49:44,513 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:50:37,590 :: INFO :: ----- test -----
2023-05-14 15:50:37,590 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:50:37,590 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-14 15:50:37,590 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-14 15:50:43,513 :: INFO :: Epoch 45: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-14 15:51:36,637 :: INFO :: ----- val -----
2023-05-14 15:51:36,637 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:51:36,637 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:51:36,637 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:52:29,544 :: INFO :: ----- test -----
2023-05-14 15:52:29,544 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:52:29,544 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-14 15:52:29,544 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-14 15:52:35,403 :: INFO :: Epoch 50: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0
2023-05-14 15:53:28,106 :: INFO :: ----- val -----
2023-05-14 15:53:28,106 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:53:28,106 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:53:28,106 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:54:20,497 :: INFO :: ----- test -----
2023-05-14 15:54:20,497 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:54:20,497 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-14 15:54:20,497 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-14 15:54:26,325 :: INFO :: Epoch 55: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 1.1316389990877196e-13
2023-05-14 15:55:18,403 :: INFO :: ----- val -----
2023-05-14 15:55:18,403 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:55:18,403 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:55:18,403 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:56:11,559 :: INFO :: ----- test -----
2023-05-14 15:56:11,559 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 15:56:11,559 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 6.823516567498226e-05]
2023-05-14 15:56:11,559 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 2.817088325401091e-05]
2023-05-14 15:56:17,590 :: INFO :: Epoch 60: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001841008081100881
2023-05-14 15:57:10,778 :: INFO :: ----- val -----
2023-05-14 15:57:10,778 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 15:57:10,778 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 15:57:10,778 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 15:58:04,106 :: INFO :: ----- test -----
2023-05-14 15:58:04,106 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-14 15:58:04,106 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 15:58:04,106 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 4.841236026043215e-05]
2023-05-14 15:58:09,997 :: INFO :: Epoch 65: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001379496301524341
2023-05-14 15:59:03,309 :: INFO :: ----- val -----
2023-05-14 15:59:03,309 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 15:59:03,309 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 15:59:03,309 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 15:59:56,419 :: INFO :: ----- test -----
2023-05-14 15:59:56,419 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-14 15:59:56,419 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-14 15:59:56,419 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.583095775853448e-05]
2023-05-14 16:00:02,356 :: INFO :: Epoch 70: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014113355427980423
2023-05-14 16:00:55,653 :: INFO :: ----- val -----
2023-05-14 16:00:55,653 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:00:55,653 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:00:55,653 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 16:01:48,044 :: INFO :: ----- test -----
2023-05-14 16:01:48,044 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.637643976199574e-05]
2023-05-14 16:01:48,044 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00015011736448496097]
2023-05-14 16:01:48,044 :: INFO :: ndcg [0.0, 3.287248683177238e-05, 6.0150578212626316e-05]
2023-05-14 16:01:53,950 :: INFO :: Epoch 75: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0014098979299888015
2023-05-14 16:02:46,294 :: INFO :: ----- val -----
2023-05-14 16:02:46,294 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:02:46,294 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:02:46,294 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:03:38,044 :: INFO :: ----- test -----
2023-05-14 16:03:38,044 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.910584638899503e-05]
2023-05-14 16:03:38,044 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00019105846388995032]
2023-05-14 16:03:38,044 :: INFO :: ndcg [0.0, 2.3509829033975273e-05, 6.49713103366677e-05]
2023-05-14 16:03:43,809 :: INFO :: Epoch 80: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0015362224075943232
2023-05-14 16:04:35,997 :: INFO :: ----- val -----
2023-05-14 16:04:35,997 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:04:35,997 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:04:35,997 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:05:28,372 :: INFO :: ----- test -----
2023-05-14 16:05:28,372 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 16:05:28,372 :: INFO :: Recall [0.0, 5.458813253998581e-05, 6.823516567498226e-05]
2023-05-14 16:05:28,372 :: INFO :: ndcg [0.0, 2.231370067438001e-05, 2.9693589738670138e-05]
2023-05-14 16:05:34,216 :: INFO :: Epoch 85: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0015673765446990728
2023-05-14 16:06:26,825 :: INFO :: ----- val -----
2023-05-14 16:06:26,825 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:06:26,825 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:06:26,825 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 16:07:20,184 :: INFO :: ----- test -----
2023-05-14 16:07:20,184 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 16:07:20,184 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 16:07:20,184 :: INFO :: ndcg [0.0, 0.0, 3.879161182866777e-05]
2023-05-14 16:07:25,919 :: INFO :: Epoch 90: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017552105709910393
2023-05-14 16:08:18,450 :: INFO :: ----- val -----
2023-05-14 16:08:18,450 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:08:18,450 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:08:18,450 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 16:09:11,481 :: INFO :: ----- test -----
2023-05-14 16:09:11,481 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 16:09:11,481 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 16:09:11,481 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.7058859936081616e-05]
2023-05-14 16:09:17,247 :: INFO :: Epoch 95: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0016143056564033031
2023-05-14 16:10:09,122 :: INFO :: ----- val -----
2023-05-14 16:10:09,122 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:10:09,122 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:10:09,122 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:11:00,888 :: INFO :: ----- test -----
2023-05-14 16:11:00,888 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 16:11:00,888 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 16:11:00,888 :: INFO :: ndcg [0.0, 0.0, 8.610319252844827e-06]
2023-05-14 16:11:06,575 :: INFO :: Epoch 100: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002092750510200858
2023-05-14 16:11:58,403 :: INFO :: ----- val -----
2023-05-14 16:11:58,403 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:11:58,403 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:11:58,403 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-14 16:12:51,091 :: INFO :: ----- test -----
2023-05-14 16:12:51,091 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-14 16:12:51,091 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-14 16:12:51,091 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 2.5110393031534075e-05]
2023-05-14 16:12:56,950 :: INFO :: Epoch 105: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0021997957956045866
2023-05-14 16:13:48,559 :: INFO :: ----- val -----
2023-05-14 16:13:48,559 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:13:48,559 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:13:48,559 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:14:40,013 :: INFO :: ----- test -----
2023-05-14 16:14:40,013 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 16:14:40,013 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 16:14:40,013 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.678120250685574e-05]
2023-05-14 16:14:45,778 :: INFO :: Epoch 110: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.001765614259056747
2023-05-14 16:15:38,044 :: INFO :: ----- val -----
2023-05-14 16:15:38,044 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:15:38,044 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:15:38,044 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:16:30,247 :: INFO :: ----- test -----
2023-05-14 16:16:30,247 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 16:16:30,247 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 16:16:30,247 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.141172276437764e-05]
2023-05-14 16:16:36,091 :: INFO :: Epoch 115: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017695929855108261
2023-05-14 16:17:27,544 :: INFO :: ----- val -----
2023-05-14 16:17:27,544 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:17:27,544 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:17:27,544 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:18:19,356 :: INFO :: ----- test -----
2023-05-14 16:18:19,356 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 16:18:19,356 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 16:18:19,356 :: INFO :: ndcg [0.0, 0.0, 3.4696117958684515e-05]
2023-05-14 16:18:25,028 :: INFO :: Epoch 120: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0017368298722431064
2023-05-14 16:19:16,528 :: INFO :: ----- val -----
2023-05-14 16:19:16,528 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:19:16,528 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:19:16,528 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:20:08,919 :: INFO :: ----- test -----
2023-05-14 16:20:08,919 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 16:20:08,919 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 16:20:08,919 :: INFO :: ndcg [0.0, 3.9048980786980544e-05, 3.483273560230819e-05]
2023-05-14 16:20:14,778 :: INFO :: Epoch 125: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002028594259172678
2023-05-14 16:21:06,497 :: INFO :: ----- val -----
2023-05-14 16:21:06,497 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:21:06,497 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:21:06,497 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:21:58,856 :: INFO :: ----- test -----
2023-05-14 16:21:58,856 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.910584638899503e-05]
2023-05-14 16:21:58,856 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00019105846388995032]
2023-05-14 16:21:58,856 :: INFO :: ndcg [0.0, 3.287248683177238e-05, 6.403002292437764e-05]
2023-05-14 16:22:04,606 :: INFO :: Epoch 130: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002457983326166868
2023-05-14 16:22:56,387 :: INFO :: ----- val -----
2023-05-14 16:22:56,387 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:22:56,387 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:22:56,387 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:23:48,231 :: INFO :: ----- test -----
2023-05-14 16:23:48,231 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 16:23:48,231 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 16:23:48,231 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.879161182866777e-05]
2023-05-14 16:23:53,856 :: INFO :: Epoch 135: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0024970960803329945
2023-05-14 16:24:45,825 :: INFO :: ----- val -----
2023-05-14 16:24:45,825 :: INFO :: Precision [0.0, 0.0, 1.32772737331268e-05]
2023-05-14 16:24:45,825 :: INFO :: Recall [0.0, 0.0, 0.00013277273733126798]
2023-05-14 16:24:45,825 :: INFO :: ndcg [0.0, 0.0, 3.890947768531786e-05]
2023-05-14 16:25:38,340 :: INFO :: ----- test -----
2023-05-14 16:25:38,340 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-14 16:25:38,340 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-14 16:25:38,340 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 1.877511880830075e-05]
2023-05-14 16:25:44,231 :: INFO :: Epoch 140: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0022324244491755962
2023-05-14 16:26:36,872 :: INFO :: ----- val -----
2023-05-14 16:26:36,872 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:26:36,872 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:26:36,872 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 16:27:29,044 :: INFO :: ----- test -----
2023-05-14 16:27:29,044 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 16:27:29,044 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 16:27:29,044 :: INFO :: ndcg [0.0, 2.8975553022677294e-05, 5.441252417927532e-05]
2023-05-14 16:27:34,919 :: INFO :: Epoch 145: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0018810608889907598
2023-05-14 16:28:27,997 :: INFO :: ----- val -----
2023-05-14 16:28:27,997 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:28:27,997 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:28:27,997 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:29:20,841 :: INFO :: ----- test -----
2023-05-14 16:29:20,841 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 16:29:20,841 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 16:29:20,841 :: INFO :: ndcg [0.0, 0.0, 2.438982830453367e-05]
2023-05-14 16:29:26,732 :: INFO :: Epoch 150: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002213935134932399
2023-05-14 16:30:19,450 :: INFO :: ----- val -----
2023-05-14 16:30:19,450 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:30:19,450 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:30:19,450 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:31:11,982 :: INFO :: ----- test -----
2023-05-14 16:31:11,982 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:31:11,982 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:31:11,982 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:31:17,763 :: INFO :: Epoch 155: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0025627706199884415
2023-05-14 16:32:09,715 :: INFO :: ----- val -----
2023-05-14 16:32:09,715 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:32:09,715 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:32:09,715 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 16:33:01,512 :: INFO :: ----- test -----
2023-05-14 16:33:01,512 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.910584638899503e-05]
2023-05-14 16:33:01,512 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00019105846388995032]
2023-05-14 16:33:01,512 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 6.191790329673367e-05]
2023-05-14 16:33:07,263 :: INFO :: Epoch 160: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0025510943960398436
2023-05-14 16:33:59,732 :: INFO :: ----- val -----
2023-05-14 16:33:59,732 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:33:59,732 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:33:59,732 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 16:34:51,747 :: INFO :: ----- test -----
2023-05-14 16:34:51,747 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 16:34:51,747 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 16:34:51,747 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 4.2389724267826144e-05]
2023-05-14 16:34:57,591 :: INFO :: Epoch 165: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0024769706651568413
2023-05-14 16:35:49,012 :: INFO :: ----- val -----
2023-05-14 16:35:49,012 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:35:49,012 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:35:49,012 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 16:36:40,919 :: INFO :: ----- test -----
2023-05-14 16:36:40,919 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 16:36:40,919 :: INFO :: Recall [0.0, 0.0, 9.552923194497516e-05]
2023-05-14 16:36:40,919 :: INFO :: ndcg [0.0, 0.0, 3.371818427337134e-05]
2023-05-14 16:36:46,700 :: INFO :: Epoch 170: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002698498545214534
2023-05-14 16:37:38,763 :: INFO :: ----- val -----
2023-05-14 16:37:38,763 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:37:38,763 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:37:38,763 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 16:38:31,309 :: INFO :: ----- test -----
2023-05-14 16:38:31,309 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 16:38:31,309 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 16:38:31,309 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 3.172616168488365e-05]
2023-05-14 16:38:37,169 :: INFO :: Epoch 175: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002626648871228099
2023-05-14 16:39:29,372 :: INFO :: ----- val -----
2023-05-14 16:39:29,372 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:39:29,372 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:39:29,372 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:40:22,559 :: INFO :: ----- test -----
2023-05-14 16:40:22,559 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 16:40:22,559 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 16:40:22,559 :: INFO :: ndcg [0.0, 0.0, 2.6942981076463776e-05]
2023-05-14 16:40:28,466 :: INFO :: Epoch 180: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.002805486088618636
2023-05-14 16:41:21,137 :: INFO :: ----- val -----
2023-05-14 16:41:21,137 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:41:21,137 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:41:21,137 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:42:13,924 :: INFO :: ----- test -----
2023-05-14 16:42:13,924 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 16:42:13,924 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 16:42:13,924 :: INFO :: ndcg [0.0, 0.0, 3.3093863792594855e-05]
2023-05-14 16:42:19,845 :: INFO :: Epoch 185: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0029032323509454727
2023-05-14 16:43:11,361 :: INFO :: ----- val -----
2023-05-14 16:43:11,361 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:43:11,361 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:43:11,361 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:44:03,283 :: INFO :: ----- test -----
2023-05-14 16:44:03,283 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 8.188219880997872e-06]
2023-05-14 16:44:03,283 :: INFO :: Recall [0.0, 8.18821988099787e-05, 8.18821988099787e-05]
2023-05-14 16:44:03,283 :: INFO :: ndcg [0.0, 4.262258615767374e-05, 3.715686216897173e-05]
2023-05-14 16:44:09,095 :: INFO :: Epoch 190: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0025871354155242443
2023-05-14 16:45:01,033 :: INFO :: ----- val -----
2023-05-14 16:45:01,033 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:45:01,033 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:45:01,033 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 16:45:53,455 :: INFO :: ----- test -----
2023-05-14 16:45:53,455 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:45:53,455 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:45:53,455 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:45:59,095 :: INFO :: Epoch 195: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.003175432560965419
2023-05-14 16:46:50,080 :: INFO :: ----- val -----
2023-05-14 16:46:50,080 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:46:50,080 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:46:50,080 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 16:47:42,142 :: INFO :: ----- test -----
2023-05-14 16:47:42,142 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 16:47:42,142 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 16:47:42,142 :: INFO :: ndcg [0.0, 0.0, 9.098022089997634e-06]
2023-05-14 16:47:47,908 :: INFO :: Epoch 200: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0028780533466488123
2023-05-14 16:48:38,377 :: INFO :: ----- val -----
2023-05-14 16:48:38,377 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:48:38,377 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:48:38,377 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 16:49:29,892 :: INFO :: ----- test -----
2023-05-14 16:49:29,892 :: INFO :: Precision [0.0, 5.458813253998581e-06, 2.7294066269992907e-06]
2023-05-14 16:49:29,892 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 2.7294066269992904e-05]
2023-05-14 16:49:29,892 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 1.7220638505689654e-05]
2023-05-14 16:49:35,486 :: INFO :: Epoch 205: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0029748212546110153
2023-05-14 16:50:26,488 :: INFO :: ----- val -----
2023-05-14 16:50:26,488 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:50:26,488 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:50:26,488 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 16:51:17,503 :: INFO :: ----- test -----
2023-05-14 16:51:17,503 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:51:17,503 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:51:17,503 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:51:23,097 :: INFO :: Epoch 210: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0032656590919941664
2023-05-14 16:52:14,206 :: INFO :: ----- val -----
2023-05-14 16:52:14,206 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-14 16:52:14,206 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-14 16:52:14,206 :: INFO :: ndcg [0.0, 0.0, 2.611616826651335e-05]
2023-05-14 16:53:06,037 :: INFO :: ----- test -----
2023-05-14 16:53:06,037 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-14 16:53:06,037 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-14 16:53:06,037 :: INFO :: ndcg [0.0, 0.0, 1.6826651903753206e-05]
2023-05-14 16:53:11,787 :: INFO :: Epoch 215: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.003299984149634838
2023-05-14 16:54:04,068 :: INFO :: ----- val -----
2023-05-14 16:54:04,068 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-14 16:54:04,068 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-14 16:54:04,068 :: INFO :: ndcg [0.0, 0.0, 2.6645717695417676e-05]
2023-05-14 16:54:56,833 :: INFO :: ----- test -----
2023-05-14 16:54:56,833 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 16:54:56,833 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 16:54:56,833 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-14 16:55:02,615 :: INFO :: Epoch 220: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0032267728820443153
2023-05-14 16:55:55,461 :: INFO :: ----- val -----
2023-05-14 16:55:55,461 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 16:55:55,461 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 16:55:55,461 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-14 16:56:47,539 :: INFO :: ----- test -----
2023-05-14 16:56:47,539 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-14 16:56:47,539 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-14 16:56:47,539 :: INFO :: ndcg [0.0, 0.0, 1.9444685141548238e-05]
2023-05-14 16:56:53,320 :: INFO :: Epoch 225: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0034117831382900476
2023-05-14 16:57:45,211 :: INFO :: ----- val -----
2023-05-14 16:57:45,211 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:57:45,211 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:57:45,211 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 16:58:37,586 :: INFO :: ----- test -----
2023-05-14 16:58:37,586 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 16:58:37,586 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 16:58:37,586 :: INFO :: ndcg [0.0, 2.231370067438001e-05, 5.351330033291588e-05]
2023-05-14 16:58:43,273 :: INFO :: Epoch 230: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0, MLP.norm 0.0035216573160141706
2023-05-14 16:59:35,602 :: INFO :: ----- val -----
2023-05-14 16:59:35,602 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 16:59:35,602 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 16:59:35,602 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:00:26,601 :: INFO :: ----- test -----
2023-05-14 17:00:26,601 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 17:00:26,601 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 17:00:26,601 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.8581566420740843e-05]
2023-05-14 17:00:32,492 :: INFO :: Epoch 235: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 7.583124124292684e-13, MLP.norm 0.003477072576060891
2023-05-14 17:01:24,617 :: INFO :: ----- val -----
2023-05-14 17:01:24,617 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:01:24,617 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:01:24,617 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:02:16,976 :: INFO :: ----- test -----
2023-05-14 17:02:16,976 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 17:02:16,976 :: INFO :: Recall [0.0, 0.0, 6.823516567498226e-05]
2023-05-14 17:02:16,976 :: INFO :: ndcg [0.0, 0.0, 2.5204109266750434e-05]
2023-05-14 17:02:22,851 :: INFO :: Epoch 240: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.6791441832283454e-07, MLP.norm 0.003642839612439275
2023-05-14 17:03:14,742 :: INFO :: ----- val -----
2023-05-14 17:03:14,742 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:03:14,742 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:03:14,742 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-14 17:04:07,072 :: INFO :: ----- test -----
2023-05-14 17:04:07,072 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-14 17:04:07,072 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-14 17:04:07,072 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 4.841236026043216e-05]
2023-05-14 17:04:12,697 :: INFO :: Epoch 245: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 9.815149724090588e-07, MLP.norm 0.003748259274289012
2023-05-14 17:05:03,057 :: INFO :: ----- val -----
2023-05-14 17:05:03,057 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:05:03,057 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:05:03,057 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:05:53,791 :: INFO :: ----- test -----
2023-05-14 17:05:53,791 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.910584638899503e-05]
2023-05-14 17:05:53,791 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00019105846388995032]
2023-05-14 17:05:53,791 :: INFO :: ndcg [0.0, 3.08676716406861e-05, 7.463477660989554e-05]
2023-05-14 17:05:59,432 :: INFO :: Epoch 250: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 6.631664291489869e-06, MLP.norm 0.0039023286662995815
2023-05-14 17:06:50,044 :: INFO :: ----- val -----
2023-05-14 17:06:50,044 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:06:50,044 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:06:50,044 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:07:40,684 :: INFO :: ----- test -----
2023-05-14 17:07:40,684 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 17:07:40,684 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00012282329821496806]
2023-05-14 17:07:40,684 :: INFO :: ndcg [0.0, 2.8975553022677294e-05, 5.441252417927533e-05]
2023-05-14 17:07:46,310 :: INFO :: Epoch 255: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 6.52989774607704e-06, MLP.norm 0.00384576921351254
2023-05-14 17:08:38,606 :: INFO :: ----- val -----
2023-05-14 17:08:38,606 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:08:38,606 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:08:38,606 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:09:29,263 :: INFO :: ----- test -----
2023-05-14 17:09:29,263 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 17:09:29,263 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 17:09:29,263 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-14 17:09:35,153 :: INFO :: Epoch 260: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 7.297667252714746e-07, MLP.norm 0.003941775299608707
2023-05-14 17:10:26,575 :: INFO :: ----- val -----
2023-05-14 17:10:26,575 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:10:26,575 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:10:26,575 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 17:11:18,590 :: INFO :: ----- test -----
2023-05-14 17:11:18,590 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.0917626507997163e-05]
2023-05-14 17:11:18,590 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00010917626507997162]
2023-05-14 17:11:18,590 :: INFO :: ndcg [0.0, 2.7294066269992904e-05, 4.422231231275584e-05]
2023-05-14 17:11:24,325 :: INFO :: Epoch 265: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 5.113606312079355e-05, MLP.norm 0.0038660329300910234
2023-05-14 17:12:15,497 :: INFO :: ----- val -----
2023-05-14 17:12:15,497 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:12:15,497 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:12:15,497 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 17:13:07,122 :: INFO :: ----- test -----
2023-05-14 17:13:07,122 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 17:13:07,122 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 17:13:07,122 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.6089473549206764e-05]
2023-05-14 17:13:12,872 :: INFO :: Epoch 270: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 3.361275958013721e-05, MLP.norm 0.004030887503176928
2023-05-14 17:14:05,263 :: INFO :: ----- val -----
2023-05-14 17:14:05,263 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:14:05,263 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:14:05,263 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 17:14:57,856 :: INFO :: ----- test -----
2023-05-14 17:14:57,856 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 17:14:57,856 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 17:14:57,856 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.875742616653053e-05]
2023-05-14 17:15:03,638 :: INFO :: Epoch 275: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.250193160260096e-05, MLP.norm 0.004180516581982374
2023-05-14 17:15:55,747 :: INFO :: ----- val -----
2023-05-14 17:15:55,747 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:15:55,747 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:15:55,747 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 17:16:48,263 :: INFO :: ----- test -----
2023-05-14 17:16:48,263 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 17:16:48,263 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 17:16:48,263 :: INFO :: ndcg [0.0, 0.0, 7.889754525844421e-06]
2023-05-14 17:16:53,903 :: INFO :: Epoch 280: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 4.5748554839519784e-05, MLP.norm 0.004129128996282816
2023-05-14 17:17:45,544 :: INFO :: ----- val -----
2023-05-14 17:17:45,544 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:17:45,544 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:17:45,544 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:18:36,220 :: INFO :: ----- test -----
2023-05-14 17:18:36,220 :: INFO :: Precision [0.0, 5.458813253998581e-06, 5.458813253998581e-06]
2023-05-14 17:18:36,220 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 5.458813253998581e-05]
2023-05-14 17:18:36,220 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.1477257087761755e-05]
2023-05-14 17:18:41,735 :: INFO :: Epoch 285: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00026327432715334, MLP.norm 0.004417008254677057
2023-05-14 17:19:31,908 :: INFO :: ----- val -----
2023-05-14 17:19:31,908 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-14 17:19:31,908 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-14 17:19:31,908 :: INFO :: ndcg [0.0, 0.0, 2.7284570588612964e-05]
2023-05-14 17:20:22,829 :: INFO :: ----- test -----
2023-05-14 17:20:22,829 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 17:20:22,829 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 17:20:22,829 :: INFO :: ndcg [0.0, 3.9048980786980544e-05, 3.758587227552212e-05]
2023-05-14 17:20:28,548 :: INFO :: Epoch 290: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00023139060067478567, MLP.norm 0.004135964438319206
2023-05-14 17:21:21,251 :: INFO :: ----- val -----
2023-05-14 17:21:21,251 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:21:21,251 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:21:21,251 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 17:22:12,751 :: INFO :: ----- test -----
2023-05-14 17:22:12,751 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 17:22:12,751 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 17:22:12,751 :: INFO :: ndcg [0.0, 1.3647033134996452e-05, 3.0634809750838504e-05]
2023-05-14 17:22:18,345 :: INFO :: Epoch 295: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00021630126866512, MLP.norm 0.004245659802109003
2023-05-14 17:23:09,001 :: INFO :: ----- val -----
2023-05-14 17:23:09,001 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:23:09,001 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:23:09,001 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 17:24:00,517 :: INFO :: ----- test -----
2023-05-14 17:24:00,517 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 17:24:00,517 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 17:24:00,517 :: INFO :: ndcg [2.7294066269992904e-05, 4.094109940498935e-05, 6.716351575445273e-05]
2023-05-14 17:24:06,345 :: INFO :: Epoch 300: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00021417118841782212, MLP.norm 0.0043572476133704185
2023-05-14 17:24:56,518 :: INFO :: ----- val -----
2023-05-14 17:24:56,518 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:24:56,518 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:24:56,518 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:25:47,393 :: INFO :: ----- test -----
2023-05-14 17:25:47,393 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-14 17:25:47,393 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-14 17:25:47,393 :: INFO :: ndcg [0.0, 0.0, 1.643266530181676e-05]
2023-05-14 17:25:53,252 :: INFO :: Epoch 305: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00019379190052859485, MLP.norm 0.004359204787760973
2023-05-14 17:26:44,862 :: INFO :: ----- val -----
2023-05-14 17:26:44,862 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:26:44,862 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:26:44,862 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:27:37,518 :: INFO :: ----- test -----
2023-05-14 17:27:37,518 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 17:27:37,518 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 17:27:37,518 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-14 17:27:43,221 :: INFO :: Epoch 310: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00020378170302137733, MLP.norm 0.004566463176161051
2023-05-14 17:28:34,833 :: INFO :: ----- val -----
2023-05-14 17:28:34,833 :: INFO :: Precision [0.0, 0.0, 2.6554547466253595e-05]
2023-05-14 17:28:34,833 :: INFO :: Recall [0.0, 0.0, 0.00026554547466253595]
2023-05-14 17:28:34,833 :: INFO :: ndcg [0.0, 0.0, 8.285903413059295e-05]
2023-05-14 17:29:26,177 :: INFO :: ----- test -----
2023-05-14 17:29:26,177 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 17:29:26,177 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 17:29:26,177 :: INFO :: ndcg [2.7294066269992904e-05, 3.9048980786980544e-05, 6.608567809866068e-05]
2023-05-14 17:29:31,849 :: INFO :: Epoch 315: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00023662770399823785, MLP.norm 0.004689569119364023
2023-05-14 17:30:24,645 :: INFO :: ----- val -----
2023-05-14 17:30:24,645 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:30:24,645 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:30:24,645 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:31:17,396 :: INFO :: ----- test -----
2023-05-14 17:31:17,396 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 17:31:17,396 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 17:31:17,396 :: INFO :: ndcg [0.0, 0.0, 2.8055004394393064e-05]
2023-05-14 17:31:22,942 :: INFO :: Epoch 320: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0012168299872428179, MLP.norm 0.004499765112996101
2023-05-14 17:32:14,192 :: INFO :: ----- val -----
2023-05-14 17:32:14,192 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:32:14,192 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:32:14,192 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:33:06,583 :: INFO :: ----- test -----
2023-05-14 17:33:06,599 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 17:33:06,599 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 17:33:06,599 :: INFO :: ndcg [0.0, 0.0, 7.889754525844421e-06]
2023-05-14 17:33:12,317 :: INFO :: Epoch 325: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0011401616502553225, MLP.norm 0.004697650205343962
2023-05-14 17:34:04,538 :: INFO :: ----- val -----
2023-05-14 17:34:04,538 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:34:04,538 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:34:04,538 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:34:57,257 :: INFO :: ----- test -----
2023-05-14 17:34:57,257 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.637643976199574e-05]
2023-05-14 17:34:57,257 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.0001637643976199574]
2023-05-14 17:34:57,257 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 6.0757409596059037e-05]
2023-05-14 17:35:03,194 :: INFO :: Epoch 330: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0011565012391656637, MLP.norm 0.004296980798244476
2023-05-14 17:35:54,929 :: INFO :: ----- val -----
2023-05-14 17:35:54,929 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:35:54,929 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:35:54,929 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:36:47,147 :: INFO :: ----- test -----
2023-05-14 17:36:47,147 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 17:36:47,147 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 17:36:47,147 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 2.786100169374044e-05]
2023-05-14 17:36:52,835 :: INFO :: Epoch 335: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0016304036835208535, MLP.norm 0.004577870946377516
2023-05-14 17:37:44,679 :: INFO :: ----- val -----
2023-05-14 17:37:44,679 :: INFO :: Precision [0.0, 0.0, 1.32772737331268e-05]
2023-05-14 17:37:44,679 :: INFO :: Recall [0.0, 0.0, 0.00013277273733126798]
2023-05-14 17:37:44,679 :: INFO :: ndcg [0.0, 0.0, 4.447910587417943e-05]
2023-05-14 17:38:35,898 :: INFO :: ----- test -----
2023-05-14 17:38:35,898 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 17:38:35,898 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 17:38:35,898 :: INFO :: ndcg [0.0, 2.420581929238882e-05, 3.392816186316294e-05]
2023-05-14 17:38:41,710 :: INFO :: Epoch 340: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0009687130805104971, MLP.norm 0.004411452449858189
2023-05-14 17:39:34,585 :: INFO :: ----- val -----
2023-05-14 17:39:34,585 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:39:34,585 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:39:34,585 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 17:40:27,429 :: INFO :: ----- test -----
2023-05-14 17:40:27,429 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 17:40:27,429 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 17:40:27,429 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 5.000902029579606e-05]
2023-05-14 17:40:33,272 :: INFO :: Epoch 345: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0009187445393763483, MLP.norm 0.004625658039003611
2023-05-14 17:41:23,538 :: INFO :: ----- val -----
2023-05-14 17:41:23,538 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:41:23,538 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:41:23,538 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:42:14,726 :: INFO :: ----- test -----
2023-05-14 17:42:14,726 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 17:42:14,726 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 17:42:14,726 :: INFO :: ndcg [0.0, 0.0, 2.6548994474527325e-05]
2023-05-14 17:42:20,491 :: INFO :: Epoch 350: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.000957423762883991, MLP.norm 0.004535454325377941
2023-05-14 17:43:11,569 :: INFO :: ----- val -----
2023-05-14 17:43:11,569 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:43:11,569 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:43:11,569 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:44:02,038 :: INFO :: ----- test -----
2023-05-14 17:44:02,038 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-14 17:44:02,038 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-14 17:44:02,038 :: INFO :: ndcg [0.0, 0.0, 1.6987776615842055e-05]
2023-05-14 17:44:07,538 :: INFO :: Epoch 355: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0010009407997131348, MLP.norm 0.004360357765108347
2023-05-14 17:44:57,991 :: INFO :: ----- val -----
2023-05-14 17:44:57,991 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:44:57,991 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:44:57,991 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 17:45:49,632 :: INFO :: ----- test -----
2023-05-14 17:45:49,632 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 17:45:49,632 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 17:45:49,632 :: INFO :: ndcg [0.0, 0.0, 7.889754525844421e-06]
2023-05-14 17:45:55,319 :: INFO :: Epoch 360: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0009920097654685378, MLP.norm 0.004445434547960758
2023-05-14 17:46:46,647 :: INFO :: ----- val -----
2023-05-14 17:46:46,647 :: INFO :: Precision [0.0, 8.851515822084533e-06, 8.851515822084533e-06]
2023-05-14 17:46:46,647 :: INFO :: Recall [0.0, 4.425757911042266e-05, 8.851515822084532e-05]
2023-05-14 17:46:46,647 :: INFO :: ndcg [0.0, 1.7121168720271814e-05, 3.0444027567980656e-05]
2023-05-14 17:47:38,576 :: INFO :: ----- test -----
2023-05-14 17:47:38,576 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 17:47:38,576 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 17:47:38,576 :: INFO :: ndcg [0.0, 2.7779424663082022e-05, 5.3607854410608945e-05]
2023-05-14 17:47:44,139 :: INFO :: Epoch 365: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0009978683665394783, MLP.norm 0.004534542094916105
2023-05-14 17:48:36,154 :: INFO :: ----- val -----
2023-05-14 17:48:36,154 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:48:36,154 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:48:36,154 :: INFO :: ndcg [0.0, 0.0, 1.4752526370140887e-05]
2023-05-14 17:49:26,779 :: INFO :: ----- test -----
2023-05-14 17:49:26,779 :: INFO :: Precision [2.7294066269992904e-05, 1.6376439761995743e-05, 1.3647033134996452e-05]
2023-05-14 17:49:26,779 :: INFO :: Recall [2.7294066269992904e-05, 8.18821988099787e-05, 0.00013647033134996453]
2023-05-14 17:49:26,779 :: INFO :: ndcg [2.7294066269992904e-05, 5.8161737910679013e-05, 8.40513883998159e-05]
2023-05-14 17:49:32,592 :: INFO :: Epoch 370: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0010052014840766788, MLP.norm 0.004658313002437353
2023-05-14 17:50:24,327 :: INFO :: ----- val -----
2023-05-14 17:50:24,327 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:50:24,327 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:50:24,327 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 17:51:15,295 :: INFO :: ----- test -----
2023-05-14 17:51:15,295 :: INFO :: Precision [0.0, 5.458813253998581e-06, 8.188219880997872e-06]
2023-05-14 17:51:15,295 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 8.18821988099787e-05]
2023-05-14 17:51:15,295 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 3.0087576340606586e-05]
2023-05-14 17:51:21,108 :: INFO :: Epoch 375: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0009946647332981229, MLP.norm 0.004493752494454384
2023-05-14 17:52:13,295 :: INFO :: ----- val -----
2023-05-14 17:52:13,295 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:52:13,295 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:52:13,295 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:53:05,391 :: INFO :: ----- test -----
2023-05-14 17:53:05,391 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 17:53:05,391 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 17:53:05,391 :: INFO :: ndcg [0.0, 0.0, 2.5042984554661586e-05]
2023-05-14 17:53:11,094 :: INFO :: Epoch 380: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0010307047050446272, MLP.norm 0.004585767164826393
2023-05-14 17:54:01,281 :: INFO :: ----- val -----
2023-05-14 17:54:01,281 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:54:01,281 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:54:01,281 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:54:52,266 :: INFO :: ----- test -----
2023-05-14 17:54:52,266 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 1.637643976199574e-05]
2023-05-14 17:54:52,266 :: INFO :: Recall [0.0, 5.458813253998581e-05, 0.0001637643976199574]
2023-05-14 17:54:52,266 :: INFO :: ndcg [0.0, 2.5401947651984092e-05, 6.042464373573256e-05]
2023-05-14 17:54:57,922 :: INFO :: Epoch 385: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0014133324148133397, MLP.norm 0.004768623039126396
2023-05-14 17:55:48,266 :: INFO :: ----- val -----
2023-05-14 17:55:48,266 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:55:48,266 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:55:48,266 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:56:39,547 :: INFO :: ----- test -----
2023-05-14 17:56:39,547 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 1.0917626507997163e-05]
2023-05-14 17:56:39,547 :: INFO :: Recall [0.0, 8.18821988099787e-05, 0.00010917626507997162]
2023-05-14 17:56:39,547 :: INFO :: ndcg [0.0, 3.526474355096291e-05, 4.625488425896936e-05]
2023-05-14 17:56:45,235 :: INFO :: Epoch 390: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0013912777649238706, MLP.norm 0.004417239222675562
2023-05-14 17:57:35,657 :: INFO :: ----- val -----
2023-05-14 17:57:35,657 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 17:57:35,657 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 17:57:35,657 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 17:58:25,563 :: INFO :: ----- test -----
2023-05-14 17:58:25,563 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:58:25,563 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:58:25,563 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 17:58:31,110 :: INFO :: Epoch 395: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0014115050435066223, MLP.norm 0.004623583052307367
2023-05-14 17:59:21,047 :: INFO :: ----- val -----
2023-05-14 17:59:21,047 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 17:59:21,047 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 17:59:21,047 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:00:12,063 :: INFO :: ----- test -----
2023-05-14 18:00:12,063 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 18:00:12,063 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 18:00:12,063 :: INFO :: ndcg [0.0, 2.1117572314784737e-05, 2.933390496569312e-05]
2023-05-14 18:00:17,563 :: INFO :: Epoch 400: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0016491570277139544, MLP.norm 0.004519149661064148
2023-05-14 18:01:07,735 :: INFO :: ----- val -----
2023-05-14 18:01:07,735 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:01:07,735 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:01:07,735 :: INFO :: ndcg [0.0, 0.0, 1.3322858847708838e-05]
2023-05-14 18:01:57,735 :: INFO :: ----- test -----
2023-05-14 18:01:57,735 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 18:01:57,735 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 18:01:57,735 :: INFO :: ndcg [0.0, 0.0, 2.5436971156598033e-05]
2023-05-14 18:02:03,328 :: INFO :: Epoch 405: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0013797929277643561, MLP.norm 0.004356579389423132
2023-05-14 18:02:53,907 :: INFO :: ----- val -----
2023-05-14 18:02:53,907 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:02:53,907 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:02:53,907 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 18:03:44,391 :: INFO :: ----- test -----
2023-05-14 18:03:44,407 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 18:03:44,407 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 18:03:44,407 :: INFO :: ndcg [2.7294066269992904e-05, 4.094109940498935e-05, 6.546135761788419e-05]
2023-05-14 18:03:50,032 :: INFO :: Epoch 410: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0013632314512506127, MLP.norm 0.004544843919575214
2023-05-14 18:04:41,360 :: INFO :: ----- val -----
2023-05-14 18:04:41,360 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:04:41,360 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:04:41,360 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 18:05:32,188 :: INFO :: ----- test -----
2023-05-14 18:05:32,188 :: INFO :: Precision [0.0, 0.0, 5.458813253998581e-06]
2023-05-14 18:05:32,188 :: INFO :: Recall [0.0, 0.0, 5.458813253998581e-05]
2023-05-14 18:05:32,188 :: INFO :: ndcg [0.0, 0.0, 1.7938675221682498e-05]
2023-05-14 18:05:37,938 :: INFO :: Epoch 415: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0013627421576529741, MLP.norm 0.004490935709327459
2023-05-14 18:06:29,173 :: INFO :: ----- val -----
2023-05-14 18:06:29,173 :: INFO :: Precision [0.0, 8.851515822084533e-06, 1.32772737331268e-05]
2023-05-14 18:06:29,173 :: INFO :: Recall [0.0, 4.425757911042266e-05, 0.00013277273733126798]
2023-05-14 18:06:29,173 :: INFO :: ndcg [0.0, 1.7121168720271814e-05, 4.3766886415689494e-05]
2023-05-14 18:07:20,485 :: INFO :: ----- test -----
2023-05-14 18:07:20,485 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 18:07:20,485 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 18:07:20,485 :: INFO :: ndcg [0.0, 0.0, 3.5944758920237485e-05]
2023-05-14 18:07:26,157 :: INFO :: Epoch 420: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0014148212503641844, MLP.norm 0.004584077745676041
2023-05-14 18:08:16,563 :: INFO :: ----- val -----
2023-05-14 18:08:16,563 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:08:16,563 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:08:16,563 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:09:07,360 :: INFO :: ----- test -----
2023-05-14 18:09:07,360 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-14 18:09:07,360 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-14 18:09:07,360 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 4.4373214676987636e-05]
2023-05-14 18:09:13,016 :: INFO :: Epoch 425: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0014417526545003057, MLP.norm 0.00451861135661602
2023-05-14 18:10:02,594 :: INFO :: ----- val -----
2023-05-14 18:10:02,594 :: INFO :: Precision [0.0, 8.851515822084533e-06, 4.4257579110422665e-06]
2023-05-14 18:10:02,594 :: INFO :: Recall [0.0, 4.425757911042266e-05, 4.425757911042266e-05]
2023-05-14 18:10:02,594 :: INFO :: ndcg [0.0, 1.7121168720271814e-05, 1.7121168720271814e-05]
2023-05-14 18:10:53,000 :: INFO :: ----- test -----
2023-05-14 18:10:53,000 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 18:10:53,000 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 18:10:53,000 :: INFO :: ndcg [0.0, 0.0, 3.4599873712460595e-05]
2023-05-14 18:10:58,626 :: INFO :: Epoch 430: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0014656780986115336, MLP.norm 0.00438506668433547
2023-05-14 18:11:48,737 :: INFO :: ----- val -----
2023-05-14 18:11:48,737 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:11:48,737 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:11:48,737 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:12:38,518 :: INFO :: ----- test -----
2023-05-14 18:12:38,518 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.0917626507997163e-05]
2023-05-14 18:12:38,518 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00010917626507997162]
2023-05-14 18:12:38,518 :: INFO :: ndcg [0.0, 1.0558786157392369e-05, 3.627133704530144e-05]
2023-05-14 18:12:44,034 :: INFO :: Epoch 435: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0015103010227903724, MLP.norm 0.004523154813796282
2023-05-14 18:13:33,612 :: INFO :: ----- val -----
2023-05-14 18:13:33,612 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:13:33,612 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:13:33,612 :: INFO :: ndcg [0.0, 0.0, 1.5764867763134417e-05]
2023-05-14 18:14:24,081 :: INFO :: ----- test -----
2023-05-14 18:14:24,081 :: INFO :: Precision [0.0, 0.0, 1.910584638899503e-05]
2023-05-14 18:14:24,081 :: INFO :: Recall [0.0, 0.0, 0.00017741143075495388]
2023-05-14 18:14:24,081 :: INFO :: ndcg [0.0, 0.0, 6.154285478892436e-05]
2023-05-14 18:14:29,721 :: INFO :: Epoch 440: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0017718293238431215, MLP.norm 0.0043207937851548195
2023-05-14 18:15:20,315 :: INFO :: ----- val -----
2023-05-14 18:15:20,315 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:15:20,315 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:15:20,315 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-14 18:16:10,924 :: INFO :: ----- test -----
2023-05-14 18:16:10,924 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 18:16:10,924 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 18:16:10,924 :: INFO :: ndcg [0.0, 0.0, 2.6548994474527325e-05]
2023-05-14 18:16:16,565 :: INFO :: Epoch 445: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.001771287526935339, MLP.norm 0.004534542560577393
2023-05-14 18:17:06,643 :: INFO :: ----- val -----
2023-05-14 18:17:06,643 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:17:06,643 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:17:06,643 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:17:56,514 :: INFO :: ----- test -----
2023-05-14 18:17:56,514 :: INFO :: Precision [0.0, 0.0, 1.0917626507997163e-05]
2023-05-14 18:17:56,514 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 18:17:56,514 :: INFO :: ndcg [0.0, 0.0, 3.339160614830738e-05]
2023-05-14 18:18:02,170 :: INFO :: Epoch 450: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0018230099231004715, MLP.norm 0.004540285561233759
2023-05-14 18:18:51,858 :: INFO :: ----- val -----
2023-05-14 18:18:51,858 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:18:51,858 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:18:51,858 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:19:42,467 :: INFO :: ----- test -----
2023-05-14 18:19:42,467 :: INFO :: Precision [0.0, 0.0, 1.3647033134996452e-05]
2023-05-14 18:19:42,467 :: INFO :: Recall [0.0, 0.0, 0.00012282329821496806]
2023-05-14 18:19:42,467 :: INFO :: ndcg [0.0, 0.0, 4.131019644350324e-05]
2023-05-14 18:19:48,172 :: INFO :: Epoch 455: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.001964023569598794, MLP.norm 0.00426054559648037
2023-05-14 18:20:38,719 :: INFO :: ----- val -----
2023-05-14 18:20:38,719 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:20:38,719 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:20:38,719 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:21:29,032 :: INFO :: ----- test -----
2023-05-14 18:21:29,032 :: INFO :: Precision [0.0, 1.0917626507997163e-05, 8.188219880997872e-06]
2023-05-14 18:21:29,032 :: INFO :: Recall [0.0, 5.458813253998581e-05, 8.18821988099787e-05]
2023-05-14 18:21:29,032 :: INFO :: ndcg [0.0, 3.08676716406861e-05, 3.869789559345141e-05]
2023-05-14 18:21:34,548 :: INFO :: Epoch 460: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0018126048380509019, MLP.norm 0.004436624702066183
2023-05-14 18:22:25,204 :: INFO :: ----- val -----
2023-05-14 18:22:25,204 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:22:25,204 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:22:25,204 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:23:16,407 :: INFO :: ----- test -----
2023-05-14 18:23:16,407 :: INFO :: Precision [0.0, 5.458813253998581e-06, 1.3647033134996452e-05]
2023-05-14 18:23:16,407 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 0.00013647033134996453]
2023-05-14 18:23:16,407 :: INFO :: ndcg [0.0, 1.1754914516987637e-05, 5.100004352067097e-05]
2023-05-14 18:23:22,078 :: INFO :: Epoch 465: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.001839100499637425, MLP.norm 0.004544978961348534
2023-05-14 18:24:12,658 :: INFO :: ----- val -----
2023-05-14 18:24:12,658 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-14 18:24:12,658 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-14 18:24:12,658 :: INFO :: ndcg [0.0, 0.0, 2.8558177181938924e-05]
2023-05-14 18:25:04,033 :: INFO :: ----- test -----
2023-05-14 18:25:04,033 :: INFO :: Precision [0.0, 0.0, 8.188219880997872e-06]
2023-05-14 18:25:04,033 :: INFO :: Recall [0.0, 0.0, 8.18821988099787e-05]
2023-05-14 18:25:04,033 :: INFO :: ndcg [0.0, 0.0, 2.592467399375084e-05]
2023-05-14 18:25:09,704 :: INFO :: Epoch 470: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0018687559058889747, MLP.norm 0.004571021534502506
2023-05-14 18:26:00,048 :: INFO :: ----- val -----
2023-05-14 18:26:00,048 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:26:00,048 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:26:00,048 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:26:50,127 :: INFO :: ----- test -----
2023-05-14 18:26:50,127 :: INFO :: Precision [0.0, 0.0, 2.7294066269992907e-06]
2023-05-14 18:26:50,127 :: INFO :: Recall [0.0, 0.0, 2.7294066269992904e-05]
2023-05-14 18:26:50,127 :: INFO :: ndcg [0.0, 0.0, 8.21633265090838e-06]
2023-05-14 18:26:55,689 :: INFO :: Epoch 475: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0019146777922287583, MLP.norm 0.004454093985259533
2023-05-14 18:27:45,861 :: INFO :: ----- val -----
2023-05-14 18:27:45,861 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:27:45,861 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:27:45,861 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:28:35,939 :: INFO :: ----- test -----
2023-05-14 18:28:35,939 :: INFO :: Precision [0.0, 0.0, 1.3647033134996452e-05]
2023-05-14 18:28:35,939 :: INFO :: Recall [0.0, 0.0, 0.00010917626507997162]
2023-05-14 18:28:35,939 :: INFO :: ndcg [0.0, 0.0, 4.304906825321657e-05]
2023-05-14 18:28:41,470 :: INFO :: Epoch 480: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0019414776470512152, MLP.norm 0.0044362302869558334
2023-05-14 18:29:31,345 :: INFO :: ----- val -----
2023-05-14 18:29:31,345 :: INFO :: Precision [0.0, 0.0, 8.851515822084533e-06]
2023-05-14 18:29:31,345 :: INFO :: Recall [0.0, 0.0, 8.851515822084532e-05]
2023-05-14 18:29:31,345 :: INFO :: ndcg [0.0, 0.0, 2.5586618837609017e-05]
2023-05-14 18:30:21,892 :: INFO :: ----- test -----
2023-05-14 18:30:21,892 :: INFO :: Precision [0.0, 2.1835253015994326e-05, 1.3647033134996452e-05]
2023-05-14 18:30:21,892 :: INFO :: Recall [0.0, 0.00010917626507997162, 0.00013647033134996453]
2023-05-14 18:30:21,892 :: INFO :: ndcg [0.0, 4.6519519966768826e-05, 5.3899409031058956e-05]
2023-05-14 18:30:27,627 :: INFO :: Epoch 485: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.0019638489466160536, MLP.norm 0.004527033772319555
2023-05-14 18:31:18,470 :: INFO :: ----- val -----
2023-05-14 18:31:18,470 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:31:18,470 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:31:18,470 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:32:09,111 :: INFO :: ----- test -----
2023-05-14 18:32:09,111 :: INFO :: Precision [0.0, 5.458813253998581e-06, 2.7294066269992907e-06]
2023-05-14 18:32:09,111 :: INFO :: Recall [0.0, 2.7294066269992904e-05, 2.7294066269992904e-05]
2023-05-14 18:32:09,111 :: INFO :: ndcg [0.0, 1.7220638505689654e-05, 1.7220638505689654e-05]
2023-05-14 18:32:14,845 :: INFO :: Epoch 490: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.002154058078303933, MLP.norm 0.004545389674603939
2023-05-14 18:33:05,330 :: INFO :: ----- val -----
2023-05-14 18:33:05,330 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:33:05,330 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:33:05,330 :: INFO :: ndcg [0.0, 0.0, 1.3961711740904128e-05]
2023-05-14 18:33:55,767 :: INFO :: ----- test -----
2023-05-14 18:33:55,767 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:33:55,767 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:33:55,767 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:34:01,486 :: INFO :: Epoch 495: loss tensor(1.9273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 0.0, V.norm 0.00217407732270658, MLP.norm 0.004358211532235146
2023-05-14 18:34:51,127 :: INFO :: ----- val -----
2023-05-14 18:34:51,127 :: INFO :: Precision [0.0, 0.0, 4.4257579110422665e-06]
2023-05-14 18:34:51,127 :: INFO :: Recall [0.0, 0.0, 4.425757911042266e-05]
2023-05-14 18:34:51,127 :: INFO :: ndcg [0.0, 0.0, 1.2793309418804509e-05]
2023-05-14 18:35:41,846 :: INFO :: ----- test -----
2023-05-14 18:35:41,846 :: INFO :: Precision [5.458813253998581e-05, 1.6376439761995743e-05, 1.910584638899503e-05]
2023-05-14 18:35:41,846 :: INFO :: Recall [5.458813253998581e-05, 8.18821988099787e-05, 0.00017741143075495388]
2023-05-14 18:35:41,846 :: INFO :: ndcg [5.458813253998581e-05, 6.514691869737817e-05, 9.775307965282023e-05]
2023-05-14 18:35:46,392 :: INFO :: Epoch 500:
2023-05-14 18:36:36,517 :: INFO :: ----- val -----
2023-05-14 18:36:36,517 :: INFO :: Precision [0.0, 0.0, 0.0]
2023-05-14 18:36:36,517 :: INFO :: Recall [0.0, 0.0, 0.0]
2023-05-14 18:36:36,517 :: INFO :: ndcg [0.0, 0.0, 0.0]
2023-05-14 18:37:27,472 :: INFO :: ----- test -----
2023-05-14 18:37:27,472 :: INFO :: Precision [0.0, 1.6376439761995743e-05, 8.188219880997872e-06]
2023-05-14 18:37:27,472 :: INFO :: Recall [0.0, 8.18821988099787e-05, 8.18821988099787e-05]
2023-05-14 18:37:27,472 :: INFO :: ndcg [0.0, 3.785285242738527e-05, 3.785285242738527e-05]
2023-05-14 18:37:27,472 :: INFO :: final:
2023-05-14 18:37:27,472 :: INFO :: ----- test -----
2023-05-14 18:37:27,472 :: INFO :: Precision [2.7294066269992904e-05, 1.0917626507997163e-05, 1.3647033134996452e-05]
2023-05-14 18:37:27,472 :: INFO :: Recall [2.7294066269992904e-05, 5.458813253998581e-05, 0.00013647033134996453]
2023-05-14 18:37:27,472 :: INFO :: ndcg [2.7294066269992904e-05, 3.9048980786980544e-05, 6.608567809866068e-05]
2023-05-14 18:37:27,472 :: INFO :: max_epoch 310:
2023-05-14 18:37:27,472 :: INFO :: ----- frontend -----
2023-05-14 18:37:27,488 :: INFO :: Environment 0
2023-05-14 18:37:37,363 :: INFO :: Epoch 5: loss tensor(798.4201, device='cuda:0'), U.norm 13.25998306274414, V.norm 16.784385681152344, MLP.norm 1.4442007541656494
2023-05-14 18:37:37,582 :: INFO :: Epoch 10: loss tensor(783.0152, device='cuda:0'), U.norm 10.54831600189209, V.norm 16.030141830444336, MLP.norm 2.6594717502593994
2023-05-14 18:37:37,816 :: INFO :: Epoch 15: loss tensor(757.2093, device='cuda:0'), U.norm 8.963632583618164, V.norm 15.660483360290527, MLP.norm 4.25753927230835
2023-05-14 18:37:38,051 :: INFO :: Epoch 20: loss tensor(726.0339, device='cuda:0'), U.norm 7.935264587402344, V.norm 15.410852432250977, MLP.norm 5.880320072174072
2023-05-14 18:37:38,066 :: INFO :: Environment 1
2023-05-14 18:37:47,598 :: INFO :: Epoch 5: loss tensor(934.5164, device='cuda:0'), U.norm 13.28286075592041, V.norm 17.005956649780273, MLP.norm 1.603946566581726
2023-05-14 18:37:47,832 :: INFO :: Epoch 10: loss tensor(909.8007, device='cuda:0'), U.norm 10.600824356079102, V.norm 16.341230392456055, MLP.norm 3.0925259590148926
2023-05-14 18:37:48,066 :: INFO :: Epoch 15: loss tensor(871.5157, device='cuda:0'), U.norm 9.050189018249512, V.norm 16.00921058654785, MLP.norm 4.94800329208374
2023-05-14 18:37:48,301 :: INFO :: Epoch 20: loss tensor(827.5177, device='cuda:0'), U.norm 8.058183670043945, V.norm 15.78073787689209, MLP.norm 6.780886173248291
2023-05-14 18:37:48,316 :: INFO :: Environment 2
2023-05-14 18:37:57,863 :: INFO :: Epoch 5: loss tensor(871.9562, device='cuda:0'), U.norm 13.271371841430664, V.norm 16.900554656982422, MLP.norm 1.5500469207763672
2023-05-14 18:37:58,098 :: INFO :: Epoch 10: loss tensor(851.2852, device='cuda:0'), U.norm 10.5757474899292, V.norm 16.197463989257812, MLP.norm 2.935919761657715
2023-05-14 18:37:58,332 :: INFO :: Epoch 15: loss tensor(819.3005, device='cuda:0'), U.norm 9.008700370788574, V.norm 15.8501558303833, MLP.norm 4.671650409698486
2023-05-14 18:37:58,566 :: INFO :: Epoch 20: loss tensor(782.6771, device='cuda:0'), U.norm 7.998384952545166, V.norm 15.614413261413574, MLP.norm 6.417741775512695
2023-05-14 18:37:58,582 :: INFO :: Environment 3
2023-05-14 18:38:08,098 :: INFO :: Epoch 5: loss tensor(857.0401, device='cuda:0'), U.norm 13.26793098449707, V.norm 16.877418518066406, MLP.norm 1.5527396202087402
2023-05-14 18:38:08,332 :: INFO :: Epoch 10: loss tensor(838.3056, device='cuda:0'), U.norm 10.56797981262207, V.norm 16.16180419921875, MLP.norm 2.907344102859497
2023-05-14 18:38:08,566 :: INFO :: Epoch 15: loss tensor(809.6127, device='cuda:0'), U.norm 8.995901107788086, V.norm 15.809808731079102, MLP.norm 4.618527412414551
2023-05-14 18:38:08,801 :: INFO :: Epoch 20: loss tensor(775.3506, device='cuda:0'), U.norm 7.980867385864258, V.norm 15.570246696472168, MLP.norm 6.340190410614014
2023-05-14 18:38:08,816 :: INFO :: Environment 4
2023-05-14 18:38:18,144 :: INFO :: Epoch 5: loss tensor(843.7411, device='cuda:0'), U.norm 13.264195442199707, V.norm 16.856107711791992, MLP.norm 1.5248535871505737
2023-05-14 18:38:18,379 :: INFO :: Epoch 10: loss tensor(825.5051, device='cuda:0'), U.norm 10.559724807739258, V.norm 16.13385772705078, MLP.norm 2.8548998832702637
2023-05-14 18:38:18,597 :: INFO :: Epoch 15: loss tensor(796.6486, device='cuda:0'), U.norm 8.983424186706543, V.norm 15.777629852294922, MLP.norm 4.540010452270508
2023-05-14 18:38:18,847 :: INFO :: Epoch 20: loss tensor(762.4000, device='cuda:0'), U.norm 7.964512825012207, V.norm 15.537331581115723, MLP.norm 6.222078800201416
2023-05-14 18:38:18,847 :: INFO :: Environment 5
2023-05-14 18:38:28,207 :: INFO :: Epoch 5: loss tensor(849.6940, device='cuda:0'), U.norm 13.268026351928711, V.norm 16.867691040039062, MLP.norm 1.5236575603485107
2023-05-14 18:38:28,426 :: INFO :: Epoch 10: loss tensor(832.1733, device='cuda:0'), U.norm 10.567368507385254, V.norm 16.151851654052734, MLP.norm 2.8384528160095215
2023-05-14 18:38:28,660 :: INFO :: Epoch 15: loss tensor(804.2520, device='cuda:0'), U.norm 8.9955472946167, V.norm 15.801326751708984, MLP.norm 4.505459308624268
2023-05-14 18:38:28,894 :: INFO :: Epoch 20: loss tensor(770.9559, device='cuda:0'), U.norm 7.981695175170898, V.norm 15.564591407775879, MLP.norm 6.169553279876709
2023-05-14 18:38:28,910 :: INFO :: Environment 6
2023-05-14 18:38:38,472 :: INFO :: Epoch 5: loss tensor(798.3011, device='cuda:0'), U.norm 13.25956916809082, V.norm 16.780065536499023, MLP.norm 1.4853324890136719
2023-05-14 18:38:38,707 :: INFO :: Epoch 10: loss tensor(782.2439, device='cuda:0'), U.norm 10.548248291015625, V.norm 16.02176856994629, MLP.norm 2.711915969848633
2023-05-14 18:38:38,941 :: INFO :: Epoch 15: loss tensor(756.5068, device='cuda:0'), U.norm 8.963465690612793, V.norm 15.650062561035156, MLP.norm 4.29388427734375
2023-05-14 18:38:39,191 :: INFO :: Epoch 20: loss tensor(726.0785, device='cuda:0'), U.norm 7.935239791870117, V.norm 15.399632453918457, MLP.norm 5.893378734588623
2023-05-14 18:38:39,191 :: INFO :: Environment 7
2023-05-14 18:38:48,723 :: INFO :: Epoch 5: loss tensor(868.9902, device='cuda:0'), U.norm 13.270179748535156, V.norm 16.88640022277832, MLP.norm 1.4964715242385864
2023-05-14 18:38:48,957 :: INFO :: Epoch 10: loss tensor(850.5989, device='cuda:0'), U.norm 10.572920799255371, V.norm 16.1787109375, MLP.norm 2.800001382827759
2023-05-14 18:38:49,207 :: INFO :: Epoch 15: loss tensor(820.7410, device='cuda:0'), U.norm 9.005054473876953, V.norm 15.831290245056152, MLP.norm 4.512025356292725
2023-05-14 18:38:49,441 :: INFO :: Epoch 20: loss tensor(784.6423, device='cuda:0'), U.norm 7.994807243347168, V.norm 15.596452713012695, MLP.norm 6.255831718444824
2023-05-14 18:38:49,457 :: INFO :: Environment 8
2023-05-14 18:38:58,785 :: INFO :: Epoch 5: loss tensor(855.4340, device='cuda:0'), U.norm 13.269423484802246, V.norm 16.875566482543945, MLP.norm 1.5130032300949097
2023-05-14 18:38:59,019 :: INFO :: Epoch 10: loss tensor(837.0385, device='cuda:0'), U.norm 10.570347785949707, V.norm 16.162477493286133, MLP.norm 2.8481197357177734
2023-05-14 18:38:59,253 :: INFO :: Epoch 15: loss tensor(807.4118, device='cuda:0'), U.norm 9.000364303588867, V.norm 15.81157112121582, MLP.norm 4.558589458465576
2023-05-14 18:38:59,472 :: INFO :: Epoch 20: loss tensor(772.2823, device='cuda:0'), U.norm 7.988377094268799, V.norm 15.573942184448242, MLP.norm 6.283932685852051
2023-05-14 18:38:59,488 :: INFO :: Environment 9
2023-05-14 18:39:09,051 :: INFO :: Epoch 5: loss tensor(791.1132, device='cuda:0'), U.norm 13.258875846862793, V.norm 16.758790969848633, MLP.norm 1.5067017078399658
2023-05-14 18:39:09,269 :: INFO :: Epoch 10: loss tensor(775.1561, device='cuda:0'), U.norm 10.547319412231445, V.norm 15.994401931762695, MLP.norm 2.7394189834594727
2023-05-14 18:39:09,488 :: INFO :: Epoch 15: loss tensor(750.2038, device='cuda:0'), U.norm 8.961766242980957, V.norm 15.621944427490234, MLP.norm 4.305996417999268
2023-05-14 18:39:09,723 :: INFO :: Epoch 20: loss tensor(720.5311, device='cuda:0'), U.norm 7.932240962982178, V.norm 15.372553825378418, MLP.norm 5.882960796356201
2023-05-14 18:39:09,738 :: INFO :: Ite = 1, Delta = 959
2023-05-14 18:39:09,754 :: INFO :: ----- backend -----
2023-05-14 18:39:13,738 :: INFO :: Epoch 5: loss tensor(219.1996, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 1.1524e-02,  1.3177e-02,  1.8833e-02,  1.3653e-02,  1.0872e-02,
         1.3956e-02,  9.9410e-03,  1.2885e-02,  4.8906e-03,  1.8623e-02,
         1.7557e-02,  1.5702e-02,  6.7389e-03,  1.2443e-02,  1.9221e-02,
         1.8164e-02,  1.8823e-02,  1.6628e-02,  1.8851e-02,  1.5404e-02,
         1.0096e-02,  1.8385e-02,  1.9376e-02,  7.3313e-03,  1.9417e-02,
         1.2432e-02,  1.2455e-02,  8.6308e-03,  1.9627e-02,  1.5777e-02,
         1.8723e-02,  1.8724e-02,  9.9546e-03,  1.8237e-02,  2.8461e-03,
         6.5016e-03,  1.5769e-02,  1.8944e-02,  9.5684e-03,  1.9130e-02,
         1.2786e-02,  5.1521e-03,  1.9327e-02,  1.4914e-02,  1.1856e-02,
         1.9027e-02,  1.4910e-02,  1.4228e-03,  1.8777e-02,  1.3610e-02,
         1.9709e-02,  9.0670e-03,  1.8178e-02,  1.1133e-02,  1.9136e-02,
         1.5957e-02,  1.0251e-02,  1.8461e-02,  6.9054e-03,  7.1179e-03,
         1.8493e-02,  1.7929e-02,  6.1996e-03,  1.4933e-02,  1.4836e-02,
         4.3795e-03,  1.9031e-02,  1.0294e-02,  1.8978e-02,  1.1244e-02,
         1.9196e-02,  6.2787e-03,  1.8973e-02,  6.3826e-03,  1.5941e-02,
         7.1764e-03,  1.8426e-02,  9.0685e-03,  1.1735e-02,  1.3634e-02,
         1.9257e-02,  1.0575e-02,  1.8953e-02,  6.9698e-03,  2.4578e-03,
         1.9231e-02,  1.8966e-02,  1.4321e-02,  6.5802e-03,  3.7802e-03,
         1.0392e-02,  1.4112e-02,  1.9217e-02,  1.5493e-02,  1.0368e-02,
         1.9018e-02,  1.9394e-02,  1.8738e-02,  1.9073e-02,  1.8163e-02,
         7.5661e-03,  1.3121e-02,  1.4198e-02, -1.2483e-03,  1.9146e-02,
         1.6181e-02,  1.6331e-02,  1.9092e-02,  1.2811e-02,  1.5743e-02,
         1.9253e-02,  9.3721e-03,  1.9599e-02,  1.7354e-02,  1.2603e-02,
         1.0174e-02,  1.2933e-02,  1.7531e-02,  1.1800e-02,  1.3731e-02,
         1.8877e-02,  1.5278e-02,  9.0296e-03,  1.1050e-02,  3.5866e-03,
         1.8928e-02,  8.3135e-03,  1.8784e-02,  1.3800e-02,  1.9024e-02,
         1.9702e-02,  1.8192e-02,  1.6320e-02,  1.0999e-02,  6.9154e-03,
         3.4465e-03,  7.7706e-03,  9.7176e-03,  8.7705e-03,  6.5256e-03,
         1.9528e-02,  1.3507e-02,  7.8042e-03,  1.8602e-02,  1.8981e-02,
         1.9221e-02,  7.9882e-03,  1.5862e-02,  1.9633e-02,  1.2329e-02,
         1.5711e-02,  1.4608e-02,  1.9738e-02, -1.7327e-02,  1.0173e-02,
         1.1908e-02,  1.3956e-02, -5.9419e-03,  1.7615e-02, -4.3139e-04,
         1.8628e-02, -1.0607e-02,  1.4043e-02,  1.9633e-02,  1.8899e-02,
         1.2651e-02,  1.1511e-02,  6.5030e-03,  1.3105e-02,  1.0765e-02,
         1.6836e-02,  1.4091e-02, -6.7416e-03,  1.8451e-02,  1.9434e-02,
         1.6564e-02,  8.9620e-03,  1.6230e-02,  1.1996e-02,  1.9297e-02,
         1.1172e-02,  1.5817e-02,  1.3923e-02, -3.0123e-03,  1.8971e-02,
         1.6161e-02,  6.1085e-03,  1.2357e-02,  1.3915e-02, -1.3657e-02,
         7.2049e-03,  1.4700e-02,  1.9331e-02,  1.0470e-02,  1.0115e-02,
         1.4880e-02,  1.3610e-02,  1.2549e-02,  1.9388e-02,  1.0998e-02,
         1.9556e-02,  1.0354e-02,  1.9589e-02,  1.5676e-02,  1.9472e-02,
         1.2704e-02,  1.3215e-02, -1.3499e-02,  1.5573e-02,  5.0678e-03,
         1.9332e-02,  1.9232e-02,  1.1955e-02,  1.1972e-02,  1.8662e-02,
         1.9053e-02,  6.8788e-03,  3.9824e-03,  1.9566e-02,  1.4689e-03,
         1.2262e-02,  8.0834e-03,  1.7790e-02,  1.9261e-02,  1.5950e-02,
         9.7974e-03,  9.3290e-03,  1.3839e-02,  8.4227e-03,  1.9725e-02,
         1.9538e-02,  1.5281e-02,  1.8426e-02,  1.3466e-02,  1.9015e-02,
         1.1270e-02,  9.9740e-03,  1.5150e-02,  7.3430e-03,  1.5538e-02,
         5.7970e-03,  1.4720e-02,  1.2138e-02,  1.9012e-02,  1.5970e-02,
         1.9407e-02,  2.4825e-03,  1.6662e-02,  1.6644e-02,  1.6499e-02,
         1.3627e-02,  1.8856e-02,  9.8491e-03,  1.9647e-02, -1.5633e-05,
         9.8487e-03,  1.1395e-02,  1.4733e-02,  1.3615e-02,  1.9529e-02,
         1.9620e-02,  1.1334e-02,  9.2547e-03,  1.1182e-02,  1.9576e-02,
         5.5607e-03,  1.9348e-02,  1.9598e-02,  1.2606e-02,  1.9430e-02,
         8.8766e-03,  1.0147e-02,  1.6225e-02,  1.3691e-02,  1.5987e-02,
         1.4855e-02,  9.0279e-03,  1.9393e-02,  1.9739e-02,  1.2662e-02,
         1.3806e-02,  1.4422e-02,  1.9628e-02,  1.1081e-02,  1.3404e-02,
         1.5947e-02,  1.0999e-02,  1.9188e-02,  1.9558e-02,  1.3536e-02,
         1.3020e-02,  1.1210e-02,  1.2531e-02,  1.5783e-02,  1.0599e-02,
         1.9542e-02,  1.6441e-02,  1.8812e-02,  1.3511e-02,  1.9345e-02,
         1.2322e-02,  1.9690e-02,  1.6762e-02,  5.4874e-03,  1.3891e-02,
         1.5793e-02,  1.9514e-02,  1.0805e-02,  1.2713e-02,  9.8927e-03,
         1.0623e-02,  1.3988e-02,  1.6717e-02,  1.5598e-02,  8.1347e-03,
         1.6745e-02,  1.0660e-02,  1.3388e-02,  1.9626e-02,  1.9007e-02,
         1.9714e-02,  1.3288e-02,  1.9654e-02,  1.8960e-02,  1.4227e-02,
         1.9913e-02,  4.2967e-03,  1.3460e-02,  1.0422e-02,  1.6586e-02,
         1.9601e-02,  1.2163e-02,  1.1773e-02,  7.2268e-03,  1.9278e-02,
         1.3968e-02,  1.1768e-02,  1.6706e-02,  1.9059e-02,  1.2569e-02,
         1.5817e-02,  1.3138e-02,  1.1489e-02,  1.1598e-02,  9.7648e-03,
         1.5778e-02,  1.9038e-02,  1.9404e-02,  1.0081e-02,  1.9639e-02,
         1.0888e-02,  1.4302e-02,  1.3906e-02,  7.3473e-03,  1.3873e-02,
         1.9044e-02,  5.3637e-03,  1.5962e-02,  1.2436e-02,  1.8907e-02,
         1.1631e-02,  9.6606e-03,  1.9393e-02,  7.6984e-03,  1.9106e-02,
         1.9647e-02,  1.8960e-02,  1.4604e-02,  1.9669e-02,  1.4740e-02,
         1.8974e-02,  1.6683e-03,  1.4253e-02,  1.1353e-02,  1.5316e-02,
         1.9456e-02,  1.5192e-02,  1.3950e-02,  1.4831e-02,  9.8158e-03,
         1.9678e-02,  1.0430e-02,  1.8924e-02,  1.6347e-02], device='cuda:0',
       requires_grad=True) MLP.norm tensor(2.6705, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:17,738 :: INFO :: Epoch 10: loss tensor(217.8349, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0319,  0.0326,  0.0403,  0.0346,  0.0304,  0.0357,  0.0302,  0.0327,
         0.0254,  0.0356,  0.0380,  0.0364,  0.0274,  0.0332,  0.0397,  0.0378,
         0.0380,  0.0353,  0.0374,  0.0356,  0.0299,  0.0356,  0.0391,  0.0290,
         0.0399,  0.0324,  0.0316,  0.0304,  0.0404,  0.0351,  0.0395,  0.0375,
         0.0298,  0.0376,  0.0242,  0.0271,  0.0347,  0.0399,  0.0305,  0.0387,
         0.0324,  0.0252,  0.0394,  0.0349,  0.0329,  0.0384,  0.0340,  0.0221,
         0.0386,  0.0322,  0.0404,  0.0285,  0.0368,  0.0309,  0.0400,  0.0374,
         0.0295,  0.0404,  0.0285,  0.0289,  0.0367,  0.0367,  0.0269,  0.0349,
         0.0358,  0.0247,  0.0365,  0.0313,  0.0374,  0.0323,  0.0383,  0.0266,
         0.0371,  0.0278,  0.0349,  0.0289,  0.0369,  0.0300,  0.0309,  0.0328,
         0.0400,  0.0320,  0.0402,  0.0277,  0.0228,  0.0392,  0.0378,  0.0351,
         0.0271,  0.0236,  0.0319,  0.0354,  0.0387,  0.0339,  0.0320,  0.0378,
         0.0390,  0.0385,  0.0395,  0.0386,  0.0290,  0.0310,  0.0349,  0.0187,
         0.0399,  0.0371,  0.0353,  0.0384,  0.0327,  0.0363,  0.0397,  0.0293,
         0.0406,  0.0379,  0.0343,  0.0309,  0.0323,  0.0372,  0.0303,  0.0348,
         0.0391,  0.0367,  0.0304,  0.0300,  0.0241,  0.0388,  0.0293,  0.0396,
         0.0333,  0.0405,  0.0414,  0.0324,  0.0332,  0.0150,  0.0292,  0.0259,
         0.0308,  0.0061,  0.0306,  0.0289,  0.0413,  0.0360,  0.0299,  0.0404,
         0.0412,  0.0395,  0.0201,  0.0388,  0.0414,  0.0348,  0.0382,  0.0355,
         0.0413, -0.0303,  0.0324,  0.0346,  0.0363,  0.0136,  0.0330,  0.0213,
         0.0380,  0.0003,  0.0218,  0.0414,  0.0388,  0.0186,  0.0340,  0.0290,
         0.0354,  0.0324,  0.0372,  0.0349,  0.0122,  0.0403,  0.0407,  0.0392,
         0.0095,  0.0378,  0.0342,  0.0412,  0.0332,  0.0384,  0.0284,  0.0182,
         0.0399,  0.0365,  0.0284,  0.0329,  0.0336, -0.0110,  0.0300,  0.0371,
         0.0399,  0.0330,  0.0279,  0.0335,  0.0359,  0.0125,  0.0401,  0.0337,
         0.0413,  0.0328,  0.0408,  0.0179,  0.0411,  0.0353,  0.0355, -0.0128,
         0.0351,  0.0275,  0.0409,  0.0389,  0.0146,  0.0344,  0.0398,  0.0411,
         0.0294,  0.0265,  0.0407,  0.0229,  0.0338,  0.0299,  0.0354,  0.0398,
         0.0278,  0.0311,  0.0309,  0.0359,  0.0263,  0.0409,  0.0407,  0.0292,
         0.0314,  0.0213,  0.0392,  0.0342,  0.0326,  0.0374,  0.0054,  0.0309,
         0.0275,  0.0371,  0.0350,  0.0388,  0.0385,  0.0410,  0.0250,  0.0384,
         0.0388,  0.0387,  0.0368,  0.0377,  0.0326,  0.0421,  0.0226,  0.0320,
         0.0344,  0.0378,  0.0366,  0.0420,  0.0423,  0.0339,  0.0323,  0.0343,
         0.0420,  0.0284,  0.0419,  0.0424,  0.0356,  0.0419,  0.0317,  0.0332,
         0.0389,  0.0365,  0.0391,  0.0376,  0.0325,  0.0419,  0.0422,  0.0354,
         0.0367,  0.0375,  0.0423,  0.0342,  0.0367,  0.0385,  0.0340,  0.0420,
         0.0421,  0.0364,  0.0357,  0.0343,  0.0358,  0.0384,  0.0334,  0.0421,
         0.0392,  0.0416,  0.0362,  0.0419,  0.0355,  0.0423,  0.0394,  0.0288,
         0.0368,  0.0387,  0.0423,  0.0340,  0.0353,  0.0328,  0.0336,  0.0371,
         0.0395,  0.0382,  0.0312,  0.0395,  0.0334,  0.0365,  0.0422,  0.0420,
         0.0424,  0.0361,  0.0423,  0.0417,  0.0372,  0.0424,  0.0274,  0.0364,
         0.0334,  0.0394,  0.0423,  0.0353,  0.0349,  0.0303,  0.0417,  0.0369,
         0.0341,  0.0396,  0.0419,  0.0354,  0.0386,  0.0361,  0.0345,  0.0341,
         0.0328,  0.0378,  0.0417,  0.0423,  0.0330,  0.0423,  0.0340,  0.0369,
         0.0368,  0.0304,  0.0364,  0.0420,  0.0285,  0.0389,  0.0350,  0.0417,
         0.0350,  0.0329,  0.0420,  0.0313,  0.0419,  0.0421,  0.0418,  0.0374,
         0.0423,  0.0376,  0.0418,  0.0246,  0.0368,  0.0343,  0.0382,  0.0420,
         0.0380,  0.0352,  0.0378,  0.0329,  0.0421,  0.0335,  0.0417,  0.0392],
       device='cuda:0', requires_grad=True) MLP.norm tensor(4.1882, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:21,707 :: INFO :: Epoch 15: loss tensor(210.7776, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0513,  0.0504,  0.0623,  0.0549,  0.0455,  0.0572,  0.0468,  0.0490,
         0.0429,  0.0489,  0.0568,  0.0551,  0.0439,  0.0530,  0.0603,  0.0562,
         0.0531,  0.0503,  0.0528,  0.0537,  0.0482,  0.0472,  0.0574,  0.0509,
         0.0597,  0.0482,  0.0462,  0.0524,  0.0602,  0.0517,  0.0601,  0.0526,
         0.0462,  0.0542,  0.0458,  0.0473,  0.0506,  0.0595,  0.0492,  0.0554,
         0.0496,  0.0413,  0.0577,  0.0532,  0.0515,  0.0547,  0.0499,  0.0422,
         0.0558,  0.0464,  0.0610,  0.0463,  0.0519,  0.0455,  0.0598,  0.0583,
         0.0435,  0.0624,  0.0495,  0.0505,  0.0501,  0.0518,  0.0435,  0.0536,
         0.0558,  0.0422,  0.0487,  0.0512,  0.0528,  0.0517,  0.0553,  0.0445,
         0.0508,  0.0460,  0.0505,  0.0500,  0.0526,  0.0496,  0.0471,  0.0483,
         0.0607,  0.0527,  0.0613,  0.0472,  0.0407,  0.0573,  0.0532,  0.0546,
         0.0447,  0.0409,  0.0527,  0.0558,  0.0553,  0.0472,  0.0530,  0.0529,
         0.0562,  0.0570,  0.0589,  0.0578,  0.0490,  0.0423,  0.0556,  0.0342,
         0.0612,  0.0563,  0.0517,  0.0553,  0.0493,  0.0550,  0.0598,  0.0443,
         0.0613,  0.0566,  0.0558,  0.0511,  0.0480,  0.0534,  0.0429,  0.0559,
         0.0584,  0.0578,  0.0498,  0.0442,  0.0419,  0.0568,  0.0497,  0.0601,
         0.0465,  0.0607,  0.0631,  0.0334,  0.0403,  0.0125,  0.0509,  0.0489,
         0.0553,  0.0041,  0.0510,  0.0516,  0.0622,  0.0588,  0.0511,  0.0625,
         0.0635,  0.0568,  0.0172,  0.0629,  0.0633,  0.0579,  0.0614,  0.0535,
         0.0621, -0.0355,  0.0539,  0.0584,  0.0591,  0.0376,  0.0364,  0.0440,
         0.0521,  0.0203,  0.0198,  0.0629,  0.0549,  0.0177,  0.0567,  0.0517,
         0.0581,  0.0520,  0.0546,  0.0529,  0.0357,  0.0615,  0.0608,  0.0629,
         0.0087,  0.0578,  0.0560,  0.0627,  0.0540,  0.0618,  0.0287,  0.0422,
         0.0586,  0.0530,  0.0521,  0.0503,  0.0496,  0.0063,  0.0537,  0.0598,
         0.0579,  0.0565,  0.0329,  0.0453,  0.0578,  0.0086,  0.0587,  0.0568,
         0.0628,  0.0550,  0.0609,  0.0150,  0.0623,  0.0578,  0.0578,  0.0007,
         0.0473,  0.0499,  0.0613,  0.0540,  0.0142,  0.0574,  0.0592,  0.0625,
         0.0521,  0.0497,  0.0601,  0.0466,  0.0532,  0.0504,  0.0450,  0.0582,
         0.0305,  0.0499,  0.0496,  0.0569,  0.0331,  0.0616,  0.0605,  0.0288,
         0.0300,  0.0215,  0.0545,  0.0583,  0.0556,  0.0599,  0.0029,  0.0352,
         0.0475,  0.0596,  0.0586,  0.0556,  0.0616,  0.0620,  0.0493,  0.0599,
         0.0609,  0.0611,  0.0610,  0.0496,  0.0556,  0.0650,  0.0476,  0.0532,
         0.0589,  0.0627,  0.0616,  0.0652,  0.0659,  0.0574,  0.0571,  0.0591,
         0.0659,  0.0521,  0.0654,  0.0665,  0.0599,  0.0650,  0.0558,  0.0577,
         0.0629,  0.0609,  0.0634,  0.0616,  0.0572,  0.0656,  0.0658,  0.0594,
         0.0609,  0.0624,  0.0663,  0.0593,  0.0614,  0.0617,  0.0586,  0.0663,
         0.0651,  0.0607,  0.0591,  0.0587,  0.0608,  0.0618,  0.0576,  0.0650,
         0.0630,  0.0657,  0.0587,  0.0652,  0.0606,  0.0663,  0.0635,  0.0543,
         0.0607,  0.0634,  0.0656,  0.0583,  0.0592,  0.0575,  0.0576,  0.0613,
         0.0639,  0.0619,  0.0555,  0.0633,  0.0566,  0.0614,  0.0662,  0.0668,
         0.0663,  0.0602,  0.0659,  0.0658,  0.0614,  0.0659,  0.0527,  0.0606,
         0.0576,  0.0635,  0.0662,  0.0597,  0.0598,  0.0551,  0.0657,  0.0600,
         0.0557,  0.0639,  0.0658,  0.0593,  0.0628,  0.0606,  0.0589,  0.0577,
         0.0571,  0.0598,  0.0650,  0.0668,  0.0570,  0.0663,  0.0586,  0.0607,
         0.0611,  0.0545,  0.0599,  0.0663,  0.0538,  0.0627,  0.0578,  0.0658,
         0.0600,  0.0582,  0.0659,  0.0568,  0.0655,  0.0653,  0.0657,  0.0616,
         0.0660,  0.0621,  0.0662,  0.0501,  0.0605,  0.0584,  0.0627,  0.0659,
         0.0615,  0.0564,  0.0617,  0.0581,  0.0656,  0.0582,  0.0660,  0.0628],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.2841, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:25,738 :: INFO :: Epoch 20: loss tensor(209.8761, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0662,  0.0627,  0.0826,  0.0714,  0.0537,  0.0762,  0.0570,  0.0600,
         0.0530,  0.0553,  0.0714,  0.0688,  0.0538,  0.0682,  0.0782,  0.0704,
         0.0614,  0.0586,  0.0625,  0.0680,  0.0628,  0.0511,  0.0708,  0.0696,
         0.0764,  0.0584,  0.0535,  0.0722,  0.0764,  0.0627,  0.0776,  0.0614,
         0.0561,  0.0642,  0.0640,  0.0628,  0.0605,  0.0763,  0.0631,  0.0663,
         0.0614,  0.0500,  0.0722,  0.0673,  0.0653,  0.0654,  0.0597,  0.0580,
         0.0684,  0.0535,  0.0785,  0.0576,  0.0608,  0.0530,  0.0765,  0.0772,
         0.0490,  0.0828,  0.0669,  0.0699,  0.0570,  0.0614,  0.0535,  0.0672,
         0.0714,  0.0539,  0.0542,  0.0666,  0.0629,  0.0672,  0.0672,  0.0559,
         0.0585,  0.0595,  0.0595,  0.0674,  0.0623,  0.0654,  0.0567,  0.0577,
         0.0785,  0.0692,  0.0792,  0.0620,  0.0526,  0.0709,  0.0632,  0.0699,
         0.0562,  0.0524,  0.0705,  0.0730,  0.0679,  0.0527,  0.0706,  0.0617,
         0.0692,  0.0719,  0.0755,  0.0735,  0.0658,  0.0467,  0.0725,  0.0433,
         0.0800,  0.0720,  0.0618,  0.0681,  0.0605,  0.0690,  0.0774,  0.0515,
         0.0796,  0.0705,  0.0743,  0.0686,  0.0576,  0.0637,  0.0489,  0.0749,
         0.0740,  0.0756,  0.0651,  0.0506,  0.0541,  0.0716,  0.0663,  0.0771,
         0.0497,  0.0776,  0.0828,  0.0258,  0.0376,  0.0094,  0.0691,  0.0699,
         0.0793,  0.0019,  0.0668,  0.0720,  0.0790,  0.0801,  0.0681,  0.0826,
         0.0834,  0.0683,  0.0122,  0.0862,  0.0833,  0.0792,  0.0836,  0.0655,
         0.0799, -0.0360,  0.0724,  0.0817,  0.0807,  0.0610,  0.0303,  0.0657,
         0.0571,  0.0411,  0.0173,  0.0824,  0.0642,  0.0156,  0.0778,  0.0723,
         0.0789,  0.0667,  0.0651,  0.0642,  0.0582,  0.0800,  0.0772,  0.0860,
         0.0069,  0.0729,  0.0748,  0.0818,  0.0702,  0.0843,  0.0216,  0.0654,
         0.0720,  0.0619,  0.0747,  0.0598,  0.0571,  0.0263,  0.0766,  0.0807,
         0.0704,  0.0790,  0.0250,  0.0470,  0.0772,  0.0081,  0.0728,  0.0779,
         0.0819,  0.0740,  0.0774,  0.0119,  0.0804,  0.0778,  0.0782,  0.0178,
         0.0481,  0.0695,  0.0772,  0.0609,  0.0107,  0.0788,  0.0733,  0.0804,
         0.0725,  0.0711,  0.0751,  0.0694,  0.0669,  0.0665,  0.0447,  0.0715,
         0.0280,  0.0625,  0.0616,  0.0737,  0.0289,  0.0792,  0.0763,  0.0202,
         0.0200,  0.0192,  0.0611,  0.0812,  0.0766,  0.0808,  0.0034,  0.0328,
         0.0626,  0.0803,  0.0813,  0.0674,  0.0836,  0.0806,  0.0727,  0.0788,
         0.0813,  0.0820,  0.0847,  0.0517,  0.0766,  0.0863,  0.0724,  0.0707,
         0.0837,  0.0873,  0.0872,  0.0872,  0.0891,  0.0806,  0.0815,  0.0835,
         0.0891,  0.0750,  0.0875,  0.0900,  0.0831,  0.0865,  0.0790,  0.0817,
         0.0865,  0.0845,  0.0870,  0.0853,  0.0819,  0.0892,  0.0888,  0.0828,
         0.0846,  0.0877,  0.0895,  0.0846,  0.0856,  0.0836,  0.0829,  0.0904,
         0.0870,  0.0851,  0.0819,  0.0827,  0.0862,  0.0843,  0.0812,  0.0873,
         0.0857,  0.0893,  0.0789,  0.0876,  0.0859,  0.0899,  0.0876,  0.0801,
         0.0846,  0.0879,  0.0874,  0.0821,  0.0821,  0.0822,  0.0810,  0.0857,
         0.0885,  0.0850,  0.0790,  0.0864,  0.0786,  0.0864,  0.0898,  0.0919,
         0.0897,  0.0840,  0.0887,  0.0892,  0.0852,  0.0886,  0.0782,  0.0848,
         0.0807,  0.0876,  0.0899,  0.0841,  0.0845,  0.0794,  0.0901,  0.0802,
         0.0747,  0.0879,  0.0892,  0.0817,  0.0864,  0.0847,  0.0832,  0.0812,
         0.0805,  0.0792,  0.0871,  0.0909,  0.0806,  0.0902,  0.0826,  0.0834,
         0.0853,  0.0775,  0.0834,  0.0898,  0.0793,  0.0860,  0.0787,  0.0896,
         0.0847,  0.0840,  0.0894,  0.0825,  0.0886,  0.0874,  0.0893,  0.0856,
         0.0889,  0.0865,  0.0904,  0.0764,  0.0834,  0.0815,  0.0876,  0.0893,
         0.0836,  0.0752,  0.0849,  0.0839,  0.0885,  0.0827,  0.0903,  0.0855],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0567, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:29,879 :: INFO :: Epoch 25: loss tensor(205.4037, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0764,  0.0695,  0.1017,  0.0841,  0.0572,  0.0921,  0.0613,  0.0656,
         0.0574,  0.0560,  0.0821,  0.0788,  0.0583,  0.0787,  0.0935,  0.0804,
         0.0641,  0.0624,  0.0667,  0.0792,  0.0733,  0.0493,  0.0793,  0.0844,
         0.0899,  0.0633,  0.0557,  0.0897,  0.0888,  0.0685,  0.0912,  0.0652,
         0.0598,  0.0685,  0.0784,  0.0734,  0.0641,  0.0884,  0.0711,  0.0718,
         0.0676,  0.0529,  0.0816,  0.0773,  0.0742,  0.0709,  0.0649,  0.0683,
         0.0773,  0.0541,  0.0928,  0.0626,  0.0642,  0.0564,  0.0901,  0.0937,
         0.0494,  0.1017,  0.0809,  0.0857,  0.0593,  0.0669,  0.0579,  0.0761,
         0.0828,  0.0602,  0.0552,  0.0765,  0.0672,  0.0783,  0.0739,  0.0612,
         0.0617,  0.0678,  0.0630,  0.0808,  0.0665,  0.0776,  0.0599,  0.0614,
         0.0928,  0.0811,  0.0940,  0.0719,  0.0583,  0.0797,  0.0684,  0.0806,
         0.0617,  0.0579,  0.0838,  0.0865,  0.0761,  0.0523,  0.0844,  0.0648,
         0.0771,  0.0827,  0.0892,  0.0864,  0.0782,  0.0486,  0.0850,  0.0467,
         0.0962,  0.0840,  0.0660,  0.0761,  0.0671,  0.0781,  0.0925,  0.0539,
         0.0947,  0.0795,  0.0901,  0.0832,  0.0618,  0.0687,  0.0516,  0.0912,
         0.0855,  0.0898,  0.0771,  0.0514,  0.0613,  0.0825,  0.0784,  0.0904,
         0.0461,  0.0909,  0.1005,  0.0194,  0.0323,  0.0084,  0.0839,  0.0891,
         0.1026,  0.0019,  0.0784,  0.0902,  0.0913,  0.0999,  0.0802,  0.1013,
         0.1007,  0.0738,  0.0105,  0.1084,  0.1013,  0.0985,  0.1047,  0.0724,
         0.0947, -0.0333,  0.0874,  0.1041,  0.1008,  0.0832,  0.0240,  0.0871,
         0.0559,  0.0622,  0.0170,  0.0998,  0.0677,  0.0140,  0.0974,  0.0909,
         0.0980,  0.0769,  0.0694,  0.0689,  0.0797,  0.0952,  0.0893,  0.1085,
         0.0066,  0.0827,  0.0909,  0.0983,  0.0819,  0.1058,  0.0170,  0.0876,
         0.0797,  0.0651,  0.0960,  0.0618,  0.0581,  0.0467,  0.0992,  0.0999,
         0.0772,  0.1007,  0.0161,  0.0436,  0.0939,  0.0065,  0.0825,  0.0967,
         0.0983,  0.0893,  0.0903,  0.0101,  0.0954,  0.0950,  0.0970,  0.0363,
         0.0425,  0.0863,  0.0884,  0.0608,  0.0099,  0.0988,  0.0823,  0.0948,
         0.0905,  0.0905,  0.0847,  0.0912,  0.0758,  0.0786,  0.0411,  0.0809,
         0.0248,  0.0693,  0.0666,  0.0867,  0.0235,  0.0934,  0.0881,  0.0144,
         0.0136,  0.0176,  0.0600,  0.1028,  0.0958,  0.0998,  0.0025,  0.0299,
         0.0727,  0.0993,  0.1025,  0.0746,  0.1044,  0.0968,  0.0954,  0.0952,
         0.0997,  0.1011,  0.1079,  0.0481,  0.0953,  0.1057,  0.0966,  0.0838,
         0.1078,  0.1112,  0.1126,  0.1072,  0.1116,  0.1024,  0.1052,  0.1073,
         0.1111,  0.0969,  0.1074,  0.1127,  0.1052,  0.1056,  0.1008,  0.1046,
         0.1089,  0.1067,  0.1094,  0.1077,  0.1061,  0.1117,  0.1110,  0.1047,
         0.1070,  0.1130,  0.1113,  0.1091,  0.1090,  0.1033,  0.1065,  0.1138,
         0.1070,  0.1088,  0.1030,  0.1056,  0.1114,  0.1044,  0.1036,  0.1077,
         0.1063,  0.1117,  0.0955,  0.1084,  0.1108,  0.1129,  0.1106,  0.1056,
         0.1076,  0.1121,  0.1072,  0.1048,  0.1037,  0.1059,  0.1026,  0.1091,
         0.1123,  0.1069,  0.1013,  0.1081,  0.0982,  0.1110,  0.1125,  0.1166,
         0.1124,  0.1069,  0.1102,  0.1114,  0.1078,  0.1101,  0.1032,  0.1081,
         0.1019,  0.1104,  0.1127,  0.1072,  0.1085,  0.1029,  0.1140,  0.0963,
         0.0898,  0.1110,  0.1116,  0.1021,  0.1087,  0.1080,  0.1065,  0.1032,
         0.1024,  0.0960,  0.1068,  0.1147,  0.1023,  0.1135,  0.1056,  0.1043,
         0.1088,  0.0990,  0.1055,  0.1122,  0.1045,  0.1089,  0.0964,  0.1123,
         0.1090,  0.1094,  0.1120,  0.1081,  0.1106,  0.1074,  0.1117,  0.1092,
         0.1106,  0.1101,  0.1140,  0.1024,  0.1045,  0.1029,  0.1122,  0.1119,
         0.1034,  0.0907,  0.1067,  0.1095,  0.1101,  0.1064,  0.1139,  0.1065],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.6279, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:33,910 :: INFO :: Epoch 30: loss tensor(204.3773, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0834,  0.0743,  0.1189,  0.0936,  0.0580,  0.1046,  0.0632,  0.0681,
         0.0600,  0.0561,  0.0905,  0.0870,  0.0601,  0.0864,  0.1066,  0.0871,
         0.0652,  0.0647,  0.0692,  0.0875,  0.0801,  0.0477,  0.0853,  0.0969,
         0.1009,  0.0652,  0.0568,  0.1046,  0.0986,  0.0723,  0.1026,  0.0677,
         0.0608,  0.0707,  0.0911,  0.0821,  0.0664,  0.0965,  0.0755,  0.0753,
         0.0712,  0.0549,  0.0875,  0.0846,  0.0803,  0.0737,  0.0680,  0.0759,
         0.0830,  0.0540,  0.1047,  0.0657,  0.0658,  0.0583,  0.1000,  0.1075,
         0.0489,  0.1186,  0.0915,  0.0989,  0.0597,  0.0699,  0.0586,  0.0819,
         0.0908,  0.0640,  0.0555,  0.0841,  0.0694,  0.0861,  0.0786,  0.0644,
         0.0627,  0.0715,  0.0637,  0.0914,  0.0686,  0.0862,  0.0612,  0.0624,
         0.1048,  0.0895,  0.1057,  0.0791,  0.0616,  0.0862,  0.0715,  0.0890,
         0.0650,  0.0607,  0.0940,  0.0970,  0.0807,  0.0510,  0.0948,  0.0663,
         0.0820,  0.0903,  0.0997,  0.0961,  0.0877,  0.0487,  0.0950,  0.0475,
         0.1101,  0.0934,  0.0681,  0.0818,  0.0713,  0.0838,  0.1048,  0.0553,
         0.1073,  0.0857,  0.1035,  0.0951,  0.0641,  0.0710,  0.0517,  0.1046,
         0.0945,  0.1009,  0.0855,  0.0521,  0.0655,  0.0901,  0.0877,  0.1004,
         0.0393,  0.1003,  0.1155,  0.0153,  0.0280,  0.0083,  0.0947,  0.1058,
         0.1245,  0.0012,  0.0844,  0.1057,  0.0990,  0.1178,  0.0872,  0.1176,
         0.1147,  0.0732,  0.0089,  0.1289,  0.1165,  0.1153,  0.1243,  0.0737,
         0.1058, -0.0283,  0.0986,  0.1250,  0.1188,  0.1038,  0.0200,  0.1058,
         0.0504,  0.0807,  0.0151,  0.1147,  0.0673,  0.0136,  0.1148,  0.1065,
         0.1148,  0.0818,  0.0695,  0.0686,  0.0992,  0.1060,  0.0964,  0.1295,
         0.0054,  0.0870,  0.1038,  0.1122,  0.0883,  0.1257,  0.0142,  0.1081,
         0.0820,  0.0632,  0.1158,  0.0587,  0.0560,  0.0654,  0.1197,  0.1167,
         0.0791,  0.1210,  0.0109,  0.0380,  0.1069,  0.0060,  0.0869,  0.1125,
         0.1113,  0.1002,  0.0991,  0.0088,  0.1064,  0.1084,  0.1133,  0.0530,
         0.0345,  0.0997,  0.0939,  0.0561,  0.0096,  0.1168,  0.0862,  0.1044,
         0.1056,  0.1075,  0.0887,  0.1114,  0.0784,  0.0858,  0.0363,  0.0854,
         0.0223,  0.0706,  0.0655,  0.0946,  0.0194,  0.1038,  0.0956,  0.0112,
         0.0105,  0.0164,  0.0545,  0.1222,  0.1125,  0.1165,  0.0025,  0.0277,
         0.0777,  0.1156,  0.1217,  0.0781,  0.1235,  0.1102,  0.1167,  0.1083,
         0.1155,  0.1180,  0.1294,  0.0417,  0.1106,  0.1225,  0.1195,  0.0923,
         0.1313,  0.1343,  0.1377,  0.1245,  0.1328,  0.1232,  0.1278,  0.1297,
         0.1323,  0.1167,  0.1263,  0.1339,  0.1255,  0.1219,  0.1212,  0.1262,
         0.1299,  0.1277,  0.1304,  0.1292,  0.1291,  0.1333,  0.1319,  0.1247,
         0.1283,  0.1377,  0.1316,  0.1331,  0.1312,  0.1203,  0.1293,  0.1363,
         0.1247,  0.1319,  0.1227,  0.1273,  0.1356,  0.1226,  0.1244,  0.1260,
         0.1247,  0.1334,  0.1083,  0.1273,  0.1352,  0.1348,  0.1328,  0.1307,
         0.1293,  0.1355,  0.1247,  0.1261,  0.1230,  0.1292,  0.1229,  0.1312,
         0.1355,  0.1271,  0.1225,  0.1280,  0.1152,  0.1349,  0.1342,  0.1406,
         0.1340,  0.1290,  0.1306,  0.1322,  0.1292,  0.1298,  0.1276,  0.1306,
         0.1214,  0.1325,  0.1343,  0.1295,  0.1317,  0.1251,  0.1373,  0.1085,
         0.1008,  0.1330,  0.1328,  0.1196,  0.1293,  0.1298,  0.1290,  0.1233,
         0.1224,  0.1096,  0.1241,  0.1374,  0.1220,  0.1358,  0.1274,  0.1239,
         0.1312,  0.1189,  0.1267,  0.1330,  0.1293,  0.1299,  0.1116,  0.1336,
         0.1322,  0.1346,  0.1334,  0.1329,  0.1309,  0.1255,  0.1330,  0.1322,
         0.1309,  0.1326,  0.1367,  0.1281,  0.1236,  0.1217,  0.1360,  0.1334,
         0.1212,  0.1016,  0.1267,  0.1347,  0.1299,  0.1290,  0.1368,  0.1258],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.0588, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:37,863 :: INFO :: Epoch 35: loss tensor(204.5385, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0869,  0.0762,  0.1338,  0.1003,  0.0581,  0.1144,  0.0632,  0.0695,
         0.0617,  0.0564,  0.0955,  0.0932,  0.0599,  0.0916,  0.1177,  0.0912,
         0.0663,  0.0659,  0.0703,  0.0921,  0.0851,  0.0467,  0.0891,  0.1064,
         0.1101,  0.0654,  0.0581,  0.1171,  0.1051,  0.0736,  0.1108,  0.0684,
         0.0610,  0.0711,  0.1008,  0.0887,  0.0673,  0.1023,  0.0790,  0.0767,
         0.0736,  0.0551,  0.0904,  0.0893,  0.0838,  0.0747,  0.0686,  0.0816,
         0.0854,  0.0550,  0.1131,  0.0673,  0.0668,  0.0586,  0.1065,  0.1182,
         0.0481,  0.1335,  0.0985,  0.1092,  0.0595,  0.0717,  0.0576,  0.0845,
         0.0963,  0.0661,  0.0550,  0.0879,  0.0702,  0.0915,  0.0808,  0.0656,
         0.0622,  0.0734,  0.0637,  0.0995,  0.0689,  0.0910,  0.0614,  0.0615,
         0.1140,  0.0950,  0.1145,  0.0837,  0.0635,  0.0889,  0.0727,  0.0940,
         0.0672,  0.0627,  0.1019,  0.1039,  0.0829,  0.0500,  0.1017,  0.0655,
         0.0849,  0.0948,  0.1078,  0.1029,  0.0938,  0.0486,  0.1022,  0.0491,
         0.1207,  0.0999,  0.0686,  0.0846,  0.0727,  0.0870,  0.1132,  0.0557,
         0.1168,  0.0892,  0.1146,  0.1043,  0.0649,  0.0721,  0.0516,  0.1151,
         0.1009,  0.1099,  0.0900,  0.0522,  0.0684,  0.0958,  0.0943,  0.1073,
         0.0339,  0.1065,  0.1280,  0.0123,  0.0244,  0.0074,  0.1018,  0.1202,
         0.1450,  0.0010,  0.0866,  0.1182,  0.1019,  0.1338,  0.0898,  0.1321,
         0.1255,  0.0691,  0.0073,  0.1476,  0.1288,  0.1294,  0.1423,  0.0719,
         0.1134, -0.0227,  0.1061,  0.1444,  0.1351,  0.1227,  0.0193,  0.1231,
         0.0449,  0.0985,  0.0135,  0.1268,  0.0652,  0.0133,  0.1303,  0.1197,
         0.1291,  0.0828,  0.0673,  0.0657,  0.1167,  0.1132,  0.0992,  0.1492,
         0.0058,  0.0875,  0.1134,  0.1240,  0.0907,  0.1439,  0.0133,  0.1269,
         0.0805,  0.0585,  0.1344,  0.0532,  0.0529,  0.0821,  0.1390,  0.1316,
         0.0777,  0.1398,  0.0085,  0.0324,  0.1167,  0.0063,  0.0874,  0.1253,
         0.1212,  0.1076,  0.1047,  0.0084,  0.1134,  0.1186,  0.1270,  0.0682,
         0.0288,  0.1102,  0.0950,  0.0501,  0.0090,  0.1330,  0.0860,  0.1102,
         0.1179,  0.1224,  0.0888,  0.1297,  0.0771,  0.0884,  0.0326,  0.0858,
         0.0206,  0.0689,  0.0614,  0.0984,  0.0169,  0.1103,  0.0991,  0.0095,
         0.0089,  0.0154,  0.0479,  0.1393,  0.1268,  0.1315,  0.0024,  0.0264,
         0.0785,  0.1293,  0.1389,  0.0785,  0.1411,  0.1209,  0.1366,  0.1180,
         0.1291,  0.1330,  0.1495,  0.0357,  0.1226,  0.1366,  0.1411,  0.0969,
         0.1538,  0.1560,  0.1620,  0.1396,  0.1533,  0.1414,  0.1495,  0.1512,
         0.1521,  0.1351,  0.1432,  0.1542,  0.1444,  0.1360,  0.1400,  0.1461,
         0.1496,  0.1476,  0.1499,  0.1497,  0.1511,  0.1534,  0.1513,  0.1434,
         0.1483,  0.1615,  0.1504,  0.1560,  0.1517,  0.1354,  0.1509,  0.1578,
         0.1403,  0.1540,  0.1411,  0.1480,  0.1592,  0.1388,  0.1440,  0.1431,
         0.1413,  0.1538,  0.1173,  0.1446,  0.1587,  0.1550,  0.1546,  0.1549,
         0.1498,  0.1577,  0.1398,  0.1461,  0.1412,  0.1518,  0.1413,  0.1522,
         0.1578,  0.1461,  0.1428,  0.1464,  0.1300,  0.1576,  0.1549,  0.1639,
         0.1544,  0.1503,  0.1493,  0.1520,  0.1491,  0.1484,  0.1507,  0.1521,
         0.1392,  0.1539,  0.1550,  0.1510,  0.1540,  0.1463,  0.1601,  0.1176,
         0.1092,  0.1541,  0.1526,  0.1348,  0.1484,  0.1505,  0.1505,  0.1418,
         0.1405,  0.1204,  0.1398,  0.1589,  0.1401,  0.1573,  0.1479,  0.1415,
         0.1528,  0.1373,  0.1457,  0.1524,  0.1528,  0.1500,  0.1243,  0.1538,
         0.1544,  0.1588,  0.1531,  0.1571,  0.1497,  0.1420,  0.1530,  0.1542,
         0.1498,  0.1542,  0.1585,  0.1530,  0.1406,  0.1383,  0.1590,  0.1533,
         0.1369,  0.1089,  0.1451,  0.1592,  0.1485,  0.1507,  0.1588,  0.1425],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.3880, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:41,910 :: INFO :: Epoch 40: loss tensor(200.5867, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0888,  0.0773,  0.1469,  0.1058,  0.0578,  0.1216,  0.0620,  0.0693,
         0.0621,  0.0554,  0.0989,  0.0959,  0.0583,  0.0939,  0.1258,  0.0931,
         0.0663,  0.0656,  0.0703,  0.0948,  0.0866,  0.0454,  0.0904,  0.1139,
         0.1163,  0.0639,  0.0577,  0.1273,  0.1097,  0.0738,  0.1167,  0.0676,
         0.0604,  0.0711,  0.1079,  0.0928,  0.0674,  0.1052,  0.0811,  0.0770,
         0.0742,  0.0546,  0.0911,  0.0913,  0.0850,  0.0747,  0.0680,  0.0845,
         0.0871,  0.0548,  0.1194,  0.0683,  0.0657,  0.0581,  0.1108,  0.1263,
         0.0468,  0.1468,  0.1039,  0.1168,  0.0583,  0.0716,  0.0562,  0.0857,
         0.0995,  0.0673,  0.0542,  0.0893,  0.0692,  0.0948,  0.0812,  0.0657,
         0.0608,  0.0726,  0.0629,  0.1054,  0.0688,  0.0945,  0.0608,  0.0604,
         0.1204,  0.0983,  0.1204,  0.0861,  0.0640,  0.0899,  0.0722,  0.0969,
         0.0677,  0.0632,  0.1076,  0.1082,  0.0833,  0.0486,  0.1071,  0.0641,
         0.0860,  0.0978,  0.1141,  0.1070,  0.0979,  0.0472,  0.1072,  0.0481,
         0.1300,  0.1048,  0.0682,  0.0855,  0.0730,  0.0880,  0.1199,  0.0551,
         0.1241,  0.0906,  0.1239,  0.1116,  0.0650,  0.0716,  0.0495,  0.1240,
         0.1052,  0.1158,  0.0930,  0.0521,  0.0685,  0.0975,  0.0989,  0.1120,
         0.0302,  0.1086,  0.1375,  0.0099,  0.0217,  0.0071,  0.1045,  0.1316,
         0.1640,  0.0008,  0.0849,  0.1272,  0.1009,  0.1473,  0.0888,  0.1437,
         0.1327,  0.0621,  0.0062,  0.1643,  0.1380,  0.1405,  0.1583,  0.0690,
         0.1171, -0.0158,  0.1092,  0.1620,  0.1489,  0.1395,  0.0178,  0.1384,
         0.0393,  0.1130,  0.0131,  0.1358,  0.0613,  0.0127,  0.1429,  0.1295,
         0.1403,  0.0799,  0.0629,  0.0604,  0.1315,  0.1165,  0.0981,  0.1674,
         0.0053,  0.0838,  0.1191,  0.1324,  0.0890,  0.1601,  0.0118,  0.1439,
         0.0756,  0.0533,  0.1507,  0.0475,  0.0492,  0.0962,  0.1567,  0.1435,
         0.0733,  0.1568,  0.0063,  0.0280,  0.1239,  0.0057,  0.0843,  0.1345,
         0.1275,  0.1115,  0.1062,  0.0078,  0.1164,  0.1259,  0.1376,  0.0806,
         0.0238,  0.1167,  0.0928,  0.0429,  0.0086,  0.1466,  0.0822,  0.1124,
         0.1269,  0.1342,  0.0858,  0.1464,  0.0735,  0.0873,  0.0302,  0.0831,
         0.0191,  0.0654,  0.0551,  0.0996,  0.0150,  0.1128,  0.0984,  0.0076,
         0.0077,  0.0137,  0.0410,  0.1542,  0.1378,  0.1436,  0.0018,  0.0244,
         0.0756,  0.1399,  0.1535,  0.0758,  0.1566,  0.1282,  0.1548,  0.1241,
         0.1397,  0.1452,  0.1683,  0.0307,  0.1317,  0.1476,  0.1614,  0.0972,
         0.1752,  0.1763,  0.1857,  0.1522,  0.1726,  0.1577,  0.1703,  0.1719,
         0.1698,  0.1511,  0.1574,  0.1737,  0.1618,  0.1478,  0.1574,  0.1646,
         0.1679,  0.1657,  0.1681,  0.1681,  0.1716,  0.1721,  0.1698,  0.1605,
         0.1669,  0.1844,  0.1676,  0.1778,  0.1708,  0.1483,  0.1715,  0.1783,
         0.1544,  0.1752,  0.1575,  0.1676,  0.1817,  0.1529,  0.1622,  0.1585,
         0.1560,  0.1727,  0.1231,  0.1601,  0.1818,  0.1742,  0.1750,  0.1782,
         0.1689,  0.1785,  0.1523,  0.1651,  0.1573,  0.1729,  0.1574,  0.1721,
         0.1789,  0.1638,  0.1615,  0.1633,  0.1423,  0.1794,  0.1745,  0.1867,
         0.1737,  0.1705,  0.1666,  0.1702,  0.1675,  0.1656,  0.1729,  0.1724,
         0.1547,  0.1738,  0.1743,  0.1711,  0.1754,  0.1663,  0.1813,  0.1235,
         0.1141,  0.1742,  0.1709,  0.1481,  0.1661,  0.1701,  0.1705,  0.1580,
         0.1572,  0.1274,  0.1538,  0.1795,  0.1563,  0.1781,  0.1671,  0.1572,
         0.1734,  0.1545,  0.1628,  0.1703,  0.1755,  0.1690,  0.1333,  0.1726,
         0.1754,  0.1823,  0.1714,  0.1803,  0.1672,  0.1564,  0.1719,  0.1754,
         0.1676,  0.1746,  0.1793,  0.1776,  0.1561,  0.1533,  0.1809,  0.1710,
         0.1500,  0.1137,  0.1616,  0.1831,  0.1656,  0.1714,  0.1798,  0.1578],
       device='cuda:0', requires_grad=True) MLP.norm tensor(7.6221, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:39:41,910 :: INFO :: ----- frontend -----
2023-05-14 18:39:41,910 :: INFO :: Environment 0
2023-05-14 18:39:51,597 :: INFO :: Epoch 5: loss tensor(868.8260, device='cuda:0'), U.norm 13.269248962402344, V.norm 16.90117835998535, MLP.norm 1.543752908706665
2023-05-14 18:39:51,832 :: INFO :: Epoch 10: loss tensor(849.7977, device='cuda:0'), U.norm 10.570664405822754, V.norm 16.195486068725586, MLP.norm 2.9164345264434814
2023-05-14 18:39:52,082 :: INFO :: Epoch 15: loss tensor(820.1113, device='cuda:0'), U.norm 9.00104808807373, V.norm 15.848071098327637, MLP.norm 4.659160614013672
2023-05-14 18:39:52,316 :: INFO :: Epoch 20: loss tensor(785.6570, device='cuda:0'), U.norm 7.988292217254639, V.norm 15.61239242553711, MLP.norm 6.409659385681152
2023-05-14 18:39:52,332 :: INFO :: Environment 1
2023-05-14 18:40:01,832 :: INFO :: Epoch 5: loss tensor(815.6840, device='cuda:0'), U.norm 13.262187957763672, V.norm 16.801706314086914, MLP.norm 1.5330144166946411
2023-05-14 18:40:02,066 :: INFO :: Epoch 10: loss tensor(798.5073, device='cuda:0'), U.norm 10.553956031799316, V.norm 16.05792808532715, MLP.norm 2.8100903034210205
2023-05-14 18:40:02,301 :: INFO :: Epoch 15: loss tensor(772.3824, device='cuda:0'), U.norm 8.972064018249512, V.norm 15.695120811462402, MLP.norm 4.422370433807373
2023-05-14 18:40:02,535 :: INFO :: Epoch 20: loss tensor(741.3544, device='cuda:0'), U.norm 7.945885181427002, V.norm 15.449297904968262, MLP.norm 6.050278663635254
2023-05-14 18:40:02,551 :: INFO :: Environment 2
2023-05-14 18:40:12,098 :: INFO :: Epoch 5: loss tensor(852.8897, device='cuda:0'), U.norm 13.266322135925293, V.norm 16.868412017822266, MLP.norm 1.5064330101013184
2023-05-14 18:40:12,332 :: INFO :: Epoch 10: loss tensor(834.3795, device='cuda:0'), U.norm 10.564473152160645, V.norm 16.151119232177734, MLP.norm 2.824754238128662
2023-05-14 18:40:12,582 :: INFO :: Epoch 15: loss tensor(805.0594, device='cuda:0'), U.norm 8.991634368896484, V.norm 15.79880428314209, MLP.norm 4.522092342376709
2023-05-14 18:40:12,816 :: INFO :: Epoch 20: loss tensor(769.0016, device='cuda:0'), U.norm 7.976443767547607, V.norm 15.559946060180664, MLP.norm 6.236034870147705
2023-05-14 18:40:12,832 :: INFO :: Environment 3
2023-05-14 18:40:22,394 :: INFO :: Epoch 5: loss tensor(849.2927, device='cuda:0'), U.norm 13.268733978271484, V.norm 16.86554527282715, MLP.norm 1.5083585977554321
2023-05-14 18:40:22,629 :: INFO :: Epoch 10: loss tensor(832.1730, device='cuda:0'), U.norm 10.568923950195312, V.norm 16.14788055419922, MLP.norm 2.802579164505005
2023-05-14 18:40:22,863 :: INFO :: Epoch 15: loss tensor(804.2995, device='cuda:0'), U.norm 8.997880935668945, V.norm 15.794621467590332, MLP.norm 4.479874134063721
2023-05-14 18:40:23,097 :: INFO :: Epoch 20: loss tensor(770.8347, device='cuda:0'), U.norm 7.984827995300293, V.norm 15.555608749389648, MLP.norm 6.18445348739624
2023-05-14 18:40:23,097 :: INFO :: Environment 4
2023-05-14 18:40:32,598 :: INFO :: Epoch 5: loss tensor(879.6739, device='cuda:0'), U.norm 13.273152351379395, V.norm 16.923303604125977, MLP.norm 1.5653507709503174
2023-05-14 18:40:32,832 :: INFO :: Epoch 10: loss tensor(859.9504, device='cuda:0'), U.norm 10.578450202941895, V.norm 16.228775024414062, MLP.norm 2.9350056648254395
2023-05-14 18:40:33,066 :: INFO :: Epoch 15: loss tensor(829.4108, device='cuda:0'), U.norm 9.012568473815918, V.norm 15.882644653320312, MLP.norm 4.674463748931885
2023-05-14 18:40:33,316 :: INFO :: Epoch 20: loss tensor(792.8263, device='cuda:0'), U.norm 8.004118919372559, V.norm 15.648268699645996, MLP.norm 6.417145252227783
2023-05-14 18:40:33,316 :: INFO :: Environment 5
2023-05-14 18:40:42,676 :: INFO :: Epoch 5: loss tensor(845.7803, device='cuda:0'), U.norm 13.267839431762695, V.norm 16.864459991455078, MLP.norm 1.5342333316802979
2023-05-14 18:40:42,910 :: INFO :: Epoch 10: loss tensor(827.8239, device='cuda:0'), U.norm 10.56678295135498, V.norm 16.144514083862305, MLP.norm 2.8570444583892822
2023-05-14 18:40:43,145 :: INFO :: Epoch 15: loss tensor(799.9927, device='cuda:0'), U.norm 8.993658065795898, V.norm 15.790337562561035, MLP.norm 4.542102813720703
2023-05-14 18:40:43,379 :: INFO :: Epoch 20: loss tensor(767.2351, device='cuda:0'), U.norm 7.977240562438965, V.norm 15.551026344299316, MLP.norm 6.237143516540527
2023-05-14 18:40:43,379 :: INFO :: Environment 6
2023-05-14 18:40:52,926 :: INFO :: Epoch 5: loss tensor(795.5427, device='cuda:0'), U.norm 13.258285522460938, V.norm 16.77216148376465, MLP.norm 1.5208404064178467
2023-05-14 18:40:53,160 :: INFO :: Epoch 10: loss tensor(776.3954, device='cuda:0'), U.norm 10.546452522277832, V.norm 16.010597229003906, MLP.norm 2.8270914554595947
2023-05-14 18:40:53,379 :: INFO :: Epoch 15: loss tensor(746.7942, device='cuda:0'), U.norm 8.961080551147461, V.norm 15.634468078613281, MLP.norm 4.456185817718506
2023-05-14 18:40:53,613 :: INFO :: Epoch 20: loss tensor(712.8627, device='cuda:0'), U.norm 7.930950164794922, V.norm 15.380010604858398, MLP.norm 6.074059009552002
2023-05-14 18:40:53,628 :: INFO :: Environment 7
2023-05-14 18:41:03,160 :: INFO :: Epoch 5: loss tensor(778.9897, device='cuda:0'), U.norm 13.25659465789795, V.norm 16.740360260009766, MLP.norm 1.4986275434494019
2023-05-14 18:41:03,394 :: INFO :: Epoch 10: loss tensor(763.1631, device='cuda:0'), U.norm 10.541733741760254, V.norm 15.96599006652832, MLP.norm 2.7084314823150635
2023-05-14 18:41:03,629 :: INFO :: Epoch 15: loss tensor(738.5403, device='cuda:0'), U.norm 8.952631950378418, V.norm 15.589587211608887, MLP.norm 4.246389389038086
2023-05-14 18:41:03,863 :: INFO :: Epoch 20: loss tensor(708.8947, device='cuda:0'), U.norm 7.919128894805908, V.norm 15.337140083312988, MLP.norm 5.791946887969971
2023-05-14 18:41:03,879 :: INFO :: Environment 8
2023-05-14 18:41:13,222 :: INFO :: Epoch 5: loss tensor(860.8089, device='cuda:0'), U.norm 13.269658088684082, V.norm 16.88032341003418, MLP.norm 1.5288355350494385
2023-05-14 18:41:13,457 :: INFO :: Epoch 10: loss tensor(841.7737, device='cuda:0'), U.norm 10.571049690246582, V.norm 16.166006088256836, MLP.norm 2.8618876934051514
2023-05-14 18:41:13,676 :: INFO :: Epoch 15: loss tensor(811.5773, device='cuda:0'), U.norm 9.000880241394043, V.norm 15.813982963562012, MLP.norm 4.556501865386963
2023-05-14 18:41:13,910 :: INFO :: Epoch 20: loss tensor(775.1007, device='cuda:0'), U.norm 7.988139629364014, V.norm 15.57462215423584, MLP.norm 6.259405612945557
2023-05-14 18:41:13,925 :: INFO :: Environment 9
2023-05-14 18:41:23,472 :: INFO :: Epoch 5: loss tensor(920.8940, device='cuda:0'), U.norm 13.279986381530762, V.norm 16.97808074951172, MLP.norm 1.5956311225891113
2023-05-14 18:41:23,722 :: INFO :: Epoch 10: loss tensor(898.7396, device='cuda:0'), U.norm 10.594837188720703, V.norm 16.308998107910156, MLP.norm 3.066791296005249
2023-05-14 18:41:23,957 :: INFO :: Epoch 15: loss tensor(864.6385, device='cuda:0'), U.norm 9.039863586425781, V.norm 15.977579116821289, MLP.norm 4.9015703201293945
2023-05-14 18:41:24,191 :: INFO :: Epoch 20: loss tensor(825.2922, device='cuda:0'), U.norm 8.041982650756836, V.norm 15.751955032348633, MLP.norm 6.734873294830322
2023-05-14 18:41:24,207 :: INFO :: Ite = 1, Delta = 986
2023-05-14 18:41:24,207 :: INFO :: ----- backend -----
2023-05-14 18:41:28,176 :: INFO :: Epoch 5: loss tensor(214.5067, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0112,  0.0132,  0.0184,  0.0128,  0.0119,  0.0153,  0.0107,  0.0130,
         0.0055,  0.0188,  0.0169,  0.0151,  0.0079,  0.0120,  0.0196,  0.0190,
         0.0197,  0.0160,  0.0186,  0.0158,  0.0095,  0.0189,  0.0189,  0.0071,
         0.0195,  0.0147,  0.0133,  0.0086,  0.0195,  0.0179,  0.0189,  0.0189,
         0.0091,  0.0188,  0.0021,  0.0041,  0.0161,  0.0190,  0.0105,  0.0191,
         0.0122,  0.0053,  0.0198,  0.0140,  0.0125,  0.0197,  0.0148,  0.0013,
         0.0191,  0.0129,  0.0193,  0.0092,  0.0186,  0.0118,  0.0193,  0.0163,
         0.0113,  0.0157,  0.0060,  0.0072,  0.0190,  0.0162,  0.0064,  0.0155,
         0.0139,  0.0045,  0.0194,  0.0102,  0.0194,  0.0133,  0.0192,  0.0059,
         0.0191,  0.0067,  0.0153,  0.0062,  0.0183,  0.0086,  0.0117,  0.0129,
         0.0192,  0.0108,  0.0191,  0.0072,  0.0032,  0.0192,  0.0194,  0.0135,
         0.0071,  0.0029,  0.0107,  0.0146,  0.0193,  0.0178,  0.0101,  0.0195,
         0.0196,  0.0192,  0.0192,  0.0164,  0.0059,  0.0147,  0.0141, -0.0004,
         0.0186,  0.0159,  0.0156,  0.0192,  0.0121,  0.0165,  0.0190,  0.0097,
         0.0195,  0.0181,  0.0116,  0.0104,  0.0122,  0.0157,  0.0117,  0.0127,
         0.0194,  0.0155,  0.0078,  0.0103,  0.0036,  0.0198,  0.0074,  0.0189,
         0.0153,  0.0197,  0.0202,  0.0191,  0.0188,  0.0105,  0.0073,  0.0044,
         0.0098,  0.0100,  0.0090,  0.0076,  0.0197,  0.0158,  0.0084,  0.0197,
         0.0194,  0.0198,  0.0085,  0.0162,  0.0202,  0.0125,  0.0171,  0.0149,
         0.0201, -0.0141,  0.0108,  0.0145,  0.0160, -0.0006,  0.0180,  0.0007,
         0.0192, -0.0120,  0.0147,  0.0201,  0.0195,  0.0125,  0.0139,  0.0073,
         0.0149,  0.0115,  0.0154,  0.0143, -0.0035,  0.0167,  0.0198,  0.0194,
         0.0084,  0.0170,  0.0129,  0.0194,  0.0113,  0.0167,  0.0143,  0.0016,
         0.0194,  0.0168,  0.0123,  0.0129,  0.0138, -0.0115,  0.0084,  0.0160,
         0.0199,  0.0141,  0.0103,  0.0152,  0.0160,  0.0117,  0.0199,  0.0116,
         0.0200,  0.0110,  0.0201,  0.0143,  0.0201,  0.0130,  0.0153, -0.0139,
         0.0164,  0.0059,  0.0197,  0.0196,  0.0102,  0.0140,  0.0192,  0.0187,
         0.0075,  0.0055,  0.0201,  0.0085,  0.0125,  0.0090,  0.0183,  0.0199,
         0.0153,  0.0109,  0.0095,  0.0151,  0.0091,  0.0199,  0.0199,  0.0166,
         0.0191,  0.0137,  0.0198,  0.0118,  0.0109,  0.0165,  0.0076,  0.0147,
         0.0064,  0.0157,  0.0130,  0.0199,  0.0167,  0.0200,  0.0078,  0.0195,
         0.0172,  0.0172,  0.0127,  0.0191,  0.0104,  0.0197,  0.0030,  0.0101,
         0.0090,  0.0197,  0.0138,  0.0200,  0.0199,  0.0129,  0.0083,  0.0108,
         0.0201,  0.0052,  0.0199,  0.0199,  0.0126,  0.0200,  0.0107,  0.0100,
         0.0198,  0.0164,  0.0161,  0.0168,  0.0085,  0.0198,  0.0202,  0.0124,
         0.0135,  0.0157,  0.0198,  0.0133,  0.0123,  0.0166,  0.0108,  0.0188,
         0.0200,  0.0132,  0.0120,  0.0114,  0.0125,  0.0168,  0.0113,  0.0201,
         0.0173,  0.0174,  0.0142,  0.0197,  0.0190,  0.0201,  0.0173,  0.0060,
         0.0152,  0.0158,  0.0174,  0.0111,  0.0129,  0.0109,  0.0119,  0.0150,
         0.0195,  0.0161,  0.0088,  0.0170,  0.0113,  0.0162,  0.0198,  0.0199,
         0.0200,  0.0150,  0.0200,  0.0197,  0.0146,  0.0203,  0.0035,  0.0140,
         0.0112,  0.0188,  0.0199,  0.0124,  0.0112,  0.0068,  0.0200,  0.0145,
         0.0129,  0.0166,  0.0171,  0.0132,  0.0165,  0.0129,  0.0120,  0.0130,
         0.0101,  0.0162,  0.0199,  0.0198,  0.0114,  0.0198,  0.0112,  0.0144,
         0.0147,  0.0075,  0.0143,  0.0172,  0.0062,  0.0169,  0.0133,  0.0191,
         0.0120,  0.0116,  0.0199,  0.0066,  0.0200,  0.0202,  0.0196,  0.0164,
         0.0197,  0.0149,  0.0164,  0.0026,  0.0153,  0.0115,  0.0172,  0.0204,
         0.0161,  0.0149,  0.0160,  0.0132,  0.0201,  0.0100,  0.0186,  0.0166],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.7022, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:32,113 :: INFO :: Epoch 10: loss tensor(209.4690, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0326,  0.0340,  0.0404,  0.0345,  0.0324,  0.0380,  0.0320,  0.0342,
         0.0267,  0.0376,  0.0383,  0.0369,  0.0295,  0.0341,  0.0414,  0.0402,
         0.0396,  0.0357,  0.0382,  0.0367,  0.0305,  0.0373,  0.0396,  0.0293,
         0.0409,  0.0356,  0.0338,  0.0309,  0.0406,  0.0379,  0.0404,  0.0382,
         0.0297,  0.0391,  0.0240,  0.0258,  0.0360,  0.0406,  0.0321,  0.0399,
         0.0320,  0.0260,  0.0409,  0.0358,  0.0343,  0.0402,  0.0354,  0.0232,
         0.0401,  0.0325,  0.0404,  0.0299,  0.0390,  0.0321,  0.0409,  0.0383,
         0.0313,  0.0381,  0.0279,  0.0299,  0.0384,  0.0367,  0.0279,  0.0368,
         0.0352,  0.0257,  0.0385,  0.0320,  0.0395,  0.0348,  0.0393,  0.0269,
         0.0385,  0.0285,  0.0352,  0.0284,  0.0381,  0.0306,  0.0321,  0.0330,
         0.0407,  0.0330,  0.0413,  0.0288,  0.0245,  0.0397,  0.0393,  0.0351,
         0.0284,  0.0238,  0.0326,  0.0363,  0.0400,  0.0368,  0.0321,  0.0391,
         0.0398,  0.0397,  0.0409,  0.0382,  0.0280,  0.0331,  0.0357,  0.0202,
         0.0398,  0.0374,  0.0359,  0.0398,  0.0329,  0.0378,  0.0408,  0.0305,
         0.0411,  0.0397,  0.0335,  0.0320,  0.0326,  0.0368,  0.0313,  0.0343,
         0.0406,  0.0372,  0.0298,  0.0304,  0.0247,  0.0416,  0.0294,  0.0401,
         0.0355,  0.0421,  0.0426,  0.0342,  0.0361,  0.0137,  0.0302,  0.0275,
         0.0332,  0.0058,  0.0317,  0.0308,  0.0415,  0.0389,  0.0309,  0.0419,
         0.0421,  0.0408,  0.0208,  0.0394,  0.0426,  0.0354,  0.0401,  0.0362,
         0.0422, -0.0212,  0.0335,  0.0378,  0.0389,  0.0216,  0.0328,  0.0235,
         0.0392,  0.0029,  0.0199,  0.0426,  0.0407,  0.0177,  0.0369,  0.0304,
         0.0378,  0.0340,  0.0360,  0.0361,  0.0183,  0.0393,  0.0418,  0.0425,
         0.0093,  0.0393,  0.0358,  0.0418,  0.0337,  0.0399,  0.0271,  0.0243,
         0.0412,  0.0376,  0.0353,  0.0340,  0.0342, -0.0002,  0.0316,  0.0390,
         0.0411,  0.0372,  0.0280,  0.0351,  0.0381,  0.0102,  0.0416,  0.0346,
         0.0424,  0.0335,  0.0418,  0.0168,  0.0423,  0.0355,  0.0382, -0.0063,
         0.0365,  0.0291,  0.0410,  0.0400,  0.0135,  0.0370,  0.0412,  0.0408,
         0.0305,  0.0287,  0.0417,  0.0311,  0.0345,  0.0318,  0.0362,  0.0410,
         0.0282,  0.0322,  0.0316,  0.0374,  0.0270,  0.0417,  0.0419,  0.0308,
         0.0319,  0.0231,  0.0407,  0.0351,  0.0342,  0.0393,  0.0049,  0.0292,
         0.0290,  0.0388,  0.0361,  0.0409,  0.0398,  0.0424,  0.0309,  0.0421,
         0.0400,  0.0401,  0.0360,  0.0382,  0.0335,  0.0425,  0.0264,  0.0328,
         0.0329,  0.0434,  0.0376,  0.0433,  0.0431,  0.0365,  0.0321,  0.0346,
         0.0431,  0.0283,  0.0433,  0.0432,  0.0360,  0.0434,  0.0343,  0.0338,
         0.0430,  0.0400,  0.0397,  0.0404,  0.0320,  0.0433,  0.0434,  0.0360,
         0.0372,  0.0396,  0.0432,  0.0372,  0.0358,  0.0399,  0.0345,  0.0425,
         0.0431,  0.0368,  0.0355,  0.0351,  0.0365,  0.0401,  0.0349,  0.0432,
         0.0407,  0.0408,  0.0370,  0.0430,  0.0425,  0.0435,  0.0403,  0.0300,
         0.0385,  0.0395,  0.0407,  0.0343,  0.0367,  0.0345,  0.0353,  0.0384,
         0.0430,  0.0396,  0.0324,  0.0405,  0.0346,  0.0401,  0.0432,  0.0434,
         0.0433,  0.0383,  0.0433,  0.0431,  0.0382,  0.0434,  0.0275,  0.0375,
         0.0346,  0.0425,  0.0433,  0.0362,  0.0350,  0.0306,  0.0434,  0.0376,
         0.0360,  0.0401,  0.0399,  0.0367,  0.0401,  0.0366,  0.0358,  0.0365,
         0.0338,  0.0388,  0.0431,  0.0433,  0.0350,  0.0432,  0.0348,  0.0377,
         0.0383,  0.0308,  0.0379,  0.0407,  0.0301,  0.0403,  0.0365,  0.0427,
         0.0357,  0.0355,  0.0431,  0.0304,  0.0433,  0.0434,  0.0431,  0.0397,
         0.0428,  0.0386,  0.0400,  0.0263,  0.0388,  0.0351,  0.0405,  0.0440,
         0.0394,  0.0366,  0.0396,  0.0369,  0.0433,  0.0338,  0.0422,  0.0396],
       device='cuda:0', requires_grad=True) MLP.norm tensor(4.2771, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:36,066 :: INFO :: Epoch 15: loss tensor(210.3786, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0517,  0.0513,  0.0627,  0.0552,  0.0483,  0.0603,  0.0481,  0.0524,
         0.0440,  0.0507,  0.0569,  0.0566,  0.0468,  0.0538,  0.0625,  0.0600,
         0.0546,  0.0506,  0.0533,  0.0553,  0.0497,  0.0478,  0.0572,  0.0507,
         0.0609,  0.0525,  0.0484,  0.0531,  0.0599,  0.0543,  0.0605,  0.0530,
         0.0454,  0.0551,  0.0455,  0.0459,  0.0517,  0.0616,  0.0517,  0.0560,
         0.0483,  0.0417,  0.0596,  0.0556,  0.0537,  0.0567,  0.0520,  0.0434,
         0.0584,  0.0462,  0.0609,  0.0472,  0.0553,  0.0460,  0.0613,  0.0595,
         0.0447,  0.0611,  0.0487,  0.0517,  0.0521,  0.0531,  0.0450,  0.0560,
         0.0549,  0.0436,  0.0512,  0.0513,  0.0557,  0.0547,  0.0562,  0.0440,
         0.0532,  0.0475,  0.0505,  0.0497,  0.0537,  0.0508,  0.0474,  0.0487,
         0.0611,  0.0534,  0.0626,  0.0479,  0.0426,  0.0577,  0.0552,  0.0546,
         0.0464,  0.0415,  0.0540,  0.0570,  0.0574,  0.0490,  0.0530,  0.0541,
         0.0575,  0.0585,  0.0616,  0.0583,  0.0486,  0.0434,  0.0563,  0.0374,
         0.0603,  0.0574,  0.0525,  0.0572,  0.0498,  0.0571,  0.0616,  0.0462,
         0.0617,  0.0588,  0.0552,  0.0530,  0.0485,  0.0542,  0.0440,  0.0556,
         0.0601,  0.0585,  0.0496,  0.0443,  0.0434,  0.0619,  0.0496,  0.0601,
         0.0492,  0.0632,  0.0647,  0.0334,  0.0411,  0.0111,  0.0523,  0.0512,
         0.0578,  0.0031,  0.0529,  0.0537,  0.0620,  0.0620,  0.0522,  0.0644,
         0.0644,  0.0585,  0.0166,  0.0636,  0.0647,  0.0584,  0.0634,  0.0555,
         0.0633, -0.0202,  0.0550,  0.0618,  0.0621,  0.0457,  0.0336,  0.0474,
         0.0533,  0.0252,  0.0196,  0.0645,  0.0577,  0.0171,  0.0599,  0.0536,
         0.0607,  0.0545,  0.0531,  0.0546,  0.0424,  0.0612,  0.0623,  0.0664,
         0.0077,  0.0596,  0.0577,  0.0634,  0.0545,  0.0634,  0.0254,  0.0484,
         0.0603,  0.0548,  0.0588,  0.0506,  0.0494,  0.0212,  0.0563,  0.0619,
         0.0593,  0.0607,  0.0299,  0.0475,  0.0606,  0.0074,  0.0609,  0.0578,
         0.0643,  0.0561,  0.0620,  0.0135,  0.0633,  0.0586,  0.0609,  0.0123,
         0.0478,  0.0519,  0.0613,  0.0553,  0.0113,  0.0601,  0.0606,  0.0625,
         0.0536,  0.0522,  0.0616,  0.0546,  0.0546,  0.0531,  0.0462,  0.0600,
         0.0290,  0.0512,  0.0503,  0.0593,  0.0325,  0.0624,  0.0620,  0.0272,
         0.0288,  0.0217,  0.0558,  0.0593,  0.0573,  0.0621,  0.0040,  0.0332,
         0.0498,  0.0617,  0.0600,  0.0587,  0.0632,  0.0641,  0.0550,  0.0640,
         0.0626,  0.0628,  0.0610,  0.0503,  0.0570,  0.0654,  0.0513,  0.0542,
         0.0588,  0.0688,  0.0635,  0.0674,  0.0674,  0.0612,  0.0574,  0.0599,
         0.0677,  0.0531,  0.0671,  0.0675,  0.0604,  0.0667,  0.0588,  0.0589,
         0.0675,  0.0645,  0.0644,  0.0652,  0.0573,  0.0682,  0.0675,  0.0606,
         0.0620,  0.0656,  0.0676,  0.0628,  0.0613,  0.0638,  0.0596,  0.0677,
         0.0667,  0.0620,  0.0601,  0.0601,  0.0625,  0.0642,  0.0596,  0.0671,
         0.0649,  0.0654,  0.0601,  0.0671,  0.0679,  0.0685,  0.0651,  0.0564,
         0.0632,  0.0652,  0.0646,  0.0589,  0.0615,  0.0601,  0.0600,  0.0636,
         0.0683,  0.0643,  0.0577,  0.0647,  0.0583,  0.0659,  0.0677,  0.0687,
         0.0675,  0.0631,  0.0675,  0.0675,  0.0630,  0.0672,  0.0539,  0.0625,
         0.0591,  0.0676,  0.0680,  0.0614,  0.0603,  0.0558,  0.0686,  0.0603,
         0.0586,  0.0650,  0.0642,  0.0608,  0.0649,  0.0615,  0.0611,  0.0615,
         0.0587,  0.0611,  0.0670,  0.0681,  0.0597,  0.0677,  0.0597,  0.0618,
         0.0632,  0.0552,  0.0631,  0.0653,  0.0562,  0.0650,  0.0602,  0.0674,
         0.0613,  0.0616,  0.0680,  0.0566,  0.0678,  0.0674,  0.0677,  0.0646,
         0.0669,  0.0635,  0.0650,  0.0527,  0.0632,  0.0598,  0.0658,  0.0690,
         0.0633,  0.0582,  0.0643,  0.0628,  0.0672,  0.0591,  0.0672,  0.0639],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.4054, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:40,035 :: INFO :: Epoch 20: loss tensor(205.0597, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0672,  0.0637,  0.0834,  0.0726,  0.0567,  0.0802,  0.0575,  0.0639,
         0.0551,  0.0573,  0.0719,  0.0712,  0.0569,  0.0693,  0.0807,  0.0755,
         0.0632,  0.0592,  0.0620,  0.0698,  0.0639,  0.0512,  0.0701,  0.0696,
         0.0775,  0.0611,  0.0561,  0.0727,  0.0758,  0.0649,  0.0773,  0.0619,
         0.0546,  0.0655,  0.0637,  0.0623,  0.0612,  0.0776,  0.0655,  0.0669,
         0.0597,  0.0501,  0.0745,  0.0704,  0.0681,  0.0677,  0.0625,  0.0603,
         0.0720,  0.0531,  0.0785,  0.0585,  0.0644,  0.0519,  0.0787,  0.0783,
         0.0505,  0.0822,  0.0661,  0.0711,  0.0583,  0.0632,  0.0555,  0.0700,
         0.0706,  0.0554,  0.0567,  0.0671,  0.0659,  0.0707,  0.0682,  0.0560,
         0.0611,  0.0599,  0.0587,  0.0678,  0.0639,  0.0673,  0.0564,  0.0573,
         0.0790,  0.0707,  0.0801,  0.0629,  0.0552,  0.0708,  0.0660,  0.0703,
         0.0580,  0.0532,  0.0715,  0.0741,  0.0695,  0.0539,  0.0709,  0.0631,
         0.0706,  0.0731,  0.0786,  0.0750,  0.0649,  0.0469,  0.0736,  0.0463,
         0.0785,  0.0739,  0.0635,  0.0701,  0.0605,  0.0713,  0.0798,  0.0543,
         0.0799,  0.0738,  0.0741,  0.0706,  0.0588,  0.0653,  0.0482,  0.0743,
         0.0760,  0.0762,  0.0652,  0.0513,  0.0552,  0.0781,  0.0670,  0.0768,
         0.0513,  0.0808,  0.0848,  0.0241,  0.0361,  0.0098,  0.0708,  0.0730,
         0.0821,  0.0022,  0.0693,  0.0745,  0.0787,  0.0837,  0.0692,  0.0858,
         0.0845,  0.0684,  0.0117,  0.0872,  0.0853,  0.0802,  0.0858,  0.0681,
         0.0815, -0.0131,  0.0734,  0.0852,  0.0840,  0.0692,  0.0262,  0.0702,
         0.0570,  0.0476,  0.0168,  0.0846,  0.0679,  0.0151,  0.0813,  0.0747,
         0.0819,  0.0699,  0.0636,  0.0660,  0.0655,  0.0801,  0.0788,  0.0898,
         0.0073,  0.0743,  0.0769,  0.0828,  0.0708,  0.0862,  0.0179,  0.0717,
         0.0735,  0.0637,  0.0814,  0.0596,  0.0573,  0.0435,  0.0801,  0.0832,
         0.0716,  0.0834,  0.0206,  0.0480,  0.0806,  0.0063,  0.0751,  0.0790,
         0.0836,  0.0756,  0.0785,  0.0117,  0.0812,  0.0792,  0.0818,  0.0329,
         0.0462,  0.0720,  0.0769,  0.0612,  0.0112,  0.0817,  0.0744,  0.0806,
         0.0743,  0.0742,  0.0760,  0.0774,  0.0691,  0.0699,  0.0455,  0.0740,
         0.0264,  0.0633,  0.0613,  0.0767,  0.0278,  0.0804,  0.0779,  0.0169,
         0.0189,  0.0196,  0.0606,  0.0827,  0.0787,  0.0833,  0.0027,  0.0301,
         0.0652,  0.0828,  0.0829,  0.0715,  0.0854,  0.0833,  0.0784,  0.0832,
         0.0834,  0.0840,  0.0854,  0.0504,  0.0784,  0.0871,  0.0763,  0.0717,
         0.0848,  0.0935,  0.0899,  0.0903,  0.0913,  0.0854,  0.0826,  0.0849,
         0.0918,  0.0769,  0.0899,  0.0915,  0.0841,  0.0887,  0.0827,  0.0837,
         0.0915,  0.0884,  0.0886,  0.0897,  0.0826,  0.0927,  0.0911,  0.0845,
         0.0864,  0.0918,  0.0914,  0.0882,  0.0861,  0.0861,  0.0847,  0.0929,
         0.0884,  0.0871,  0.0840,  0.0846,  0.0884,  0.0873,  0.0838,  0.0898,
         0.0880,  0.0894,  0.0806,  0.0902,  0.0935,  0.0931,  0.0900,  0.0827,
         0.0875,  0.0906,  0.0872,  0.0827,  0.0853,  0.0856,  0.0833,  0.0886,
         0.0937,  0.0881,  0.0825,  0.0882,  0.0804,  0.0916,  0.0920,  0.0940,
         0.0914,  0.0877,  0.0909,  0.0915,  0.0875,  0.0902,  0.0803,  0.0874,
         0.0824,  0.0926,  0.0924,  0.0865,  0.0855,  0.0809,  0.0938,  0.0807,
         0.0776,  0.0897,  0.0876,  0.0836,  0.0891,  0.0862,  0.0862,  0.0857,
         0.0825,  0.0812,  0.0896,  0.0927,  0.0828,  0.0923,  0.0842,  0.0851,
         0.0881,  0.0787,  0.0878,  0.0895,  0.0825,  0.0889,  0.0820,  0.0919,
         0.0870,  0.0878,  0.0923,  0.0831,  0.0913,  0.0902,  0.0918,  0.0894,
         0.0906,  0.0884,  0.0900,  0.0793,  0.0866,  0.0836,  0.0914,  0.0938,
         0.0858,  0.0781,  0.0882,  0.0890,  0.0907,  0.0842,  0.0923,  0.0873],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.1925, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:44,035 :: INFO :: Epoch 25: loss tensor(204.6002, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0769,  0.0704,  0.1020,  0.0849,  0.0591,  0.0969,  0.0602,  0.0682,
         0.0588,  0.0579,  0.0823,  0.0807,  0.0600,  0.0793,  0.0956,  0.0856,
         0.0655,  0.0622,  0.0662,  0.0795,  0.0731,  0.0487,  0.0778,  0.0849,
         0.0905,  0.0637,  0.0574,  0.0894,  0.0877,  0.0697,  0.0899,  0.0645,
         0.0577,  0.0695,  0.0776,  0.0744,  0.0639,  0.0893,  0.0729,  0.0719,
         0.0648,  0.0520,  0.0842,  0.0802,  0.0771,  0.0723,  0.0665,  0.0715,
         0.0796,  0.0532,  0.0927,  0.0633,  0.0663,  0.0528,  0.0922,  0.0941,
         0.0504,  0.1013,  0.0791,  0.0871,  0.0592,  0.0677,  0.0585,  0.0782,
         0.0816,  0.0609,  0.0569,  0.0770,  0.0689,  0.0817,  0.0743,  0.0616,
         0.0639,  0.0663,  0.0613,  0.0817,  0.0685,  0.0791,  0.0587,  0.0601,
         0.0934,  0.0833,  0.0935,  0.0717,  0.0612,  0.0791,  0.0708,  0.0804,
         0.0628,  0.0586,  0.0848,  0.0870,  0.0762,  0.0530,  0.0845,  0.0655,
         0.0778,  0.0837,  0.0918,  0.0881,  0.0766,  0.0464,  0.0865,  0.0494,
         0.0941,  0.0857,  0.0674,  0.0777,  0.0648,  0.0796,  0.0950,  0.0565,
         0.0942,  0.0826,  0.0893,  0.0843,  0.0624,  0.0696,  0.0482,  0.0896,
         0.0870,  0.0903,  0.0761,  0.0516,  0.0618,  0.0896,  0.0795,  0.0890,
         0.0452,  0.0937,  0.1024,  0.0172,  0.0292,  0.0088,  0.0850,  0.0924,
         0.1055,  0.0014,  0.0800,  0.0926,  0.0896,  0.1037,  0.0806,  0.1048,
         0.1014,  0.0707,  0.0096,  0.1094,  0.1032,  0.0993,  0.1069,  0.0735,
         0.0960, -0.0015,  0.0876,  0.1078,  0.1040,  0.0916,  0.0200,  0.0907,
         0.0530,  0.0674,  0.0161,  0.1020,  0.0704,  0.0134,  0.1006,  0.0930,
         0.1008,  0.0791,  0.0673,  0.0695,  0.0869,  0.0950,  0.0901,  0.1123,
         0.0059,  0.0826,  0.0925,  0.0989,  0.0816,  0.1077,  0.0146,  0.0939,
         0.0798,  0.0651,  0.1027,  0.0596,  0.0575,  0.0650,  0.1025,  0.1023,
         0.0771,  0.1051,  0.0122,  0.0424,  0.0969,  0.0051,  0.0833,  0.0974,
         0.0997,  0.0904,  0.0903,  0.0096,  0.0948,  0.0963,  0.1004,  0.0520,
         0.0378,  0.0885,  0.0867,  0.0590,  0.0101,  0.1015,  0.0820,  0.0942,
         0.0921,  0.0936,  0.0845,  0.0994,  0.0766,  0.0810,  0.0405,  0.0821,
         0.0232,  0.0679,  0.0637,  0.0895,  0.0219,  0.0942,  0.0890,  0.0119,
         0.0122,  0.0171,  0.0568,  0.1043,  0.0977,  0.1023,  0.0028,  0.0271,
         0.0742,  0.1015,  0.1041,  0.0779,  0.1063,  0.0997,  0.1012,  0.0991,
         0.1019,  0.1030,  0.1087,  0.0442,  0.0969,  0.1063,  0.1007,  0.0841,
         0.1106,  0.1171,  0.1161,  0.1105,  0.1145,  0.1090,  0.1068,  0.1089,
         0.1147,  0.0995,  0.1103,  0.1144,  0.1063,  0.1083,  0.1047,  0.1073,
         0.1142,  0.1108,  0.1115,  0.1129,  0.1073,  0.1160,  0.1136,  0.1067,
         0.1093,  0.1178,  0.1139,  0.1129,  0.1098,  0.1062,  0.1089,  0.1172,
         0.1080,  0.1116,  0.1068,  0.1078,  0.1142,  0.1086,  0.1064,  0.1110,
         0.1094,  0.1123,  0.0963,  0.1115,  0.1190,  0.1167,  0.1145,  0.1088,
         0.1109,  0.1152,  0.1074,  0.1058,  0.1076,  0.1103,  0.1048,  0.1130,
         0.1182,  0.1106,  0.1062,  0.1102,  0.1001,  0.1169,  0.1153,  0.1193,
         0.1143,  0.1115,  0.1125,  0.1143,  0.1114,  0.1120,  0.1063,  0.1118,
         0.1037,  0.1166,  0.1159,  0.1104,  0.1099,  0.1049,  0.1186,  0.0979,
         0.0926,  0.1135,  0.1100,  0.1040,  0.1121,  0.1098,  0.1105,  0.1086,
         0.1045,  0.0974,  0.1099,  0.1165,  0.1045,  0.1162,  0.1071,  0.1067,
         0.1117,  0.1005,  0.1111,  0.1120,  0.1083,  0.1120,  0.1009,  0.1153,
         0.1120,  0.1138,  0.1155,  0.1094,  0.1137,  0.1112,  0.1147,  0.1137,
         0.1127,  0.1122,  0.1141,  0.1060,  0.1081,  0.1049,  0.1168,  0.1175,
         0.1062,  0.0940,  0.1103,  0.1152,  0.1128,  0.1085,  0.1166,  0.1083],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.7578, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:48,019 :: INFO :: Epoch 30: loss tensor(203.6617, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0827, 0.0744, 0.1189, 0.0938, 0.0601, 0.1094, 0.0593, 0.0699, 0.0597,
        0.0580, 0.0889, 0.0876, 0.0608, 0.0866, 0.1083, 0.0929, 0.0660, 0.0638,
        0.0673, 0.0862, 0.0794, 0.0454, 0.0819, 0.0966, 0.1010, 0.0639, 0.0581,
        0.1039, 0.0961, 0.0720, 0.0996, 0.0654, 0.0576, 0.0699, 0.0894, 0.0839,
        0.0646, 0.0973, 0.0774, 0.0734, 0.0673, 0.0525, 0.0894, 0.0863, 0.0831,
        0.0734, 0.0686, 0.0781, 0.0836, 0.0521, 0.1045, 0.0652, 0.0665, 0.0543,
        0.1016, 0.1071, 0.0492, 0.1187, 0.0881, 0.0992, 0.0594, 0.0693, 0.0580,
        0.0829, 0.0888, 0.0628, 0.0558, 0.0823, 0.0692, 0.0891, 0.0777, 0.0636,
        0.0638, 0.0689, 0.0621, 0.0919, 0.0698, 0.0871, 0.0591, 0.0603, 0.1053,
        0.0910, 0.1042, 0.0772, 0.0642, 0.0835, 0.0724, 0.0871, 0.0649, 0.0617,
        0.0957, 0.0968, 0.0794, 0.0512, 0.0942, 0.0651, 0.0816, 0.0912, 0.1020,
        0.0983, 0.0844, 0.0470, 0.0970, 0.0509, 0.1070, 0.0939, 0.0682, 0.0818,
        0.0665, 0.0847, 0.1067, 0.0569, 0.1048, 0.0871, 0.1020, 0.0955, 0.0634,
        0.0708, 0.0477, 0.1024, 0.0950, 0.1019, 0.0824, 0.0508, 0.0656, 0.0975,
        0.0878, 0.0980, 0.0376, 0.1030, 0.1181, 0.0147, 0.0253, 0.0079, 0.0955,
        0.1099, 0.1279, 0.0010, 0.0858, 0.1084, 0.0956, 0.1224, 0.0868, 0.1208,
        0.1152, 0.0687, 0.0088, 0.1301, 0.1187, 0.1159, 0.1272, 0.0734, 0.1074,
        0.0123, 0.0984, 0.1292, 0.1228, 0.1136, 0.0182, 0.1098, 0.0467, 0.0870,
        0.0139, 0.1172, 0.0686, 0.0129, 0.1187, 0.1093, 0.1179, 0.0834, 0.0656,
        0.0679, 0.1077, 0.1067, 0.0963, 0.1340, 0.0058, 0.0859, 0.1053, 0.1129,
        0.0878, 0.1281, 0.0129, 0.1156, 0.0816, 0.0623, 0.1235, 0.0557, 0.0542,
        0.0867, 0.1227, 0.1198, 0.0783, 0.1261, 0.0086, 0.0370, 0.1088, 0.0059,
        0.0866, 0.1131, 0.1130, 0.1005, 0.0984, 0.0087, 0.1049, 0.1093, 0.1170,
        0.0723, 0.0305, 0.1023, 0.0915, 0.0537, 0.0088, 0.1201, 0.0847, 0.1031,
        0.1077, 0.1116, 0.0881, 0.1211, 0.0777, 0.0871, 0.0350, 0.0845, 0.0208,
        0.0671, 0.0610, 0.0968, 0.0188, 0.1041, 0.0962, 0.0104, 0.0096, 0.0159,
        0.0503, 0.1238, 0.1148, 0.1198, 0.0024, 0.0244, 0.0785, 0.1183, 0.1236,
        0.0798, 0.1262, 0.1137, 0.1234, 0.1119, 0.1186, 0.1204, 0.1300, 0.0378,
        0.1125, 0.1232, 0.1244, 0.0922, 0.1347, 0.1399, 0.1416, 0.1279, 0.1363,
        0.1305, 0.1301, 0.1317, 0.1363, 0.1200, 0.1287, 0.1364, 0.1268, 0.1251,
        0.1254, 0.1293, 0.1358, 0.1318, 0.1327, 0.1347, 0.1306, 0.1379, 0.1350,
        0.1271, 0.1308, 0.1431, 0.1349, 0.1369, 0.1318, 0.1240, 0.1320, 0.1403,
        0.1256, 0.1352, 0.1272, 0.1298, 0.1394, 0.1277, 0.1276, 0.1305, 0.1283,
        0.1341, 0.1088, 0.1309, 0.1438, 0.1393, 0.1377, 0.1343, 0.1328, 0.1389,
        0.1250, 0.1274, 0.1281, 0.1340, 0.1243, 0.1361, 0.1414, 0.1314, 0.1286,
        0.1303, 0.1173, 0.1413, 0.1376, 0.1439, 0.1361, 0.1343, 0.1327, 0.1356,
        0.1335, 0.1323, 0.1314, 0.1352, 0.1232, 0.1395, 0.1381, 0.1332, 0.1332,
        0.1277, 0.1424, 0.1110, 0.1038, 0.1362, 0.1311, 0.1219, 0.1335, 0.1322,
        0.1335, 0.1294, 0.1246, 0.1104, 0.1284, 0.1393, 0.1246, 0.1391, 0.1289,
        0.1264, 0.1346, 0.1206, 0.1329, 0.1330, 0.1333, 0.1336, 0.1166, 0.1373,
        0.1360, 0.1391, 0.1375, 0.1351, 0.1348, 0.1298, 0.1363, 0.1373, 0.1335,
        0.1350, 0.1374, 0.1322, 0.1276, 0.1237, 0.1412, 0.1396, 0.1243, 0.1067,
        0.1311, 0.1409, 0.1332, 0.1317, 0.1401, 0.1271], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.1955, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:52,020 :: INFO :: Epoch 35: loss tensor(204.7953, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0853, 0.0757, 0.1338, 0.0995, 0.0581, 0.1190, 0.0587, 0.0700, 0.0604,
        0.0584, 0.0930, 0.0913, 0.0584, 0.0908, 0.1176, 0.0971, 0.0651, 0.0653,
        0.0673, 0.0905, 0.0834, 0.0445, 0.0838, 0.1057, 0.1089, 0.0613, 0.0570,
        0.1157, 0.1014, 0.0721, 0.1065, 0.0658, 0.0567, 0.0697, 0.0984, 0.0900,
        0.0646, 0.1013, 0.0796, 0.0740, 0.0687, 0.0524, 0.0921, 0.0896, 0.0843,
        0.0730, 0.0692, 0.0829, 0.0851, 0.0515, 0.1129, 0.0665, 0.0653, 0.0541,
        0.1077, 0.1175, 0.0480, 0.1337, 0.0940, 0.1084, 0.0578, 0.0692, 0.0569,
        0.0851, 0.0921, 0.0638, 0.0547, 0.0857, 0.0681, 0.0934, 0.0789, 0.0644,
        0.0611, 0.0680, 0.0606, 0.0992, 0.0691, 0.0930, 0.0592, 0.0599, 0.1146,
        0.0955, 0.1110, 0.0806, 0.0651, 0.0854, 0.0724, 0.0917, 0.0646, 0.0616,
        0.1042, 0.1034, 0.0793, 0.0498, 0.1008, 0.0639, 0.0832, 0.0950, 0.1087,
        0.1049, 0.0886, 0.0463, 0.1041, 0.0486, 0.1171, 0.1000, 0.0687, 0.0830,
        0.0674, 0.0872, 0.1146, 0.0562, 0.1132, 0.0890, 0.1114, 0.1039, 0.0634,
        0.0704, 0.0454, 0.1123, 0.1001, 0.1105, 0.0855, 0.0497, 0.0656, 0.1014,
        0.0932, 0.1038, 0.0314, 0.1079, 0.1307, 0.0127, 0.0238, 0.0080, 0.1014,
        0.1243, 0.1489, 0.0008, 0.0867, 0.1214, 0.0963, 0.1387, 0.0884, 0.1342,
        0.1254, 0.0627, 0.0074, 0.1487, 0.1314, 0.1304, 0.1456, 0.0691, 0.1146,
        0.0250, 0.1053, 0.1494, 0.1393, 0.1336, 0.0163, 0.1276, 0.0402, 0.1053,
        0.0127, 0.1297, 0.0636, 0.0125, 0.1345, 0.1226, 0.1325, 0.0830, 0.0623,
        0.0626, 0.1266, 0.1137, 0.0982, 0.1539, 0.0049, 0.0841, 0.1146, 0.1237,
        0.0889, 0.1467, 0.0111, 0.1353, 0.0783, 0.0565, 0.1423, 0.0510, 0.0511,
        0.1055, 0.1414, 0.1346, 0.0748, 0.1457, 0.0071, 0.0313, 0.1167, 0.0044,
        0.0859, 0.1252, 0.1223, 0.1063, 0.1023, 0.0079, 0.1109, 0.1178, 0.1312,
        0.0893, 0.0243, 0.1119, 0.0912, 0.0459, 0.0087, 0.1365, 0.0839, 0.1065,
        0.1196, 0.1270, 0.0864, 0.1408, 0.0743, 0.0894, 0.0308, 0.0829, 0.0201,
        0.0629, 0.0550, 0.0985, 0.0163, 0.1099, 0.0988, 0.0084, 0.0083, 0.0154,
        0.0426, 0.1409, 0.1291, 0.1345, 0.0022, 0.0218, 0.0781, 0.1318, 0.1404,
        0.0781, 0.1440, 0.1239, 0.1441, 0.1216, 0.1323, 0.1354, 0.1495, 0.0314,
        0.1239, 0.1371, 0.1470, 0.0957, 0.1583, 0.1615, 0.1663, 0.1434, 0.1571,
        0.1498, 0.1525, 0.1537, 0.1561, 0.1382, 0.1449, 0.1572, 0.1457, 0.1395,
        0.1445, 0.1501, 0.1565, 0.1515, 0.1528, 0.1553, 0.1531, 0.1588, 0.1550,
        0.1459, 0.1512, 0.1675, 0.1542, 0.1598, 0.1521, 0.1394, 0.1543, 0.1627,
        0.1413, 0.1581, 0.1462, 0.1510, 0.1638, 0.1450, 0.1474, 0.1484, 0.1451,
        0.1551, 0.1174, 0.1487, 0.1676, 0.1599, 0.1600, 0.1584, 0.1537, 0.1614,
        0.1399, 0.1477, 0.1470, 0.1571, 0.1418, 0.1583, 0.1642, 0.1512, 0.1498,
        0.1490, 0.1323, 0.1646, 0.1588, 0.1676, 0.1568, 0.1561, 0.1516, 0.1554,
        0.1546, 0.1510, 0.1553, 0.1577, 0.1407, 0.1615, 0.1593, 0.1552, 0.1559,
        0.1495, 0.1655, 0.1199, 0.1110, 0.1580, 0.1508, 0.1377, 0.1534, 0.1535,
        0.1559, 0.1488, 0.1435, 0.1194, 0.1452, 0.1612, 0.1427, 0.1613, 0.1496,
        0.1439, 0.1567, 0.1390, 0.1532, 0.1525, 0.1569, 0.1542, 0.1290, 0.1583,
        0.1587, 0.1634, 0.1577, 0.1598, 0.1544, 0.1466, 0.1568, 0.1595, 0.1528,
        0.1570, 0.1599, 0.1576, 0.1450, 0.1405, 0.1648, 0.1598, 0.1401, 0.1148,
        0.1505, 0.1657, 0.1522, 0.1540, 0.1629, 0.1438], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.5198, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:55,972 :: INFO :: Epoch 40: loss tensor(201.7350, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0862, 0.0767, 0.1470, 0.1038, 0.0573, 0.1266, 0.0575, 0.0692, 0.0607,
        0.0574, 0.0955, 0.0949, 0.0581, 0.0924, 0.1244, 0.0992, 0.0651, 0.0652,
        0.0678, 0.0930, 0.0866, 0.0436, 0.0838, 0.1120, 0.1148, 0.0616, 0.0573,
        0.1250, 0.1043, 0.0729, 0.1109, 0.0654, 0.0557, 0.0687, 0.1053, 0.0939,
        0.0650, 0.1043, 0.0822, 0.0743, 0.0697, 0.0524, 0.0931, 0.0920, 0.0857,
        0.0732, 0.0689, 0.0854, 0.0871, 0.0515, 0.1187, 0.0677, 0.0665, 0.0552,
        0.1119, 0.1254, 0.0470, 0.1470, 0.0986, 0.1154, 0.0575, 0.0701, 0.0556,
        0.0855, 0.0938, 0.0658, 0.0536, 0.0873, 0.0670, 0.0967, 0.0785, 0.0645,
        0.0597, 0.0684, 0.0594, 0.1047, 0.0681, 0.0968, 0.0582, 0.0588, 0.1209,
        0.0981, 0.1162, 0.0826, 0.0656, 0.0861, 0.0723, 0.0942, 0.0660, 0.0627,
        0.1103, 0.1078, 0.0796, 0.0491, 0.1055, 0.0624, 0.0840, 0.0981, 0.1154,
        0.1100, 0.0913, 0.0465, 0.1085, 0.0494, 0.1245, 0.1051, 0.0680, 0.0844,
        0.0678, 0.0887, 0.1209, 0.0564, 0.1193, 0.0900, 0.1200, 0.1110, 0.0644,
        0.0712, 0.0460, 0.1203, 0.1031, 0.1163, 0.0886, 0.0498, 0.0661, 0.1041,
        0.0972, 0.1082, 0.0277, 0.1087, 0.1399, 0.0101, 0.0208, 0.0068, 0.1030,
        0.1358, 0.1678, 0.0009, 0.0840, 0.1302, 0.0933, 0.1525, 0.0859, 0.1447,
        0.1314, 0.0552, 0.0060, 0.1648, 0.1402, 0.1410, 0.1617, 0.0650, 0.1173,
        0.0376, 0.1073, 0.1672, 0.1532, 0.1510, 0.0153, 0.1433, 0.0349, 0.1208,
        0.0129, 0.1384, 0.0581, 0.0121, 0.1472, 0.1324, 0.1436, 0.0788, 0.0574,
        0.0560, 0.1426, 0.1167, 0.0957, 0.1721, 0.0049, 0.0785, 0.1195, 0.1312,
        0.0859, 0.1629, 0.0091, 0.1526, 0.0724, 0.0507, 0.1591, 0.0451, 0.0469,
        0.1214, 0.1583, 0.1464, 0.0691, 0.1628, 0.0057, 0.0270, 0.1219, 0.0051,
        0.0815, 0.1336, 0.1280, 0.1087, 0.1021, 0.0068, 0.1121, 0.1231, 0.1418,
        0.1027, 0.0202, 0.1178, 0.0876, 0.0388, 0.0074, 0.1500, 0.0788, 0.1062,
        0.1281, 0.1393, 0.0822, 0.1582, 0.0694, 0.0873, 0.0292, 0.0785, 0.0178,
        0.0588, 0.0480, 0.0975, 0.0138, 0.1112, 0.0970, 0.0067, 0.0079, 0.0144,
        0.0357, 0.1552, 0.1399, 0.1466, 0.0024, 0.0210, 0.0740, 0.1419, 0.1547,
        0.0734, 0.1596, 0.1308, 0.1627, 0.1269, 0.1430, 0.1474, 0.1675, 0.0270,
        0.1321, 0.1473, 0.1669, 0.0947, 0.1811, 0.1821, 0.1905, 0.1556, 0.1762,
        0.1669, 0.1734, 0.1742, 0.1741, 0.1534, 0.1589, 0.1769, 0.1626, 0.1504,
        0.1618, 0.1688, 0.1760, 0.1698, 0.1709, 0.1741, 0.1741, 0.1781, 0.1736,
        0.1624, 0.1696, 0.1910, 0.1718, 0.1817, 0.1712, 0.1523, 0.1750, 0.1836,
        0.1544, 0.1795, 0.1634, 0.1702, 0.1870, 0.1598, 0.1654, 0.1640, 0.1593,
        0.1750, 0.1223, 0.1641, 0.1909, 0.1788, 0.1810, 0.1820, 0.1729, 0.1825,
        0.1519, 0.1662, 0.1634, 0.1793, 0.1573, 0.1790, 0.1858, 0.1695, 0.1693,
        0.1652, 0.1432, 0.1869, 0.1784, 0.1908, 0.1759, 0.1768, 0.1683, 0.1737,
        0.1736, 0.1681, 0.1782, 0.1785, 0.1559, 0.1822, 0.1789, 0.1758, 0.1774,
        0.1697, 0.1875, 0.1252, 0.1143, 0.1783, 0.1687, 0.1505, 0.1712, 0.1733,
        0.1768, 0.1658, 0.1603, 0.1248, 0.1598, 0.1820, 0.1589, 0.1821, 0.1684,
        0.1596, 0.1772, 0.1558, 0.1716, 0.1698, 0.1798, 0.1731, 0.1382, 0.1775,
        0.1801, 0.1873, 0.1762, 0.1836, 0.1719, 0.1610, 0.1755, 0.1813, 0.1704,
        0.1775, 0.1811, 0.1823, 0.1599, 0.1547, 0.1873, 0.1781, 0.1536, 0.1183,
        0.1676, 0.1903, 0.1689, 0.1749, 0.1845, 0.1586], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.7435, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:41:55,972 :: INFO :: ----- frontend -----
2023-05-14 18:41:55,972 :: INFO :: Environment 0
2023-05-14 18:42:05,425 :: INFO :: Epoch 5: loss tensor(864.8449, device='cuda:0'), U.norm 13.269613265991211, V.norm 16.887462615966797, MLP.norm 1.5326595306396484
2023-05-14 18:42:05,660 :: INFO :: Epoch 10: loss tensor(847.2820, device='cuda:0'), U.norm 10.570813179016113, V.norm 16.177854537963867, MLP.norm 2.8569741249084473
2023-05-14 18:42:05,895 :: INFO :: Epoch 15: loss tensor(819.0316, device='cuda:0'), U.norm 9.000847816467285, V.norm 15.829744338989258, MLP.norm 4.578286170959473
2023-05-14 18:42:06,129 :: INFO :: Epoch 20: loss tensor(784.7872, device='cuda:0'), U.norm 7.988624095916748, V.norm 15.593860626220703, MLP.norm 6.343695163726807
2023-05-14 18:42:06,144 :: INFO :: Environment 1
2023-05-14 18:42:15,694 :: INFO :: Epoch 5: loss tensor(888.6121, device='cuda:0'), U.norm 13.275238990783691, V.norm 16.932016372680664, MLP.norm 1.555393099784851
2023-05-14 18:42:15,928 :: INFO :: Epoch 10: loss tensor(867.4552, device='cuda:0'), U.norm 10.5838623046875, V.norm 16.243837356567383, MLP.norm 2.968233823776245
2023-05-14 18:42:16,162 :: INFO :: Epoch 15: loss tensor(834.0909, device='cuda:0'), U.norm 9.022232055664062, V.norm 15.9031343460083, MLP.norm 4.74717903137207
2023-05-14 18:42:16,397 :: INFO :: Epoch 20: loss tensor(794.5774, device='cuda:0'), U.norm 8.018089294433594, V.norm 15.670865058898926, MLP.norm 6.515614986419678
2023-05-14 18:42:16,412 :: INFO :: Environment 2
2023-05-14 18:42:25,647 :: INFO :: Epoch 5: loss tensor(755.5477, device='cuda:0'), U.norm 14.721102714538574, V.norm 17.745223999023438, MLP.norm 1.5224144458770752
2023-05-14 18:42:25,803 :: INFO :: Epoch 10: loss tensor(743.2443, device='cuda:0'), U.norm 12.193916320800781, V.norm 17.12446403503418, MLP.norm 2.3933215141296387
2023-05-14 18:42:25,959 :: INFO :: Epoch 15: loss tensor(722.9059, device='cuda:0'), U.norm 10.62123966217041, V.norm 16.845333099365234, MLP.norm 3.6568028926849365
2023-05-14 18:42:26,116 :: INFO :: Epoch 20: loss tensor(697.4398, device='cuda:0'), U.norm 9.530329704284668, V.norm 16.665586471557617, MLP.norm 4.9959025382995605
2023-05-14 18:42:26,131 :: INFO :: Environment 3
2023-05-14 18:42:35,381 :: INFO :: Epoch 5: loss tensor(888.1962, device='cuda:0'), U.norm 13.27438735961914, V.norm 16.927589416503906, MLP.norm 1.5849902629852295
2023-05-14 18:42:35,616 :: INFO :: Epoch 10: loss tensor(867.6077, device='cuda:0'), U.norm 10.580984115600586, V.norm 16.23354148864746, MLP.norm 2.9772653579711914
2023-05-14 18:42:35,834 :: INFO :: Epoch 15: loss tensor(835.8564, device='cuda:0'), U.norm 9.01708698272705, V.norm 15.888671875, MLP.norm 4.742499828338623
2023-05-14 18:42:36,069 :: INFO :: Epoch 20: loss tensor(798.0891, device='cuda:0'), U.norm 8.01068115234375, V.norm 15.65381145477295, MLP.norm 6.517408847808838
2023-05-14 18:42:36,069 :: INFO :: Environment 4
2023-05-14 18:42:45,412 :: INFO :: Epoch 5: loss tensor(830.2119, device='cuda:0'), U.norm 13.266077995300293, V.norm 16.828359603881836, MLP.norm 1.5071018934249878
2023-05-14 18:42:45,647 :: INFO :: Epoch 10: loss tensor(812.9752, device='cuda:0'), U.norm 10.562554359436035, V.norm 16.094409942626953, MLP.norm 2.7980244159698486
2023-05-14 18:42:45,881 :: INFO :: Epoch 15: loss tensor(785.5942, device='cuda:0'), U.norm 8.986733436584473, V.norm 15.733684539794922, MLP.norm 4.44406270980835
2023-05-14 18:42:46,115 :: INFO :: Epoch 20: loss tensor(753.4251, device='cuda:0'), U.norm 7.968001842498779, V.norm 15.491588592529297, MLP.norm 6.103673934936523
2023-05-14 18:42:46,131 :: INFO :: Environment 5
2023-05-14 18:42:55,491 :: INFO :: Epoch 5: loss tensor(844.0677, device='cuda:0'), U.norm 13.268597602844238, V.norm 16.852754592895508, MLP.norm 1.5533872842788696
2023-05-14 18:42:55,709 :: INFO :: Epoch 10: loss tensor(825.8746, device='cuda:0'), U.norm 10.568620681762695, V.norm 16.130464553833008, MLP.norm 2.8880844116210938
2023-05-14 18:42:55,943 :: INFO :: Epoch 15: loss tensor(797.4068, device='cuda:0'), U.norm 8.996668815612793, V.norm 15.776094436645508, MLP.norm 4.574549674987793
2023-05-14 18:42:56,178 :: INFO :: Epoch 20: loss tensor(763.8249, device='cuda:0'), U.norm 7.981308937072754, V.norm 15.537869453430176, MLP.norm 6.2681074142456055
2023-05-14 18:42:56,178 :: INFO :: Environment 6
2023-05-14 18:43:05,537 :: INFO :: Epoch 5: loss tensor(886.7230, device='cuda:0'), U.norm 13.272811889648438, V.norm 16.925920486450195, MLP.norm 1.5234166383743286
2023-05-14 18:43:05,756 :: INFO :: Epoch 10: loss tensor(867.1899, device='cuda:0'), U.norm 10.578872680664062, V.norm 16.232675552368164, MLP.norm 2.8789358139038086
2023-05-14 18:43:05,990 :: INFO :: Epoch 15: loss tensor(835.6799, device='cuda:0'), U.norm 9.01447868347168, V.norm 15.887957572937012, MLP.norm 4.6383748054504395
2023-05-14 18:43:06,209 :: INFO :: Epoch 20: loss tensor(797.1354, device='cuda:0'), U.norm 8.00827407836914, V.norm 15.653642654418945, MLP.norm 6.41995096206665
2023-05-14 18:43:06,225 :: INFO :: Environment 7
2023-05-14 18:43:15,600 :: INFO :: Epoch 5: loss tensor(883.3918, device='cuda:0'), U.norm 13.27351188659668, V.norm 16.91468620300293, MLP.norm 1.532360553741455
2023-05-14 18:43:15,834 :: INFO :: Epoch 10: loss tensor(863.3932, device='cuda:0'), U.norm 10.579872131347656, V.norm 16.219993591308594, MLP.norm 2.9150121212005615
2023-05-14 18:43:16,053 :: INFO :: Epoch 15: loss tensor(832.1974, device='cuda:0'), U.norm 9.015629768371582, V.norm 15.876657485961914, MLP.norm 4.671666145324707
2023-05-14 18:43:16,287 :: INFO :: Epoch 20: loss tensor(794.8703, device='cuda:0'), U.norm 8.009530067443848, V.norm 15.643425941467285, MLP.norm 6.443397521972656
2023-05-14 18:43:16,303 :: INFO :: Environment 8
2023-05-14 18:43:25,662 :: INFO :: Epoch 5: loss tensor(819.2993, device='cuda:0'), U.norm 13.264520645141602, V.norm 16.82094955444336, MLP.norm 1.5379703044891357
2023-05-14 18:43:25,897 :: INFO :: Epoch 10: loss tensor(801.0855, device='cuda:0'), U.norm 10.558853149414062, V.norm 16.083419799804688, MLP.norm 2.849120855331421
2023-05-14 18:43:26,131 :: INFO :: Epoch 15: loss tensor(772.4983, device='cuda:0'), U.norm 8.980210304260254, V.norm 15.719183921813965, MLP.norm 4.506556510925293
2023-05-14 18:43:26,397 :: INFO :: Epoch 20: loss tensor(739.1755, device='cuda:0'), U.norm 7.957507610321045, V.norm 15.473272323608398, MLP.norm 6.170748710632324
2023-05-14 18:43:26,397 :: INFO :: Environment 9
2023-05-14 18:43:35,709 :: INFO :: Epoch 5: loss tensor(807.7076, device='cuda:0'), U.norm 13.25751781463623, V.norm 16.795202255249023, MLP.norm 1.4505397081375122
2023-05-14 18:43:35,928 :: INFO :: Epoch 10: loss tensor(791.4763, device='cuda:0'), U.norm 10.54505729675293, V.norm 16.04389762878418, MLP.norm 2.6866047382354736
2023-05-14 18:43:36,162 :: INFO :: Epoch 15: loss tensor(764.2275, device='cuda:0'), U.norm 8.959298133850098, V.norm 15.674561500549316, MLP.norm 4.320849418640137
2023-05-14 18:43:36,381 :: INFO :: Epoch 20: loss tensor(731.9307, device='cuda:0'), U.norm 7.9297051429748535, V.norm 15.42658519744873, MLP.norm 5.974370956420898
2023-05-14 18:43:36,397 :: INFO :: Ite = 1, Delta = 974
2023-05-14 18:43:36,397 :: INFO :: ----- backend -----
2023-05-14 18:43:40,459 :: INFO :: Epoch 5: loss tensor(209.8501, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0114,  0.0135,  0.0195,  0.0139,  0.0124,  0.0142,  0.0105,  0.0137,
         0.0064,  0.0186,  0.0182,  0.0155,  0.0083,  0.0127,  0.0193,  0.0191,
         0.0191,  0.0164,  0.0191,  0.0169,  0.0116,  0.0179,  0.0192,  0.0072,
         0.0198,  0.0141,  0.0133,  0.0102,  0.0196,  0.0158,  0.0191,  0.0189,
         0.0100,  0.0187,  0.0038,  0.0073,  0.0154,  0.0188,  0.0107,  0.0191,
         0.0129,  0.0056,  0.0197,  0.0136,  0.0133,  0.0192,  0.0161,  0.0025,
         0.0191,  0.0136,  0.0194,  0.0099,  0.0178,  0.0120,  0.0190,  0.0164,
         0.0111,  0.0190,  0.0075,  0.0093,  0.0185,  0.0184,  0.0056,  0.0159,
         0.0145,  0.0045,  0.0191,  0.0103,  0.0194,  0.0142,  0.0197,  0.0071,
         0.0193,  0.0069,  0.0172,  0.0076,  0.0191,  0.0088,  0.0113,  0.0154,
         0.0194,  0.0117,  0.0195,  0.0079,  0.0046,  0.0193,  0.0192,  0.0136,
         0.0085,  0.0045,  0.0114,  0.0149,  0.0195,  0.0171,  0.0107,  0.0192,
         0.0194,  0.0194,  0.0195,  0.0188,  0.0075,  0.0142,  0.0153,  0.0004,
         0.0194,  0.0153,  0.0152,  0.0196,  0.0133,  0.0160,  0.0194,  0.0103,
         0.0195,  0.0185,  0.0143,  0.0121,  0.0137,  0.0166,  0.0128,  0.0142,
         0.0193,  0.0157,  0.0090,  0.0115,  0.0042,  0.0192,  0.0084,  0.0188,
         0.0147,  0.0193,  0.0198,  0.0185,  0.0171,  0.0098,  0.0074,  0.0044,
         0.0102,  0.0095,  0.0090,  0.0071,  0.0195,  0.0166,  0.0076,  0.0186,
         0.0190,  0.0196,  0.0081,  0.0183,  0.0198,  0.0127,  0.0197,  0.0149,
         0.0197, -0.0169,  0.0106,  0.0161,  0.0164,  0.0015,  0.0183, -0.0039,
         0.0192, -0.0128,  0.0130,  0.0198,  0.0193,  0.0118,  0.0133,  0.0073,
         0.0149,  0.0110,  0.0158,  0.0141, -0.0024,  0.0186,  0.0194,  0.0197,
         0.0071,  0.0164,  0.0125,  0.0193,  0.0112,  0.0194,  0.0139,  0.0037,
         0.0193,  0.0162,  0.0149,  0.0120,  0.0132, -0.0101,  0.0049,  0.0164,
         0.0196,  0.0191,  0.0097,  0.0146,  0.0139,  0.0104,  0.0194,  0.0111,
         0.0198,  0.0104,  0.0197,  0.0138,  0.0196,  0.0124,  0.0154, -0.0142,
         0.0155,  0.0061,  0.0194,  0.0194,  0.0095,  0.0148,  0.0186,  0.0186,
         0.0075,  0.0052,  0.0197,  0.0123,  0.0123,  0.0084,  0.0174,  0.0190,
         0.0153,  0.0103,  0.0094,  0.0141,  0.0084,  0.0193,  0.0195,  0.0153,
         0.0179,  0.0127,  0.0191,  0.0113,  0.0108,  0.0169,  0.0054,  0.0145,
         0.0064,  0.0156,  0.0130,  0.0194,  0.0193,  0.0196,  0.0094,  0.0192,
         0.0194,  0.0195,  0.0141,  0.0188,  0.0097,  0.0195,  0.0038,  0.0100,
         0.0115,  0.0157,  0.0149,  0.0196,  0.0196,  0.0129,  0.0085,  0.0106,
         0.0198,  0.0064,  0.0197,  0.0198,  0.0126,  0.0194,  0.0102,  0.0099,
         0.0170,  0.0157,  0.0162,  0.0157,  0.0093,  0.0195,  0.0198,  0.0126,
         0.0134,  0.0151,  0.0199,  0.0131,  0.0130,  0.0164,  0.0105,  0.0190,
         0.0197,  0.0135,  0.0124,  0.0111,  0.0127,  0.0159,  0.0106,  0.0197,
         0.0168,  0.0164,  0.0137,  0.0194,  0.0167,  0.0200,  0.0163,  0.0065,
         0.0150,  0.0165,  0.0189,  0.0112,  0.0128,  0.0113,  0.0118,  0.0145,
         0.0193,  0.0152,  0.0077,  0.0168,  0.0104,  0.0152,  0.0198,  0.0199,
         0.0197,  0.0139,  0.0198,  0.0194,  0.0148,  0.0199,  0.0047,  0.0137,
         0.0109,  0.0166,  0.0194,  0.0128,  0.0112,  0.0068,  0.0198,  0.0135,
         0.0132,  0.0169,  0.0165,  0.0128,  0.0159,  0.0127,  0.0120,  0.0126,
         0.0099,  0.0150,  0.0192,  0.0198,  0.0111,  0.0198,  0.0112,  0.0143,
         0.0140,  0.0073,  0.0138,  0.0190,  0.0065,  0.0164,  0.0124,  0.0191,
         0.0130,  0.0119,  0.0197,  0.0061,  0.0196,  0.0198,  0.0194,  0.0156,
         0.0198,  0.0144,  0.0187,  0.0032,  0.0148,  0.0111,  0.0168,  0.0199,
         0.0158,  0.0127,  0.0151,  0.0138,  0.0198,  0.0101,  0.0191,  0.0169],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.6644, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:43:44,396 :: INFO :: Epoch 10: loss tensor(207.3245, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0319,  0.0335,  0.0413,  0.0350,  0.0312,  0.0362,  0.0306,  0.0334,
         0.0263,  0.0353,  0.0383,  0.0357,  0.0282,  0.0338,  0.0398,  0.0387,
         0.0378,  0.0351,  0.0381,  0.0371,  0.0324,  0.0345,  0.0394,  0.0291,
         0.0406,  0.0325,  0.0318,  0.0323,  0.0404,  0.0352,  0.0400,  0.0370,
         0.0300,  0.0385,  0.0255,  0.0288,  0.0342,  0.0391,  0.0313,  0.0387,
         0.0315,  0.0256,  0.0399,  0.0337,  0.0339,  0.0383,  0.0357,  0.0239,
         0.0388,  0.0322,  0.0401,  0.0294,  0.0356,  0.0308,  0.0401,  0.0380,
         0.0302,  0.0408,  0.0291,  0.0313,  0.0362,  0.0368,  0.0263,  0.0362,
         0.0354,  0.0251,  0.0360,  0.0315,  0.0383,  0.0353,  0.0398,  0.0279,
         0.0377,  0.0273,  0.0344,  0.0293,  0.0376,  0.0300,  0.0309,  0.0341,
         0.0407,  0.0339,  0.0410,  0.0287,  0.0252,  0.0390,  0.0381,  0.0346,
         0.0286,  0.0245,  0.0333,  0.0362,  0.0383,  0.0347,  0.0326,  0.0380,
         0.0385,  0.0397,  0.0400,  0.0396,  0.0291,  0.0302,  0.0366,  0.0194,
         0.0404,  0.0362,  0.0346,  0.0391,  0.0328,  0.0361,  0.0405,  0.0296,
         0.0409,  0.0390,  0.0357,  0.0330,  0.0326,  0.0360,  0.0311,  0.0357,
         0.0400,  0.0369,  0.0303,  0.0305,  0.0247,  0.0393,  0.0302,  0.0394,
         0.0331,  0.0407,  0.0414,  0.0308,  0.0326,  0.0117,  0.0296,  0.0268,
         0.0330,  0.0064,  0.0307,  0.0294,  0.0406,  0.0390,  0.0292,  0.0403,
         0.0409,  0.0395,  0.0178,  0.0406,  0.0413,  0.0350,  0.0419,  0.0350,
         0.0409, -0.0247,  0.0325,  0.0386,  0.0388,  0.0240,  0.0299,  0.0168,
         0.0379, -0.0031,  0.0183,  0.0415,  0.0388,  0.0153,  0.0357,  0.0298,
         0.0371,  0.0325,  0.0355,  0.0350,  0.0194,  0.0402,  0.0401,  0.0421,
         0.0086,  0.0379,  0.0347,  0.0413,  0.0329,  0.0417,  0.0252,  0.0265,
         0.0400,  0.0352,  0.0375,  0.0317,  0.0325,  0.0062,  0.0273,  0.0387,
         0.0399,  0.0416,  0.0267,  0.0322,  0.0356,  0.0094,  0.0401,  0.0333,
         0.0414,  0.0321,  0.0408,  0.0157,  0.0411,  0.0342,  0.0376, -0.0084,
         0.0339,  0.0284,  0.0398,  0.0381,  0.0120,  0.0372,  0.0395,  0.0397,
         0.0298,  0.0279,  0.0405,  0.0351,  0.0333,  0.0303,  0.0333,  0.0393,
         0.0249,  0.0309,  0.0305,  0.0354,  0.0246,  0.0401,  0.0405,  0.0270,
         0.0291,  0.0185,  0.0385,  0.0338,  0.0333,  0.0391,  0.0041,  0.0282,
         0.0279,  0.0377,  0.0354,  0.0394,  0.0417,  0.0410,  0.0322,  0.0410,
         0.0414,  0.0416,  0.0368,  0.0361,  0.0321,  0.0414,  0.0266,  0.0319,
         0.0345,  0.0389,  0.0382,  0.0424,  0.0423,  0.0357,  0.0321,  0.0342,
         0.0424,  0.0296,  0.0422,  0.0427,  0.0358,  0.0423,  0.0336,  0.0333,
         0.0395,  0.0387,  0.0394,  0.0387,  0.0325,  0.0426,  0.0427,  0.0361,
         0.0369,  0.0384,  0.0430,  0.0365,  0.0361,  0.0395,  0.0340,  0.0422,
         0.0422,  0.0367,  0.0352,  0.0346,  0.0361,  0.0389,  0.0340,  0.0423,
         0.0399,  0.0389,  0.0362,  0.0423,  0.0400,  0.0430,  0.0389,  0.0300,
         0.0380,  0.0397,  0.0419,  0.0340,  0.0361,  0.0345,  0.0346,  0.0374,
         0.0423,  0.0378,  0.0304,  0.0400,  0.0332,  0.0384,  0.0430,  0.0430,
         0.0428,  0.0367,  0.0429,  0.0425,  0.0376,  0.0425,  0.0281,  0.0365,
         0.0343,  0.0398,  0.0424,  0.0362,  0.0347,  0.0304,  0.0429,  0.0364,
         0.0355,  0.0402,  0.0392,  0.0360,  0.0392,  0.0361,  0.0352,  0.0356,
         0.0331,  0.0375,  0.0419,  0.0429,  0.0344,  0.0429,  0.0347,  0.0376,
         0.0374,  0.0303,  0.0364,  0.0422,  0.0299,  0.0391,  0.0352,  0.0423,
         0.0362,  0.0354,  0.0425,  0.0294,  0.0420,  0.0424,  0.0424,  0.0387,
         0.0428,  0.0378,  0.0420,  0.0265,  0.0380,  0.0342,  0.0399,  0.0427,
         0.0390,  0.0353,  0.0383,  0.0371,  0.0427,  0.0336,  0.0424,  0.0398],
       device='cuda:0', requires_grad=True) MLP.norm tensor(4.1877, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:43:48,381 :: INFO :: Epoch 15: loss tensor(201.7978, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0498,  0.0498,  0.0630,  0.0543,  0.0450,  0.0578,  0.0460,  0.0485,
         0.0412,  0.0461,  0.0558,  0.0540,  0.0437,  0.0523,  0.0595,  0.0563,
         0.0514,  0.0477,  0.0520,  0.0545,  0.0499,  0.0438,  0.0564,  0.0501,
         0.0602,  0.0465,  0.0437,  0.0537,  0.0591,  0.0503,  0.0596,  0.0500,
         0.0451,  0.0533,  0.0463,  0.0479,  0.0480,  0.0581,  0.0487,  0.0541,
         0.0464,  0.0401,  0.0585,  0.0518,  0.0520,  0.0533,  0.0508,  0.0432,
         0.0552,  0.0446,  0.0597,  0.0447,  0.0492,  0.0433,  0.0601,  0.0592,
         0.0422,  0.0626,  0.0490,  0.0525,  0.0478,  0.0502,  0.0423,  0.0534,
         0.0543,  0.0412,  0.0460,  0.0502,  0.0531,  0.0540,  0.0559,  0.0444,
         0.0505,  0.0448,  0.0473,  0.0495,  0.0503,  0.0491,  0.0456,  0.0472,
         0.0607,  0.0546,  0.0614,  0.0468,  0.0420,  0.0559,  0.0529,  0.0528,
         0.0443,  0.0403,  0.0540,  0.0557,  0.0542,  0.0458,  0.0530,  0.0515,
         0.0548,  0.0580,  0.0589,  0.0579,  0.0487,  0.0390,  0.0562,  0.0344,
         0.0606,  0.0550,  0.0496,  0.0558,  0.0477,  0.0532,  0.0598,  0.0435,
         0.0615,  0.0566,  0.0563,  0.0524,  0.0461,  0.0511,  0.0428,  0.0559,
         0.0579,  0.0571,  0.0493,  0.0425,  0.0421,  0.0575,  0.0500,  0.0587,
         0.0444,  0.0606,  0.0625,  0.0291,  0.0357,  0.0107,  0.0511,  0.0496,
         0.0568,  0.0032,  0.0507,  0.0515,  0.0599,  0.0613,  0.0493,  0.0627,
         0.0623,  0.0552,  0.0131,  0.0639,  0.0624,  0.0572,  0.0644,  0.0524,
         0.0609, -0.0168,  0.0532,  0.0618,  0.0614,  0.0479,  0.0294,  0.0394,
         0.0507,  0.0173,  0.0176,  0.0625,  0.0542,  0.0148,  0.0581,  0.0522,
         0.0591,  0.0518,  0.0512,  0.0524,  0.0431,  0.0609,  0.0588,  0.0651,
         0.0068,  0.0570,  0.0560,  0.0626,  0.0531,  0.0644,  0.0236,  0.0503,
         0.0577,  0.0492,  0.0606,  0.0455,  0.0464,  0.0299,  0.0511,  0.0608,
         0.0572,  0.0645,  0.0282,  0.0416,  0.0572,  0.0068,  0.0581,  0.0553,
         0.0622,  0.0535,  0.0605,  0.0124,  0.0613,  0.0561,  0.0595,  0.0098,
         0.0444,  0.0503,  0.0585,  0.0512,  0.0102,  0.0596,  0.0577,  0.0601,
         0.0519,  0.0509,  0.0590,  0.0584,  0.0523,  0.0504,  0.0408,  0.0579,
         0.0246,  0.0484,  0.0483,  0.0559,  0.0274,  0.0595,  0.0595,  0.0238,
         0.0259,  0.0168,  0.0519,  0.0571,  0.0556,  0.0612,  0.0035,  0.0320,
         0.0471,  0.0596,  0.0584,  0.0564,  0.0644,  0.0616,  0.0559,  0.0620,
         0.0632,  0.0634,  0.0609,  0.0455,  0.0547,  0.0634,  0.0510,  0.0522,
         0.0594,  0.0634,  0.0636,  0.0658,  0.0662,  0.0595,  0.0573,  0.0593,
         0.0666,  0.0538,  0.0652,  0.0668,  0.0600,  0.0656,  0.0581,  0.0582,
         0.0634,  0.0628,  0.0638,  0.0628,  0.0573,  0.0669,  0.0665,  0.0607,
         0.0616,  0.0637,  0.0672,  0.0615,  0.0611,  0.0631,  0.0591,  0.0668,
         0.0651,  0.0616,  0.0590,  0.0594,  0.0615,  0.0627,  0.0586,  0.0656,
         0.0637,  0.0624,  0.0583,  0.0660,  0.0650,  0.0673,  0.0628,  0.0557,
         0.0623,  0.0644,  0.0659,  0.0581,  0.0604,  0.0594,  0.0583,  0.0619,
         0.0669,  0.0615,  0.0545,  0.0643,  0.0562,  0.0633,  0.0674,  0.0677,
         0.0670,  0.0611,  0.0669,  0.0667,  0.0618,  0.0659,  0.0537,  0.0610,
         0.0584,  0.0642,  0.0668,  0.0608,  0.0598,  0.0557,  0.0675,  0.0594,
         0.0576,  0.0650,  0.0634,  0.0601,  0.0637,  0.0609,  0.0600,  0.0593,
         0.0575,  0.0596,  0.0654,  0.0674,  0.0585,  0.0675,  0.0594,  0.0619,
         0.0623,  0.0542,  0.0603,  0.0667,  0.0554,  0.0629,  0.0581,  0.0668,
         0.0610,  0.0608,  0.0668,  0.0552,  0.0656,  0.0658,  0.0666,  0.0631,
         0.0668,  0.0626,  0.0668,  0.0524,  0.0619,  0.0583,  0.0649,  0.0667,
         0.0630,  0.0574,  0.0626,  0.0624,  0.0666,  0.0587,  0.0672,  0.0632],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.2847, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:43:52,350 :: INFO :: Epoch 20: loss tensor(199.4109, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0627,  0.0598,  0.0837,  0.0705,  0.0513,  0.0764,  0.0538,  0.0573,
         0.0489,  0.0498,  0.0690,  0.0676,  0.0513,  0.0657,  0.0762,  0.0692,
         0.0574,  0.0537,  0.0595,  0.0681,  0.0637,  0.0448,  0.0677,  0.0673,
         0.0772,  0.0541,  0.0473,  0.0730,  0.0735,  0.0601,  0.0758,  0.0563,
         0.0518,  0.0612,  0.0628,  0.0616,  0.0547,  0.0729,  0.0604,  0.0624,
         0.0556,  0.0461,  0.0716,  0.0646,  0.0650,  0.0620,  0.0591,  0.0565,
         0.0666,  0.0479,  0.0763,  0.0537,  0.0559,  0.0482,  0.0768,  0.0780,
         0.0447,  0.0827,  0.0654,  0.0703,  0.0539,  0.0587,  0.0506,  0.0650,
         0.0688,  0.0514,  0.0492,  0.0632,  0.0618,  0.0679,  0.0662,  0.0530,
         0.0565,  0.0561,  0.0532,  0.0664,  0.0563,  0.0644,  0.0524,  0.0542,
         0.0770,  0.0703,  0.0789,  0.0597,  0.0519,  0.0679,  0.0618,  0.0662,
         0.0533,  0.0498,  0.0711,  0.0721,  0.0641,  0.0484,  0.0697,  0.0578,
         0.0664,  0.0722,  0.0744,  0.0733,  0.0639,  0.0407,  0.0715,  0.0420,
         0.0786,  0.0698,  0.0567,  0.0671,  0.0574,  0.0646,  0.0767,  0.0491,
         0.0784,  0.0688,  0.0743,  0.0688,  0.0535,  0.0593,  0.0477,  0.0742,
         0.0725,  0.0739,  0.0642,  0.0460,  0.0532,  0.0721,  0.0650,  0.0748,
         0.0449,  0.0768,  0.0818,  0.0212,  0.0316,  0.0090,  0.0690,  0.0704,
         0.0806,  0.0024,  0.0654,  0.0714,  0.0757,  0.0822,  0.0652,  0.0833,
         0.0815,  0.0643,  0.0104,  0.0871,  0.0821,  0.0780,  0.0862,  0.0638,
         0.0780, -0.0033,  0.0707,  0.0846,  0.0827,  0.0708,  0.0224,  0.0608,
         0.0529,  0.0389,  0.0161,  0.0816,  0.0615,  0.0131,  0.0789,  0.0724,
         0.0795,  0.0659,  0.0592,  0.0623,  0.0655,  0.0789,  0.0735,  0.0881,
         0.0072,  0.0706,  0.0745,  0.0814,  0.0690,  0.0865,  0.0182,  0.0731,
         0.0696,  0.0566,  0.0827,  0.0523,  0.0515,  0.0525,  0.0743,  0.0814,
         0.0687,  0.0866,  0.0200,  0.0423,  0.0766,  0.0058,  0.0710,  0.0759,
         0.0809,  0.0722,  0.0763,  0.0107,  0.0787,  0.0761,  0.0797,  0.0295,
         0.0426,  0.0692,  0.0733,  0.0554,  0.0098,  0.0807,  0.0705,  0.0772,
         0.0719,  0.0719,  0.0734,  0.0806,  0.0655,  0.0659,  0.0407,  0.0715,
         0.0222,  0.0595,  0.0584,  0.0725,  0.0239,  0.0762,  0.0744,  0.0156,
         0.0166,  0.0162,  0.0560,  0.0800,  0.0762,  0.0816,  0.0030,  0.0296,
         0.0610,  0.0800,  0.0808,  0.0677,  0.0860,  0.0798,  0.0789,  0.0805,
         0.0831,  0.0836,  0.0851,  0.0450,  0.0759,  0.0843,  0.0755,  0.0684,
         0.0846,  0.0874,  0.0897,  0.0877,  0.0897,  0.0830,  0.0821,  0.0840,
         0.0904,  0.0771,  0.0872,  0.0901,  0.0833,  0.0867,  0.0815,  0.0825,
         0.0872,  0.0860,  0.0875,  0.0867,  0.0823,  0.0907,  0.0895,  0.0843,
         0.0854,  0.0897,  0.0906,  0.0867,  0.0858,  0.0851,  0.0840,  0.0913,
         0.0870,  0.0865,  0.0817,  0.0835,  0.0872,  0.0854,  0.0822,  0.0877,
         0.0860,  0.0855,  0.0773,  0.0885,  0.0903,  0.0919,  0.0868,  0.0820,
         0.0861,  0.0893,  0.0879,  0.0820,  0.0836,  0.0847,  0.0809,  0.0859,
         0.0916,  0.0848,  0.0785,  0.0877,  0.0777,  0.0887,  0.0914,  0.0928,
         0.0907,  0.0851,  0.0901,  0.0903,  0.0857,  0.0887,  0.0797,  0.0853,
         0.0812,  0.0883,  0.0909,  0.0851,  0.0848,  0.0806,  0.0923,  0.0796,
         0.0768,  0.0894,  0.0870,  0.0829,  0.0877,  0.0855,  0.0846,  0.0826,
         0.0814,  0.0788,  0.0872,  0.0916,  0.0814,  0.0918,  0.0834,  0.0845,
         0.0869,  0.0774,  0.0838,  0.0906,  0.0812,  0.0864,  0.0787,  0.0909,
         0.0860,  0.0870,  0.0909,  0.0812,  0.0890,  0.0880,  0.0903,  0.0876,
         0.0896,  0.0871,  0.0912,  0.0788,  0.0847,  0.0815,  0.0903,  0.0905,
         0.0853,  0.0757,  0.0861,  0.0883,  0.0897,  0.0834,  0.0919,  0.0850],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0643, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:43:56,303 :: INFO :: Epoch 25: loss tensor(198.5157, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0698, 0.0645, 0.1018, 0.0820, 0.0525, 0.0901, 0.0555, 0.0598, 0.0517,
        0.0503, 0.0771, 0.0769, 0.0528, 0.0743, 0.0896, 0.0768, 0.0589, 0.0555,
        0.0620, 0.0764, 0.0729, 0.0428, 0.0743, 0.0805, 0.0909, 0.0564, 0.0485,
        0.0891, 0.0828, 0.0640, 0.0879, 0.0574, 0.0532, 0.0638, 0.0754, 0.0712,
        0.0563, 0.0827, 0.0664, 0.0654, 0.0600, 0.0470, 0.0788, 0.0718, 0.0729,
        0.0647, 0.0629, 0.0657, 0.0728, 0.0473, 0.0891, 0.0571, 0.0577, 0.0487,
        0.0883, 0.0931, 0.0435, 0.1008, 0.0770, 0.0837, 0.0536, 0.0616, 0.0527,
        0.0716, 0.0780, 0.0553, 0.0483, 0.0712, 0.0644, 0.0759, 0.0709, 0.0564,
        0.0572, 0.0618, 0.0539, 0.0789, 0.0582, 0.0748, 0.0533, 0.0552, 0.0901,
        0.0799, 0.0917, 0.0663, 0.0554, 0.0736, 0.0648, 0.0751, 0.0568, 0.0536,
        0.0842, 0.0841, 0.0687, 0.0464, 0.0816, 0.0587, 0.0723, 0.0810, 0.0856,
        0.0849, 0.0731, 0.0407, 0.0829, 0.0432, 0.0937, 0.0796, 0.0591, 0.0734,
        0.0611, 0.0710, 0.0897, 0.0497, 0.0912, 0.0752, 0.0889, 0.0817, 0.0556,
        0.0617, 0.0487, 0.0890, 0.0832, 0.0869, 0.0730, 0.0461, 0.0584, 0.0821,
        0.0754, 0.0865, 0.0402, 0.0887, 0.0986, 0.0154, 0.0270, 0.0081, 0.0821,
        0.0888, 0.1035, 0.0015, 0.0746, 0.0885, 0.0867, 0.1013, 0.0753, 0.1008,
        0.0973, 0.0661, 0.0087, 0.1091, 0.0992, 0.0962, 0.1068, 0.0692, 0.0916,
        0.0110, 0.0839, 0.1064, 0.1019, 0.0923, 0.0181, 0.0796, 0.0489, 0.0576,
        0.0144, 0.0981, 0.0624, 0.0120, 0.0975, 0.0898, 0.0977, 0.0742, 0.0602,
        0.0643, 0.0859, 0.0928, 0.0831, 0.1103, 0.0065, 0.0778, 0.0894, 0.0969,
        0.0788, 0.1073, 0.0142, 0.0945, 0.0750, 0.0572, 0.1032, 0.0523, 0.0503,
        0.0730, 0.0955, 0.0998, 0.0733, 0.1074, 0.0121, 0.0377, 0.0925, 0.0056,
        0.0782, 0.0938, 0.0963, 0.0872, 0.0873, 0.0089, 0.0922, 0.0921, 0.0975,
        0.0476, 0.0346, 0.0846, 0.0831, 0.0528, 0.0088, 0.0999, 0.0770, 0.0896,
        0.0890, 0.0904, 0.0820, 0.1017, 0.0721, 0.0756, 0.0372, 0.0786, 0.0198,
        0.0641, 0.0601, 0.0837, 0.0200, 0.0891, 0.0845, 0.0110, 0.0102, 0.0150,
        0.0519, 0.1011, 0.0944, 0.0997, 0.0027, 0.0265, 0.0689, 0.0980, 0.1014,
        0.0734, 0.1062, 0.0951, 0.1007, 0.0956, 0.1007, 0.1018, 0.1077, 0.0391,
        0.0944, 0.1030, 0.0993, 0.0796, 0.1093, 0.1107, 0.1154, 0.1072, 0.1121,
        0.1050, 0.1059, 0.1076, 0.1128, 0.0988, 0.1067, 0.1122, 0.1048, 0.1056,
        0.1034, 0.1055, 0.1098, 0.1077, 0.1096, 0.1093, 0.1062, 0.1132, 0.1115,
        0.1061, 0.1080, 0.1149, 0.1125, 0.1111, 0.1096, 0.1047, 0.1076, 0.1146,
        0.1067, 0.1103, 0.1027, 0.1064, 0.1121, 0.1059, 0.1043, 0.1076, 0.1059,
        0.1074, 0.0929, 0.1087, 0.1153, 0.1149, 0.1103, 0.1077, 0.1088, 0.1135,
        0.1076, 0.1046, 0.1049, 0.1094, 0.1016, 0.1090, 0.1157, 0.1064, 0.1014,
        0.1094, 0.0965, 0.1133, 0.1140, 0.1175, 0.1133, 0.1083, 0.1115, 0.1123,
        0.1083, 0.1097, 0.1051, 0.1087, 0.1023, 0.1114, 0.1138, 0.1083, 0.1087,
        0.1039, 0.1166, 0.0958, 0.0922, 0.1126, 0.1092, 0.1029, 0.1100, 0.1083,
        0.1084, 0.1042, 0.1031, 0.0941, 0.1072, 0.1151, 0.1027, 0.1150, 0.1061,
        0.1050, 0.1104, 0.0988, 0.1060, 0.1129, 0.1063, 0.1087, 0.0956, 0.1136,
        0.1102, 0.1126, 0.1133, 0.1068, 0.1107, 0.1080, 0.1127, 0.1115, 0.1111,
        0.1104, 0.1146, 0.1050, 0.1054, 0.1022, 0.1150, 0.1130, 0.1054, 0.0897,
        0.1077, 0.1138, 0.1107, 0.1069, 0.1157, 0.1053], device='cuda:0',
       requires_grad=True) MLP.norm tensor(6.6177, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:44:00,225 :: INFO :: Epoch 30: loss tensor(198.6883, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0741, 0.0675, 0.1180, 0.0898, 0.0525, 0.1007, 0.0548, 0.0599, 0.0535,
        0.0512, 0.0826, 0.0830, 0.0526, 0.0801, 0.0997, 0.0819, 0.0600, 0.0568,
        0.0624, 0.0817, 0.0779, 0.0419, 0.0777, 0.0902, 0.1009, 0.0574, 0.0498,
        0.1026, 0.0890, 0.0662, 0.0972, 0.0580, 0.0533, 0.0647, 0.0853, 0.0787,
        0.0573, 0.0882, 0.0692, 0.0670, 0.0635, 0.0477, 0.0830, 0.0751, 0.0773,
        0.0659, 0.0647, 0.0726, 0.0767, 0.0481, 0.0995, 0.0589, 0.0589, 0.0504,
        0.0968, 0.1052, 0.0437, 0.1170, 0.0853, 0.0947, 0.0533, 0.0631, 0.0523,
        0.0744, 0.0841, 0.0569, 0.0484, 0.0767, 0.0654, 0.0809, 0.0734, 0.0590,
        0.0568, 0.0647, 0.0536, 0.0877, 0.0595, 0.0815, 0.0548, 0.0549, 0.1009,
        0.0864, 0.1007, 0.0700, 0.0571, 0.0767, 0.0658, 0.0809, 0.0594, 0.0555,
        0.0938, 0.0927, 0.0709, 0.0451, 0.0904, 0.0587, 0.0759, 0.0869, 0.0938,
        0.0936, 0.0797, 0.0418, 0.0915, 0.0433, 0.1060, 0.0872, 0.0605, 0.0770,
        0.0622, 0.0743, 0.1006, 0.0504, 0.1010, 0.0787, 0.1009, 0.0904, 0.0571,
        0.0632, 0.0493, 0.1009, 0.0898, 0.0962, 0.0790, 0.0468, 0.0611, 0.0883,
        0.0831, 0.0942, 0.0347, 0.0966, 0.1125, 0.0127, 0.0236, 0.0070, 0.0910,
        0.1044, 0.1251, 0.0009, 0.0792, 0.1028, 0.0921, 0.1186, 0.0798, 0.1160,
        0.1097, 0.0628, 0.0070, 0.1291, 0.1130, 0.1115, 0.1257, 0.0698, 0.1012,
        0.0260, 0.0934, 0.1268, 0.1194, 0.1130, 0.0171, 0.0984, 0.0433, 0.0761,
        0.0133, 0.1118, 0.0605, 0.0117, 0.1143, 0.1045, 0.1133, 0.0774, 0.0582,
        0.0618, 0.1054, 0.1023, 0.0873, 0.1310, 0.0066, 0.0796, 0.1009, 0.1099,
        0.0833, 0.1265, 0.0117, 0.1149, 0.0751, 0.0533, 0.1228, 0.0482, 0.0476,
        0.0933, 0.1156, 0.1159, 0.0728, 0.1273, 0.0088, 0.0321, 0.1045, 0.0052,
        0.0806, 0.1083, 0.1080, 0.0976, 0.0945, 0.0083, 0.1011, 0.1040, 0.1128,
        0.0657, 0.0277, 0.0966, 0.0869, 0.0469, 0.0084, 0.1172, 0.0784, 0.0970,
        0.1028, 0.1068, 0.0844, 0.1219, 0.0733, 0.0805, 0.0328, 0.0806, 0.0182,
        0.0640, 0.0571, 0.0892, 0.0168, 0.0974, 0.0899, 0.0088, 0.0087, 0.0140,
        0.0453, 0.1197, 0.1100, 0.1157, 0.0025, 0.0249, 0.0713, 0.1130, 0.1198,
        0.0747, 0.1248, 0.1073, 0.1218, 0.1067, 0.1157, 0.1177, 0.1287, 0.0329,
        0.1092, 0.1187, 0.1219, 0.0861, 0.1332, 0.1331, 0.1404, 0.1245, 0.1335,
        0.1253, 0.1289, 0.1305, 0.1339, 0.1196, 0.1239, 0.1331, 0.1246, 0.1212,
        0.1235, 0.1271, 0.1316, 0.1277, 0.1306, 0.1311, 0.1296, 0.1348, 0.1321,
        0.1269, 0.1294, 0.1401, 0.1328, 0.1347, 0.1318, 0.1219, 0.1305, 0.1375,
        0.1241, 0.1337, 0.1225, 0.1282, 0.1366, 0.1243, 0.1248, 0.1262, 0.1234,
        0.1284, 0.1039, 0.1271, 0.1395, 0.1367, 0.1330, 0.1328, 0.1309, 0.1368,
        0.1246, 0.1260, 0.1249, 0.1338, 0.1203, 0.1316, 0.1389, 0.1265, 0.1231,
        0.1294, 0.1125, 0.1372, 0.1356, 0.1414, 0.1349, 0.1308, 0.1318, 0.1329,
        0.1299, 0.1293, 0.1295, 0.1313, 0.1215, 0.1336, 0.1359, 0.1309, 0.1321,
        0.1265, 0.1404, 0.1076, 0.1041, 0.1350, 0.1308, 0.1207, 0.1309, 0.1303,
        0.1314, 0.1241, 0.1236, 0.1061, 0.1254, 0.1375, 0.1218, 0.1377, 0.1275,
        0.1240, 0.1333, 0.1185, 0.1271, 0.1342, 0.1305, 0.1293, 0.1090, 0.1351,
        0.1331, 0.1377, 0.1343, 0.1323, 0.1310, 0.1257, 0.1340, 0.1342, 0.1307,
        0.1331, 0.1374, 0.1308, 0.1244, 0.1212, 0.1394, 0.1341, 0.1232, 0.0986,
        0.1280, 0.1387, 0.1304, 0.1295, 0.1393, 0.1230], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.0419, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:44:04,240 :: INFO :: Epoch 35: loss tensor(196.8482, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0757, 0.0693, 0.1326, 0.0958, 0.0521, 0.1078, 0.0540, 0.0611, 0.0546,
        0.0516, 0.0858, 0.0872, 0.0518, 0.0834, 0.1079, 0.0845, 0.0600, 0.0584,
        0.0625, 0.0853, 0.0813, 0.0403, 0.0792, 0.0974, 0.1077, 0.0573, 0.0510,
        0.1141, 0.0933, 0.0672, 0.1039, 0.0587, 0.0530, 0.0639, 0.0930, 0.0836,
        0.0578, 0.0914, 0.0711, 0.0675, 0.0649, 0.0483, 0.0843, 0.0774, 0.0796,
        0.0664, 0.0659, 0.0770, 0.0784, 0.0480, 0.1073, 0.0603, 0.0587, 0.0524,
        0.1024, 0.1147, 0.0433, 0.1318, 0.0912, 0.1027, 0.0527, 0.0648, 0.0509,
        0.0761, 0.0872, 0.0583, 0.0483, 0.0791, 0.0652, 0.0844, 0.0748, 0.0600,
        0.0566, 0.0649, 0.0541, 0.0934, 0.0592, 0.0857, 0.0558, 0.0547, 0.1085,
        0.0897, 0.1066, 0.0723, 0.0583, 0.0784, 0.0661, 0.0840, 0.0603, 0.0571,
        0.1017, 0.0982, 0.0719, 0.0444, 0.0960, 0.0585, 0.0779, 0.0908, 0.1004,
        0.1003, 0.0832, 0.0423, 0.0972, 0.0434, 0.1160, 0.0928, 0.0615, 0.0788,
        0.0632, 0.0768, 0.1087, 0.0507, 0.1078, 0.0803, 0.1105, 0.0973, 0.0586,
        0.0629, 0.0500, 0.1108, 0.0938, 0.1029, 0.0826, 0.0466, 0.0630, 0.0915,
        0.0878, 0.0991, 0.0292, 0.1002, 0.1234, 0.0102, 0.0211, 0.0068, 0.0950,
        0.1169, 0.1447, 0.0011, 0.0786, 0.1134, 0.0922, 0.1332, 0.0794, 0.1277,
        0.1182, 0.0568, 0.0060, 0.1464, 0.1230, 0.1234, 0.1427, 0.0660, 0.1064,
        0.0392, 0.0987, 0.1451, 0.1344, 0.1314, 0.0154, 0.1123, 0.0368, 0.0889,
        0.0119, 0.1219, 0.0561, 0.0110, 0.1285, 0.1158, 0.1259, 0.0757, 0.0546,
        0.0564, 0.1222, 0.1073, 0.0868, 0.1499, 0.0050, 0.0769, 0.1084, 0.1200,
        0.0831, 0.1435, 0.0107, 0.1331, 0.0710, 0.0485, 0.1406, 0.0426, 0.0439,
        0.1094, 0.1322, 0.1291, 0.0691, 0.1453, 0.0070, 0.0277, 0.1118, 0.0051,
        0.0783, 0.1190, 0.1158, 0.1025, 0.0976, 0.0070, 0.1055, 0.1113, 0.1249,
        0.0772, 0.0227, 0.1048, 0.0851, 0.0402, 0.0071, 0.1322, 0.0755, 0.0996,
        0.1132, 0.1204, 0.0820, 0.1399, 0.0692, 0.0803, 0.0294, 0.0781, 0.0169,
        0.0603, 0.0515, 0.0903, 0.0145, 0.1011, 0.0907, 0.0075, 0.0082, 0.0136,
        0.0383, 0.1353, 0.1225, 0.1292, 0.0023, 0.0231, 0.0691, 0.1248, 0.1356,
        0.0723, 0.1414, 0.1159, 0.1412, 0.1136, 0.1277, 0.1310, 0.1464, 0.0283,
        0.1200, 0.1307, 0.1428, 0.0879, 0.1556, 0.1545, 0.1646, 0.1391, 0.1539,
        0.1435, 0.1507, 0.1523, 0.1535, 0.1375, 0.1394, 0.1530, 0.1430, 0.1340,
        0.1424, 0.1473, 0.1522, 0.1466, 0.1499, 0.1516, 0.1516, 0.1550, 0.1514,
        0.1458, 0.1493, 0.1643, 0.1518, 0.1575, 0.1516, 0.1365, 0.1521, 0.1589,
        0.1400, 0.1559, 0.1399, 0.1487, 0.1601, 0.1407, 0.1443, 0.1431, 0.1390,
        0.1486, 0.1118, 0.1433, 0.1630, 0.1573, 0.1545, 0.1571, 0.1513, 0.1592,
        0.1388, 0.1464, 0.1429, 0.1566, 0.1374, 0.1527, 0.1608, 0.1455, 0.1431,
        0.1477, 0.1255, 0.1601, 0.1561, 0.1649, 0.1550, 0.1522, 0.1502, 0.1526,
        0.1494, 0.1477, 0.1530, 0.1527, 0.1390, 0.1545, 0.1565, 0.1525, 0.1544,
        0.1477, 0.1632, 0.1162, 0.1122, 0.1562, 0.1506, 0.1361, 0.1502, 0.1511,
        0.1531, 0.1413, 0.1422, 0.1131, 0.1411, 0.1592, 0.1390, 0.1591, 0.1475,
        0.1409, 0.1548, 0.1367, 0.1459, 0.1539, 0.1538, 0.1487, 0.1194, 0.1552,
        0.1550, 0.1620, 0.1542, 0.1570, 0.1503, 0.1415, 0.1538, 0.1562, 0.1488,
        0.1546, 0.1593, 0.1557, 0.1412, 0.1378, 0.1628, 0.1535, 0.1390, 0.1034,
        0.1464, 0.1633, 0.1484, 0.1511, 0.1616, 0.1387], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.3578, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:44:08,225 :: INFO :: Epoch 40: loss tensor(197.6750, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0756, 0.0686, 0.1439, 0.0987, 0.0514, 0.1121, 0.0526, 0.0610, 0.0538,
        0.0500, 0.0861, 0.0896, 0.0502, 0.0842, 0.1139, 0.0851, 0.0587, 0.0581,
        0.0620, 0.0875, 0.0825, 0.0384, 0.0789, 0.1018, 0.1120, 0.0560, 0.0505,
        0.1221, 0.0946, 0.0663, 0.1068, 0.0577, 0.0514, 0.0616, 0.0979, 0.0860,
        0.0567, 0.0931, 0.0709, 0.0673, 0.0645, 0.0477, 0.0834, 0.0783, 0.0795,
        0.0650, 0.0652, 0.0785, 0.0784, 0.0471, 0.1116, 0.0605, 0.0575, 0.0515,
        0.1043, 0.1214, 0.0413, 0.1431, 0.0937, 0.1080, 0.0513, 0.0647, 0.0492,
        0.0752, 0.0870, 0.0583, 0.0471, 0.0793, 0.0639, 0.0856, 0.0745, 0.0587,
        0.0555, 0.0642, 0.0536, 0.0966, 0.0578, 0.0868, 0.0547, 0.0528, 0.1127,
        0.0911, 0.1094, 0.0725, 0.0578, 0.0780, 0.0648, 0.0842, 0.0601, 0.0581,
        0.1062, 0.1004, 0.0713, 0.0432, 0.0988, 0.0562, 0.0777, 0.0916, 0.1049,
        0.1032, 0.0836, 0.0407, 0.0999, 0.0434, 0.1230, 0.0957, 0.0602, 0.0783,
        0.0629, 0.0768, 0.1139, 0.0500, 0.1119, 0.0803, 0.1165, 0.1013, 0.0583,
        0.0614, 0.0481, 0.1171, 0.0952, 0.1068, 0.0836, 0.0456, 0.0629, 0.0917,
        0.0903, 0.1010, 0.0258, 0.1008, 0.1314, 0.0085, 0.0184, 0.0064, 0.0961,
        0.1269, 0.1630, 0.0008, 0.0754, 0.1212, 0.0894, 0.1458, 0.0762, 0.1379,
        0.1234, 0.0499, 0.0056, 0.1620, 0.1301, 0.1324, 0.1580, 0.0620, 0.1081,
        0.0525, 0.1004, 0.1615, 0.1473, 0.1486, 0.0141, 0.1261, 0.0319, 0.1030,
        0.0113, 0.1292, 0.0518, 0.0103, 0.1407, 0.1245, 0.1358, 0.0717, 0.0505,
        0.0507, 0.1375, 0.1088, 0.0832, 0.1676, 0.0059, 0.0715, 0.1127, 0.1276,
        0.0798, 0.1588, 0.0100, 0.1501, 0.0649, 0.0440, 0.1571, 0.0376, 0.0411,
        0.1257, 0.1482, 0.1400, 0.0637, 0.1620, 0.0057, 0.0242, 0.1165, 0.0043,
        0.0735, 0.1268, 0.1204, 0.1049, 0.0976, 0.0071, 0.1063, 0.1158, 0.1343,
        0.0907, 0.0190, 0.1099, 0.0813, 0.0342, 0.0076, 0.1452, 0.0704, 0.0990,
        0.1204, 0.1318, 0.0771, 0.1569, 0.0642, 0.0774, 0.0277, 0.0745, 0.0150,
        0.0565, 0.0455, 0.0887, 0.0128, 0.1015, 0.0884, 0.0066, 0.0073, 0.0119,
        0.0321, 0.1487, 0.1322, 0.1404, 0.0017, 0.0209, 0.0649, 0.1336, 0.1491,
        0.0686, 0.1563, 0.1214, 0.1594, 0.1173, 0.1372, 0.1417, 0.1634, 0.0251,
        0.1275, 0.1399, 0.1624, 0.0864, 0.1767, 0.1743, 0.1879, 0.1509, 0.1727,
        0.1597, 0.1714, 0.1724, 0.1705, 0.1528, 0.1523, 0.1707, 0.1594, 0.1447,
        0.1595, 0.1657, 0.1711, 0.1634, 0.1675, 0.1707, 0.1718, 0.1739, 0.1690,
        0.1629, 0.1677, 0.1874, 0.1687, 0.1790, 0.1694, 0.1486, 0.1729, 0.1793,
        0.1530, 0.1771, 0.1565, 0.1679, 0.1824, 0.1549, 0.1618, 0.1576, 0.1520,
        0.1669, 0.1153, 0.1577, 0.1851, 0.1759, 0.1747, 0.1803, 0.1697, 0.1798,
        0.1508, 0.1648, 0.1592, 0.1783, 0.1521, 0.1720, 0.1819, 0.1620, 0.1615,
        0.1641, 0.1362, 0.1815, 0.1754, 0.1875, 0.1737, 0.1720, 0.1670, 0.1704,
        0.1670, 0.1639, 0.1751, 0.1730, 0.1545, 0.1748, 0.1762, 0.1731, 0.1755,
        0.1677, 0.1850, 0.1214, 0.1171, 0.1764, 0.1675, 0.1482, 0.1675, 0.1702,
        0.1739, 0.1563, 0.1578, 0.1177, 0.1551, 0.1790, 0.1536, 0.1797, 0.1663,
        0.1560, 0.1751, 0.1528, 0.1637, 0.1715, 0.1758, 0.1664, 0.1275, 0.1738,
        0.1752, 0.1852, 0.1716, 0.1803, 0.1671, 0.1551, 0.1723, 0.1770, 0.1653,
        0.1748, 0.1804, 0.1798, 0.1551, 0.1521, 0.1848, 0.1707, 0.1522, 0.1050,
        0.1630, 0.1869, 0.1647, 0.1714, 0.1830, 0.1516], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.5765, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:44:08,240 :: INFO :: ----- frontend -----
2023-05-14 18:44:08,240 :: INFO :: Environment 0
2023-05-14 18:44:17,475 :: INFO :: Epoch 5: loss tensor(822.8890, device='cuda:0'), U.norm 13.263124465942383, V.norm 16.832990646362305, MLP.norm 1.5109163522720337
2023-05-14 18:44:17,709 :: INFO :: Epoch 10: loss tensor(806.2885, device='cuda:0'), U.norm 10.556387901306152, V.norm 16.098594665527344, MLP.norm 2.790479898452759
2023-05-14 18:44:17,943 :: INFO :: Epoch 15: loss tensor(780.0790, device='cuda:0'), U.norm 8.977272987365723, V.norm 15.73731517791748, MLP.norm 4.435971260070801
2023-05-14 18:44:18,162 :: INFO :: Epoch 20: loss tensor(748.1486, device='cuda:0'), U.norm 7.955288887023926, V.norm 15.492949485778809, MLP.norm 6.105287551879883
2023-05-14 18:44:18,178 :: INFO :: Environment 1
2023-05-14 18:44:27,631 :: INFO :: Epoch 5: loss tensor(737.1567, device='cuda:0'), U.norm 14.71830940246582, V.norm 17.707233428955078, MLP.norm 1.525078296661377
2023-05-14 18:44:27,818 :: INFO :: Epoch 10: loss tensor(725.2401, device='cuda:0'), U.norm 12.18832778930664, V.norm 17.0710391998291, MLP.norm 2.4000470638275146
2023-05-14 18:44:28,006 :: INFO :: Epoch 15: loss tensor(705.4636, device='cuda:0'), U.norm 10.612478256225586, V.norm 16.786144256591797, MLP.norm 3.667787551879883
2023-05-14 18:44:28,147 :: INFO :: Epoch 20: loss tensor(680.9070, device='cuda:0'), U.norm 9.517948150634766, V.norm 16.60577392578125, MLP.norm 5.037581920623779
2023-05-14 18:44:28,162 :: INFO :: Environment 2
2023-05-14 18:44:37,365 :: INFO :: Epoch 5: loss tensor(854.4619, device='cuda:0'), U.norm 13.266634941101074, V.norm 16.875829696655273, MLP.norm 1.5138895511627197
2023-05-14 18:44:37,584 :: INFO :: Epoch 10: loss tensor(836.1941, device='cuda:0'), U.norm 10.564665794372559, V.norm 16.160411834716797, MLP.norm 2.833326816558838
2023-05-14 18:44:37,818 :: INFO :: Epoch 15: loss tensor(806.6884, device='cuda:0'), U.norm 8.991414070129395, V.norm 15.807415008544922, MLP.norm 4.537137031555176
2023-05-14 18:44:38,037 :: INFO :: Epoch 20: loss tensor(771.6609, device='cuda:0'), U.norm 7.975274562835693, V.norm 15.568206787109375, MLP.norm 6.260405540466309
2023-05-14 18:44:38,053 :: INFO :: Environment 3
2023-05-14 18:44:47,412 :: INFO :: Epoch 5: loss tensor(899.3395, device='cuda:0'), U.norm 13.276338577270508, V.norm 16.945802688598633, MLP.norm 1.531456470489502
2023-05-14 18:44:47,646 :: INFO :: Epoch 10: loss tensor(879.5978, device='cuda:0'), U.norm 10.5853910446167, V.norm 16.262969970703125, MLP.norm 2.8784635066986084
2023-05-14 18:44:47,881 :: INFO :: Epoch 15: loss tensor(848.4118, device='cuda:0'), U.norm 9.024953842163086, V.norm 15.924729347229004, MLP.norm 4.6257171630859375
2023-05-14 18:44:48,115 :: INFO :: Epoch 20: loss tensor(809.7092, device='cuda:0'), U.norm 8.02388858795166, V.norm 15.695075035095215, MLP.norm 6.3985748291015625
2023-05-14 18:44:48,131 :: INFO :: Environment 4
2023-05-14 18:44:57,444 :: INFO :: Epoch 5: loss tensor(860.1146, device='cuda:0'), U.norm 13.269386291503906, V.norm 16.87199592590332, MLP.norm 1.5611848831176758
2023-05-14 18:44:57,678 :: INFO :: Epoch 10: loss tensor(841.3038, device='cuda:0'), U.norm 10.57036304473877, V.norm 16.157306671142578, MLP.norm 2.9178848266601562
2023-05-14 18:44:57,912 :: INFO :: Epoch 15: loss tensor(812.7294, device='cuda:0'), U.norm 8.998568534851074, V.norm 15.806513786315918, MLP.norm 4.630619049072266
2023-05-14 18:44:58,162 :: INFO :: Epoch 20: loss tensor(778.6561, device='cuda:0'), U.norm 7.98267936706543, V.norm 15.570127487182617, MLP.norm 6.353709697723389
2023-05-14 18:44:58,178 :: INFO :: Environment 5
2023-05-14 18:45:07,694 :: INFO :: Epoch 5: loss tensor(827.8419, device='cuda:0'), U.norm 13.261857032775879, V.norm 16.83074951171875, MLP.norm 1.518411636352539
2023-05-14 18:45:07,943 :: INFO :: Epoch 10: loss tensor(809.9802, device='cuda:0'), U.norm 10.554926872253418, V.norm 16.095054626464844, MLP.norm 2.813476324081421
2023-05-14 18:45:08,162 :: INFO :: Epoch 15: loss tensor(781.9587, device='cuda:0'), U.norm 8.975650787353516, V.norm 15.733183860778809, MLP.norm 4.463759422302246
2023-05-14 18:45:08,397 :: INFO :: Epoch 20: loss tensor(748.8450, device='cuda:0'), U.norm 7.952915191650391, V.norm 15.487950325012207, MLP.norm 6.13175630569458
2023-05-14 18:45:08,412 :: INFO :: Environment 6
2023-05-14 18:45:17,772 :: INFO :: Epoch 5: loss tensor(823.7991, device='cuda:0'), U.norm 13.264546394348145, V.norm 16.818004608154297, MLP.norm 1.531197428703308
2023-05-14 18:45:17,990 :: INFO :: Epoch 10: loss tensor(805.7697, device='cuda:0'), U.norm 10.558911323547363, V.norm 16.080720901489258, MLP.norm 2.8365612030029297
2023-05-14 18:45:18,225 :: INFO :: Epoch 15: loss tensor(777.5372, device='cuda:0'), U.norm 8.98017406463623, V.norm 15.719612121582031, MLP.norm 4.4724040031433105
2023-05-14 18:45:18,459 :: INFO :: Epoch 20: loss tensor(744.7174, device='cuda:0'), U.norm 7.957509994506836, V.norm 15.477060317993164, MLP.norm 6.112488746643066
2023-05-14 18:45:18,475 :: INFO :: Environment 7
2023-05-14 18:45:28,006 :: INFO :: Epoch 5: loss tensor(836.6802, device='cuda:0'), U.norm 13.266668319702148, V.norm 16.845603942871094, MLP.norm 1.5710999965667725
2023-05-14 18:45:28,240 :: INFO :: Epoch 10: loss tensor(817.6686, device='cuda:0'), U.norm 10.564319610595703, V.norm 16.118450164794922, MLP.norm 2.903897762298584
2023-05-14 18:45:28,490 :: INFO :: Epoch 15: loss tensor(788.5531, device='cuda:0'), U.norm 8.989770889282227, V.norm 15.758312225341797, MLP.norm 4.559482097625732
2023-05-14 18:45:28,725 :: INFO :: Epoch 20: loss tensor(754.8024, device='cuda:0'), U.norm 7.9719648361206055, V.norm 15.515469551086426, MLP.norm 6.222979545593262
2023-05-14 18:45:28,725 :: INFO :: Environment 8
2023-05-14 18:45:38,256 :: INFO :: Epoch 5: loss tensor(894.4894, device='cuda:0'), U.norm 13.278480529785156, V.norm 16.944520950317383, MLP.norm 1.563374638557434
2023-05-14 18:45:38,506 :: INFO :: Epoch 10: loss tensor(874.5935, device='cuda:0'), U.norm 10.589582443237305, V.norm 16.256946563720703, MLP.norm 2.957247257232666
2023-05-14 18:45:38,740 :: INFO :: Epoch 15: loss tensor(843.1343, device='cuda:0'), U.norm 9.030729293823242, V.norm 15.914590835571289, MLP.norm 4.731921672821045
2023-05-14 18:45:38,975 :: INFO :: Epoch 20: loss tensor(805.3164, device='cuda:0'), U.norm 8.03039264678955, V.norm 15.681401252746582, MLP.norm 6.520806312561035
2023-05-14 18:45:38,990 :: INFO :: Environment 9
2023-05-14 18:45:48,506 :: INFO :: Epoch 5: loss tensor(910.3990, device='cuda:0'), U.norm 13.277620315551758, V.norm 16.96880531311035, MLP.norm 1.6195515394210815
2023-05-14 18:45:48,740 :: INFO :: Epoch 10: loss tensor(887.9507, device='cuda:0'), U.norm 10.589564323425293, V.norm 16.29440689086914, MLP.norm 3.0900745391845703
2023-05-14 18:45:48,975 :: INFO :: Epoch 15: loss tensor(853.0906, device='cuda:0'), U.norm 9.03154468536377, V.norm 15.959345817565918, MLP.norm 4.9088969230651855
2023-05-14 18:45:49,209 :: INFO :: Epoch 20: loss tensor(812.9445, device='cuda:0'), U.norm 8.030937194824219, V.norm 15.73095703125, MLP.norm 6.730666160583496
2023-05-14 18:45:49,225 :: INFO :: Ite = 1, Delta = 973
2023-05-14 18:45:49,225 :: INFO :: ----- backend -----
2023-05-14 18:45:53,365 :: INFO :: Epoch 5: loss tensor(210.9289, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0120,  0.0144,  0.0166,  0.0135,  0.0121,  0.0156,  0.0103,  0.0144,
         0.0059,  0.0182,  0.0168,  0.0168,  0.0079,  0.0133,  0.0193,  0.0198,
         0.0180,  0.0172,  0.0189,  0.0155,  0.0117,  0.0177,  0.0187,  0.0077,
         0.0195,  0.0120,  0.0144,  0.0101,  0.0184,  0.0163,  0.0195,  0.0189,
         0.0100,  0.0177,  0.0036,  0.0071,  0.0150,  0.0188,  0.0110,  0.0181,
         0.0132,  0.0069,  0.0188,  0.0154,  0.0132,  0.0184,  0.0147,  0.0021,
         0.0181,  0.0142,  0.0191,  0.0093,  0.0186,  0.0126,  0.0187,  0.0164,
         0.0109,  0.0187,  0.0075,  0.0088,  0.0180,  0.0169,  0.0056,  0.0161,
         0.0153,  0.0049,  0.0180,  0.0110,  0.0192,  0.0139,  0.0190,  0.0074,
         0.0188,  0.0069,  0.0155,  0.0079,  0.0191,  0.0092,  0.0121,  0.0149,
         0.0194,  0.0109,  0.0193,  0.0080,  0.0053,  0.0186,  0.0187,  0.0138,
         0.0073,  0.0044,  0.0108,  0.0147,  0.0187,  0.0153,  0.0107,  0.0184,
         0.0192,  0.0195,  0.0188,  0.0187,  0.0073,  0.0148,  0.0149,  0.0012,
         0.0184,  0.0158,  0.0153,  0.0185,  0.0138,  0.0186,  0.0186,  0.0106,
         0.0187,  0.0174,  0.0136,  0.0112,  0.0125,  0.0174,  0.0131,  0.0135,
         0.0189,  0.0154,  0.0086,  0.0108,  0.0041,  0.0191,  0.0082,  0.0189,
         0.0150,  0.0196,  0.0199,  0.0182,  0.0167,  0.0099,  0.0074,  0.0046,
         0.0099,  0.0089,  0.0091,  0.0077,  0.0191,  0.0164,  0.0083,  0.0197,
         0.0189,  0.0196,  0.0082,  0.0163,  0.0197,  0.0126,  0.0193,  0.0150,
         0.0197, -0.0161,  0.0110,  0.0154,  0.0164,  0.0032,  0.0182,  0.0033,
         0.0194, -0.0077,  0.0118,  0.0198,  0.0197,  0.0118,  0.0143,  0.0076,
         0.0154,  0.0112,  0.0155,  0.0146,  0.0013,  0.0187,  0.0192,  0.0194,
         0.0065,  0.0167,  0.0129,  0.0194,  0.0114,  0.0193,  0.0128,  0.0046,
         0.0194,  0.0160,  0.0131,  0.0123,  0.0136, -0.0086,  0.0085,  0.0163,
         0.0198,  0.0161,  0.0101,  0.0147,  0.0149,  0.0109,  0.0195,  0.0116,
         0.0197,  0.0106,  0.0199,  0.0121,  0.0194,  0.0126,  0.0158, -0.0113,
         0.0167,  0.0060,  0.0194,  0.0195,  0.0091,  0.0148,  0.0186,  0.0180,
         0.0077,  0.0060,  0.0191,  0.0129,  0.0124,  0.0086,  0.0172,  0.0196,
         0.0147,  0.0104,  0.0098,  0.0146,  0.0092,  0.0194,  0.0196,  0.0157,
         0.0184,  0.0130,  0.0194,  0.0119,  0.0109,  0.0170,  0.0054,  0.0143,
         0.0061,  0.0158,  0.0131,  0.0196,  0.0190,  0.0200,  0.0101,  0.0193,
         0.0194,  0.0196,  0.0129,  0.0186,  0.0099,  0.0191,  0.0033,  0.0102,
         0.0087,  0.0192,  0.0141,  0.0194,  0.0193,  0.0126,  0.0086,  0.0105,
         0.0192,  0.0063,  0.0189,  0.0194,  0.0123,  0.0190,  0.0097,  0.0100,
         0.0163,  0.0147,  0.0158,  0.0159,  0.0083,  0.0190,  0.0196,  0.0126,
         0.0136,  0.0152,  0.0194,  0.0108,  0.0131,  0.0155,  0.0109,  0.0189,
         0.0194,  0.0129,  0.0124,  0.0116,  0.0135,  0.0153,  0.0106,  0.0195,
         0.0187,  0.0160,  0.0129,  0.0193,  0.0155,  0.0196,  0.0156,  0.0065,
         0.0147,  0.0152,  0.0188,  0.0102,  0.0131,  0.0101,  0.0098,  0.0140,
         0.0164,  0.0158,  0.0079,  0.0165,  0.0105,  0.0148,  0.0196,  0.0162,
         0.0196,  0.0133,  0.0197,  0.0189,  0.0138,  0.0196,  0.0049,  0.0133,
         0.0105,  0.0163,  0.0195,  0.0124,  0.0114,  0.0068,  0.0186,  0.0133,
         0.0123,  0.0166,  0.0189,  0.0122,  0.0154,  0.0128,  0.0108,  0.0118,
         0.0094,  0.0150,  0.0193,  0.0191,  0.0103,  0.0196,  0.0110,  0.0135,
         0.0145,  0.0070,  0.0138,  0.0189,  0.0070,  0.0149,  0.0119,  0.0167,
         0.0111,  0.0109,  0.0194,  0.0061,  0.0168,  0.0195,  0.0189,  0.0144,
         0.0192,  0.0145,  0.0164,  0.0022,  0.0147,  0.0111,  0.0189,  0.0198,
         0.0158,  0.0142,  0.0145,  0.0120,  0.0196,  0.0103,  0.0186,  0.0142],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.6603, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:45:57,412 :: INFO :: Epoch 10: loss tensor(209.2040, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0324,  0.0336,  0.0383,  0.0344,  0.0305,  0.0370,  0.0294,  0.0331,
         0.0252,  0.0348,  0.0363,  0.0373,  0.0274,  0.0335,  0.0395,  0.0397,
         0.0352,  0.0342,  0.0361,  0.0348,  0.0317,  0.0327,  0.0376,  0.0292,
         0.0393,  0.0308,  0.0312,  0.0320,  0.0382,  0.0354,  0.0403,  0.0367,
         0.0290,  0.0359,  0.0248,  0.0281,  0.0334,  0.0386,  0.0312,  0.0363,
         0.0313,  0.0257,  0.0379,  0.0351,  0.0333,  0.0367,  0.0334,  0.0228,
         0.0375,  0.0312,  0.0391,  0.0293,  0.0359,  0.0296,  0.0389,  0.0378,
         0.0289,  0.0403,  0.0283,  0.0300,  0.0347,  0.0347,  0.0257,  0.0359,
         0.0360,  0.0247,  0.0338,  0.0315,  0.0372,  0.0338,  0.0374,  0.0271,
         0.0358,  0.0270,  0.0327,  0.0290,  0.0369,  0.0298,  0.0304,  0.0330,
         0.0401,  0.0321,  0.0404,  0.0286,  0.0252,  0.0381,  0.0367,  0.0341,
         0.0269,  0.0238,  0.0320,  0.0358,  0.0376,  0.0324,  0.0319,  0.0361,
         0.0380,  0.0393,  0.0389,  0.0390,  0.0283,  0.0294,  0.0360,  0.0198,
         0.0388,  0.0359,  0.0336,  0.0374,  0.0326,  0.0376,  0.0389,  0.0293,
         0.0392,  0.0368,  0.0348,  0.0320,  0.0309,  0.0357,  0.0293,  0.0350,
         0.0387,  0.0366,  0.0294,  0.0285,  0.0242,  0.0377,  0.0290,  0.0396,
         0.0334,  0.0410,  0.0413,  0.0308,  0.0317,  0.0108,  0.0296,  0.0270,
         0.0326,  0.0054,  0.0307,  0.0299,  0.0396,  0.0386,  0.0298,  0.0415,
         0.0406,  0.0394,  0.0172,  0.0385,  0.0410,  0.0345,  0.0414,  0.0350,
         0.0407, -0.0213,  0.0328,  0.0378,  0.0385,  0.0254,  0.0293,  0.0252,
         0.0375,  0.0099,  0.0167,  0.0413,  0.0392,  0.0145,  0.0365,  0.0300,
         0.0376,  0.0324,  0.0345,  0.0354,  0.0233,  0.0402,  0.0397,  0.0415,
         0.0078,  0.0380,  0.0347,  0.0412,  0.0327,  0.0415,  0.0217,  0.0270,
         0.0400,  0.0345,  0.0352,  0.0318,  0.0322,  0.0095,  0.0311,  0.0383,
         0.0399,  0.0383,  0.0264,  0.0316,  0.0363,  0.0095,  0.0399,  0.0336,
         0.0410,  0.0323,  0.0409,  0.0139,  0.0404,  0.0344,  0.0378,  0.0027,
         0.0350,  0.0282,  0.0395,  0.0382,  0.0109,  0.0370,  0.0392,  0.0386,
         0.0299,  0.0284,  0.0393,  0.0350,  0.0334,  0.0302,  0.0325,  0.0398,
         0.0238,  0.0307,  0.0310,  0.0353,  0.0243,  0.0398,  0.0404,  0.0271,
         0.0288,  0.0185,  0.0389,  0.0343,  0.0332,  0.0391,  0.0041,  0.0261,
         0.0276,  0.0378,  0.0353,  0.0393,  0.0412,  0.0413,  0.0326,  0.0408,
         0.0414,  0.0416,  0.0356,  0.0348,  0.0319,  0.0408,  0.0261,  0.0320,
         0.0321,  0.0421,  0.0373,  0.0417,  0.0419,  0.0356,  0.0318,  0.0336,
         0.0413,  0.0290,  0.0407,  0.0418,  0.0351,  0.0414,  0.0326,  0.0331,
         0.0386,  0.0372,  0.0387,  0.0389,  0.0312,  0.0419,  0.0422,  0.0357,
         0.0366,  0.0386,  0.0421,  0.0339,  0.0361,  0.0379,  0.0340,  0.0419,
         0.0417,  0.0359,  0.0348,  0.0347,  0.0366,  0.0380,  0.0336,  0.0420,
         0.0414,  0.0383,  0.0347,  0.0420,  0.0385,  0.0421,  0.0382,  0.0298,
         0.0376,  0.0383,  0.0413,  0.0329,  0.0359,  0.0332,  0.0323,  0.0367,
         0.0391,  0.0383,  0.0307,  0.0392,  0.0328,  0.0381,  0.0423,  0.0389,
         0.0422,  0.0357,  0.0423,  0.0415,  0.0368,  0.0419,  0.0281,  0.0362,
         0.0334,  0.0392,  0.0422,  0.0355,  0.0346,  0.0300,  0.0414,  0.0352,
         0.0346,  0.0396,  0.0411,  0.0349,  0.0382,  0.0359,  0.0339,  0.0342,
         0.0325,  0.0372,  0.0417,  0.0417,  0.0330,  0.0423,  0.0341,  0.0361,
         0.0375,  0.0296,  0.0366,  0.0417,  0.0303,  0.0372,  0.0342,  0.0395,
         0.0339,  0.0342,  0.0420,  0.0292,  0.0390,  0.0417,  0.0418,  0.0373,
         0.0418,  0.0376,  0.0393,  0.0253,  0.0375,  0.0338,  0.0417,  0.0425,
         0.0384,  0.0355,  0.0375,  0.0349,  0.0422,  0.0334,  0.0416,  0.0361],
       device='cuda:0', requires_grad=True) MLP.norm tensor(4.1642, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:01,459 :: INFO :: Epoch 15: loss tensor(207.2263, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0502,  0.0491,  0.0597,  0.0543,  0.0434,  0.0576,  0.0436,  0.0466,
         0.0397,  0.0455,  0.0533,  0.0548,  0.0417,  0.0518,  0.0589,  0.0581,
         0.0475,  0.0465,  0.0477,  0.0515,  0.0491,  0.0408,  0.0541,  0.0494,
         0.0582,  0.0448,  0.0419,  0.0532,  0.0558,  0.0503,  0.0598,  0.0498,
         0.0429,  0.0491,  0.0450,  0.0473,  0.0465,  0.0574,  0.0481,  0.0504,
         0.0451,  0.0382,  0.0544,  0.0525,  0.0502,  0.0511,  0.0475,  0.0416,
         0.0537,  0.0415,  0.0582,  0.0455,  0.0479,  0.0403,  0.0576,  0.0586,
         0.0402,  0.0618,  0.0472,  0.0504,  0.0453,  0.0474,  0.0415,  0.0530,
         0.0548,  0.0402,  0.0435,  0.0491,  0.0509,  0.0507,  0.0521,  0.0423,
         0.0475,  0.0429,  0.0456,  0.0487,  0.0500,  0.0486,  0.0436,  0.0456,
         0.0599,  0.0513,  0.0607,  0.0465,  0.0405,  0.0547,  0.0498,  0.0518,
         0.0417,  0.0392,  0.0521,  0.0558,  0.0529,  0.0431,  0.0518,  0.0486,
         0.0539,  0.0566,  0.0569,  0.0573,  0.0472,  0.0363,  0.0558,  0.0324,
         0.0582,  0.0535,  0.0475,  0.0529,  0.0474,  0.0532,  0.0578,  0.0423,
         0.0585,  0.0531,  0.0550,  0.0514,  0.0440,  0.0496,  0.0383,  0.0554,
         0.0567,  0.0571,  0.0473,  0.0396,  0.0410,  0.0548,  0.0477,  0.0591,
         0.0429,  0.0608,  0.0624,  0.0287,  0.0347,  0.0088,  0.0508,  0.0497,
         0.0564,  0.0032,  0.0501,  0.0519,  0.0586,  0.0608,  0.0494,  0.0626,
         0.0618,  0.0554,  0.0129,  0.0616,  0.0619,  0.0566,  0.0639,  0.0501,
         0.0607, -0.0160,  0.0534,  0.0609,  0.0609,  0.0491,  0.0279,  0.0477,
         0.0480,  0.0324,  0.0151,  0.0623,  0.0536,  0.0135,  0.0587,  0.0523,
         0.0596,  0.0512,  0.0485,  0.0524,  0.0467,  0.0607,  0.0584,  0.0646,
         0.0064,  0.0571,  0.0556,  0.0624,  0.0520,  0.0642,  0.0196,  0.0505,
         0.0579,  0.0479,  0.0580,  0.0463,  0.0443,  0.0326,  0.0539,  0.0603,
         0.0568,  0.0610,  0.0275,  0.0399,  0.0562,  0.0068,  0.0575,  0.0556,
         0.0618,  0.0524,  0.0603,  0.0114,  0.0603,  0.0551,  0.0595,  0.0237,
         0.0439,  0.0500,  0.0573,  0.0512,  0.0097,  0.0594,  0.0570,  0.0576,
         0.0521,  0.0513,  0.0573,  0.0579,  0.0507,  0.0497,  0.0376,  0.0565,
         0.0246,  0.0462,  0.0484,  0.0542,  0.0264,  0.0591,  0.0591,  0.0228,
         0.0235,  0.0186,  0.0524,  0.0573,  0.0555,  0.0612,  0.0024,  0.0288,
         0.0468,  0.0597,  0.0581,  0.0551,  0.0637,  0.0618,  0.0561,  0.0614,
         0.0631,  0.0635,  0.0590,  0.0433,  0.0538,  0.0625,  0.0504,  0.0523,
         0.0573,  0.0665,  0.0624,  0.0651,  0.0655,  0.0596,  0.0567,  0.0582,
         0.0642,  0.0526,  0.0629,  0.0652,  0.0589,  0.0640,  0.0566,  0.0575,
         0.0619,  0.0608,  0.0627,  0.0630,  0.0555,  0.0662,  0.0656,  0.0602,
         0.0608,  0.0637,  0.0656,  0.0589,  0.0601,  0.0607,  0.0586,  0.0664,
         0.0641,  0.0606,  0.0582,  0.0591,  0.0615,  0.0613,  0.0579,  0.0650,
         0.0650,  0.0617,  0.0561,  0.0655,  0.0631,  0.0657,  0.0624,  0.0554,
         0.0617,  0.0630,  0.0642,  0.0566,  0.0596,  0.0578,  0.0556,  0.0611,
         0.0637,  0.0619,  0.0549,  0.0629,  0.0553,  0.0629,  0.0662,  0.0633,
         0.0658,  0.0599,  0.0658,  0.0652,  0.0608,  0.0649,  0.0535,  0.0605,
         0.0570,  0.0636,  0.0662,  0.0600,  0.0595,  0.0548,  0.0660,  0.0570,
         0.0563,  0.0638,  0.0647,  0.0583,  0.0624,  0.0603,  0.0586,  0.0581,
         0.0568,  0.0583,  0.0650,  0.0656,  0.0563,  0.0664,  0.0583,  0.0595,
         0.0620,  0.0529,  0.0608,  0.0659,  0.0555,  0.0604,  0.0567,  0.0636,
         0.0582,  0.0594,  0.0656,  0.0543,  0.0620,  0.0645,  0.0657,  0.0614,
         0.0652,  0.0621,  0.0637,  0.0512,  0.0613,  0.0575,  0.0662,  0.0666,
         0.0617,  0.0559,  0.0615,  0.0599,  0.0656,  0.0581,  0.0662,  0.0586],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.2542, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:05,475 :: INFO :: Epoch 20: loss tensor(205.3318, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0629,  0.0585,  0.0796,  0.0707,  0.0485,  0.0752,  0.0499,  0.0531,
         0.0475,  0.0492,  0.0658,  0.0672,  0.0482,  0.0647,  0.0757,  0.0724,
         0.0528,  0.0528,  0.0529,  0.0636,  0.0612,  0.0415,  0.0652,  0.0663,
         0.0735,  0.0525,  0.0461,  0.0721,  0.0692,  0.0590,  0.0751,  0.0565,
         0.0491,  0.0562,  0.0611,  0.0614,  0.0529,  0.0721,  0.0593,  0.0582,
         0.0527,  0.0426,  0.0660,  0.0647,  0.0611,  0.0588,  0.0548,  0.0550,
         0.0645,  0.0446,  0.0742,  0.0553,  0.0532,  0.0442,  0.0728,  0.0767,
         0.0437,  0.0819,  0.0622,  0.0674,  0.0495,  0.0552,  0.0494,  0.0642,
         0.0685,  0.0492,  0.0470,  0.0615,  0.0575,  0.0630,  0.0615,  0.0507,
         0.0519,  0.0516,  0.0519,  0.0646,  0.0571,  0.0626,  0.0498,  0.0517,
         0.0766,  0.0661,  0.0770,  0.0590,  0.0486,  0.0662,  0.0570,  0.0642,
         0.0495,  0.0468,  0.0687,  0.0724,  0.0627,  0.0461,  0.0674,  0.0545,
         0.0642,  0.0696,  0.0715,  0.0714,  0.0609,  0.0380,  0.0720,  0.0368,
         0.0747,  0.0669,  0.0536,  0.0625,  0.0554,  0.0630,  0.0738,  0.0484,
         0.0740,  0.0642,  0.0721,  0.0674,  0.0508,  0.0566,  0.0398,  0.0730,
         0.0704,  0.0750,  0.0606,  0.0430,  0.0522,  0.0678,  0.0614,  0.0755,
         0.0419,  0.0768,  0.0816,  0.0212,  0.0309,  0.0089,  0.0684,  0.0705,
         0.0801,  0.0019,  0.0643,  0.0719,  0.0734,  0.0819,  0.0642,  0.0820,
         0.0809,  0.0640,  0.0101,  0.0844,  0.0813,  0.0772,  0.0857,  0.0584,
         0.0776, -0.0025,  0.0710,  0.0838,  0.0821,  0.0722,  0.0214,  0.0689,
         0.0488,  0.0537,  0.0140,  0.0814,  0.0598,  0.0128,  0.0796,  0.0726,
         0.0800,  0.0646,  0.0556,  0.0621,  0.0692,  0.0783,  0.0729,  0.0874,
         0.0058,  0.0707,  0.0741,  0.0813,  0.0671,  0.0864,  0.0153,  0.0734,
         0.0692,  0.0531,  0.0802,  0.0532,  0.0483,  0.0556,  0.0760,  0.0809,
         0.0673,  0.0832,  0.0192,  0.0388,  0.0735,  0.0068,  0.0698,  0.0758,
         0.0802,  0.0692,  0.0762,  0.0101,  0.0776,  0.0734,  0.0796,  0.0448,
         0.0406,  0.0689,  0.0705,  0.0544,  0.0094,  0.0806,  0.0695,  0.0732,
         0.0719,  0.0723,  0.0703,  0.0802,  0.0616,  0.0647,  0.0358,  0.0683,
         0.0231,  0.0550,  0.0582,  0.0687,  0.0227,  0.0755,  0.0741,  0.0151,
         0.0152,  0.0178,  0.0557,  0.0797,  0.0761,  0.0818,  0.0024,  0.0258,
         0.0605,  0.0800,  0.0801,  0.0654,  0.0854,  0.0798,  0.0791,  0.0795,
         0.0832,  0.0838,  0.0821,  0.0421,  0.0737,  0.0829,  0.0750,  0.0686,
         0.0831,  0.0909,  0.0882,  0.0873,  0.0889,  0.0837,  0.0814,  0.0828,
         0.0868,  0.0756,  0.0831,  0.0885,  0.0821,  0.0846,  0.0795,  0.0817,
         0.0852,  0.0834,  0.0864,  0.0868,  0.0799,  0.0902,  0.0885,  0.0840,
         0.0844,  0.0888,  0.0887,  0.0840,  0.0840,  0.0819,  0.0832,  0.0908,
         0.0853,  0.0855,  0.0809,  0.0831,  0.0866,  0.0836,  0.0813,  0.0871,
         0.0876,  0.0846,  0.0752,  0.0880,  0.0882,  0.0892,  0.0865,  0.0814,
         0.0858,  0.0878,  0.0859,  0.0798,  0.0822,  0.0832,  0.0778,  0.0857,
         0.0886,  0.0848,  0.0789,  0.0858,  0.0762,  0.0878,  0.0901,  0.0883,
         0.0892,  0.0837,  0.0888,  0.0885,  0.0845,  0.0872,  0.0791,  0.0849,
         0.0794,  0.0877,  0.0902,  0.0841,  0.0843,  0.0795,  0.0911,  0.0765,
         0.0742,  0.0880,  0.0876,  0.0801,  0.0860,  0.0845,  0.0832,  0.0814,
         0.0800,  0.0769,  0.0871,  0.0895,  0.0786,  0.0906,  0.0822,  0.0821,
         0.0863,  0.0754,  0.0844,  0.0895,  0.0809,  0.0834,  0.0768,  0.0873,
         0.0826,  0.0849,  0.0884,  0.0799,  0.0848,  0.0861,  0.0894,  0.0857,
         0.0880,  0.0865,  0.0881,  0.0779,  0.0843,  0.0804,  0.0911,  0.0902,
         0.0835,  0.0748,  0.0848,  0.0855,  0.0884,  0.0827,  0.0908,  0.0800],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.0384, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:09,475 :: INFO :: Epoch 25: loss tensor(204.3427, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0713, 0.0642, 0.0980, 0.0835, 0.0508, 0.0892, 0.0516, 0.0559, 0.0514,
        0.0506, 0.0751, 0.0752, 0.0514, 0.0735, 0.0892, 0.0816, 0.0553, 0.0569,
        0.0556, 0.0724, 0.0687, 0.0402, 0.0717, 0.0800, 0.0851, 0.0562, 0.0477,
        0.0879, 0.0792, 0.0636, 0.0872, 0.0591, 0.0513, 0.0596, 0.0738, 0.0718,
        0.0555, 0.0825, 0.0659, 0.0619, 0.0579, 0.0447, 0.0737, 0.0721, 0.0686,
        0.0631, 0.0591, 0.0639, 0.0716, 0.0457, 0.0868, 0.0603, 0.0558, 0.0465,
        0.0844, 0.0915, 0.0442, 0.0998, 0.0739, 0.0811, 0.0513, 0.0603, 0.0516,
        0.0710, 0.0774, 0.0540, 0.0483, 0.0696, 0.0602, 0.0717, 0.0673, 0.0552,
        0.0531, 0.0553, 0.0542, 0.0769, 0.0599, 0.0727, 0.0524, 0.0541, 0.0900,
        0.0760, 0.0894, 0.0673, 0.0530, 0.0734, 0.0607, 0.0730, 0.0532, 0.0514,
        0.0815, 0.0851, 0.0684, 0.0454, 0.0799, 0.0571, 0.0706, 0.0792, 0.0835,
        0.0823, 0.0706, 0.0389, 0.0840, 0.0394, 0.0886, 0.0774, 0.0560, 0.0691,
        0.0600, 0.0683, 0.0874, 0.0502, 0.0862, 0.0715, 0.0868, 0.0799, 0.0542,
        0.0601, 0.0419, 0.0878, 0.0803, 0.0890, 0.0697, 0.0446, 0.0577, 0.0778,
        0.0716, 0.0873, 0.0364, 0.0886, 0.0984, 0.0165, 0.0259, 0.0077, 0.0811,
        0.0890, 0.1029, 0.0014, 0.0726, 0.0891, 0.0829, 0.1012, 0.0729, 0.0990,
        0.0966, 0.0657, 0.0082, 0.1057, 0.0980, 0.0951, 0.1064, 0.0613, 0.0908,
        0.0167, 0.0845, 0.1055, 0.1017, 0.0944, 0.0176, 0.0886, 0.0441, 0.0734,
        0.0133, 0.0977, 0.0601, 0.0125, 0.0987, 0.0902, 0.0981, 0.0722, 0.0566,
        0.0644, 0.0903, 0.0919, 0.0821, 0.1094, 0.0051, 0.0774, 0.0890, 0.0975,
        0.0765, 0.1072, 0.0117, 0.0952, 0.0740, 0.0532, 0.1016, 0.0527, 0.0477,
        0.0778, 0.0969, 0.0992, 0.0716, 0.1046, 0.0115, 0.0348, 0.0875, 0.0057,
        0.0764, 0.0930, 0.0954, 0.0815, 0.0874, 0.0086, 0.0909, 0.0883, 0.0972,
        0.0645, 0.0327, 0.0845, 0.0780, 0.0509, 0.0081, 0.1001, 0.0757, 0.0847,
        0.0890, 0.0912, 0.0772, 0.1018, 0.0660, 0.0741, 0.0320, 0.0740, 0.0206,
        0.0581, 0.0600, 0.0784, 0.0193, 0.0880, 0.0839, 0.0117, 0.0099, 0.0152,
        0.0517, 0.1004, 0.0944, 0.1003, 0.0021, 0.0232, 0.0680, 0.0979, 0.1004,
        0.0705, 0.1057, 0.0951, 0.1015, 0.0942, 0.1008, 0.1020, 0.1045, 0.0374,
        0.0910, 0.1008, 0.0988, 0.0798, 0.1078, 0.1145, 0.1137, 0.1071, 0.1112,
        0.1060, 0.1053, 0.1064, 0.1086, 0.0968, 0.1019, 0.1110, 0.1037, 0.1029,
        0.1011, 0.1043, 0.1079, 0.1051, 0.1086, 0.1092, 0.1033, 0.1129, 0.1104,
        0.1062, 0.1070, 0.1141, 0.1107, 0.1084, 0.1066, 0.1008, 0.1066, 0.1142,
        0.1047, 0.1093, 0.1021, 0.1058, 0.1113, 0.1035, 0.1037, 0.1071, 0.1082,
        0.1067, 0.0904, 0.1083, 0.1129, 0.1117, 0.1097, 0.1072, 0.1089, 0.1120,
        0.1049, 0.1023, 0.1027, 0.1081, 0.0984, 0.1092, 0.1122, 0.1066, 0.1016,
        0.1072, 0.0939, 0.1123, 0.1128, 0.1129, 0.1115, 0.1068, 0.1102, 0.1106,
        0.1069, 0.1085, 0.1041, 0.1081, 0.0998, 0.1107, 0.1128, 0.1074, 0.1083,
        0.1029, 0.1153, 0.0920, 0.0882, 0.1110, 0.1099, 0.0997, 0.1081, 0.1076,
        0.1069, 0.1025, 0.1019, 0.0915, 0.1071, 0.1129, 0.0997, 0.1139, 0.1046,
        0.1031, 0.1096, 0.0965, 0.1066, 0.1118, 0.1062, 0.1055, 0.0934, 0.1099,
        0.1065, 0.1102, 0.1103, 0.1052, 0.1067, 0.1057, 0.1115, 0.1094, 0.1097,
        0.1097, 0.1115, 0.1043, 0.1052, 0.1010, 0.1156, 0.1124, 0.1033, 0.0894,
        0.1063, 0.1110, 0.1095, 0.1063, 0.1147, 0.0998], device='cuda:0',
       requires_grad=True) MLP.norm tensor(6.6141, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:13,443 :: INFO :: Epoch 30: loss tensor(203.8992, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0744, 0.0653, 0.1133, 0.0911, 0.0490, 0.0997, 0.0510, 0.0550, 0.0521,
        0.0491, 0.0802, 0.0784, 0.0505, 0.0775, 0.0981, 0.0864, 0.0553, 0.0573,
        0.0566, 0.0772, 0.0718, 0.0379, 0.0742, 0.0896, 0.0931, 0.0562, 0.0472,
        0.0998, 0.0846, 0.0651, 0.0949, 0.0582, 0.0499, 0.0602, 0.0816, 0.0778,
        0.0549, 0.0873, 0.0678, 0.0626, 0.0594, 0.0444, 0.0770, 0.0750, 0.0711,
        0.0636, 0.0591, 0.0690, 0.0752, 0.0446, 0.0947, 0.0610, 0.0553, 0.0471,
        0.0921, 0.1024, 0.0428, 0.1152, 0.0812, 0.0911, 0.0501, 0.0618, 0.0510,
        0.0729, 0.0810, 0.0552, 0.0472, 0.0735, 0.0590, 0.0766, 0.0679, 0.0561,
        0.0510, 0.0562, 0.0535, 0.0845, 0.0596, 0.0791, 0.0515, 0.0529, 0.0987,
        0.0817, 0.0973, 0.0708, 0.0543, 0.0762, 0.0603, 0.0776, 0.0536, 0.0523,
        0.0891, 0.0927, 0.0704, 0.0429, 0.0881, 0.0566, 0.0726, 0.0841, 0.0913,
        0.0880, 0.0767, 0.0387, 0.0917, 0.0394, 0.0987, 0.0837, 0.0563, 0.0720,
        0.0605, 0.0700, 0.0973, 0.0490, 0.0951, 0.0749, 0.0977, 0.0881, 0.0551,
        0.0611, 0.0412, 0.0984, 0.0857, 0.0980, 0.0757, 0.0439, 0.0581, 0.0829,
        0.0777, 0.0947, 0.0302, 0.0949, 0.1117, 0.0130, 0.0220, 0.0075, 0.0886,
        0.1042, 0.1241, 0.0011, 0.0750, 0.1025, 0.0870, 0.1178, 0.0763, 0.1131,
        0.1084, 0.0616, 0.0069, 0.1252, 0.1113, 0.1101, 0.1251, 0.0610, 0.0994,
        0.0358, 0.0928, 0.1255, 0.1187, 0.1144, 0.0148, 0.1054, 0.0375, 0.0901,
        0.0122, 0.1106, 0.0565, 0.0118, 0.1148, 0.1041, 0.1131, 0.0737, 0.0545,
        0.0606, 0.1088, 0.1008, 0.0857, 0.1299, 0.0050, 0.0777, 0.0995, 0.1098,
        0.0799, 0.1261, 0.0112, 0.1149, 0.0723, 0.0504, 0.1209, 0.0484, 0.0454,
        0.0980, 0.1158, 0.1147, 0.0700, 0.1240, 0.0078, 0.0305, 0.0980, 0.0049,
        0.0773, 0.1067, 0.1064, 0.0899, 0.0934, 0.0078, 0.0993, 0.0991, 0.1115,
        0.0811, 0.0251, 0.0954, 0.0807, 0.0437, 0.0075, 0.1169, 0.0759, 0.0913,
        0.1021, 0.1070, 0.0788, 0.1215, 0.0653, 0.0777, 0.0295, 0.0749, 0.0185,
        0.0582, 0.0555, 0.0834, 0.0158, 0.0958, 0.0883, 0.0092, 0.0081, 0.0133,
        0.0442, 0.1186, 0.1094, 0.1159, 0.0022, 0.0221, 0.0691, 0.1124, 0.1183,
        0.0708, 0.1239, 0.1064, 0.1221, 0.1048, 0.1152, 0.1173, 0.1250, 0.0320,
        0.1049, 0.1159, 0.1213, 0.0855, 0.1315, 0.1369, 0.1383, 0.1247, 0.1326,
        0.1263, 0.1281, 0.1290, 0.1290, 0.1159, 0.1182, 0.1326, 0.1238, 0.1181,
        0.1215, 0.1255, 0.1295, 0.1251, 0.1293, 0.1307, 0.1261, 0.1346, 0.1309,
        0.1268, 0.1281, 0.1388, 0.1312, 0.1320, 0.1275, 0.1177, 0.1291, 0.1365,
        0.1223, 0.1323, 0.1214, 0.1275, 0.1357, 0.1214, 0.1250, 0.1257, 0.1268,
        0.1278, 0.1012, 0.1271, 0.1368, 0.1332, 0.1315, 0.1322, 0.1306, 0.1351,
        0.1212, 0.1237, 0.1217, 0.1321, 0.1171, 0.1318, 0.1345, 0.1273, 0.1227,
        0.1269, 0.1093, 0.1358, 0.1345, 0.1366, 0.1327, 0.1292, 0.1299, 0.1317,
        0.1280, 0.1287, 0.1278, 0.1304, 0.1188, 0.1328, 0.1345, 0.1298, 0.1314,
        0.1251, 0.1385, 0.1033, 0.0991, 0.1331, 0.1316, 0.1169, 0.1288, 0.1296,
        0.1298, 0.1213, 0.1222, 0.1023, 0.1252, 0.1350, 0.1195, 0.1362, 0.1260,
        0.1218, 0.1319, 0.1162, 0.1269, 0.1327, 0.1303, 0.1264, 0.1071, 0.1313,
        0.1291, 0.1346, 0.1307, 0.1299, 0.1270, 0.1235, 0.1326, 0.1323, 0.1296,
        0.1322, 0.1343, 0.1298, 0.1247, 0.1199, 0.1395, 0.1328, 0.1210, 0.1000,
        0.1265, 0.1359, 0.1294, 0.1289, 0.1379, 0.1171], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.0315, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:17,490 :: INFO :: Epoch 35: loss tensor(203.6365, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0755, 0.0659, 0.1267, 0.0964, 0.0486, 0.1078, 0.0500, 0.0554, 0.0536,
        0.0501, 0.0838, 0.0823, 0.0500, 0.0793, 0.1060, 0.0903, 0.0552, 0.0575,
        0.0575, 0.0805, 0.0743, 0.0375, 0.0753, 0.0963, 0.0990, 0.0570, 0.0473,
        0.1088, 0.0876, 0.0657, 0.1008, 0.0577, 0.0489, 0.0604, 0.0877, 0.0821,
        0.0547, 0.0906, 0.0696, 0.0628, 0.0610, 0.0448, 0.0783, 0.0767, 0.0723,
        0.0637, 0.0588, 0.0719, 0.0775, 0.0444, 0.1000, 0.0627, 0.0565, 0.0480,
        0.0963, 0.1104, 0.0422, 0.1287, 0.0859, 0.0986, 0.0504, 0.0635, 0.0501,
        0.0743, 0.0829, 0.0572, 0.0474, 0.0754, 0.0589, 0.0802, 0.0679, 0.0566,
        0.0495, 0.0572, 0.0540, 0.0894, 0.0599, 0.0834, 0.0509, 0.0518, 0.1045,
        0.0849, 0.1029, 0.0735, 0.0555, 0.0767, 0.0601, 0.0806, 0.0555, 0.0539,
        0.0944, 0.0974, 0.0718, 0.0422, 0.0932, 0.0560, 0.0749, 0.0879, 0.0977,
        0.0918, 0.0806, 0.0392, 0.0964, 0.0410, 0.1064, 0.0887, 0.0569, 0.0736,
        0.0613, 0.0716, 0.1047, 0.0489, 0.1015, 0.0768, 0.1065, 0.0946, 0.0565,
        0.0623, 0.0422, 0.1070, 0.0897, 0.1047, 0.0794, 0.0449, 0.0591, 0.0864,
        0.0816, 0.1001, 0.0253, 0.0974, 0.1228, 0.0108, 0.0199, 0.0064, 0.0919,
        0.1171, 0.1441, 0.0011, 0.0734, 0.1133, 0.0869, 0.1328, 0.0763, 0.1233,
        0.1172, 0.0553, 0.0061, 0.1427, 0.1222, 0.1224, 0.1426, 0.0576, 0.1048,
        0.0521, 0.0978, 0.1445, 0.1341, 0.1335, 0.0138, 0.1200, 0.0317, 0.1044,
        0.0110, 0.1211, 0.0514, 0.0112, 0.1291, 0.1155, 0.1258, 0.0711, 0.0512,
        0.0546, 0.1261, 0.1062, 0.0860, 0.1493, 0.0040, 0.0747, 0.1068, 0.1194,
        0.0797, 0.1437, 0.0092, 0.1337, 0.0678, 0.0468, 0.1394, 0.0448, 0.0420,
        0.1162, 0.1317, 0.1283, 0.0654, 0.1427, 0.0065, 0.0271, 0.1038, 0.0047,
        0.0748, 0.1174, 0.1144, 0.0935, 0.0958, 0.0068, 0.1044, 0.1055, 0.1235,
        0.0956, 0.0200, 0.1032, 0.0800, 0.0378, 0.0073, 0.1323, 0.0732, 0.0936,
        0.1126, 0.1208, 0.0775, 0.1402, 0.0610, 0.0774, 0.0265, 0.0711, 0.0180,
        0.0550, 0.0490, 0.0845, 0.0152, 0.0999, 0.0893, 0.0078, 0.0070, 0.0142,
        0.0368, 0.1346, 0.1222, 0.1296, 0.0016, 0.0207, 0.0672, 0.1247, 0.1344,
        0.0677, 0.1409, 0.1151, 0.1419, 0.1123, 0.1276, 0.1307, 0.1431, 0.0277,
        0.1153, 0.1281, 0.1429, 0.0878, 0.1540, 0.1577, 0.1619, 0.1391, 0.1526,
        0.1441, 0.1498, 0.1505, 0.1468, 0.1331, 0.1322, 0.1524, 0.1418, 0.1296,
        0.1396, 0.1453, 0.1495, 0.1433, 0.1482, 0.1508, 0.1473, 0.1549, 0.1497,
        0.1453, 0.1475, 0.1621, 0.1496, 0.1538, 0.1465, 0.1316, 0.1504, 0.1577,
        0.1370, 0.1544, 0.1382, 0.1475, 0.1583, 0.1368, 0.1447, 0.1424, 0.1426,
        0.1477, 0.1069, 0.1438, 0.1595, 0.1525, 0.1523, 0.1558, 0.1508, 0.1568,
        0.1344, 0.1429, 0.1384, 0.1550, 0.1331, 0.1527, 0.1562, 0.1466, 0.1426,
        0.1445, 0.1216, 0.1577, 0.1549, 0.1592, 0.1524, 0.1499, 0.1474, 0.1512,
        0.1472, 0.1468, 0.1503, 0.1516, 0.1353, 0.1538, 0.1548, 0.1510, 0.1534,
        0.1460, 0.1606, 0.1096, 0.1056, 0.1541, 0.1505, 0.1311, 0.1473, 0.1501,
        0.1514, 0.1384, 0.1405, 0.1093, 0.1408, 0.1559, 0.1368, 0.1575, 0.1456,
        0.1383, 0.1529, 0.1335, 0.1452, 0.1515, 0.1530, 0.1456, 0.1176, 0.1514,
        0.1501, 0.1578, 0.1490, 0.1537, 0.1452, 0.1391, 0.1522, 0.1537, 0.1478,
        0.1534, 0.1559, 0.1542, 0.1416, 0.1361, 0.1623, 0.1508, 0.1358, 0.1062,
        0.1446, 0.1596, 0.1474, 0.1504, 0.1597, 0.1317], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.3446, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:21,584 :: INFO :: Epoch 40: loss tensor(205.4249, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0768, 0.0674, 0.1386, 0.0998, 0.0479, 0.1126, 0.0494, 0.0562, 0.0549,
        0.0502, 0.0860, 0.0865, 0.0500, 0.0807, 0.1121, 0.0919, 0.0550, 0.0573,
        0.0578, 0.0825, 0.0759, 0.0368, 0.0764, 0.1012, 0.1036, 0.0573, 0.0477,
        0.1160, 0.0891, 0.0664, 0.1052, 0.0580, 0.0491, 0.0607, 0.0936, 0.0849,
        0.0552, 0.0925, 0.0707, 0.0630, 0.0623, 0.0448, 0.0785, 0.0782, 0.0735,
        0.0638, 0.0595, 0.0743, 0.0788, 0.0447, 0.1031, 0.0641, 0.0572, 0.0488,
        0.0990, 0.1163, 0.0415, 0.1408, 0.0892, 0.1043, 0.0499, 0.0636, 0.0483,
        0.0755, 0.0834, 0.0592, 0.0472, 0.0774, 0.0595, 0.0826, 0.0689, 0.0573,
        0.0499, 0.0581, 0.0534, 0.0931, 0.0602, 0.0858, 0.0510, 0.0513, 0.1090,
        0.0871, 0.1062, 0.0751, 0.0562, 0.0765, 0.0596, 0.0826, 0.0574, 0.0559,
        0.0989, 0.1002, 0.0722, 0.0421, 0.0971, 0.0554, 0.0759, 0.0905, 0.1030,
        0.0945, 0.0824, 0.0395, 0.1000, 0.0413, 0.1124, 0.0920, 0.0571, 0.0743,
        0.0619, 0.0730, 0.1098, 0.0502, 0.1065, 0.0779, 0.1140, 0.0995, 0.0571,
        0.0623, 0.0421, 0.1140, 0.0922, 0.1095, 0.0820, 0.0460, 0.0595, 0.0884,
        0.0847, 0.1033, 0.0216, 0.0966, 0.1308, 0.0090, 0.0178, 0.0065, 0.0919,
        0.1271, 0.1626, 0.0008, 0.0689, 0.1207, 0.0837, 0.1454, 0.0736, 0.1319,
        0.1222, 0.0484, 0.0051, 0.1583, 0.1300, 0.1321, 0.1581, 0.0537, 0.1061,
        0.0641, 0.0987, 0.1614, 0.1472, 0.1504, 0.0139, 0.1325, 0.0270, 0.1160,
        0.0104, 0.1285, 0.0472, 0.0105, 0.1409, 0.1236, 0.1356, 0.0660, 0.0483,
        0.0482, 0.1410, 0.1076, 0.0832, 0.1671, 0.0044, 0.0689, 0.1104, 0.1263,
        0.0762, 0.1591, 0.0076, 0.1503, 0.0616, 0.0429, 0.1560, 0.0412, 0.0401,
        0.1298, 0.1460, 0.1389, 0.0598, 0.1594, 0.0050, 0.0243, 0.1066, 0.0049,
        0.0695, 0.1244, 0.1188, 0.0940, 0.0945, 0.0060, 0.1055, 0.1090, 0.1324,
        0.1052, 0.0169, 0.1073, 0.0767, 0.0326, 0.0069, 0.1451, 0.0681, 0.0930,
        0.1197, 0.1316, 0.0731, 0.1569, 0.0560, 0.0742, 0.0241, 0.0659, 0.0168,
        0.0516, 0.0421, 0.0835, 0.0135, 0.1006, 0.0864, 0.0064, 0.0061, 0.0136,
        0.0309, 0.1484, 0.1319, 0.1405, 0.0020, 0.0188, 0.0628, 0.1336, 0.1477,
        0.0631, 0.1557, 0.1204, 0.1601, 0.1161, 0.1369, 0.1414, 0.1598, 0.0243,
        0.1220, 0.1371, 0.1629, 0.0859, 0.1756, 0.1775, 0.1850, 0.1504, 0.1715,
        0.1595, 0.1702, 0.1707, 0.1630, 0.1483, 0.1438, 0.1704, 0.1576, 0.1382,
        0.1562, 0.1633, 0.1683, 0.1591, 0.1653, 0.1697, 0.1678, 0.1740, 0.1671,
        0.1622, 0.1651, 0.1849, 0.1663, 0.1749, 0.1641, 0.1428, 0.1707, 0.1778,
        0.1498, 0.1757, 0.1534, 0.1662, 0.1804, 0.1499, 0.1622, 0.1571, 0.1556,
        0.1662, 0.1092, 0.1583, 0.1813, 0.1704, 0.1727, 0.1786, 0.1697, 0.1771,
        0.1448, 0.1606, 0.1533, 0.1770, 0.1464, 0.1726, 0.1769, 0.1637, 0.1612,
        0.1599, 0.1308, 0.1789, 0.1741, 0.1808, 0.1706, 0.1695, 0.1630, 0.1684,
        0.1649, 0.1629, 0.1719, 0.1716, 0.1495, 0.1738, 0.1736, 0.1710, 0.1744,
        0.1655, 0.1825, 0.1127, 0.1089, 0.1739, 0.1682, 0.1426, 0.1641, 0.1689,
        0.1718, 0.1532, 0.1568, 0.1138, 0.1539, 0.1754, 0.1521, 0.1777, 0.1635,
        0.1528, 0.1730, 0.1493, 0.1622, 0.1684, 0.1745, 0.1632, 0.1240, 0.1699,
        0.1696, 0.1803, 0.1653, 0.1771, 0.1621, 0.1516, 0.1706, 0.1740, 0.1643,
        0.1735, 0.1765, 0.1783, 0.1565, 0.1498, 0.1845, 0.1673, 0.1480, 0.1087,
        0.1607, 0.1827, 0.1636, 0.1705, 0.1810, 0.1437], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.5834, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:46:21,584 :: INFO :: ----- frontend -----
2023-05-14 18:46:21,584 :: INFO :: Environment 0
2023-05-14 18:46:31,069 :: INFO :: Epoch 5: loss tensor(840.3748, device='cuda:0'), U.norm 13.266053199768066, V.norm 16.84870147705078, MLP.norm 1.5382355451583862
2023-05-14 18:46:31,318 :: INFO :: Epoch 10: loss tensor(821.5062, device='cuda:0'), U.norm 10.563305854797363, V.norm 16.119827270507812, MLP.norm 2.8574888706207275
2023-05-14 18:46:31,553 :: INFO :: Epoch 15: loss tensor(791.8987, device='cuda:0'), U.norm 8.988336563110352, V.norm 15.761070251464844, MLP.norm 4.531423568725586
2023-05-14 18:46:31,787 :: INFO :: Epoch 20: loss tensor(757.3172, device='cuda:0'), U.norm 7.96982479095459, V.norm 15.517905235290527, MLP.norm 6.228738784790039
2023-05-14 18:46:31,803 :: INFO :: Environment 1
2023-05-14 18:46:41,131 :: INFO :: Epoch 5: loss tensor(840.0598, device='cuda:0'), U.norm 13.264959335327148, V.norm 16.846452713012695, MLP.norm 1.4857946634292603
2023-05-14 18:46:41,350 :: INFO :: Epoch 10: loss tensor(822.6320, device='cuda:0'), U.norm 10.560778617858887, V.norm 16.12102699279785, MLP.norm 2.7552082538604736
2023-05-14 18:46:41,584 :: INFO :: Epoch 15: loss tensor(794.1815, device='cuda:0'), U.norm 8.984989166259766, V.norm 15.764745712280273, MLP.norm 4.431951999664307
2023-05-14 18:46:41,819 :: INFO :: Epoch 20: loss tensor(759.3340, device='cuda:0'), U.norm 7.966625690460205, V.norm 15.525157928466797, MLP.norm 6.13342809677124
2023-05-14 18:46:41,819 :: INFO :: Environment 2
2023-05-14 18:46:51,209 :: INFO :: Epoch 5: loss tensor(812.6832, device='cuda:0'), U.norm 13.261759757995605, V.norm 16.800195693969727, MLP.norm 1.4712477922439575
2023-05-14 18:46:51,459 :: INFO :: Epoch 10: loss tensor(797.1816, device='cuda:0'), U.norm 10.552739143371582, V.norm 16.058273315429688, MLP.norm 2.6946287155151367
2023-05-14 18:46:51,678 :: INFO :: Epoch 15: loss tensor(771.7609, device='cuda:0'), U.norm 8.970503807067871, V.norm 15.696386337280273, MLP.norm 4.298874378204346
2023-05-14 18:46:51,912 :: INFO :: Epoch 20: loss tensor(740.8904, device='cuda:0'), U.norm 7.9447150230407715, V.norm 15.453700065612793, MLP.norm 5.93347692489624
2023-05-14 18:46:51,928 :: INFO :: Environment 3
2023-05-14 18:47:01,256 :: INFO :: Epoch 5: loss tensor(843.2357, device='cuda:0'), U.norm 13.267293930053711, V.norm 16.86133575439453, MLP.norm 1.5504300594329834
2023-05-14 18:47:01,491 :: INFO :: Epoch 10: loss tensor(824.3630, device='cuda:0'), U.norm 10.56662654876709, V.norm 16.139616012573242, MLP.norm 2.8992762565612793
2023-05-14 18:47:01,709 :: INFO :: Epoch 15: loss tensor(795.4120, device='cuda:0'), U.norm 8.993990898132324, V.norm 15.783085823059082, MLP.norm 4.5782928466796875
2023-05-14 18:47:01,943 :: INFO :: Epoch 20: loss tensor(761.3651, device='cuda:0'), U.norm 7.978131294250488, V.norm 15.542608261108398, MLP.norm 6.249936103820801
2023-05-14 18:47:01,959 :: INFO :: Environment 4
2023-05-14 18:47:11,287 :: INFO :: Epoch 5: loss tensor(877.1503, device='cuda:0'), U.norm 13.27482795715332, V.norm 16.91696548461914, MLP.norm 1.5331746339797974
2023-05-14 18:47:11,522 :: INFO :: Epoch 10: loss tensor(857.6229, device='cuda:0'), U.norm 10.582006454467773, V.norm 16.221546173095703, MLP.norm 2.8972840309143066
2023-05-14 18:47:11,756 :: INFO :: Epoch 15: loss tensor(827.0632, device='cuda:0'), U.norm 9.018623352050781, V.norm 15.880072593688965, MLP.norm 4.644093990325928
2023-05-14 18:47:11,975 :: INFO :: Epoch 20: loss tensor(789.6992, device='cuda:0'), U.norm 8.013422012329102, V.norm 15.648539543151855, MLP.norm 6.398703575134277
2023-05-14 18:47:11,991 :: INFO :: Environment 5
2023-05-14 18:47:21,396 :: INFO :: Epoch 5: loss tensor(818.3248, device='cuda:0'), U.norm 13.258648872375488, V.norm 16.81360626220703, MLP.norm 1.5094714164733887
2023-05-14 18:47:21,631 :: INFO :: Epoch 10: loss tensor(801.8136, device='cuda:0'), U.norm 10.547141075134277, V.norm 16.070383071899414, MLP.norm 2.7733075618743896
2023-05-14 18:47:21,881 :: INFO :: Epoch 15: loss tensor(775.5746, device='cuda:0'), U.norm 8.962468147277832, V.norm 15.7042818069458, MLP.norm 4.372386455535889
2023-05-14 18:47:22,115 :: INFO :: Epoch 20: loss tensor(744.7870, device='cuda:0'), U.norm 7.933807849884033, V.norm 15.457002639770508, MLP.norm 5.98103141784668
2023-05-14 18:47:22,131 :: INFO :: Environment 6
2023-05-14 18:47:31,397 :: INFO :: Epoch 5: loss tensor(861.0753, device='cuda:0'), U.norm 13.269247055053711, V.norm 16.886838912963867, MLP.norm 1.5778415203094482
2023-05-14 18:47:31,631 :: INFO :: Epoch 10: loss tensor(840.5569, device='cuda:0'), U.norm 10.571063041687012, V.norm 16.173967361450195, MLP.norm 2.9915950298309326
2023-05-14 18:47:31,850 :: INFO :: Epoch 15: loss tensor(809.3834, device='cuda:0'), U.norm 9.001032829284668, V.norm 15.821728706359863, MLP.norm 4.750147342681885
2023-05-14 18:47:32,084 :: INFO :: Epoch 20: loss tensor(772.2980, device='cuda:0'), U.norm 7.987931251525879, V.norm 15.581622123718262, MLP.norm 6.49613094329834
2023-05-14 18:47:32,084 :: INFO :: Environment 7
2023-05-14 18:47:41,443 :: INFO :: Epoch 5: loss tensor(822.3284, device='cuda:0'), U.norm 13.264004707336426, V.norm 16.823108673095703, MLP.norm 1.5166151523590088
2023-05-14 18:47:41,678 :: INFO :: Epoch 10: loss tensor(804.4844, device='cuda:0'), U.norm 10.558636665344238, V.norm 16.083120346069336, MLP.norm 2.8138926029205322
2023-05-14 18:47:41,897 :: INFO :: Epoch 15: loss tensor(776.0194, device='cuda:0'), U.norm 8.981266975402832, V.norm 15.719685554504395, MLP.norm 4.472254753112793
2023-05-14 18:47:42,131 :: INFO :: Epoch 20: loss tensor(741.9654, device='cuda:0'), U.norm 7.961152076721191, V.norm 15.473384857177734, MLP.norm 6.144412517547607
2023-05-14 18:47:42,147 :: INFO :: Environment 8
2023-05-14 18:47:51,506 :: INFO :: Epoch 5: loss tensor(871.6519, device='cuda:0'), U.norm 13.27341365814209, V.norm 16.897693634033203, MLP.norm 1.5724375247955322
2023-05-14 18:47:51,756 :: INFO :: Epoch 10: loss tensor(851.9986, device='cuda:0'), U.norm 10.57933521270752, V.norm 16.19174575805664, MLP.norm 2.9568698406219482
2023-05-14 18:47:51,990 :: INFO :: Epoch 15: loss tensor(822.0994, device='cuda:0'), U.norm 9.014513969421387, V.norm 15.84309196472168, MLP.norm 4.699093341827393
2023-05-14 18:47:52,225 :: INFO :: Epoch 20: loss tensor(786.1522, device='cuda:0'), U.norm 8.007246017456055, V.norm 15.606439590454102, MLP.norm 6.441318988800049
2023-05-14 18:47:52,241 :: INFO :: Environment 9
2023-05-14 18:48:01,584 :: INFO :: Epoch 5: loss tensor(881.6379, device='cuda:0'), U.norm 13.275670051574707, V.norm 16.91367530822754, MLP.norm 1.5772452354431152
2023-05-14 18:48:01,818 :: INFO :: Epoch 10: loss tensor(861.1547, device='cuda:0'), U.norm 10.584244728088379, V.norm 16.214271545410156, MLP.norm 2.9669442176818848
2023-05-14 18:48:02,037 :: INFO :: Epoch 15: loss tensor(829.5290, device='cuda:0'), U.norm 9.02223014831543, V.norm 15.870686531066895, MLP.norm 4.708014488220215
2023-05-14 18:48:02,272 :: INFO :: Epoch 20: loss tensor(792.3165, device='cuda:0'), U.norm 8.017885208129883, V.norm 15.636725425720215, MLP.norm 6.46409797668457
2023-05-14 18:48:02,287 :: INFO :: Ite = 1, Delta = 907
2023-05-14 18:48:02,287 :: INFO :: ----- backend -----
2023-05-14 18:48:06,334 :: INFO :: Epoch 5: loss tensor(218.0975, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0099,  0.0125,  0.0166,  0.0117,  0.0111,  0.0156,  0.0105,  0.0129,
         0.0049,  0.0160,  0.0151,  0.0145,  0.0069,  0.0100,  0.0180,  0.0175,
         0.0179,  0.0149,  0.0158,  0.0142,  0.0096,  0.0173,  0.0178,  0.0073,
         0.0185,  0.0125,  0.0116,  0.0088,  0.0185,  0.0158,  0.0184,  0.0171,
         0.0090,  0.0171,  0.0029,  0.0042,  0.0148,  0.0166,  0.0102,  0.0166,
         0.0121,  0.0066,  0.0185,  0.0131,  0.0109,  0.0177,  0.0137,  0.0010,
         0.0178,  0.0128,  0.0186,  0.0084,  0.0147,  0.0098,  0.0185,  0.0156,
         0.0108,  0.0173,  0.0070,  0.0084,  0.0148,  0.0141,  0.0068,  0.0144,
         0.0133,  0.0045,  0.0172,  0.0092,  0.0175,  0.0127,  0.0180,  0.0072,
         0.0170,  0.0054,  0.0150,  0.0057,  0.0165,  0.0082,  0.0108,  0.0115,
         0.0185,  0.0107,  0.0182,  0.0070,  0.0033,  0.0182,  0.0178,  0.0127,
         0.0069,  0.0025,  0.0104,  0.0133,  0.0180,  0.0150,  0.0106,  0.0184,
         0.0182,  0.0178,  0.0181,  0.0173,  0.0064,  0.0128,  0.0134, -0.0007,
         0.0180,  0.0170,  0.0142,  0.0180,  0.0119,  0.0156,  0.0182,  0.0081,
         0.0187,  0.0166,  0.0123,  0.0094,  0.0117,  0.0152,  0.0096,  0.0124,
         0.0179,  0.0141,  0.0072,  0.0101,  0.0032,  0.0160,  0.0069,  0.0177,
         0.0138,  0.0185,  0.0191,  0.0172,  0.0162,  0.0087,  0.0066,  0.0036,
         0.0094,  0.0072,  0.0085,  0.0065,  0.0185,  0.0138,  0.0078,  0.0186,
         0.0183,  0.0184,  0.0081,  0.0159,  0.0190,  0.0122,  0.0188,  0.0133,
         0.0189, -0.0150,  0.0101,  0.0133,  0.0159, -0.0028,  0.0162, -0.0081,
         0.0182, -0.0135,  0.0106,  0.0192,  0.0181,  0.0094,  0.0115,  0.0061,
         0.0139,  0.0104,  0.0146,  0.0134, -0.0066,  0.0160,  0.0186,  0.0190,
         0.0064,  0.0157,  0.0117,  0.0190,  0.0104,  0.0186,  0.0121, -0.0015,
         0.0181,  0.0150,  0.0107,  0.0123,  0.0130, -0.0105,  0.0031,  0.0154,
         0.0187,  0.0123,  0.0095,  0.0133,  0.0134,  0.0090,  0.0188,  0.0109,
         0.0188,  0.0101,  0.0191,  0.0109,  0.0188,  0.0118,  0.0136, -0.0137,
         0.0151,  0.0054,  0.0184,  0.0182,  0.0083,  0.0129,  0.0180,  0.0175,
         0.0066,  0.0037,  0.0180,  0.0054,  0.0116,  0.0080,  0.0160,  0.0187,
         0.0142,  0.0098,  0.0089,  0.0130,  0.0088,  0.0190,  0.0189,  0.0138,
         0.0171,  0.0113,  0.0181,  0.0113,  0.0101,  0.0180,  0.0049,  0.0127,
         0.0061,  0.0145,  0.0119,  0.0185,  0.0184,  0.0189,  0.0056,  0.0185,
         0.0186,  0.0186,  0.0106,  0.0172,  0.0092,  0.0188,  0.0028,  0.0097,
         0.0102,  0.0188,  0.0142,  0.0191,  0.0191,  0.0104,  0.0091,  0.0109,
         0.0189,  0.0068,  0.0187,  0.0194,  0.0122,  0.0191,  0.0098,  0.0098,
         0.0188,  0.0151,  0.0155,  0.0156,  0.0086,  0.0190,  0.0193,  0.0124,
         0.0134,  0.0140,  0.0191,  0.0104,  0.0109,  0.0162,  0.0108,  0.0188,
         0.0188,  0.0135,  0.0117,  0.0113,  0.0114,  0.0152,  0.0106,  0.0192,
         0.0158,  0.0189,  0.0131,  0.0189,  0.0147,  0.0189,  0.0184,  0.0048,
         0.0148,  0.0122,  0.0188,  0.0102,  0.0131,  0.0101,  0.0113,  0.0138,
         0.0163,  0.0153,  0.0079,  0.0164,  0.0109,  0.0141,  0.0193,  0.0193,
         0.0194,  0.0144,  0.0193,  0.0192,  0.0140,  0.0195,  0.0041,  0.0131,
         0.0109,  0.0163,  0.0194,  0.0121,  0.0113,  0.0067,  0.0190,  0.0131,
         0.0120,  0.0165,  0.0186,  0.0122,  0.0159,  0.0126,  0.0118,  0.0121,
         0.0094,  0.0147,  0.0190,  0.0192,  0.0097,  0.0194,  0.0108,  0.0132,
         0.0141,  0.0067,  0.0147,  0.0166,  0.0060,  0.0164,  0.0120,  0.0187,
         0.0117,  0.0100,  0.0183,  0.0057,  0.0185,  0.0193,  0.0189,  0.0144,
         0.0191,  0.0147,  0.0188,  0.0037,  0.0144,  0.0110,  0.0188,  0.0189,
         0.0154,  0.0124,  0.0148,  0.0103,  0.0193,  0.0102,  0.0188,  0.0163],
       device='cuda:0', requires_grad=True) MLP.norm tensor(2.6002, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:10,381 :: INFO :: Epoch 10: loss tensor(214.9705, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0294,  0.0308,  0.0368,  0.0316,  0.0282,  0.0362,  0.0290,  0.0313,
         0.0237,  0.0307,  0.0340,  0.0334,  0.0254,  0.0294,  0.0369,  0.0361,
         0.0342,  0.0308,  0.0327,  0.0326,  0.0288,  0.0312,  0.0361,  0.0283,
         0.0377,  0.0293,  0.0275,  0.0295,  0.0372,  0.0328,  0.0380,  0.0332,
         0.0268,  0.0341,  0.0232,  0.0238,  0.0317,  0.0352,  0.0291,  0.0336,
         0.0299,  0.0240,  0.0368,  0.0316,  0.0300,  0.0345,  0.0311,  0.0213,
         0.0350,  0.0294,  0.0377,  0.0267,  0.0317,  0.0254,  0.0378,  0.0356,
         0.0273,  0.0376,  0.0271,  0.0287,  0.0307,  0.0305,  0.0261,  0.0328,
         0.0329,  0.0235,  0.0313,  0.0288,  0.0347,  0.0314,  0.0354,  0.0261,
         0.0329,  0.0248,  0.0321,  0.0263,  0.0327,  0.0284,  0.0284,  0.0287,
         0.0382,  0.0306,  0.0384,  0.0268,  0.0224,  0.0360,  0.0341,  0.0320,
         0.0255,  0.0213,  0.0308,  0.0333,  0.0352,  0.0306,  0.0311,  0.0348,
         0.0356,  0.0359,  0.0369,  0.0364,  0.0262,  0.0265,  0.0331,  0.0176,
         0.0376,  0.0354,  0.0313,  0.0352,  0.0295,  0.0338,  0.0369,  0.0261,
         0.0384,  0.0350,  0.0323,  0.0289,  0.0288,  0.0323,  0.0251,  0.0327,
         0.0363,  0.0342,  0.0267,  0.0269,  0.0225,  0.0341,  0.0271,  0.0370,
         0.0308,  0.0389,  0.0397,  0.0276,  0.0296,  0.0111,  0.0279,  0.0252,
         0.0313,  0.0062,  0.0292,  0.0280,  0.0382,  0.0352,  0.0283,  0.0394,
         0.0390,  0.0369,  0.0162,  0.0373,  0.0395,  0.0335,  0.0401,  0.0320,
         0.0389, -0.0236,  0.0312,  0.0349,  0.0373,  0.0176,  0.0268,  0.0098,
         0.0349, -0.0073,  0.0154,  0.0398,  0.0366,  0.0141,  0.0330,  0.0278,
         0.0351,  0.0309,  0.0321,  0.0332,  0.0121,  0.0365,  0.0381,  0.0404,
         0.0071,  0.0357,  0.0329,  0.0400,  0.0310,  0.0399,  0.0212,  0.0194,
         0.0377,  0.0319,  0.0323,  0.0301,  0.0306,  0.0008,  0.0250,  0.0367,
         0.0377,  0.0340,  0.0243,  0.0285,  0.0338,  0.0090,  0.0383,  0.0320,
         0.0392,  0.0308,  0.0392,  0.0129,  0.0389,  0.0324,  0.0349, -0.0116,
         0.0319,  0.0268,  0.0373,  0.0354,  0.0110,  0.0344,  0.0376,  0.0368,
         0.0281,  0.0254,  0.0369,  0.0267,  0.0315,  0.0289,  0.0296,  0.0378,
         0.0222,  0.0288,  0.0289,  0.0329,  0.0221,  0.0385,  0.0387,  0.0231,
         0.0265,  0.0161,  0.0364,  0.0327,  0.0316,  0.0392,  0.0040,  0.0236,
         0.0268,  0.0355,  0.0334,  0.0375,  0.0397,  0.0392,  0.0272,  0.0392,
         0.0397,  0.0398,  0.0326,  0.0322,  0.0304,  0.0395,  0.0247,  0.0305,
         0.0324,  0.0409,  0.0366,  0.0411,  0.0411,  0.0321,  0.0318,  0.0337,
         0.0404,  0.0290,  0.0399,  0.0416,  0.0345,  0.0409,  0.0318,  0.0325,
         0.0405,  0.0369,  0.0380,  0.0375,  0.0309,  0.0410,  0.0412,  0.0348,
         0.0359,  0.0366,  0.0414,  0.0329,  0.0336,  0.0381,  0.0335,  0.0409,
         0.0404,  0.0360,  0.0330,  0.0341,  0.0341,  0.0373,  0.0329,  0.0409,
         0.0381,  0.0405,  0.0347,  0.0411,  0.0373,  0.0407,  0.0399,  0.0276,
         0.0369,  0.0345,  0.0406,  0.0322,  0.0354,  0.0321,  0.0329,  0.0362,
         0.0385,  0.0370,  0.0299,  0.0388,  0.0330,  0.0365,  0.0416,  0.0417,
         0.0416,  0.0362,  0.0415,  0.0413,  0.0359,  0.0413,  0.0267,  0.0353,
         0.0332,  0.0385,  0.0417,  0.0344,  0.0340,  0.0296,  0.0411,  0.0348,
         0.0333,  0.0390,  0.0400,  0.0346,  0.0384,  0.0353,  0.0342,  0.0342,
         0.0317,  0.0362,  0.0404,  0.0416,  0.0315,  0.0418,  0.0333,  0.0349,
         0.0368,  0.0286,  0.0366,  0.0390,  0.0286,  0.0383,  0.0338,  0.0410,
         0.0341,  0.0327,  0.0399,  0.0284,  0.0402,  0.0410,  0.0412,  0.0365,
         0.0410,  0.0372,  0.0414,  0.0263,  0.0365,  0.0336,  0.0409,  0.0409,
         0.0374,  0.0338,  0.0373,  0.0329,  0.0414,  0.0330,  0.0412,  0.0378],
       device='cuda:0', requires_grad=True) MLP.norm tensor(4.0680, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:14,412 :: INFO :: Epoch 15: loss tensor(210.4937, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0457,  0.0448,  0.0570,  0.0497,  0.0401,  0.0561,  0.0425,  0.0447,
         0.0371,  0.0400,  0.0502,  0.0497,  0.0385,  0.0464,  0.0550,  0.0521,
         0.0451,  0.0415,  0.0454,  0.0485,  0.0440,  0.0381,  0.0514,  0.0480,
         0.0551,  0.0404,  0.0373,  0.0494,  0.0542,  0.0456,  0.0555,  0.0447,
         0.0390,  0.0472,  0.0425,  0.0417,  0.0434,  0.0520,  0.0439,  0.0465,
         0.0425,  0.0358,  0.0523,  0.0474,  0.0455,  0.0470,  0.0438,  0.0385,
         0.0498,  0.0385,  0.0558,  0.0406,  0.0420,  0.0353,  0.0552,  0.0546,
         0.0362,  0.0583,  0.0452,  0.0478,  0.0405,  0.0428,  0.0401,  0.0484,
         0.0500,  0.0376,  0.0396,  0.0453,  0.0473,  0.0476,  0.0490,  0.0401,
         0.0439,  0.0391,  0.0442,  0.0449,  0.0445,  0.0459,  0.0404,  0.0410,
         0.0564,  0.0487,  0.0573,  0.0437,  0.0374,  0.0512,  0.0462,  0.0488,
         0.0387,  0.0346,  0.0497,  0.0515,  0.0491,  0.0399,  0.0498,  0.0463,
         0.0495,  0.0519,  0.0542,  0.0534,  0.0438,  0.0334,  0.0513,  0.0293,
         0.0565,  0.0516,  0.0434,  0.0494,  0.0429,  0.0486,  0.0548,  0.0386,
         0.0567,  0.0503,  0.0516,  0.0467,  0.0405,  0.0449,  0.0334,  0.0514,
         0.0529,  0.0526,  0.0437,  0.0364,  0.0379,  0.0495,  0.0450,  0.0548,
         0.0399,  0.0577,  0.0598,  0.0258,  0.0331,  0.0096,  0.0480,  0.0472,
         0.0544,  0.0031,  0.0477,  0.0494,  0.0562,  0.0569,  0.0469,  0.0591,
         0.0594,  0.0513,  0.0129,  0.0595,  0.0598,  0.0550,  0.0620,  0.0459,
         0.0579, -0.0226,  0.0513,  0.0575,  0.0591,  0.0409,  0.0257,  0.0309,
         0.0451,  0.0098,  0.0143,  0.0601,  0.0498,  0.0139,  0.0547,  0.0494,
         0.0565,  0.0487,  0.0452,  0.0488,  0.0346,  0.0560,  0.0558,  0.0627,
         0.0054,  0.0534,  0.0533,  0.0601,  0.0496,  0.0620,  0.0195,  0.0425,
         0.0542,  0.0437,  0.0547,  0.0432,  0.0416,  0.0214,  0.0469,  0.0579,
         0.0535,  0.0564,  0.0258,  0.0362,  0.0529,  0.0064,  0.0551,  0.0530,
         0.0590,  0.0501,  0.0576,  0.0110,  0.0581,  0.0522,  0.0561,  0.0016,
         0.0401,  0.0478,  0.0540,  0.0469,  0.0098,  0.0561,  0.0547,  0.0547,
         0.0494,  0.0475,  0.0535,  0.0494,  0.0476,  0.0476,  0.0348,  0.0538,
         0.0235,  0.0428,  0.0450,  0.0508,  0.0244,  0.0569,  0.0567,  0.0202,
         0.0225,  0.0169,  0.0489,  0.0549,  0.0532,  0.0605,  0.0029,  0.0258,
         0.0452,  0.0565,  0.0555,  0.0526,  0.0616,  0.0589,  0.0504,  0.0591,
         0.0606,  0.0609,  0.0554,  0.0399,  0.0515,  0.0605,  0.0485,  0.0499,
         0.0563,  0.0644,  0.0609,  0.0634,  0.0639,  0.0551,  0.0560,  0.0578,
         0.0630,  0.0521,  0.0613,  0.0646,  0.0577,  0.0627,  0.0550,  0.0565,
         0.0636,  0.0596,  0.0614,  0.0609,  0.0547,  0.0642,  0.0638,  0.0582,
         0.0594,  0.0606,  0.0646,  0.0570,  0.0571,  0.0603,  0.0576,  0.0645,
         0.0621,  0.0600,  0.0553,  0.0579,  0.0583,  0.0599,  0.0564,  0.0631,
         0.0610,  0.0634,  0.0557,  0.0639,  0.0612,  0.0637,  0.0630,  0.0523,
         0.0600,  0.0583,  0.0628,  0.0549,  0.0585,  0.0559,  0.0556,  0.0597,
         0.0620,  0.0600,  0.0535,  0.0620,  0.0551,  0.0605,  0.0650,  0.0653,
         0.0648,  0.0594,  0.0645,  0.0646,  0.0592,  0.0638,  0.0512,  0.0590,
         0.0559,  0.0621,  0.0652,  0.0582,  0.0581,  0.0538,  0.0646,  0.0564,
         0.0540,  0.0628,  0.0626,  0.0576,  0.0618,  0.0591,  0.0580,  0.0572,
         0.0553,  0.0572,  0.0628,  0.0647,  0.0543,  0.0653,  0.0569,  0.0576,
         0.0606,  0.0510,  0.0594,  0.0625,  0.0529,  0.0610,  0.0555,  0.0646,
         0.0577,  0.0571,  0.0628,  0.0531,  0.0625,  0.0630,  0.0645,  0.0599,
         0.0636,  0.0613,  0.0651,  0.0513,  0.0595,  0.0570,  0.0646,  0.0638,
         0.0599,  0.0535,  0.0608,  0.0571,  0.0644,  0.0571,  0.0651,  0.0591],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.1352, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:18,443 :: INFO :: Epoch 20: loss tensor(208.7911, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0565,  0.0531,  0.0747,  0.0637,  0.0456,  0.0730,  0.0487,  0.0512,
         0.0437,  0.0434,  0.0619,  0.0604,  0.0452,  0.0579,  0.0696,  0.0636,
         0.0504,  0.0464,  0.0522,  0.0594,  0.0541,  0.0399,  0.0606,  0.0632,
         0.0689,  0.0469,  0.0410,  0.0662,  0.0667,  0.0529,  0.0694,  0.0501,
         0.0445,  0.0536,  0.0573,  0.0544,  0.0489,  0.0642,  0.0530,  0.0542,
         0.0497,  0.0407,  0.0633,  0.0577,  0.0558,  0.0539,  0.0500,  0.0507,
         0.0600,  0.0415,  0.0692,  0.0483,  0.0473,  0.0399,  0.0685,  0.0706,
         0.0389,  0.0766,  0.0588,  0.0637,  0.0450,  0.0487,  0.0466,  0.0583,
         0.0618,  0.0454,  0.0428,  0.0562,  0.0536,  0.0592,  0.0561,  0.0474,
         0.0482,  0.0478,  0.0495,  0.0587,  0.0503,  0.0586,  0.0452,  0.0474,
         0.0713,  0.0629,  0.0721,  0.0543,  0.0454,  0.0611,  0.0528,  0.0598,
         0.0455,  0.0418,  0.0638,  0.0656,  0.0575,  0.0421,  0.0644,  0.0516,
         0.0584,  0.0634,  0.0680,  0.0657,  0.0566,  0.0357,  0.0651,  0.0341,
         0.0720,  0.0637,  0.0492,  0.0590,  0.0504,  0.0580,  0.0692,  0.0434,
         0.0712,  0.0610,  0.0672,  0.0607,  0.0468,  0.0519,  0.0373,  0.0667,
         0.0648,  0.0674,  0.0559,  0.0398,  0.0457,  0.0605,  0.0576,  0.0684,
         0.0394,  0.0726,  0.0780,  0.0192,  0.0288,  0.0078,  0.0641,  0.0670,
         0.0772,  0.0020,  0.0609,  0.0684,  0.0703,  0.0770,  0.0608,  0.0768,
         0.0775,  0.0590,  0.0095,  0.0813,  0.0781,  0.0745,  0.0830,  0.0540,
         0.0737, -0.0141,  0.0679,  0.0794,  0.0792,  0.0636,  0.0198,  0.0503,
         0.0465,  0.0271,  0.0135,  0.0780,  0.0553,  0.0124,  0.0748,  0.0685,
         0.0758,  0.0606,  0.0512,  0.0571,  0.0559,  0.0725,  0.0688,  0.0847,
         0.0049,  0.0654,  0.0706,  0.0777,  0.0637,  0.0832,  0.0154,  0.0648,
         0.0647,  0.0489,  0.0762,  0.0487,  0.0451,  0.0430,  0.0681,  0.0775,
         0.0633,  0.0780,  0.0184,  0.0357,  0.0696,  0.0061,  0.0659,  0.0722,
         0.0764,  0.0668,  0.0720,  0.0095,  0.0742,  0.0700,  0.0751,  0.0177,
         0.0370,  0.0657,  0.0669,  0.0501,  0.0086,  0.0765,  0.0660,  0.0698,
         0.0682,  0.0674,  0.0657,  0.0715,  0.0580,  0.0611,  0.0337,  0.0643,
         0.0210,  0.0510,  0.0538,  0.0645,  0.0212,  0.0722,  0.0704,  0.0138,
         0.0144,  0.0152,  0.0519,  0.0764,  0.0729,  0.0799,  0.0024,  0.0248,
         0.0576,  0.0755,  0.0767,  0.0616,  0.0824,  0.0758,  0.0730,  0.0760,
         0.0794,  0.0802,  0.0782,  0.0395,  0.0708,  0.0800,  0.0723,  0.0650,
         0.0803,  0.0875,  0.0856,  0.0845,  0.0863,  0.0778,  0.0800,  0.0819,
         0.0848,  0.0738,  0.0815,  0.0874,  0.0804,  0.0826,  0.0773,  0.0800,
         0.0867,  0.0816,  0.0844,  0.0838,  0.0784,  0.0870,  0.0858,  0.0808,
         0.0824,  0.0851,  0.0874,  0.0811,  0.0799,  0.0814,  0.0815,  0.0880,
         0.0827,  0.0840,  0.0768,  0.0813,  0.0826,  0.0813,  0.0792,  0.0847,
         0.0833,  0.0861,  0.0741,  0.0859,  0.0855,  0.0867,  0.0856,  0.0774,
         0.0825,  0.0820,  0.0834,  0.0775,  0.0804,  0.0800,  0.0771,  0.0831,
         0.0855,  0.0823,  0.0763,  0.0846,  0.0754,  0.0846,  0.0881,  0.0892,
         0.0876,  0.0825,  0.0869,  0.0874,  0.0823,  0.0858,  0.0757,  0.0824,
         0.0776,  0.0853,  0.0884,  0.0815,  0.0823,  0.0777,  0.0882,  0.0752,
         0.0724,  0.0864,  0.0848,  0.0796,  0.0847,  0.0829,  0.0817,  0.0793,
         0.0782,  0.0747,  0.0837,  0.0880,  0.0762,  0.0887,  0.0800,  0.0788,
         0.0844,  0.0724,  0.0816,  0.0857,  0.0773,  0.0833,  0.0749,  0.0878,
         0.0815,  0.0818,  0.0852,  0.0781,  0.0846,  0.0838,  0.0875,  0.0830,
         0.0853,  0.0851,  0.0886,  0.0766,  0.0817,  0.0794,  0.0886,  0.0864,
         0.0811,  0.0700,  0.0836,  0.0818,  0.0868,  0.0812,  0.0890,  0.0801],
       device='cuda:0', requires_grad=True) MLP.norm tensor(5.8894, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:22,490 :: INFO :: Epoch 25: loss tensor(206.9278, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([ 0.0624,  0.0566,  0.0902,  0.0734,  0.0465,  0.0857,  0.0499,  0.0532,
         0.0460,  0.0440,  0.0691,  0.0669,  0.0463,  0.0648,  0.0807,  0.0703,
         0.0515,  0.0482,  0.0541,  0.0668,  0.0601,  0.0385,  0.0649,  0.0743,
         0.0790,  0.0483,  0.0421,  0.0796,  0.0753,  0.0557,  0.0792,  0.0511,
         0.0457,  0.0557,  0.0679,  0.0624,  0.0503,  0.0722,  0.0571,  0.0572,
         0.0536,  0.0412,  0.0692,  0.0634,  0.0611,  0.0566,  0.0526,  0.0576,
         0.0655,  0.0411,  0.0789,  0.0511,  0.0489,  0.0404,  0.0782,  0.0830,
         0.0381,  0.0923,  0.0681,  0.0753,  0.0462,  0.0512,  0.0475,  0.0638,
         0.0692,  0.0481,  0.0425,  0.0619,  0.0550,  0.0663,  0.0589,  0.0502,
         0.0490,  0.0509,  0.0502,  0.0680,  0.0517,  0.0671,  0.0450,  0.0488,
         0.0823,  0.0721,  0.0825,  0.0605,  0.0485,  0.0659,  0.0558,  0.0666,
         0.0475,  0.0445,  0.0739,  0.0756,  0.0609,  0.0415,  0.0746,  0.0524,
         0.0627,  0.0706,  0.0781,  0.0741,  0.0645,  0.0351,  0.0746,  0.0351,
         0.0842,  0.0716,  0.0513,  0.0641,  0.0537,  0.0626,  0.0797,  0.0443,
         0.0822,  0.0666,  0.0794,  0.0708,  0.0490,  0.0537,  0.0376,  0.0787,
         0.0729,  0.0780,  0.0629,  0.0392,  0.0496,  0.0675,  0.0658,  0.0779,
         0.0343,  0.0832,  0.0935,  0.0148,  0.0242,  0.0073,  0.0759,  0.0842,
         0.0987,  0.0013,  0.0685,  0.0845,  0.0781,  0.0952,  0.0688,  0.0920,
         0.0921,  0.0603,  0.0081,  0.1013,  0.0938,  0.0913,  0.1025,  0.0556,
         0.0855, -0.0030,  0.0804,  0.1000,  0.0974,  0.0847,  0.0161,  0.0671,
         0.0424,  0.0431,  0.0115,  0.0933,  0.0551,  0.0106,  0.0929,  0.0849,
         0.0928,  0.0671,  0.0520,  0.0589,  0.0757,  0.0849,  0.0771,  0.1053,
         0.0044,  0.0713,  0.0843,  0.0926,  0.0718,  0.1028,  0.0115,  0.0857,
         0.0692,  0.0483,  0.0964,  0.0484,  0.0438,  0.0638,  0.0870,  0.0946,
         0.0669,  0.0984,  0.0117,  0.0315,  0.0816,  0.0051,  0.0714,  0.0880,
         0.0902,  0.0781,  0.0823,  0.0076,  0.0859,  0.0829,  0.0917,  0.0343,
         0.0298,  0.0800,  0.0735,  0.0476,  0.0082,  0.0948,  0.0712,  0.0793,
         0.0838,  0.0851,  0.0715,  0.0921,  0.0617,  0.0695,  0.0295,  0.0695,
         0.0191,  0.0527,  0.0554,  0.0721,  0.0175,  0.0835,  0.0791,  0.0102,
         0.0097,  0.0142,  0.0483,  0.0956,  0.0900,  0.0971,  0.0024,  0.0219,
         0.0643,  0.0918,  0.0956,  0.0658,  0.1015,  0.0896,  0.0943,  0.0893,
         0.0959,  0.0974,  0.0985,  0.0350,  0.0859,  0.0966,  0.0949,  0.0753,
         0.1035,  0.1097,  0.1100,  0.1033,  0.1078,  0.0984,  0.1031,  0.1050,
         0.1054,  0.0940,  0.1001,  0.1094,  0.1020,  0.1007,  0.0985,  0.1023,
         0.1083,  0.1030,  0.1061,  0.1055,  0.1011,  0.1087,  0.1071,  0.1020,
         0.1040,  0.1093,  0.1088,  0.1045,  0.1017,  0.1007,  0.1047,  0.1106,
         0.1016,  0.1072,  0.0965,  0.1035,  0.1065,  0.1014,  0.1010,  0.1045,
         0.1037,  0.1081,  0.0895,  0.1063,  0.1095,  0.1088,  0.1077,  0.1020,
         0.1042,  0.1051,  0.1020,  0.0995,  0.1009,  0.1032,  0.0969,  0.1054,
         0.1086,  0.1030,  0.0979,  0.1057,  0.0934,  0.1080,  0.1103,  0.1129,
         0.1095,  0.1048,  0.1077,  0.1090,  0.1040,  0.1067,  0.1000,  0.1052,
         0.0978,  0.1076,  0.1107,  0.1042,  0.1057,  0.1007,  0.1112,  0.0909,
         0.0871,  0.1091,  0.1060,  0.0997,  0.1065,  0.1056,  0.1048,  0.1000,
         0.0990,  0.0890,  0.1027,  0.1108,  0.0965,  0.1115,  0.1023,  0.0987,
         0.1073,  0.0933,  0.1026,  0.1074,  0.1016,  0.1045,  0.0913,  0.1100,
         0.1046,  0.1062,  0.1064,  0.1026,  0.1052,  0.1031,  0.1094,  0.1059,
         0.1063,  0.1081,  0.1115,  0.1016,  0.1019,  0.1002,  0.1119,  0.1078,
         0.1003,  0.0835,  0.1047,  0.1066,  0.1079,  0.1043,  0.1121,  0.0995],
       device='cuda:0', requires_grad=True) MLP.norm tensor(6.4450, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:26,475 :: INFO :: Epoch 30: loss tensor(205.5358, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0664, 0.0597, 0.1045, 0.0809, 0.0469, 0.0943, 0.0493, 0.0554, 0.0490,
        0.0455, 0.0738, 0.0725, 0.0470, 0.0702, 0.0899, 0.0751, 0.0516, 0.0498,
        0.0554, 0.0725, 0.0645, 0.0379, 0.0679, 0.0838, 0.0871, 0.0499, 0.0435,
        0.0911, 0.0810, 0.0577, 0.0874, 0.0523, 0.0472, 0.0562, 0.0773, 0.0690,
        0.0511, 0.0783, 0.0605, 0.0588, 0.0581, 0.0421, 0.0720, 0.0675, 0.0648,
        0.0581, 0.0546, 0.0627, 0.0687, 0.0416, 0.0877, 0.0546, 0.0507, 0.0430,
        0.0854, 0.0930, 0.0378, 0.1071, 0.0754, 0.0843, 0.0467, 0.0537, 0.0464,
        0.0679, 0.0746, 0.0506, 0.0428, 0.0667, 0.0559, 0.0712, 0.0608, 0.0522,
        0.0490, 0.0531, 0.0508, 0.0753, 0.0532, 0.0735, 0.0466, 0.0492, 0.0915,
        0.0777, 0.0913, 0.0647, 0.0513, 0.0689, 0.0573, 0.0717, 0.0502, 0.0469,
        0.0827, 0.0833, 0.0628, 0.0412, 0.0815, 0.0529, 0.0660, 0.0764, 0.0862,
        0.0803, 0.0702, 0.0364, 0.0819, 0.0371, 0.0942, 0.0776, 0.0531, 0.0671,
        0.0558, 0.0667, 0.0883, 0.0458, 0.0904, 0.0698, 0.0906, 0.0796, 0.0509,
        0.0553, 0.0387, 0.0892, 0.0791, 0.0870, 0.0680, 0.0404, 0.0522, 0.0726,
        0.0724, 0.0845, 0.0286, 0.0899, 0.1063, 0.0123, 0.0217, 0.0070, 0.0837,
        0.0990, 0.1188, 0.0012, 0.0714, 0.0977, 0.0807, 0.1114, 0.0721, 0.1051,
        0.1029, 0.0566, 0.0071, 0.1193, 0.1065, 0.1056, 0.1203, 0.0538, 0.0934,
        0.0084, 0.0886, 0.1194, 0.1135, 0.1040, 0.0147, 0.0821, 0.0367, 0.0578,
        0.0104, 0.1061, 0.0528, 0.0114, 0.1087, 0.0985, 0.1074, 0.0690, 0.0504,
        0.0572, 0.0935, 0.0930, 0.0808, 0.1246, 0.0041, 0.0725, 0.0947, 0.1046,
        0.0749, 0.1208, 0.0106, 0.1047, 0.0689, 0.0446, 0.1147, 0.0456, 0.0416,
        0.0815, 0.1043, 0.1094, 0.0661, 0.1172, 0.0083, 0.0273, 0.0899, 0.0050,
        0.0724, 0.1004, 0.1003, 0.0844, 0.0884, 0.0071, 0.0935, 0.0921, 0.1059,
        0.0480, 0.0236, 0.0910, 0.0749, 0.0423, 0.0074, 0.1110, 0.0720, 0.0842,
        0.0965, 0.1004, 0.0728, 0.1112, 0.0610, 0.0735, 0.0260, 0.0705, 0.0181,
        0.0514, 0.0525, 0.0754, 0.0152, 0.0908, 0.0837, 0.0083, 0.0079, 0.0134,
        0.0424, 0.1127, 0.1044, 0.1119, 0.0019, 0.0202, 0.0663, 0.1054, 0.1122,
        0.0664, 0.1190, 0.1005, 0.1144, 0.0993, 0.1098, 0.1124, 0.1171, 0.0300,
        0.0975, 0.1102, 0.1166, 0.0811, 0.1262, 0.1310, 0.1339, 0.1200, 0.1284,
        0.1169, 0.1252, 0.1270, 0.1247, 0.1123, 0.1155, 0.1297, 0.1213, 0.1158,
        0.1174, 0.1232, 0.1286, 0.1224, 0.1262, 0.1261, 0.1225, 0.1295, 0.1267,
        0.1216, 0.1243, 0.1326, 0.1284, 0.1273, 0.1222, 0.1173, 0.1268, 0.1323,
        0.1176, 0.1297, 0.1145, 0.1245, 0.1295, 0.1190, 0.1208, 0.1224, 0.1214,
        0.1280, 0.1013, 0.1247, 0.1326, 0.1291, 0.1289, 0.1259, 0.1249, 0.1265,
        0.1182, 0.1197, 0.1197, 0.1257, 0.1148, 0.1268, 0.1308, 0.1222, 0.1183,
        0.1249, 0.1089, 0.1303, 0.1315, 0.1361, 0.1302, 0.1258, 0.1269, 0.1290,
        0.1243, 0.1257, 0.1232, 0.1272, 0.1163, 0.1293, 0.1320, 0.1258, 0.1280,
        0.1224, 0.1338, 0.1027, 0.0974, 0.1307, 0.1253, 0.1164, 0.1265, 0.1267,
        0.1269, 0.1188, 0.1174, 0.1000, 0.1196, 0.1319, 0.1148, 0.1333, 0.1231,
        0.1164, 0.1290, 0.1119, 0.1222, 0.1272, 0.1248, 0.1241, 0.1047, 0.1307,
        0.1264, 0.1299, 0.1257, 0.1265, 0.1240, 0.1204, 0.1302, 0.1278, 0.1254,
        0.1300, 0.1337, 0.1263, 0.1203, 0.1184, 0.1347, 0.1274, 0.1170, 0.0931,
        0.1242, 0.1307, 0.1270, 0.1263, 0.1344, 0.1162], device='cuda:0',
       requires_grad=True) MLP.norm tensor(6.8716, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:30,491 :: INFO :: Epoch 35: loss tensor(204.0463, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0680, 0.0618, 0.1168, 0.0858, 0.0478, 0.0998, 0.0475, 0.0555, 0.0502,
        0.0460, 0.0764, 0.0770, 0.0485, 0.0729, 0.0957, 0.0773, 0.0521, 0.0506,
        0.0562, 0.0749, 0.0671, 0.0367, 0.0695, 0.0906, 0.0925, 0.0522, 0.0456,
        0.1002, 0.0837, 0.0586, 0.0935, 0.0522, 0.0465, 0.0556, 0.0838, 0.0730,
        0.0513, 0.0831, 0.0635, 0.0596, 0.0601, 0.0426, 0.0730, 0.0700, 0.0668,
        0.0585, 0.0558, 0.0656, 0.0704, 0.0420, 0.0929, 0.0559, 0.0524, 0.0450,
        0.0899, 0.0995, 0.0373, 0.1192, 0.0801, 0.0913, 0.0466, 0.0552, 0.0449,
        0.0693, 0.0768, 0.0522, 0.0429, 0.0690, 0.0565, 0.0750, 0.0611, 0.0525,
        0.0482, 0.0561, 0.0503, 0.0799, 0.0532, 0.0769, 0.0469, 0.0488, 0.0981,
        0.0803, 0.0969, 0.0669, 0.0525, 0.0700, 0.0573, 0.0743, 0.0518, 0.0491,
        0.0883, 0.0882, 0.0639, 0.0398, 0.0856, 0.0520, 0.0676, 0.0796, 0.0926,
        0.0837, 0.0734, 0.0373, 0.0869, 0.0393, 0.1012, 0.0817, 0.0529, 0.0692,
        0.0569, 0.0693, 0.0944, 0.0467, 0.0960, 0.0713, 0.0998, 0.0863, 0.0518,
        0.0567, 0.0407, 0.0969, 0.0831, 0.0935, 0.0715, 0.0415, 0.0544, 0.0764,
        0.0765, 0.0884, 0.0252, 0.0928, 0.1166, 0.0099, 0.0191, 0.0064, 0.0875,
        0.1111, 0.1380, 0.0008, 0.0709, 0.1077, 0.0806, 0.1256, 0.0719, 0.1162,
        0.1107, 0.0515, 0.0058, 0.1361, 0.1162, 0.1170, 0.1368, 0.0526, 0.0979,
        0.0213, 0.0928, 0.1372, 0.1277, 0.1223, 0.0138, 0.0958, 0.0323, 0.0724,
        0.0113, 0.1158, 0.0497, 0.0103, 0.1224, 0.1092, 0.1192, 0.0676, 0.0480,
        0.0527, 0.1097, 0.0976, 0.0806, 0.1431, 0.0045, 0.0700, 0.1014, 0.1137,
        0.0742, 0.1372, 0.0092, 0.1225, 0.0655, 0.0419, 0.1322, 0.0406, 0.0396,
        0.0994, 0.1208, 0.1220, 0.0624, 0.1349, 0.0064, 0.0245, 0.0966, 0.0045,
        0.0703, 0.1100, 0.1073, 0.0891, 0.0905, 0.0063, 0.0973, 0.0992, 0.1171,
        0.0620, 0.0194, 0.0986, 0.0741, 0.0371, 0.0070, 0.1252, 0.0694, 0.0871,
        0.1062, 0.1132, 0.0715, 0.1292, 0.0588, 0.0736, 0.0258, 0.0689, 0.0157,
        0.0506, 0.0473, 0.0772, 0.0135, 0.0943, 0.0840, 0.0068, 0.0068, 0.0116,
        0.0363, 0.1278, 0.1164, 0.1244, 0.0017, 0.0200, 0.0646, 0.1165, 0.1272,
        0.0642, 0.1349, 0.1087, 0.1335, 0.1059, 0.1211, 0.1250, 0.1351, 0.0274,
        0.1070, 0.1216, 0.1374, 0.0826, 0.1476, 0.1513, 0.1572, 0.1341, 0.1476,
        0.1340, 0.1460, 0.1478, 0.1429, 0.1292, 0.1293, 0.1489, 0.1391, 0.1285,
        0.1347, 0.1425, 0.1477, 0.1406, 0.1448, 0.1455, 0.1431, 0.1489, 0.1450,
        0.1393, 0.1433, 0.1554, 0.1467, 0.1492, 0.1409, 0.1311, 0.1476, 0.1529,
        0.1313, 0.1511, 0.1307, 0.1441, 0.1516, 0.1345, 0.1395, 0.1384, 0.1366,
        0.1469, 0.1084, 0.1404, 0.1550, 0.1483, 0.1494, 0.1493, 0.1441, 0.1474,
        0.1315, 0.1388, 0.1364, 0.1477, 0.1305, 0.1472, 0.1518, 0.1405, 0.1374,
        0.1422, 0.1216, 0.1518, 0.1514, 0.1586, 0.1495, 0.1461, 0.1442, 0.1477,
        0.1432, 0.1437, 0.1456, 0.1478, 0.1327, 0.1496, 0.1517, 0.1465, 0.1494,
        0.1428, 0.1554, 0.1107, 0.1043, 0.1511, 0.1431, 0.1307, 0.1445, 0.1463,
        0.1480, 0.1357, 0.1346, 0.1074, 0.1343, 0.1523, 0.1319, 0.1541, 0.1423,
        0.1323, 0.1495, 0.1288, 0.1406, 0.1450, 0.1472, 0.1424, 0.1145, 0.1502,
        0.1473, 0.1529, 0.1437, 0.1499, 0.1412, 0.1356, 0.1493, 0.1489, 0.1432,
        0.1506, 0.1547, 0.1503, 0.1368, 0.1341, 0.1569, 0.1456, 0.1312, 0.0982,
        0.1418, 0.1544, 0.1444, 0.1472, 0.1556, 0.1306], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.1974, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:34,475 :: INFO :: Epoch 40: loss tensor(208.4830, device='cuda:0', grad_fn=<AddBackward0>), reg tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<MulBackward0>) mu Parameter containing:
tensor([0.0699, 0.0636, 0.1283, 0.0898, 0.0472, 0.1042, 0.0482, 0.0550, 0.0512,
        0.0466, 0.0792, 0.0801, 0.0482, 0.0748, 0.1001, 0.0787, 0.0527, 0.0501,
        0.0577, 0.0771, 0.0686, 0.0366, 0.0705, 0.0962, 0.0966, 0.0513, 0.0457,
        0.1077, 0.0861, 0.0599, 0.0982, 0.0528, 0.0468, 0.0561, 0.0890, 0.0767,
        0.0527, 0.0850, 0.0650, 0.0610, 0.0618, 0.0439, 0.0740, 0.0715, 0.0675,
        0.0590, 0.0559, 0.0686, 0.0719, 0.0429, 0.0963, 0.0575, 0.0522, 0.0447,
        0.0935, 0.1049, 0.0375, 0.1292, 0.0841, 0.0976, 0.0464, 0.0553, 0.0438,
        0.0705, 0.0779, 0.0544, 0.0436, 0.0708, 0.0566, 0.0781, 0.0612, 0.0537,
        0.0476, 0.0560, 0.0496, 0.0835, 0.0532, 0.0804, 0.0474, 0.0494, 0.1030,
        0.0825, 0.0998, 0.0689, 0.0540, 0.0708, 0.0578, 0.0768, 0.0530, 0.0503,
        0.0917, 0.0913, 0.0643, 0.0396, 0.0895, 0.0523, 0.0685, 0.0820, 0.0974,
        0.0860, 0.0761, 0.0365, 0.0906, 0.0381, 0.1073, 0.0853, 0.0534, 0.0709,
        0.0576, 0.0702, 0.0994, 0.0470, 0.1010, 0.0728, 0.1076, 0.0919, 0.0530,
        0.0573, 0.0400, 0.1037, 0.0865, 0.0974, 0.0745, 0.0421, 0.0543, 0.0773,
        0.0806, 0.0913, 0.0227, 0.0926, 0.1237, 0.0082, 0.0177, 0.0061, 0.0878,
        0.1202, 0.1556, 0.0007, 0.0676, 0.1147, 0.0779, 0.1374, 0.0695, 0.1246,
        0.1150, 0.0455, 0.0054, 0.1508, 0.1232, 0.1260, 0.1513, 0.0504, 0.0990,
        0.0317, 0.0936, 0.1532, 0.1396, 0.1382, 0.0133, 0.1076, 0.0286, 0.0841,
        0.0098, 0.1227, 0.0462, 0.0104, 0.1335, 0.1169, 0.1282, 0.0638, 0.0451,
        0.0472, 0.1231, 0.0988, 0.0779, 0.1598, 0.0041, 0.0648, 0.1048, 0.1202,
        0.0708, 0.1517, 0.0080, 0.1380, 0.0602, 0.0385, 0.1478, 0.0373, 0.0383,
        0.1143, 0.1354, 0.1318, 0.0570, 0.1508, 0.0052, 0.0213, 0.1005, 0.0043,
        0.0664, 0.1166, 0.1109, 0.0912, 0.0896, 0.0065, 0.0977, 0.1029, 0.1257,
        0.0732, 0.0166, 0.1024, 0.0719, 0.0320, 0.0073, 0.1373, 0.0648, 0.0867,
        0.1126, 0.1233, 0.0678, 0.1451, 0.0555, 0.0711, 0.0241, 0.0653, 0.0154,
        0.0484, 0.0414, 0.0761, 0.0119, 0.0947, 0.0813, 0.0058, 0.0062, 0.0117,
        0.0308, 0.1404, 0.1254, 0.1344, 0.0019, 0.0190, 0.0608, 0.1244, 0.1397,
        0.0608, 0.1487, 0.1133, 0.1508, 0.1094, 0.1295, 0.1349, 0.1510, 0.0239,
        0.1129, 0.1300, 0.1565, 0.0809, 0.1682, 0.1695, 0.1797, 0.1455, 0.1656,
        0.1484, 0.1657, 0.1675, 0.1593, 0.1440, 0.1404, 0.1665, 0.1547, 0.1378,
        0.1501, 0.1602, 0.1657, 0.1565, 0.1615, 0.1631, 0.1626, 0.1671, 0.1616,
        0.1551, 0.1605, 0.1775, 0.1630, 0.1692, 0.1578, 0.1425, 0.1674, 0.1724,
        0.1430, 0.1717, 0.1454, 0.1623, 0.1732, 0.1476, 0.1564, 0.1525, 0.1492,
        0.1648, 0.1110, 0.1542, 0.1760, 0.1660, 0.1688, 0.1714, 0.1614, 0.1667,
        0.1416, 0.1564, 0.1510, 0.1687, 0.1435, 0.1660, 0.1719, 0.1571, 0.1551,
        0.1574, 0.1314, 0.1722, 0.1699, 0.1800, 0.1674, 0.1653, 0.1599, 0.1649,
        0.1606, 0.1596, 0.1666, 0.1675, 0.1465, 0.1686, 0.1702, 0.1659, 0.1698,
        0.1618, 0.1761, 0.1143, 0.1080, 0.1702, 0.1599, 0.1422, 0.1608, 0.1646,
        0.1678, 0.1499, 0.1501, 0.1112, 0.1470, 0.1713, 0.1456, 0.1737, 0.1600,
        0.1461, 0.1689, 0.1440, 0.1568, 0.1612, 0.1683, 0.1591, 0.1210, 0.1681,
        0.1670, 0.1752, 0.1601, 0.1725, 0.1570, 0.1481, 0.1667, 0.1685, 0.1592,
        0.1703, 0.1749, 0.1734, 0.1512, 0.1474, 0.1781, 0.1617, 0.1427, 0.0996,
        0.1573, 0.1772, 0.1603, 0.1670, 0.1758, 0.1421], device='cuda:0',
       requires_grad=True) MLP.norm tensor(7.4395, device='cuda:0', grad_fn=<CopyBackwards>)
2023-05-14 18:48:34,506 :: INFO :: mask tensor([0.5699, 0.5636, 0.6283, 0.5898, 0.5472, 0.6042, 0.5482, 0.5550, 0.5512,
        0.5466, 0.5792, 0.5801, 0.5482, 0.5748, 0.6001, 0.5787, 0.5527, 0.5501,
        0.5577, 0.5771, 0.5686, 0.5366, 0.5705, 0.5962, 0.5966, 0.5513, 0.5457,
        0.6077, 0.5861, 0.5599, 0.5982, 0.5528, 0.5468, 0.5561, 0.5890, 0.5767,
        0.5527, 0.5850, 0.5650, 0.5610, 0.5618, 0.5439, 0.5740, 0.5715, 0.5675,
        0.5590, 0.5559, 0.5686, 0.5719, 0.5429, 0.5963, 0.5575, 0.5522, 0.5447,
        0.5935, 0.6049, 0.5375, 0.6292, 0.5841, 0.5976, 0.5464, 0.5553, 0.5438,
        0.5705, 0.5779, 0.5544, 0.5436, 0.5708, 0.5566, 0.5781, 0.5612, 0.5537,
        0.5476, 0.5560, 0.5496, 0.5835, 0.5532, 0.5804, 0.5474, 0.5494, 0.6030,
        0.5825, 0.5998, 0.5689, 0.5540, 0.5708, 0.5578, 0.5768, 0.5530, 0.5503,
        0.5917, 0.5913, 0.5643, 0.5396, 0.5895, 0.5523, 0.5685, 0.5820, 0.5974,
        0.5860, 0.5761, 0.5365, 0.5906, 0.5381, 0.6073, 0.5853, 0.5534, 0.5709,
        0.5576, 0.5702, 0.5994, 0.5470, 0.6010, 0.5728, 0.6076, 0.5919, 0.5530,
        0.5573, 0.5400, 0.6037, 0.5865, 0.5974, 0.5745, 0.5421, 0.5543, 0.5773,
        0.5806, 0.5913, 0.5227, 0.5926, 0.6237, 0.5082, 0.5177, 0.5061, 0.5878,
        0.6202, 0.6556, 0.5007, 0.5676, 0.6147, 0.5779, 0.6374, 0.5695, 0.6246,
        0.6150, 0.5455, 0.5054, 0.6508, 0.6232, 0.6260, 0.6513, 0.5504, 0.5990,
        0.5317, 0.5936, 0.6532, 0.6396, 0.6382, 0.5133, 0.6076, 0.5286, 0.5841,
        0.5098, 0.6227, 0.5462, 0.5104, 0.6335, 0.6169, 0.6282, 0.5638, 0.5451,
        0.5472, 0.6231, 0.5988, 0.5779, 0.6598, 0.5041, 0.5648, 0.6048, 0.6202,
        0.5708, 0.6517, 0.5080, 0.6380, 0.5602, 0.5385, 0.6478, 0.5373, 0.5383,
        0.6143, 0.6354, 0.6318, 0.5570, 0.6508, 0.5052, 0.5213, 0.6005, 0.5043,
        0.5664, 0.6166, 0.6109, 0.5912, 0.5896, 0.5065, 0.5977, 0.6029, 0.6257,
        0.5732, 0.5166, 0.6024, 0.5719, 0.5320, 0.5073, 0.6373, 0.5648, 0.5867,
        0.6126, 0.6233, 0.5678, 0.6451, 0.5555, 0.5711, 0.5241, 0.5653, 0.5154,
        0.5484, 0.5414, 0.5761, 0.5119, 0.5947, 0.5813, 0.5058, 0.5062, 0.5117,
        0.5308, 0.6404, 0.6254, 0.6344, 0.5019, 0.5190, 0.5608, 0.6244, 0.6397,
        0.5608, 0.6487, 0.6133, 0.6508, 0.6094, 0.6295, 0.6349, 0.6510, 0.5239,
        0.6129, 0.6300, 0.6565, 0.5809, 0.6682, 0.6695, 0.6797, 0.6455, 0.6656,
        0.6484, 0.6657, 0.6675, 0.6593, 0.6440, 0.6404, 0.6665, 0.6547, 0.6378,
        0.6501, 0.6602, 0.6657, 0.6565, 0.6615, 0.6631, 0.6626, 0.6671, 0.6616,
        0.6551, 0.6605, 0.6775, 0.6630, 0.6692, 0.6578, 0.6425, 0.6674, 0.6724,
        0.6430, 0.6717, 0.6454, 0.6623, 0.6732, 0.6476, 0.6564, 0.6525, 0.6492,
        0.6648, 0.6110, 0.6542, 0.6760, 0.6660, 0.6688, 0.6714, 0.6614, 0.6667,
        0.6416, 0.6564, 0.6510, 0.6687, 0.6435, 0.6660, 0.6719, 0.6571, 0.6551,
        0.6574, 0.6314, 0.6722, 0.6699, 0.6800, 0.6674, 0.6653, 0.6599, 0.6649,
        0.6606, 0.6596, 0.6666, 0.6675, 0.6465, 0.6686, 0.6702, 0.6659, 0.6698,
        0.6618, 0.6761, 0.6143, 0.6080, 0.6702, 0.6599, 0.6422, 0.6608, 0.6646,
        0.6678, 0.6499, 0.6501, 0.6112, 0.6470, 0.6713, 0.6456, 0.6737, 0.6600,
        0.6461, 0.6689, 0.6440, 0.6568, 0.6612, 0.6683, 0.6591, 0.6210, 0.6681,
        0.6670, 0.6752, 0.6601, 0.6725, 0.6570, 0.6481, 0.6667, 0.6685, 0.6592,
        0.6703, 0.6749, 0.6734, 0.6512, 0.6474, 0.6781, 0.6617, 0.6427, 0.5996,
        0.6573, 0.6772, 0.6603, 0.6670, 0.6758, 0.6421], device='cuda:0')
2023-05-14 18:48:50,975 :: INFO :: Epoch 5: loss tensor(24.0779, device='cuda:0', grad_fn=<AddBackward0>), U.norm 16.034015655517578, V.norm 15.672560691833496, MLP.norm 9.737021446228027
2023-05-14 18:49:40,959 :: INFO :: ----- val -----
2023-05-14 18:49:40,959 :: INFO :: Precision [0.009780924983403407, 0.004992254923655723, 0.004284133657888865]
2023-05-14 18:49:40,959 :: INFO :: Recall [0.002526331928898167, 0.008305772267405129, 0.013908628366491697]
2023-05-14 18:49:40,959 :: INFO :: ndcg [0.009780924983403407, 0.016727295690025457, 0.021907711889346832]
2023-05-14 18:50:30,414 :: INFO :: ----- test -----
2023-05-14 18:50:30,414 :: INFO :: Precision [0.005622577651618538, 0.003215241006605195, 0.0028986298378732025]
2023-05-14 18:50:30,414 :: INFO :: Recall [0.0020958552321774637, 0.007876547644470065, 0.0145660791385498]
2023-05-14 18:50:30,414 :: INFO :: ndcg [0.005622577651618538, 0.010437970814014222, 0.01436860697422728]
2023-05-14 18:50:36,273 :: INFO :: Epoch 10: loss tensor(28.6382, device='cuda:0', grad_fn=<AddBackward0>), U.norm 26.370882034301758, V.norm 23.227375030517578, MLP.norm 11.430318832397461
2023-05-14 18:51:26,602 :: INFO :: ----- val -----
2023-05-14 18:51:26,602 :: INFO :: Precision [0.010666076565611861, 0.004319539721177289, 0.004828501880947036]
2023-05-14 18:51:26,602 :: INFO :: Recall [0.002625241403775777, 0.005452486443043474, 0.014786465814439214]
2023-05-14 18:51:26,602 :: INFO :: ndcg [0.010666076565611861, 0.01563687747258292, 0.02376840573245141]
2023-05-14 18:52:16,930 :: INFO :: ----- test -----
2023-05-14 18:52:16,930 :: INFO :: Precision [0.005513401386538566, 0.002440089524537385, 0.003299852612042079]
2023-05-14 18:52:16,930 :: INFO :: Recall [0.0018404049337390112, 0.005088359881250437, 0.016002676558760948]
2023-05-14 18:52:16,930 :: INFO :: ndcg [0.005513401386538566, 0.008589500181788891, 0.014959135526031939]
2023-05-14 18:52:22,930 :: INFO :: Epoch 15: loss tensor(27.5050, device='cuda:0', grad_fn=<AddBackward0>), U.norm 35.54965591430664, V.norm 39.30569839477539, MLP.norm 11.213916778564453
2023-05-14 18:53:13,383 :: INFO :: ----- val -----
2023-05-14 18:53:13,383 :: INFO :: Precision [0.011329940252268201, 0.007187430847532613, 0.006997123257357635]
2023-05-14 18:53:13,383 :: INFO :: Recall [0.002940401029623609, 0.011008399440677141, 0.02308010546307306]
2023-05-14 18:53:13,383 :: INFO :: ndcg [0.011329940252268201, 0.021896532986224612, 0.031867779562822045]
2023-05-14 18:54:04,008 :: INFO :: ----- test -----
2023-05-14 18:54:04,008 :: INFO :: Precision [0.006195753043288389, 0.004579944320104785, 0.004645450079152653]
2023-05-14 18:54:04,008 :: INFO :: Recall [0.0022698315763868986, 0.010164734709989501, 0.022165756990942875]
2023-05-14 18:54:04,008 :: INFO :: ndcg [0.006195753043288389, 0.013440092844980222, 0.02066582235392652]
2023-05-14 18:54:09,836 :: INFO :: Epoch 20: loss tensor(25.8107, device='cuda:0', grad_fn=<AddBackward0>), U.norm 45.47259521484375, V.norm 68.7549819946289, MLP.norm 10.887343406677246
2023-05-14 18:55:00,523 :: INFO :: ----- val -----
2023-05-14 18:55:00,523 :: INFO :: Precision [0.011639743306041159, 0.01026775835361787, 0.007868997565832907]
2023-05-14 18:55:00,523 :: INFO :: Recall [0.0031269379823695566, 0.01672513148896578, 0.026406821369176586]
2023-05-14 18:55:00,523 :: INFO :: ndcg [0.011639743306041159, 0.02941196554468366, 0.037413768031248476]
2023-05-14 18:55:51,289 :: INFO :: ----- test -----
2023-05-14 18:55:51,289 :: INFO :: Precision [0.006823516567498226, 0.006883563513292065, 0.00519679021780648]
2023-05-14 18:55:51,289 :: INFO :: Recall [0.0024700156321307556, 0.015825327080796144, 0.024723299108010546]
2023-05-14 18:55:51,289 :: INFO :: ndcg [0.006823516567498226, 0.019105678023659426, 0.024434077625788125]
2023-05-14 18:55:57,242 :: INFO :: Epoch 25: loss tensor(23.7796, device='cuda:0', grad_fn=<AddBackward0>), U.norm 56.762901306152344, V.norm 106.24687194824219, MLP.norm 10.311328887939453
2023-05-14 18:56:47,320 :: INFO :: ----- val -----
2023-05-14 18:56:47,320 :: INFO :: Precision [0.013410046470458067, 0.01252489488824931, 0.00996680681566684]
2023-05-14 18:56:47,320 :: INFO :: Recall [0.0035333711126752064, 0.020679832474726347, 0.031934082713532465]
2023-05-14 18:56:47,320 :: INFO :: ndcg [0.013410046470458067, 0.03608443696678261, 0.04605844380662432]
2023-05-14 18:57:37,242 :: INFO :: ----- test -----
2023-05-14 18:57:37,242 :: INFO :: Precision [0.007915279218297941, 0.008237349200283637, 0.006356788034281118]
2023-05-14 18:57:37,242 :: INFO :: Recall [0.0028687811445824458, 0.01912422518541249, 0.029110798670903014]
2023-05-14 18:57:37,242 :: INFO :: ndcg [0.007915279218297941, 0.023220695469117213, 0.029564887586976216]
2023-05-14 18:57:43,195 :: INFO :: Epoch 30: loss tensor(29.2038, device='cuda:0', grad_fn=<AddBackward0>), U.norm 68.6696548461914, V.norm 144.68997192382812, MLP.norm 9.755081176757812
2023-05-14 18:58:33,242 :: INFO :: ----- val -----
2023-05-14 18:58:33,242 :: INFO :: Precision [0.015003319318433283, 0.014658110201371562, 0.011861031201592964]
2023-05-14 18:58:33,242 :: INFO :: Recall [0.004144981467519835, 0.02527278762003035, 0.0387413140846239]
2023-05-14 18:58:33,242 :: INFO :: ndcg [0.015003319318433283, 0.04229825032405939, 0.05431859568934741]
2023-05-14 18:59:23,461 :: INFO :: ----- test -----
2023-05-14 18:59:23,461 :: INFO :: Precision [0.009334570664337573, 0.009853157923467127, 0.0075522681369068825]
2023-05-14 18:59:23,461 :: INFO :: Recall [0.0034584988386563826, 0.02333489350867577, 0.03447937917112527]
2023-05-14 18:59:23,461 :: INFO :: ndcg [0.009334570664337573, 0.028352364317954397, 0.035497657351710336]
2023-05-14 18:59:29,336 :: INFO :: Epoch 35: loss tensor(30.0318, device='cuda:0', grad_fn=<AddBackward0>), U.norm 80.1942367553711, V.norm 178.59664916992188, MLP.norm 9.177959442138672
2023-05-14 19:00:19,867 :: INFO :: ----- val -----
2023-05-14 19:00:19,867 :: INFO :: Precision [0.01593272847975216, 0.015445895109537042, 0.012732905510068435]
2023-05-14 19:00:19,867 :: INFO :: Recall [0.0046641653464191786, 0.02732061520885709, 0.04178345070450962]
2023-05-14 19:00:19,867 :: INFO :: ndcg [0.01593272847975216, 0.04432136821214318, 0.057289171118714864]
2023-05-14 19:01:10,555 :: INFO :: ----- test -----
2023-05-14 19:01:10,555 :: INFO :: Precision [0.009662099459577488, 0.010491839074184923, 0.008163655221354852]
2023-05-14 19:01:10,555 :: INFO :: Recall [0.0037658234530528883, 0.02548118211959969, 0.03769656626479733]
2023-05-14 19:01:10,555 :: INFO :: ndcg [0.009662099459577488, 0.03014536348680906, 0.038278376901569655]
2023-05-14 19:01:16,555 :: INFO :: Epoch 40: loss tensor(26.8941, device='cuda:0', grad_fn=<AddBackward0>), U.norm 90.94540405273438, V.norm 206.33395385742188, MLP.norm 9.143259048461914
2023-05-14 19:02:06,524 :: INFO :: ----- val -----
2023-05-14 19:02:06,524 :: INFO :: Precision [0.01845541048904625, 0.01571144058419957, 0.01331710555432611]
2023-05-14 19:02:06,524 :: INFO :: Recall [0.006110066180816944, 0.028137637438517854, 0.04468919652477802]
2023-05-14 19:02:06,524 :: INFO :: ndcg [0.01845541048904625, 0.04613690802428104, 0.0603350754581832]
2023-05-14 19:02:56,931 :: INFO :: ----- test -----
2023-05-14 19:02:56,931 :: INFO :: Precision [0.012527976417926743, 0.010841203122440813, 0.008755936459413822]
2023-05-14 19:02:56,931 :: INFO :: Recall [0.005961894777147032, 0.02667184178531416, 0.040667610840913]
2023-05-14 19:02:56,931 :: INFO :: ndcg [0.012527976417926743, 0.03237683128452673, 0.04169910962719198]
2023-05-14 19:03:02,775 :: INFO :: Epoch 45: loss tensor(28.5879, device='cuda:0', grad_fn=<AddBackward0>), U.norm 100.71662139892578, V.norm 227.35662841796875, MLP.norm 8.648353576660156
2023-05-14 19:03:52,431 :: INFO :: ----- val -----
2023-05-14 19:03:52,431 :: INFO :: Precision [0.01929630449214428, 0.015950431511395833, 0.013622482850188074]
2023-05-14 19:03:52,431 :: INFO :: Recall [0.006305778738803689, 0.029178047967877273, 0.047120088046278805]
2023-05-14 19:03:52,431 :: INFO :: ndcg [0.01929630449214428, 0.047545874299474905, 0.062129927118613384]
2023-05-14 19:04:42,432 :: INFO :: ----- test -----
2023-05-14 19:04:42,432 :: INFO :: Precision [0.013428680604836509, 0.01134887275506265, 0.0090507123751298]
2023-05-14 19:04:42,432 :: INFO :: Recall [0.00633582407983641, 0.02861359872430937, 0.04258998150201161]
2023-05-14 19:04:42,432 :: INFO :: ndcg [0.013428680604836509, 0.03424427468378393, 0.04361231926796136]
2023-05-14 19:04:48,385 :: INFO :: Epoch 50: loss tensor(32.6545, device='cuda:0', grad_fn=<AddBackward0>), U.norm 109.451904296875, V.norm 244.8556365966797, MLP.norm 8.26992130279541
2023-05-14 19:05:37,807 :: INFO :: ----- val -----
2023-05-14 19:05:37,807 :: INFO :: Precision [0.019030759017481743, 0.01619827395441419, 0.013755255587519373]
2023-05-14 19:05:37,807 :: INFO :: Recall [0.006546040537271399, 0.029635076536765927, 0.04764490057780829]
2023-05-14 19:05:37,807 :: INFO :: ndcg [0.019030759017481743, 0.047996788218358864, 0.06259810594332886]
2023-05-14 19:06:27,823 :: INFO :: ----- test -----
2023-05-14 19:06:27,823 :: INFO :: Precision [0.013374092472296522, 0.011496260712920606, 0.009170806266717789]
2023-05-14 19:06:27,823 :: INFO :: Recall [0.00656378481024858, 0.02937032424024924, 0.043909032698548366]
2023-05-14 19:06:27,823 :: INFO :: ndcg [0.013374092472296522, 0.034569532932103034, 0.04424896513585186]
2023-05-14 19:06:33,667 :: INFO :: Epoch 55: loss tensor(30.3660, device='cuda:0', grad_fn=<AddBackward0>), U.norm 117.3285140991211, V.norm 257.998291015625, MLP.norm 7.838742256164551
2023-05-14 19:07:23,041 :: INFO :: ----- val -----
2023-05-14 19:07:23,041 :: INFO :: Precision [0.019738880283248505, 0.016154016375303764, 0.013803938924540822]
2023-05-14 19:07:23,041 :: INFO :: Recall [0.007377113371838727, 0.02975669718161883, 0.04840709303124843]
2023-05-14 19:07:23,041 :: INFO :: ndcg [0.019738880283248505, 0.04830817685049711, 0.06344811649790415]
2023-05-14 19:08:12,791 :: INFO :: ----- test -----
2023-05-14 19:08:12,791 :: INFO :: Precision [0.014902560183416125, 0.011736448496096528, 0.00935094710409977]
2023-05-14 19:08:12,791 :: INFO :: Recall [0.007815008547022727, 0.030275948277815223, 0.04502050984801518]
2023-05-14 19:08:12,791 :: INFO :: ndcg [0.014902560183416125, 0.03585484706211649, 0.045379220175990324]
2023-05-14 19:08:18,791 :: INFO :: Epoch 60: loss tensor(31.5216, device='cuda:0', grad_fn=<AddBackward0>), U.norm 124.53419494628906, V.norm 267.78961181640625, MLP.norm 7.802200794219971
2023-05-14 19:09:08,229 :: INFO :: ----- val -----
2023-05-14 19:09:08,229 :: INFO :: Precision [0.02009294091613189, 0.016348749723389615, 0.01406063288338132]
2023-05-14 19:09:08,229 :: INFO :: Recall [0.007431262482223305, 0.030407768569116134, 0.049976413233515184]
2023-05-14 19:09:08,229 :: INFO :: ndcg [0.02009294091613189, 0.048599782702653985, 0.0640070963767171]
2023-05-14 19:09:58,151 :: INFO :: ----- test -----
2023-05-14 19:09:58,151 :: INFO :: Precision [0.014738795785796168, 0.01195480102625646, 0.009528358534854762]
2023-05-14 19:09:58,151 :: INFO :: Recall [0.007809846199620086, 0.031252536413617256, 0.046404336236227]
2023-05-14 19:09:58,151 :: INFO :: ndcg [0.014738795785796168, 0.03631881783106868, 0.04619524398537385]
2023-05-14 19:10:04,088 :: INFO :: Epoch 65: loss tensor(32.1707, device='cuda:0', grad_fn=<AddBackward0>), U.norm 130.9207305908203, V.norm 275.07611083984375, MLP.norm 7.606046676635742
2023-05-14 19:10:53,635 :: INFO :: ----- val -----
2023-05-14 19:10:53,635 :: INFO :: Precision [0.018853728701040053, 0.01618057092277002, 0.014180128346979475]
2023-05-14 19:10:53,635 :: INFO :: Recall [0.0072612103494845625, 0.03075677385201138, 0.05121033822364406]
2023-05-14 19:10:53,635 :: INFO :: ndcg [0.018853728701040053, 0.047991227676406074, 0.06403562247644515]
2023-05-14 19:11:43,635 :: INFO :: ----- test -----
2023-05-14 19:11:43,635 :: INFO :: Precision [0.014902560183416125, 0.011883836453954482, 0.009642993613188756]
2023-05-14 19:11:43,635 :: INFO :: Recall [0.008118411263883398, 0.03122260158173679, 0.04796656454400925]
2023-05-14 19:11:43,635 :: INFO :: ndcg [0.014902560183416125, 0.03625497760505513, 0.04681959456246547]
2023-05-14 19:11:49,572 :: INFO :: Epoch 70: loss tensor(31.7178, device='cuda:0', grad_fn=<AddBackward0>), U.norm 136.79209899902344, V.norm 280.1266174316406, MLP.norm 7.719054222106934
2023-05-14 19:12:39,388 :: INFO :: ----- val -----
2023-05-14 19:12:39,388 :: INFO :: Precision [0.01898650143837132, 0.01633989820756753, 0.013808364682451878]
2023-05-14 19:12:39,388 :: INFO :: Recall [0.007447275305506325, 0.03124689711938922, 0.05065016909022913]
2023-05-14 19:12:39,388 :: INFO :: ndcg [0.01898650143837132, 0.048057829733494296, 0.0632230429878362]
2023-05-14 19:13:29,529 :: INFO :: ----- test -----
2023-05-14 19:13:29,529 :: INFO :: Precision [0.014520443255636225, 0.011938424586494462, 0.009572029040886762]
2023-05-14 19:13:29,529 :: INFO :: Recall [0.008151053864351752, 0.031441104760807484, 0.048395286723749056]
2023-05-14 19:13:29,529 :: INFO :: ndcg [0.014520443255636225, 0.03630584027089093, 0.046482397354017754]
2023-05-14 19:13:35,560 :: INFO :: Epoch 75: loss tensor(35.3710, device='cuda:0', grad_fn=<AddBackward0>), U.norm 142.2221221923828, V.norm 283.9345397949219, MLP.norm 7.377323627471924
2023-05-14 19:14:25,825 :: INFO :: ----- val -----
2023-05-14 19:14:25,825 :: INFO :: Precision [0.01916353175481301, 0.016260234565168775, 0.013715423766319967]
2023-05-14 19:14:25,825 :: INFO :: Recall [0.007584335896720929, 0.03185690853943462, 0.0510315684005253]
2023-05-14 19:14:25,825 :: INFO :: ndcg [0.01916353175481301, 0.0481167219270028, 0.06308599680647024]
2023-05-14 19:15:15,966 :: INFO :: ----- test -----
2023-05-14 19:15:15,966 :: INFO :: Precision [0.015230088978656041, 0.012260494568480364, 0.009632075986680765]
2023-05-14 19:15:15,966 :: INFO :: Recall [0.00867741609986842, 0.032862409078658644, 0.049841479299665765]
2023-05-14 19:15:15,966 :: INFO :: ndcg [0.015230088978656041, 0.037254996056653535, 0.04723768777231997]
2023-05-14 19:15:21,888 :: INFO :: Epoch 80: loss tensor(31.6744, device='cuda:0', grad_fn=<AddBackward0>), U.norm 147.27439880371094, V.norm 286.9224548339844, MLP.norm 7.296565055847168
2023-05-14 19:16:12,451 :: INFO :: ----- val -----
2023-05-14 19:16:12,451 :: INFO :: Precision [0.01876521354281921, 0.0163576012392117, 0.013622482850188068]
2023-05-14 19:16:12,451 :: INFO :: Recall [0.007573924329072199, 0.032038644209112475, 0.05108194924803193]
2023-05-14 19:16:12,451 :: INFO :: ndcg [0.01876521354281921, 0.04806027737504154, 0.06271292643707738]
2023-05-14 19:17:03,091 :: INFO :: ----- test -----
2023-05-14 19:17:03,091 :: INFO :: Precision [0.01476608985206616, 0.012309623887766347, 0.00974944047164175]
2023-05-14 19:17:03,091 :: INFO :: Recall [0.008535578802498292, 0.03296956757548708, 0.05073885832391092]
2023-05-14 19:17:03,091 :: INFO :: ndcg [0.01476608985206616, 0.03728792433683041, 0.04740102733547798]
2023-05-14 19:17:08,982 :: INFO :: Epoch 85: loss tensor(32.9066, device='cuda:0', grad_fn=<AddBackward0>), U.norm 152.03318786621094, V.norm 288.7900085449219, MLP.norm 7.175955772399902
2023-05-14 19:17:58,466 :: INFO :: ----- val -----
2023-05-14 19:17:58,466 :: INFO :: Precision [0.01863244080548794, 0.01634874972338961, 0.013618057092277024]
2023-05-14 19:17:58,466 :: INFO :: Recall [0.007674765843623553, 0.03239574314155173, 0.05153797777263656]
2023-05-14 19:17:58,466 :: INFO :: ndcg [0.01863244080548794, 0.04806212884303862, 0.06266720887246378]
2023-05-14 19:18:47,607 :: INFO :: ----- test -----
2023-05-14 19:18:47,607 :: INFO :: Precision [0.015284677111196026, 0.012353294393798332, 0.009801299197554734]
2023-05-14 19:18:47,607 :: INFO :: Recall [0.009042019825408316, 0.0332965743882464, 0.05161257920735128]
2023-05-14 19:18:47,607 :: INFO :: ndcg [0.015284677111196026, 0.03771735965018755, 0.047885734753320204]
2023-05-14 19:18:53,435 :: INFO :: Epoch 90: loss tensor(31.8495, device='cuda:0', grad_fn=<AddBackward0>), U.norm 156.43931579589844, V.norm 290.3960876464844, MLP.norm 6.927112579345703
2023-05-14 19:19:43,138 :: INFO :: ----- val -----
2023-05-14 19:19:43,138 :: INFO :: Precision [0.01646381942907723, 0.016162867891125848, 0.013343660101792372]
2023-05-14 19:19:43,138 :: INFO :: Recall [0.0071295965398793385, 0.03191901356701905, 0.05087631178464976]
2023-05-14 19:19:43,138 :: INFO :: ndcg [0.01646381942907723, 0.04690806982646209, 0.06124009675193928]
2023-05-14 19:20:32,716 :: INFO :: ----- test -----
2023-05-14 19:20:32,716 :: INFO :: Precision [0.01446585512309624, 0.012287788634750354, 0.009850428516840736]
2023-05-14 19:20:32,716 :: INFO :: Recall [0.008791359962801842, 0.03369308269773022, 0.05261065753737529]
2023-05-14 19:20:32,716 :: INFO :: ndcg [0.01446585512309624, 0.03729020057603933, 0.0478637174600659]
2023-05-14 19:20:38,560 :: INFO :: Epoch 95: loss tensor(32.5076, device='cuda:0', grad_fn=<AddBackward0>), U.norm 160.67166137695312, V.norm 291.14569091796875, MLP.norm 7.138429164886475
2023-05-14 19:21:28,263 :: INFO :: ----- val -----
2023-05-14 19:21:28,263 :: INFO :: Precision [0.01717194069484399, 0.016224828501880437, 0.013356937375525504]
2023-05-14 19:21:28,263 :: INFO :: Recall [0.007406458118466964, 0.03198779635916799, 0.05066292045364087]
2023-05-14 19:21:28,263 :: INFO :: ndcg [0.01717194069484399, 0.04704601378064142, 0.06134306955081516]
2023-05-14 19:22:17,966 :: INFO :: ----- test -----
2023-05-14 19:22:17,966 :: INFO :: Precision [0.015175500846116054, 0.012298706261258348, 0.009896828429499733]
2023-05-14 19:22:17,966 :: INFO :: Recall [0.00930380999555866, 0.033732002010149496, 0.0525650787452243]
2023-05-14 19:22:17,966 :: INFO :: ndcg [0.015175500846116054, 0.037625159940299795, 0.04837447704901189]
2023-05-14 19:22:23,935 :: INFO :: Epoch 100: loss tensor(31.2983, device='cuda:0', grad_fn=<AddBackward0>), U.norm 164.63702392578125, V.norm 291.4022521972656, MLP.norm 6.976181983947754
2023-05-14 19:23:13,560 :: INFO :: ----- val -----
2023-05-14 19:23:13,560 :: INFO :: Precision [0.01849966806815667, 0.016012392122150415, 0.01349856162867888]
2023-05-14 19:23:13,560 :: INFO :: Recall [0.0077313208838435815, 0.03170361013964306, 0.05167837807017703]
2023-05-14 19:23:13,560 :: INFO :: ndcg [0.01849966806815667, 0.046870022693168055, 0.06184412724042182]
2023-05-14 19:24:03,404 :: INFO :: ----- test -----
2023-05-14 19:24:03,404 :: INFO :: Precision [0.015148206779846062, 0.01220590643594038, 0.009883181396364736]
2023-05-14 19:24:03,404 :: INFO :: Recall [0.008917178951501027, 0.03331462785377389, 0.0524036943074481]
2023-05-14 19:24:03,404 :: INFO :: ndcg [0.015148206779846062, 0.03721488295364679, 0.04801925368241731]
2023-05-14 19:24:09,310 :: INFO :: Epoch 105: loss tensor(32.6525, device='cuda:0', grad_fn=<AddBackward0>), U.norm 168.36102294921875, V.norm 292.0810852050781, MLP.norm 6.861545085906982
2023-05-14 19:24:57,201 :: INFO :: ----- val -----
2023-05-14 19:24:57,216 :: INFO :: Precision [0.01845541048904625, 0.015835361805708735, 0.013423323744191156]
2023-05-14 19:24:57,216 :: INFO :: Recall [0.007700275072242871, 0.031490315672642, 0.0520234428058552]
2023-05-14 19:24:57,216 :: INFO :: ndcg [0.01845541048904625, 0.04663175311930622, 0.06178066503898895]
2023-05-14 19:25:47,107 :: INFO :: ----- test -----
2023-05-14 19:25:47,107 :: INFO :: Precision [0.015475735575085976, 0.012194988809432383, 0.009948687155412736]
2023-05-14 19:25:47,107 :: INFO :: Recall [0.009319554732581977, 0.03371893685297127, 0.05382850662395756]
2023-05-14 19:25:47,107 :: INFO :: ndcg [0.015475735575085976, 0.037323214111258836, 0.04836477076642411]
2023-05-14 19:25:53,029 :: INFO :: Epoch 110: loss tensor(35.4441, device='cuda:0', grad_fn=<AddBackward0>), U.norm 171.99945068359375, V.norm 291.9544982910156, MLP.norm 6.850497245788574
2023-05-14 19:26:42,216 :: INFO :: ----- val -----
2023-05-14 19:26:42,216 :: INFO :: Precision [0.01845541048904625, 0.015720292100021644, 0.013299402522681941]
2023-05-14 19:26:42,216 :: INFO :: Recall [0.007689535584296038, 0.031461955713834526, 0.05156467934285215]
2023-05-14 19:26:42,216 :: INFO :: ndcg [0.01845541048904625, 0.04639962896657011, 0.06131567655036418]
2023-05-14 19:27:30,638 :: INFO :: ----- test -----
2023-05-14 19:27:30,638 :: INFO :: Precision [0.01561220590643594, 0.012249576941972365, 0.009954145968666738]
2023-05-14 19:27:30,638 :: INFO :: Recall [0.009348739296948592, 0.034025840813868356, 0.05368064047460392]
2023-05-14 19:27:30,638 :: INFO :: ndcg [0.01561220590643594, 0.037556746181217945, 0.04847773971470605]
2023-05-14 19:27:36,529 :: INFO :: Epoch 115: loss tensor(31.2049, device='cuda:0', grad_fn=<AddBackward0>), U.norm 175.52874755859375, V.norm 291.70941162109375, MLP.norm 6.966142654418945
2023-05-14 19:28:25,341 :: INFO :: ----- val -----
2023-05-14 19:28:25,341 :: INFO :: Precision [0.01880947112192963, 0.015684886036733313, 0.013414472228369058]
2023-05-14 19:28:25,341 :: INFO :: Recall [0.007933988808965488, 0.03158309714961617, 0.05199973871629326]
2023-05-14 19:28:25,341 :: INFO :: ndcg [0.01880947112192963, 0.04653478758833743, 0.061852011938787414]
2023-05-14 19:29:14,763 :: INFO :: ----- test -----
2023-05-14 19:29:14,763 :: INFO :: Precision [0.015530323707625962, 0.012173153556416388, 0.009995087068071727]
2023-05-14 19:29:14,763 :: INFO :: Recall [0.009177184317459336, 0.03375622249707225, 0.05395448912172621]
2023-05-14 19:29:14,763 :: INFO :: ndcg [0.015530323707625962, 0.03744975878961408, 0.04860697246531212]
2023-05-14 19:29:20,622 :: INFO :: Epoch 120: loss tensor(35.7992, device='cuda:0', grad_fn=<AddBackward0>), U.norm 178.9412384033203, V.norm 291.4561767578125, MLP.norm 6.7586822509765625
2023-05-14 19:30:09,138 :: INFO :: ----- val -----
2023-05-14 19:30:09,138 :: INFO :: Precision [0.019075016596592165, 0.01592387696392959, 0.013449878291657398]
2023-05-14 19:30:09,138 :: INFO :: Recall [0.008168027102308935, 0.03193229393433256, 0.051867258604846304]
2023-05-14 19:30:09,138 :: INFO :: ndcg [0.019075016596592165, 0.04699395983824484, 0.062227682995250666]
2023-05-14 19:30:57,107 :: INFO :: ----- test -----
2023-05-14 19:30:57,107 :: INFO :: Precision [0.015284677111196026, 0.012309623887766345, 0.01010153392652472]
2023-05-14 19:30:57,107 :: INFO :: Recall [0.00918932849549685, 0.03430356590212042, 0.054853673779199634]
2023-05-14 19:30:57,107 :: INFO :: ndcg [0.015284677111196026, 0.03768872427947485, 0.049030343980809285]
2023-05-14 19:31:02,997 :: INFO :: Epoch 125: loss tensor(33.5503, device='cuda:0', grad_fn=<AddBackward0>), U.norm 182.20193481445312, V.norm 291.09649658203125, MLP.norm 6.805999279022217
2023-05-14 19:31:52,341 :: INFO :: ----- val -----
2023-05-14 19:31:52,341 :: INFO :: Precision [0.0179685771188316, 0.01618057092277002, 0.013325957070148196]
2023-05-14 19:31:52,341 :: INFO :: Recall [0.007786372087117381, 0.0324138443147602, 0.052014425316725336]
2023-05-14 19:31:52,341 :: INFO :: ndcg [0.0179685771188316, 0.04700852095832396, 0.06151404734038032]
2023-05-14 19:32:42,482 :: INFO :: ----- test -----
2023-05-14 19:32:42,482 :: INFO :: Precision [0.015666794038975928, 0.012440635405862306, 0.010066051640373727]
2023-05-14 19:32:42,482 :: INFO :: Recall [0.009704220729828215, 0.034774907574791396, 0.05502222283972345]
2023-05-14 19:32:42,482 :: INFO :: ndcg [0.015666794038975928, 0.038150599481840915, 0.04931410252512909]
2023-05-14 19:32:48,451 :: INFO :: Epoch 130: loss tensor(32.4806, device='cuda:0', grad_fn=<AddBackward0>), U.norm 185.4245147705078, V.norm 290.5204162597656, MLP.norm 6.77193546295166
2023-05-14 19:33:37,888 :: INFO :: ----- val -----
2023-05-14 19:33:37,888 :: INFO :: Precision [0.01845541048904625, 0.015985837574684168, 0.013348085859703417]
2023-05-14 19:33:37,888 :: INFO :: Recall [0.007867489655452663, 0.03212730449681221, 0.05213508727069113]
2023-05-14 19:33:37,888 :: INFO :: ndcg [0.01845541048904625, 0.04678440555969288, 0.06160484977145419]
2023-05-14 19:34:27,466 :: INFO :: ----- test -----
2023-05-14 19:34:27,466 :: INFO :: Precision [0.015339265243736012, 0.012424258966100311, 0.010090616300016732]
2023-05-14 19:34:27,466 :: INFO :: Recall [0.00948094952057788, 0.03515083334774827, 0.055558154698956766]
2023-05-14 19:34:27,466 :: INFO :: ndcg [0.015339265243736012, 0.037975495910373094, 0.04917446724674548]
2023-05-14 19:34:33,372 :: INFO :: Epoch 135: loss tensor(33.3566, device='cuda:0', grad_fn=<AddBackward0>), U.norm 188.5367431640625, V.norm 289.9140319824219, MLP.norm 6.789431571960449
2023-05-14 19:35:23,341 :: INFO :: ----- val -----
2023-05-14 19:35:23,341 :: INFO :: Precision [0.018543925647267093, 0.016224828501880437, 0.013383491922991765]
2023-05-14 19:35:23,341 :: INFO :: Recall [0.007895453645081433, 0.03244056364539241, 0.0521579905624774]
2023-05-14 19:35:23,341 :: INFO :: ndcg [0.018543925647267093, 0.04722252663495166, 0.06187791429518551]
2023-05-14 19:36:13,466 :: INFO :: ----- test -----
2023-05-14 19:36:13,466 :: INFO :: Precision [0.015639499972705934, 0.012648070309514241, 0.010147933839183727]
2023-05-14 19:36:13,466 :: INFO :: Recall [0.009534525662979597, 0.03532158772490842, 0.055772459622888765]
2023-05-14 19:36:13,466 :: INFO :: ndcg [0.015639499972705934, 0.03849195374732025, 0.049545579917719684]
2023-05-14 19:36:19,435 :: INFO :: Epoch 140: loss tensor(36.1861, device='cuda:0', grad_fn=<AddBackward0>), U.norm 191.5071258544922, V.norm 289.359375, MLP.norm 6.645724773406982
2023-05-14 19:37:08,622 :: INFO :: ----- val -----
2023-05-14 19:37:08,622 :: INFO :: Precision [0.019207789333923433, 0.015976986058862084, 0.013370214649258631]
2023-05-14 19:37:08,622 :: INFO :: Recall [0.008191936583188626, 0.03230688151931868, 0.05182282685091051]
2023-05-14 19:37:08,622 :: INFO :: ndcg [0.019207789333923433, 0.04712420997368807, 0.06204124543097691]
2023-05-14 19:37:58,669 :: INFO :: ----- test -----
2023-05-14 19:37:58,669 :: INFO :: Precision [0.015857852502865876, 0.012484305911894292, 0.010093345706643725]
2023-05-14 19:37:58,669 :: INFO :: Recall [0.009668615727485848, 0.03501826939715066, 0.05535687041374566]
2023-05-14 19:37:58,669 :: INFO :: ndcg [0.015857852502865876, 0.03828575308938521, 0.04939061881805704]
2023-05-14 19:38:04,591 :: INFO :: Epoch 145: loss tensor(31.4273, device='cuda:0', grad_fn=<AddBackward0>), U.norm 194.48826599121094, V.norm 288.94879150390625, MLP.norm 6.697438716888428
2023-05-14 19:38:54,263 :: INFO :: ----- val -----
2023-05-14 19:38:54,263 :: INFO :: Precision [0.018366895330825403, 0.015888470900641244, 0.013396769196724884]
2023-05-14 19:38:54,263 :: INFO :: Recall [0.0078096416881638575, 0.031874725794327805, 0.051697625364875696]
2023-05-14 19:38:54,263 :: INFO :: ndcg [0.018366895330825403, 0.046665726311568505, 0.06180621096257866]
2023-05-14 19:39:43,388 :: INFO :: ----- test -----
2023-05-14 19:39:43,388 :: INFO :: Precision [0.01572138217151591, 0.012566188110704266, 0.010115180959659723]
2023-05-14 19:39:43,388 :: INFO :: Recall [0.00965255130525898, 0.03539439701813074, 0.055625393496972306]
2023-05-14 19:39:43,388 :: INFO :: ndcg [0.01572138217151591, 0.038250817833715214, 0.04932997689710988]
2023-05-14 19:39:49,248 :: INFO :: Epoch 150: loss tensor(36.1680, device='cuda:0', grad_fn=<AddBackward0>), U.norm 197.45774841308594, V.norm 288.19677734375, MLP.norm 6.8673529624938965
2023-05-14 19:40:39,529 :: INFO :: ----- val -----
2023-05-14 19:40:39,529 :: INFO :: Precision [0.018189865014383712, 0.015817658774064575, 0.013564947997344522]
2023-05-14 19:40:39,544 :: INFO :: Recall [0.007523733256647989, 0.03198448235529037, 0.05235284394753864]
2023-05-14 19:40:39,544 :: INFO :: ndcg [0.018189865014383712, 0.04646784367945638, 0.06212301335359863]
2023-05-14 19:41:29,672 :: INFO :: ----- test -----
2023-05-14 19:41:29,672 :: INFO :: Precision [0.01588514656913587, 0.012571646923958266, 0.010183416125334717]
2023-05-14 19:41:29,672 :: INFO :: Recall [0.00979861670973096, 0.035488650679345576, 0.05586994096218105]
2023-05-14 19:41:29,672 :: INFO :: ndcg [0.01588514656913587, 0.03832827129366269, 0.049545997728094364]
2023-05-14 19:41:35,594 :: INFO :: Epoch 155: loss tensor(36.4170, device='cuda:0', grad_fn=<AddBackward0>), U.norm 200.35760498046875, V.norm 287.40679931640625, MLP.norm 6.8616042137146
2023-05-14 19:42:25,672 :: INFO :: ----- val -----
2023-05-14 19:42:25,672 :: INFO :: Precision [0.01814560743527329, 0.015569816331046215, 0.013502987386589919]
2023-05-14 19:42:25,672 :: INFO :: Recall [0.007886191213211536, 0.03160333892411474, 0.05183547695882123]
2023-05-14 19:42:25,672 :: INFO :: ndcg [0.01814560743527329, 0.04616576754884939, 0.061795782363666056]
2023-05-14 19:43:15,469 :: INFO :: ----- test -----
2023-05-14 19:43:15,469 :: INFO :: Precision [0.015666794038975928, 0.012500682351656288, 0.010207980784977727]
2023-05-14 19:43:15,469 :: INFO :: Recall [0.009677078978929703, 0.035908532215414374, 0.05618123591945589]
2023-05-14 19:43:15,469 :: INFO :: ndcg [0.015666794038975928, 0.038292294229459595, 0.04976113480774869]
2023-05-14 19:43:21,438 :: INFO :: Epoch 160: loss tensor(37.3787, device='cuda:0', grad_fn=<AddBackward0>), U.norm 203.13613891601562, V.norm 286.9020080566406, MLP.norm 6.620175838470459
2023-05-14 19:44:10,813 :: INFO :: ----- val -----
2023-05-14 19:44:10,813 :: INFO :: Precision [0.01717194069484399, 0.0155786678468683, 0.013317105554326117]
2023-05-14 19:44:10,813 :: INFO :: Recall [0.0076590577558256325, 0.031486679767971024, 0.052035090633204974]
2023-05-14 19:44:10,813 :: INFO :: ndcg [0.01717194069484399, 0.04588910798490433, 0.061226101603747675]
2023-05-14 19:45:00,281 :: INFO :: ----- test -----
2023-05-14 19:45:00,281 :: INFO :: Precision [0.014711501719526175, 0.012342376767290336, 0.010281674763906718]
2023-05-14 19:45:00,281 :: INFO :: Recall [0.009478354968705957, 0.03505530422381669, 0.05703135126797157]
2023-05-14 19:45:00,281 :: INFO :: ndcg [0.014711501719526175, 0.037657274413575935, 0.04948481792981189]
2023-05-14 19:45:06,157 :: INFO :: Epoch 165: loss tensor(31.8139, device='cuda:0', grad_fn=<AddBackward0>), U.norm 205.80636596679688, V.norm 286.56622314453125, MLP.norm 6.70643424987793
2023-05-14 19:45:55,719 :: INFO :: ----- val -----
2023-05-14 19:45:55,735 :: INFO :: Precision [0.01717194069484399, 0.015861916353174996, 0.01329940252268194]
2023-05-14 19:45:55,735 :: INFO :: Recall [0.007930089210689096, 0.03170437792106749, 0.051919061628452126]
2023-05-14 19:45:55,735 :: INFO :: ndcg [0.01717194069484399, 0.046365711193076344, 0.061251043222322846]
2023-05-14 19:46:46,221 :: INFO :: ----- test -----
2023-05-14 19:46:46,221 :: INFO :: Precision [0.014602325454446204, 0.012604399803482257, 0.010257110104263719]
2023-05-14 19:46:46,237 :: INFO :: Recall [0.009291618065308753, 0.03546255146812316, 0.056531474475500006]
2023-05-14 19:46:46,237 :: INFO :: ndcg [0.014602325454446204, 0.03815082589718446, 0.049534072710375514]
2023-05-14 19:46:52,221 :: INFO :: Epoch 170: loss tensor(34.7400, device='cuda:0', grad_fn=<AddBackward0>), U.norm 208.39073181152344, V.norm 285.6489562988281, MLP.norm 6.940512657165527
2023-05-14 19:47:42,409 :: INFO :: ----- val -----
2023-05-14 19:47:42,409 :: INFO :: Precision [0.01748174374861695, 0.01590617393228542, 0.013343660101792377]
2023-05-14 19:47:42,409 :: INFO :: Recall [0.007829095631606922, 0.03184632841951598, 0.05201646632925117]
2023-05-14 19:47:42,409 :: INFO :: ndcg [0.01748174374861695, 0.04666543693757626, 0.061726508148081116]
2023-05-14 19:48:32,971 :: INFO :: ----- test -----
2023-05-14 19:48:32,971 :: INFO :: Precision [0.014547737321906217, 0.012648070309514241, 0.01025438069763672]
2023-05-14 19:48:32,971 :: INFO :: Recall [0.009371630935662251, 0.03564253205009695, 0.05683296802264839]
2023-05-14 19:48:32,971 :: INFO :: ndcg [0.014547737321906217, 0.038340096420828654, 0.049603638747355745]
2023-05-14 19:48:38,878 :: INFO :: Epoch 175: loss tensor(32.4754, device='cuda:0', grad_fn=<AddBackward0>), U.norm 210.97193908691406, V.norm 284.6426696777344, MLP.norm 6.7829766273498535
2023-05-14 19:49:27,925 :: INFO :: ----- val -----
2023-05-14 19:49:27,925 :: INFO :: Precision [0.01695065279929188, 0.016277937596812942, 0.013626908608099127]
2023-05-14 19:49:27,925 :: INFO :: Recall [0.007815220593686044, 0.03257275485578158, 0.05244064426669435]
2023-05-14 19:49:27,925 :: INFO :: ndcg [0.01695065279929188, 0.047029948847790805, 0.06218273756214835]
2023-05-14 19:50:17,549 :: INFO :: ----- test -----
2023-05-14 19:50:17,549 :: INFO :: Precision [0.014793383918336154, 0.012604399803482255, 0.01029259239041472]
2023-05-14 19:50:17,549 :: INFO :: Recall [0.009379229851839694, 0.03592746292404709, 0.0567833065697006]
2023-05-14 19:50:17,549 :: INFO :: ndcg [0.014793383918336154, 0.03817035106064902, 0.04965029280753506]
2023-05-14 19:50:23,378 :: INFO :: Epoch 180: loss tensor(33.5161, device='cuda:0', grad_fn=<AddBackward0>), U.norm 213.63760375976562, V.norm 284.2789611816406, MLP.norm 6.804640293121338
2023-05-14 19:51:12,706 :: INFO :: ----- val -----
2023-05-14 19:51:12,706 :: INFO :: Precision [0.016862137641071032, 0.015950431511395836, 0.013604779818543923]
2023-05-14 19:51:12,706 :: INFO :: Recall [0.007651886879846279, 0.03202860377435481, 0.05248969287227837]
2023-05-14 19:51:12,706 :: INFO :: ndcg [0.016862137641071032, 0.04669126085743773, 0.062135664410533885]
2023-05-14 19:52:03,174 :: INFO :: ----- test -----
2023-05-14 19:52:03,174 :: INFO :: Precision [0.014547737321906217, 0.012571646923958263, 0.01025983951089072]
2023-05-14 19:52:03,174 :: INFO :: Recall [0.00928051631216322, 0.035520617190751023, 0.056548191920998976]
2023-05-14 19:52:03,174 :: INFO :: ndcg [0.014547737321906217, 0.03805979373176062, 0.049562561624769436]
2023-05-14 19:52:09,096 :: INFO :: Epoch 185: loss tensor(34.9341, device='cuda:0', grad_fn=<AddBackward0>), U.norm 216.23458862304688, V.norm 283.5711364746094, MLP.norm 6.813304424285889
2023-05-14 19:52:58,690 :: INFO :: ----- val -----
2023-05-14 19:52:58,690 :: INFO :: Precision [0.01712768311573357, 0.015755698163309982, 0.013489710112856799]
2023-05-14 19:52:58,690 :: INFO :: Recall [0.007746447935297611, 0.03156753908790521, 0.05193109661308629]
2023-05-14 19:52:58,690 :: INFO :: ndcg [0.01712768311573357, 0.04631087594556334, 0.061816533871539674]
2023-05-14 19:53:48,987 :: INFO :: ----- test -----
2023-05-14 19:53:48,987 :: INFO :: Precision [0.014602325454446204, 0.012631693869752247, 0.010259839510890722]
2023-05-14 19:53:48,987 :: INFO :: Recall [0.009334779515342848, 0.03581493713741279, 0.05633588276283884]
2023-05-14 19:53:48,987 :: INFO :: ndcg [0.014602325454446204, 0.03812987397307865, 0.0495305945904023]
2023-05-14 19:53:54,971 :: INFO :: Epoch 190: loss tensor(32.0799, device='cuda:0', grad_fn=<AddBackward0>), U.norm 218.74960327148438, V.norm 282.88140869140625, MLP.norm 6.965458869934082
2023-05-14 19:54:44,440 :: INFO :: ----- val -----
2023-05-14 19:54:44,440 :: INFO :: Precision [0.01765877406505864, 0.015950431511395826, 0.013635760123921222]
2023-05-14 19:54:44,440 :: INFO :: Recall [0.007838066423627062, 0.031850003546226864, 0.052657427638412926]
2023-05-14 19:54:44,440 :: INFO :: ndcg [0.01765877406505864, 0.04708012928205281, 0.06268902671403212]
2023-05-14 19:55:34,002 :: INFO :: ----- test -----
2023-05-14 19:55:34,002 :: INFO :: Precision [0.01465691358698619, 0.012702658442054225, 0.010450897974780713]
2023-05-14 19:55:34,002 :: INFO :: Recall [0.009359474146729985, 0.035761330181939256, 0.05751309480630302]
2023-05-14 19:55:34,002 :: INFO :: ndcg [0.01465691358698619, 0.03848060224476064, 0.05038892627776961]
2023-05-14 19:55:39,940 :: INFO :: Epoch 195: loss tensor(34.9701, device='cuda:0', grad_fn=<AddBackward0>), U.norm 221.2152557373047, V.norm 282.5521240234375, MLP.norm 6.902008056640625
2023-05-14 19:56:29,659 :: INFO :: ----- val -----
2023-05-14 19:56:29,659 :: INFO :: Precision [0.01712768311573357, 0.016047798185438757, 0.013494135870767856]
2023-05-14 19:56:29,659 :: INFO :: Recall [0.007758078009747414, 0.03195457145652709, 0.05299240471508923]
2023-05-14 19:56:29,659 :: INFO :: ndcg [0.01712768311573357, 0.04706268983974077, 0.062427746920485847]
2023-05-14 19:57:18,252 :: INFO :: ----- test -----
2023-05-14 19:57:18,252 :: INFO :: Precision [0.014411266990556252, 0.012806375893880192, 0.010448168568153709]
2023-05-14 19:57:18,252 :: INFO :: Recall [0.009248491656676268, 0.035776219921736914, 0.0578784136715174]
2023-05-14 19:57:18,252 :: INFO :: ndcg [0.014411266990556252, 0.0386711472948158, 0.05040550278607183]
2023-05-14 19:57:24,127 :: INFO :: Epoch 200: loss tensor(31.4422, device='cuda:0', grad_fn=<AddBackward0>), U.norm 223.59066772460938, V.norm 282.00274658203125, MLP.norm 6.884322166442871
2023-05-14 19:58:13,534 :: INFO :: ----- val -----
2023-05-14 19:58:13,534 :: INFO :: Precision [0.017039167957512723, 0.015985837574684168, 0.01332153131223718]
2023-05-14 19:58:13,534 :: INFO :: Recall [0.007664210411812025, 0.032023353514435846, 0.052086533583501085]
2023-05-14 19:58:13,534 :: INFO :: ndcg [0.017039167957512723, 0.046709668824528976, 0.06178537094397989]
2023-05-14 19:59:02,753 :: INFO :: ----- test -----
2023-05-14 19:59:02,753 :: INFO :: Precision [0.01446585512309624, 0.012833669960150184, 0.01028440417053373]
2023-05-14 19:59:02,753 :: INFO :: Recall [0.009191674890994067, 0.03582579401806575, 0.056862816352950284]
2023-05-14 19:59:02,753 :: INFO :: ndcg [0.01446585512309624, 0.03863831266970503, 0.049982871966194976]
2023-05-14 19:59:08,659 :: INFO :: Epoch 205: loss tensor(33.3728, device='cuda:0', grad_fn=<AddBackward0>), U.norm 225.98171997070312, V.norm 281.395751953125, MLP.norm 7.002676963806152
2023-05-14 19:59:58,002 :: INFO :: ----- val -----
2023-05-14 19:59:58,002 :: INFO :: Precision [0.017039167957512723, 0.016189422438592095, 0.013458729807479504]
2023-05-14 19:59:58,002 :: INFO :: Recall [0.007739020325140006, 0.03226666944467298, 0.052508648683559894]
2023-05-14 19:59:58,002 :: INFO :: ndcg [0.017039167957512723, 0.0471604195630922, 0.06220682265699073]
2023-05-14 20:00:47,893 :: INFO :: ----- test -----
2023-05-14 20:00:47,893 :: INFO :: Precision [0.014711501719526175, 0.012855505213166179, 0.010407227468748722]
2023-05-14 20:00:47,893 :: INFO :: Recall [0.009316772694731532, 0.03605861523354158, 0.05711449551752602]
2023-05-14 20:00:47,893 :: INFO :: ndcg [0.014711501719526175, 0.03873685661508975, 0.05029834146923559]
2023-05-14 20:00:53,831 :: INFO :: Epoch 210: loss tensor(32.9305, device='cuda:0', grad_fn=<AddBackward0>), U.norm 228.35398864746094, V.norm 280.5896911621094, MLP.norm 6.910624027252197
2023-05-14 20:01:43,518 :: INFO :: ----- val -----
2023-05-14 20:01:43,518 :: INFO :: Precision [0.016862137641071032, 0.016038946669616676, 0.013396769196724903]
2023-05-14 20:01:43,518 :: INFO :: Recall [0.007618956685317686, 0.03197150253728975, 0.05212352762211963]
2023-05-14 20:01:43,518 :: INFO :: ndcg [0.016862137641071032, 0.04682547116482695, 0.06190560336572718]
2023-05-14 20:02:33,128 :: INFO :: ----- test -----
2023-05-14 20:02:33,128 :: INFO :: Precision [0.01484797205087614, 0.012779081827610199, 0.010366286369343725]
2023-05-14 20:02:33,128 :: INFO :: Recall [0.009359576722469239, 0.03587816697546279, 0.057549922913847244]
2023-05-14 20:02:33,128 :: INFO :: ndcg [0.01484797205087614, 0.038760303734893926, 0.05039472344244509]
2023-05-14 20:02:39,034 :: INFO :: Epoch 215: loss tensor(33.4754, device='cuda:0', grad_fn=<AddBackward0>), U.norm 230.69125366210938, V.norm 280.0823669433594, MLP.norm 6.902552604675293
2023-05-14 20:03:29,145 :: INFO :: ----- val -----
2023-05-14 20:03:29,145 :: INFO :: Precision [0.0169949103784023, 0.016171719406947935, 0.013511838902412024]
2023-05-14 20:03:29,145 :: INFO :: Recall [0.007785414946903368, 0.032188650704851766, 0.052872735078571605]
2023-05-14 20:03:29,145 :: INFO :: ndcg [0.0169949103784023, 0.04754340290075036, 0.062667726806712]
2023-05-14 20:04:18,926 :: INFO :: ----- test -----
2023-05-14 20:04:18,926 :: INFO :: Precision [0.014684207653256181, 0.01293192859872215, 0.010497297887439713]
2023-05-14 20:04:18,926 :: INFO :: Recall [0.009311887923347499, 0.036343025738400044, 0.05819022912348767]
2023-05-14 20:04:18,926 :: INFO :: ndcg [0.014684207653256181, 0.03910580948993465, 0.05081712183350965]
2023-05-14 20:04:24,895 :: INFO :: Epoch 220: loss tensor(33.6007, device='cuda:0', grad_fn=<AddBackward0>), U.norm 233.02456665039062, V.norm 279.5358581542969, MLP.norm 7.085853099822998
2023-05-14 20:05:14,348 :: INFO :: ----- val -----
2023-05-14 20:05:14,348 :: INFO :: Precision [0.017216198273954413, 0.016109758796193346, 0.01359150254481079]
2023-05-14 20:05:14,348 :: INFO :: Recall [0.007973680673482182, 0.03250467830236157, 0.053076035954781636]
2023-05-14 20:05:14,348 :: INFO :: ndcg [0.017216198273954413, 0.04707919729833143, 0.06259265297091476]
2023-05-14 20:06:03,676 :: INFO :: ----- test -----
2023-05-14 20:06:03,676 :: INFO :: Precision [0.01465691358698619, 0.013002893171024129, 0.01048910966755871]
2023-05-14 20:06:03,676 :: INFO :: Recall [0.009261359843980832, 0.0367248951176578, 0.05774425187763824]
2023-05-14 20:06:03,676 :: INFO :: ndcg [0.01465691358698619, 0.039048094477370114, 0.050648162108847865]
2023-05-14 20:06:09,504 :: INFO :: Epoch 225: loss tensor(34.3807, device='cuda:0', grad_fn=<AddBackward0>), U.norm 235.27557373046875, V.norm 278.8704528808594, MLP.norm 7.096182823181152
2023-05-14 20:06:58,145 :: INFO :: ----- val -----
2023-05-14 20:06:58,145 :: INFO :: Precision [0.017083425536623148, 0.016065501217082924, 0.013781810134985637]
2023-05-14 20:06:58,145 :: INFO :: Recall [0.007822047233152647, 0.03237338960465063, 0.053888624581075725]
2023-05-14 20:06:58,145 :: INFO :: ndcg [0.017083425536623148, 0.046991227699078895, 0.06306983935044609]
2023-05-14 20:07:47,645 :: INFO :: ----- test -----
2023-05-14 20:07:47,645 :: INFO :: Precision [0.014957148315956112, 0.01312844587586609, 0.010540968393471703]
2023-05-14 20:07:47,645 :: INFO :: Recall [0.0093831594760083, 0.03721077027255648, 0.05811563073192758]
2023-05-14 20:07:47,645 :: INFO :: ndcg [0.014957148315956112, 0.039582199810333035, 0.05104190833415708]
2023-05-14 20:07:53,520 :: INFO :: Epoch 230: loss tensor(32.1600, device='cuda:0', grad_fn=<AddBackward0>), U.norm 237.51239013671875, V.norm 278.0892333984375, MLP.norm 7.012053966522217
2023-05-14 20:08:42,333 :: INFO :: ----- val -----
2023-05-14 20:08:42,333 :: INFO :: Precision [0.016729364903739767, 0.016295640628457106, 0.013781810134985643]
2023-05-14 20:08:42,333 :: INFO :: Recall [0.007798700267854361, 0.032538167125798975, 0.05351141682146687]
2023-05-14 20:08:42,333 :: INFO :: ndcg [0.016729364903739767, 0.047084124409771895, 0.06287202759677438]
2023-05-14 20:09:31,115 :: INFO :: ----- test -----
2023-05-14 20:09:31,115 :: INFO :: Precision [0.014111032261586332, 0.012964681478246143, 0.010535509580217699]
2023-05-14 20:09:31,115 :: INFO :: Recall [0.008992647758423167, 0.036433083356999824, 0.05733259943663882]
2023-05-14 20:09:31,115 :: INFO :: ndcg [0.014111032261586332, 0.03858937545988522, 0.05024843339376651]
2023-05-14 20:09:37,036 :: INFO :: Epoch 235: loss tensor(35.2947, device='cuda:0', grad_fn=<AddBackward0>), U.norm 239.75486755371094, V.norm 277.7311706542969, MLP.norm 7.048124313354492
2023-05-14 20:10:25,099 :: INFO :: ----- val -----
2023-05-14 20:10:25,099 :: INFO :: Precision [0.01779154680238991, 0.016233680017702524, 0.013958840451427361]
2023-05-14 20:10:25,099 :: INFO :: Recall [0.007591616941190962, 0.03254758535711108, 0.05383640147354834]
2023-05-14 20:10:25,099 :: INFO :: ndcg [0.01779154680238991, 0.04740041687347104, 0.06365714664011637]
2023-05-14 20:11:13,739 :: INFO :: ----- test -----
2023-05-14 20:11:13,739 :: INFO :: Precision [0.015311971177466018, 0.013062940116818111, 0.010554615426606701]
2023-05-14 20:11:13,739 :: INFO :: Recall [0.009617591338647825, 0.03660561660569095, 0.05719451404806189]
2023-05-14 20:11:13,739 :: INFO :: ndcg [0.015311971177466018, 0.03951028166724865, 0.05112392216212222]
2023-05-14 20:11:19,677 :: INFO :: Epoch 240: loss tensor(34.1251, device='cuda:0', grad_fn=<AddBackward0>), U.norm 241.9430694580078, V.norm 277.0770568847656, MLP.norm 7.0408711433410645
2023-05-14 20:12:09,304 :: INFO :: ----- val -----
2023-05-14 20:12:09,304 :: INFO :: Precision [0.018234122593494134, 0.016375304270855862, 0.014157999557424314]
2023-05-14 20:12:09,304 :: INFO :: Recall [0.007907119750666589, 0.032555324757959865, 0.05400883071819769]
2023-05-14 20:12:09,304 :: INFO :: ndcg [0.018234122593494134, 0.04783590044468973, 0.06429765482734953]
2023-05-14 20:12:58,757 :: INFO :: ----- test -----
2023-05-14 20:12:58,757 :: INFO :: Precision [0.015311971177466018, 0.013122987062612092, 0.010611932965773696]
2023-05-14 20:12:58,757 :: INFO :: Recall [0.009571264694374018, 0.03684003300738368, 0.05733491512691724]
2023-05-14 20:12:58,757 :: INFO :: ndcg [0.015311971177466018, 0.039463004298650205, 0.05115665530133879]
2023-05-14 20:13:04,585 :: INFO :: Epoch 245: loss tensor(35.2826, device='cuda:0', grad_fn=<AddBackward0>), U.norm 244.15362548828125, V.norm 276.399169921875, MLP.norm 7.069864273071289
2023-05-14 20:13:53,757 :: INFO :: ----- val -----
2023-05-14 20:13:53,757 :: INFO :: Precision [0.018057092277052444, 0.016047798185438757, 0.01367116618720956]
2023-05-14 20:13:53,757 :: INFO :: Recall [0.0077694032881740396, 0.03238458470789609, 0.05281799049259507]
2023-05-14 20:13:53,757 :: INFO :: ndcg [0.018057092277052444, 0.04717733258512314, 0.06293921787398903]
2023-05-14 20:14:44,044 :: INFO :: ----- test -----
2023-05-14 20:14:44,044 :: INFO :: Precision [0.01684043888858562, 0.012948305038484145, 0.01076205033025869]
2023-05-14 20:14:44,044 :: INFO :: Recall [0.010433391398714903, 0.03716839152055989, 0.059495209965800516]
2023-05-14 20:14:44,044 :: INFO :: ndcg [0.01684043888858562, 0.03975790086657599, 0.05211330588845195]
2023-05-14 20:14:50,185 :: INFO :: Epoch 250: loss tensor(33.7676, device='cuda:0', grad_fn=<AddBackward0>), U.norm 246.266845703125, V.norm 275.94134521484375, MLP.norm 7.015493392944336
2023-05-14 20:15:39,278 :: INFO :: ----- val -----
2023-05-14 20:15:39,278 :: INFO :: Precision [0.017393228590396104, 0.016127461827837516, 0.013737552555875195]
2023-05-14 20:15:39,278 :: INFO :: Recall [0.007656742561892044, 0.03225117028562491, 0.053167961316969055]
2023-05-14 20:15:39,278 :: INFO :: ndcg [0.017393228590396104, 0.04705729222104438, 0.06295790990222676]
2023-05-14 20:16:28,327 :: INFO :: ----- test -----
2023-05-14 20:16:28,327 :: INFO :: Precision [0.015148206779846062, 0.013041104863802116, 0.010549156613352694]
2023-05-14 20:16:28,327 :: INFO :: Recall [0.009478900456177585, 0.037031021550690144, 0.057808554965853534]
2023-05-14 20:16:28,327 :: INFO :: ndcg [0.015148206779846062, 0.03930728768279078, 0.05088722508746795]
2023-05-14 20:16:34,264 :: INFO :: Epoch 255: loss tensor(32.0703, device='cuda:0', grad_fn=<AddBackward0>), U.norm 248.35340881347656, V.norm 275.502197265625, MLP.norm 7.146561622619629
2023-05-14 20:17:23,231 :: INFO :: ----- val -----
2023-05-14 20:17:23,231 :: INFO :: Precision [0.017880061960610753, 0.016313343660101284, 0.013985394998893619]
2023-05-14 20:17:23,231 :: INFO :: Recall [0.00789581366737882, 0.03270838702190793, 0.05398335250980385]
2023-05-14 20:17:23,231 :: INFO :: ndcg [0.017880061960610753, 0.04784913436028849, 0.06408282903603654]
2023-05-14 20:18:13,529 :: INFO :: ----- test -----
2023-05-14 20:18:13,529 :: INFO :: Precision [0.015257383044926033, 0.013095692996342098, 0.010663791691686688]
2023-05-14 20:18:13,529 :: INFO :: Recall [0.009493970589021612, 0.037047561198591095, 0.0579726661186373]
2023-05-14 20:18:13,529 :: INFO :: ndcg [0.015257383044926033, 0.03963485178086811, 0.051416851316364344]
2023-05-14 20:18:19,450 :: INFO :: Epoch 260: loss tensor(31.8655, device='cuda:0', grad_fn=<AddBackward0>), U.norm 250.4355926513672, V.norm 275.13909912109375, MLP.norm 7.109453201293945
2023-05-14 20:19:08,996 :: INFO :: ----- val -----
2023-05-14 20:19:08,996 :: INFO :: Precision [0.017393228590396104, 0.016304492144279193, 0.013781810134985637]
2023-05-14 20:19:08,996 :: INFO :: Recall [0.007723376663283671, 0.03227186288124709, 0.05320592634323777]
2023-05-14 20:19:08,996 :: INFO :: ndcg [0.017393228590396104, 0.047814688050744084, 0.06351070671778467]
2023-05-14 20:19:56,917 :: INFO :: ----- test -----
2023-05-14 20:19:56,917 :: INFO :: Precision [0.014547737321906217, 0.012953763851738144, 0.010671979911567693]
2023-05-14 20:19:56,917 :: INFO :: Recall [0.00923151597998695, 0.03638685221030613, 0.05847650651001306]
2023-05-14 20:19:56,917 :: INFO :: ndcg [0.014547737321906217, 0.03911086673855387, 0.05119343949673665]
2023-05-14 20:20:02,713 :: INFO :: Epoch 265: loss tensor(36.2333, device='cuda:0', grad_fn=<AddBackward0>), U.norm 252.49855041503906, V.norm 274.6369934082031, MLP.norm 7.02341890335083
2023-05-14 20:20:50,390 :: INFO :: ----- val -----
2023-05-14 20:20:50,390 :: INFO :: Precision [0.01677362248285019, 0.016437264881610448, 0.013994246514715715]
2023-05-14 20:20:50,390 :: INFO :: Recall [0.007593810624773695, 0.03253511297279078, 0.05342747191713774]
2023-05-14 20:20:50,390 :: INFO :: ndcg [0.01677362248285019, 0.04768836834643847, 0.06378266705076076]
2023-05-14 20:21:39,123 :: INFO :: ----- test -----
2023-05-14 20:21:39,123 :: INFO :: Precision [0.014602325454446204, 0.013024728424040122, 0.010625579998908683]
2023-05-14 20:21:39,123 :: INFO :: Recall [0.00933198512284378, 0.03667473313780085, 0.05808710978820512]
2023-05-14 20:21:39,123 :: INFO :: ndcg [0.014602325454446204, 0.03926708049473388, 0.05106591621421255]
2023-05-14 20:21:44,810 :: INFO :: Epoch 270: loss tensor(33.4785, device='cuda:0', grad_fn=<AddBackward0>), U.norm 254.53799438476562, V.norm 274.1473388671875, MLP.norm 7.112092971801758
2023-05-14 20:22:32,559 :: INFO :: ----- val -----
2023-05-14 20:22:32,559 :: INFO :: Precision [0.01664084974551892, 0.016525780039831305, 0.014162425315335342]
2023-05-14 20:22:32,559 :: INFO :: Recall [0.007661400295093358, 0.0325792041189217, 0.05406789507988015]
2023-05-14 20:22:32,559 :: INFO :: ndcg [0.01664084974551892, 0.04788236193435214, 0.0641077317746237]
2023-05-14 20:23:21,469 :: INFO :: ----- test -----
2023-05-14 20:23:21,469 :: INFO :: Precision [0.014711501719526175, 0.013177575195152074, 0.010680168131448693]
2023-05-14 20:23:21,469 :: INFO :: Recall [0.009516382534846412, 0.03742825890059693, 0.058309088253809975]
2023-05-14 20:23:21,469 :: INFO :: ndcg [0.014711501719526175, 0.03979441644847812, 0.05148014234614015]
2023-05-14 20:23:27,360 :: INFO :: Epoch 275: loss tensor(32.0276, device='cuda:0', grad_fn=<AddBackward0>), U.norm 256.5152893066406, V.norm 273.71514892578125, MLP.norm 7.0575714111328125
2023-05-14 20:24:15,921 :: INFO :: ----- val -----
2023-05-14 20:24:15,921 :: INFO :: Precision [0.017039167957512723, 0.016331046691745447, 0.013989820756804671]
2023-05-14 20:24:15,921 :: INFO :: Recall [0.007649627542728252, 0.03237338658968705, 0.05325857991157464]
2023-05-14 20:24:15,921 :: INFO :: ndcg [0.017039167957512723, 0.047638983292252766, 0.06381541885439529]
2023-05-14 20:25:04,201 :: INFO :: ----- test -----
2023-05-14 20:25:04,201 :: INFO :: Precision [0.014957148315956112, 0.013095692996342098, 0.010530050766963691]
2023-05-14 20:25:04,201 :: INFO :: Recall [0.009573669353419325, 0.036562005721249305, 0.05705512818249518]
2023-05-14 20:25:04,201 :: INFO :: ndcg [0.014957148315956112, 0.039703421456450796, 0.05112397134638544]
2023-05-14 20:25:10,044 :: INFO :: Epoch 280: loss tensor(34.7393, device='cuda:0', grad_fn=<AddBackward0>), U.norm 258.479736328125, V.norm 273.554443359375, MLP.norm 7.1238579750061035
2023-05-14 20:25:59,174 :: INFO :: ----- val -----
2023-05-14 20:25:59,174 :: INFO :: Precision [0.01717194069484399, 0.016525780039831302, 0.013901305598583803]
2023-05-14 20:25:59,174 :: INFO :: Recall [0.007584474683620458, 0.03265961123073864, 0.05343320131742304]
2023-05-14 20:25:59,174 :: INFO :: ndcg [0.01717194069484399, 0.04821359264124868, 0.06383291510371697]
2023-05-14 20:26:48,610 :: INFO :: ----- test -----
2023-05-14 20:26:48,610 :: INFO :: Precision [0.014984442382226104, 0.013193951634914068, 0.010699273977837683]
2023-05-14 20:26:48,610 :: INFO :: Recall [0.009569355866928862, 0.03698367761753871, 0.058972228286974473]
2023-05-14 20:26:48,610 :: INFO :: ndcg [0.014984442382226104, 0.03994389517865467, 0.05180321803454807]
2023-05-14 20:26:54,532 :: INFO :: Epoch 285: loss tensor(39.7348, device='cuda:0', grad_fn=<AddBackward0>), U.norm 260.3448181152344, V.norm 273.18157958984375, MLP.norm 7.067424774169922
2023-05-14 20:27:42,905 :: INFO :: ----- val -----
2023-05-14 20:27:42,905 :: INFO :: Precision [0.018057092277052444, 0.01662314671387422, 0.014034078335915088]
2023-05-14 20:27:42,905 :: INFO :: Recall [0.007651948823898313, 0.032771564451636766, 0.05348022487167794]
2023-05-14 20:27:42,905 :: INFO :: ndcg [0.018057092277052444, 0.048724260511505246, 0.06441865279276458]
2023-05-14 20:28:31,393 :: INFO :: ----- test -----
2023-05-14 20:28:31,393 :: INFO :: Precision [0.014820677984606147, 0.013002893171024127, 0.010732026857361684]
2023-05-14 20:28:31,393 :: INFO :: Recall [0.009367236807612358, 0.03637606205332092, 0.05838957733379636]
2023-05-14 20:28:31,393 :: INFO :: ndcg [0.014820677984606147, 0.03933360652218914, 0.05156025790540185]
2023-05-14 20:28:37,190 :: INFO :: Epoch 290: loss tensor(34.1158, device='cuda:0', grad_fn=<AddBackward0>), U.norm 262.29595947265625, V.norm 272.84881591796875, MLP.norm 7.111080169677734
2023-05-14 20:29:26,423 :: INFO :: ----- val -----
2023-05-14 20:29:26,423 :: INFO :: Precision [0.018189865014383712, 0.016543483071475462, 0.0142819207789335]
2023-05-14 20:29:26,423 :: INFO :: Recall [0.007407464817761588, 0.032760428171670276, 0.05392681259873366]
2023-05-14 20:29:26,423 :: INFO :: ndcg [0.018189865014383712, 0.04862518897702526, 0.06494130154508564]
2023-05-14 20:30:15,671 :: INFO :: ----- test -----
2023-05-14 20:30:15,671 :: INFO :: Precision [0.015748676237785905, 0.013139363502374084, 0.010770238550139682]
2023-05-14 20:30:15,671 :: INFO :: Recall [0.009837629570963576, 0.036936089482650716, 0.05801898289703972]
2023-05-14 20:30:15,671 :: INFO :: ndcg [0.015748676237785905, 0.039882803298670604, 0.05187904670141161]
2023-05-14 20:30:21,608 :: INFO :: Epoch 295: loss tensor(35.8305, device='cuda:0', grad_fn=<AddBackward0>), U.norm 264.1819152832031, V.norm 272.70819091796875, MLP.norm 7.104879856109619
2023-05-14 20:31:10,795 :: INFO :: ----- val -----
2023-05-14 20:31:10,795 :: INFO :: Precision [0.020004425757911042, 0.016508077008187124, 0.014135870767869093]
2023-05-14 20:31:10,795 :: INFO :: Recall [0.007991417944488578, 0.032446905779177596, 0.053462419187075824]
2023-05-14 20:31:10,795 :: INFO :: ndcg [0.020004425757911042, 0.049161528172543274, 0.06499291554337464]
2023-05-14 20:31:59,204 :: INFO :: ----- test -----
2023-05-14 20:31:59,204 :: INFO :: Precision [0.016649380424695673, 0.013243080954200051, 0.010663791691686698]
2023-05-14 20:31:59,204 :: INFO :: Recall [0.0101627897869152, 0.03744856538381038, 0.05769316541527636]
2023-05-14 20:31:59,204 :: INFO :: ndcg [0.016649380424695673, 0.04056116179984331, 0.05216305948155657]
2023-05-14 20:32:05,095 :: INFO :: Epoch 300: loss tensor(34.6356, device='cuda:0', grad_fn=<AddBackward0>), U.norm 265.9869384765625, V.norm 272.4326171875, MLP.norm 7.1031975746154785
2023-05-14 20:32:53,640 :: INFO :: ----- val -----
2023-05-14 20:32:53,640 :: INFO :: Precision [0.019738880283248505, 0.01670281035627297, 0.014259791989378299]
2023-05-14 20:32:53,640 :: INFO :: Recall [0.008112013922775529, 0.0329044687757948, 0.05391667724041738]
2023-05-14 20:32:53,640 :: INFO :: ndcg [0.019738880283248505, 0.04971958169482602, 0.06555856876873183]
2023-05-14 20:33:42,811 :: INFO :: ----- test -----
2023-05-14 20:33:42,811 :: INFO :: Precision [0.016294557563185763, 0.013139363502374084, 0.010685626944702693]
2023-05-14 20:33:42,811 :: INFO :: Recall [0.010096398338556944, 0.03685041368789114, 0.05759035477313517]
2023-05-14 20:33:42,811 :: INFO :: ndcg [0.016294557563185763, 0.04021559937520345, 0.0520479072435221]
2023-05-14 20:33:48,733 :: INFO :: Epoch 305: loss tensor(40.2133, device='cuda:0', grad_fn=<AddBackward0>), U.norm 267.72235107421875, V.norm 271.91668701171875, MLP.norm 7.0849103927612305
2023-05-14 20:34:37,471 :: INFO :: ----- val -----
2023-05-14 20:34:37,471 :: INFO :: Precision [0.01783580438150033, 0.016587740650585877, 0.014290772294755597]
2023-05-14 20:34:37,471 :: INFO :: Recall [0.007067328767733838, 0.032764619582788576, 0.05468863164032676]
2023-05-14 20:34:37,471 :: INFO :: ndcg [0.01783580438150033, 0.048713788214496685, 0.06504036564982478]
2023-05-14 20:35:26,110 :: INFO :: ----- test -----
2023-05-14 20:35:26,110 :: INFO :: Precision [0.01599432283421584, 0.01303018723729412, 0.010863038375457684]
2023-05-14 20:35:26,110 :: INFO :: Recall [0.009938678663496918, 0.03678293197382686, 0.05895679750334809]
2023-05-14 20:35:26,110 :: INFO :: ndcg [0.01599432283421584, 0.03987340234543377, 0.052423882511917805]
2023-05-14 20:35:32,032 :: INFO :: Epoch 310: loss tensor(34.2901, device='cuda:0', grad_fn=<AddBackward0>), U.norm 269.5230712890625, V.norm 271.746337890625, MLP.norm 7.135689735412598
2023-05-14 20:36:21,187 :: INFO :: ----- val -----
2023-05-14 20:36:21,187 :: INFO :: Precision [0.019561849966806814, 0.016729364903739226, 0.014237663199823072]
2023-05-14 20:36:21,187 :: INFO :: Recall [0.007990159758245002, 0.03325608583883998, 0.053927203389216145]
2023-05-14 20:36:21,187 :: INFO :: ndcg [0.019561849966806814, 0.04966804325177884, 0.06561842287089523]
2023-05-14 20:37:10,033 :: INFO :: ----- test -----
2023-05-14 20:37:10,033 :: INFO :: Precision [0.01651291009334571, 0.013046563677056115, 0.010710191604345694]
2023-05-14 20:37:10,033 :: INFO :: Recall [0.009920995758956533, 0.036828275981161465, 0.058273236336653905]
2023-05-14 20:37:10,033 :: INFO :: ndcg [0.01651291009334571, 0.04010439067728828, 0.05213088543990106]
2023-05-14 20:37:15,908 :: INFO :: Epoch 315: loss tensor(35.0074, device='cuda:0', grad_fn=<AddBackward0>), U.norm 271.2975158691406, V.norm 271.7220153808594, MLP.norm 7.03986930847168
2023-05-14 20:38:04,750 :: INFO :: ----- val -----
2023-05-14 20:38:04,750 :: INFO :: Precision [0.01849966806815667, 0.016685107324628808, 0.014299623810577668]
2023-05-14 20:38:04,750 :: INFO :: Recall [0.0074130719296947945, 0.0326753738452618, 0.05433649597715632]
2023-05-14 20:38:04,750 :: INFO :: ndcg [0.01849966806815667, 0.04907804874604112, 0.06555530654088827]
2023-05-14 20:38:53,312 :: INFO :: ----- test -----
2023-05-14 20:38:53,312 :: INFO :: Precision [0.016048910966755828, 0.013172116381898075, 0.010808450242917694]
2023-05-14 20:38:53,312 :: INFO :: Recall [0.01004707158091063, 0.037259479737464844, 0.05929007780960795]
2023-05-14 20:38:53,312 :: INFO :: ndcg [0.016048910966755828, 0.040243021663437184, 0.05240077667308616]
2023-05-14 20:38:59,124 :: INFO :: Epoch 320: loss tensor(33.7485, device='cuda:0', grad_fn=<AddBackward0>), U.norm 272.9947814941406, V.norm 271.5843200683594, MLP.norm 6.9304046630859375
2023-05-14 20:39:47,998 :: INFO :: ----- val -----
2023-05-14 20:39:47,998 :: INFO :: Precision [0.019871653020579774, 0.01651692852400921, 0.014175702589068466]
2023-05-14 20:39:47,998 :: INFO :: Recall [0.007767174206788055, 0.032654881213904835, 0.053750485870090556]
2023-05-14 20:39:47,998 :: INFO :: ndcg [0.019871653020579774, 0.04915017128715425, 0.06522365841441644]
2023-05-14 20:40:37,502 :: INFO :: ----- test -----
2023-05-14 20:40:37,502 :: INFO :: Precision [0.016895027021125608, 0.012855505213166174, 0.010696544571210697]
2023-05-14 20:40:37,502 :: INFO :: Recall [0.010109392112452075, 0.036550412191779603, 0.058485399334832586]
2023-05-14 20:40:37,502 :: INFO :: ndcg [0.016895027021125608, 0.0399194307374512, 0.052163108571271946]
2023-05-14 20:40:43,376 :: INFO :: Epoch 325: loss tensor(35.9912, device='cuda:0', grad_fn=<AddBackward0>), U.norm 274.71075439453125, V.norm 271.435302734375, MLP.norm 6.996169567108154
2023-05-14 20:41:32,469 :: INFO :: ----- val -----
2023-05-14 20:41:32,469 :: INFO :: Precision [0.020314228811684, 0.016587740650585877, 0.014516485948218773]
2023-05-14 20:41:32,469 :: INFO :: Recall [0.008140300010221186, 0.032307935534773825, 0.05475614162036931]
2023-05-14 20:41:32,469 :: INFO :: ndcg [0.020314228811684, 0.04973479869260928, 0.0665292682326546]
2023-05-14 20:42:21,655 :: INFO :: ----- test -----
2023-05-14 20:42:21,655 :: INFO :: Precision [0.017167967683825537, 0.012997434357770128, 0.010846661935695681]
2023-05-14 20:42:21,655 :: INFO :: Recall [0.010240916460821566, 0.0368842366358893, 0.05903145855140074]
2023-05-14 20:42:21,655 :: INFO :: ndcg [0.017167967683825537, 0.04028843034416094, 0.052673071222787575]
2023-05-14 20:42:27,561 :: INFO :: Epoch 330: loss tensor(33.0011, device='cuda:0', grad_fn=<AddBackward0>), U.norm 276.45391845703125, V.norm 271.4108581542969, MLP.norm 6.9806413650512695
2023-05-14 20:43:16,629 :: INFO :: ----- val -----
2023-05-14 20:43:16,629 :: INFO :: Precision [0.020845319761009073, 0.016986058862579662, 0.014644832927639045]
2023-05-14 20:43:16,629 :: INFO :: Recall [0.008197488481523917, 0.03319590151289498, 0.054508381359537665]
2023-05-14 20:43:16,629 :: INFO :: ndcg [0.020845319761009073, 0.051000812982438595, 0.06749640270178527]
2023-05-14 20:44:06,378 :: INFO :: ----- test -----
2023-05-14 20:44:06,378 :: INFO :: Precision [0.017713849009225395, 0.01343959823134399, 0.010977673453791673]
2023-05-14 20:44:06,378 :: INFO :: Recall [0.010516694330595171, 0.03757129834042162, 0.05956464815310912]
2023-05-14 20:44:06,378 :: INFO :: ndcg [0.017713849009225395, 0.04154682903603333, 0.053632190650850974]
2023-05-14 20:44:12,299 :: INFO :: Epoch 335: loss tensor(35.8410, device='cuda:0', grad_fn=<AddBackward0>), U.norm 278.07586669921875, V.norm 271.0018615722656, MLP.norm 6.999090194702148
2023-05-14 20:45:01,111 :: INFO :: ----- val -----
2023-05-14 20:45:01,111 :: INFO :: Precision [0.018853728701040053, 0.01689754370435882, 0.014649258685550092]
2023-05-14 20:45:01,111 :: INFO :: Recall [0.007230824838324945, 0.033009456529939286, 0.05470062201072607]
2023-05-14 20:45:01,111 :: INFO :: ndcg [0.018853728701040053, 0.04998141268367472, 0.06666183565642302]
2023-05-14 20:45:49,959 :: INFO :: ----- test -----
2023-05-14 20:45:49,959 :: INFO :: Precision [0.016294557563185763, 0.01331404552650203, 0.010811179649544695]
2023-05-14 20:45:49,959 :: INFO :: Recall [0.009978543895496106, 0.036952466103440995, 0.058484410437800734]
2023-05-14 20:45:49,959 :: INFO :: ndcg [0.016294557563185763, 0.04073627898996126, 0.05269195466828757]
2023-05-14 20:45:55,819 :: INFO :: Epoch 340: loss tensor(32.4624, device='cuda:0', grad_fn=<AddBackward0>), U.norm 279.7271423339844, V.norm 270.65338134765625, MLP.norm 7.042166709899902
2023-05-14 20:46:44,442 :: INFO :: ----- val -----
2023-05-14 20:46:44,442 :: INFO :: Precision [0.01916353175481301, 0.01687984067271465, 0.014494357158663576]
2023-05-14 20:46:44,442 :: INFO :: Recall [0.007704813705874334, 0.03298579418355784, 0.05454752102880193]
2023-05-14 20:46:44,442 :: INFO :: ndcg [0.01916353175481301, 0.04991435712856431, 0.06627765625369841]
2023-05-14 20:47:33,457 :: INFO :: ----- test -----
2023-05-14 20:47:33,457 :: INFO :: Precision [0.016567498225885692, 0.013341339592772021, 0.010822097276052696]
2023-05-14 20:47:33,457 :: INFO :: Recall [0.009949938182559915, 0.0372251248597648, 0.058978814783618626]
2023-05-14 20:47:33,457 :: INFO :: ndcg [0.016567498225885692, 0.04083580795448889, 0.05292109325685097]
2023-05-14 20:47:39,378 :: INFO :: Epoch 345: loss tensor(34.3094, device='cuda:0', grad_fn=<AddBackward0>), U.norm 281.2705993652344, V.norm 270.6107482910156, MLP.norm 6.921540260314941
2023-05-14 20:48:28,424 :: INFO :: ----- val -----
2023-05-14 20:48:28,424 :: INFO :: Precision [0.02080106218189865, 0.01702146492586799, 0.014543040495685063]
2023-05-14 20:48:28,424 :: INFO :: Recall [0.008227426619502964, 0.03304583281684215, 0.05441031411397084]
2023-05-14 20:48:28,424 :: INFO :: ndcg [0.02080106218189865, 0.051021262679766075, 0.06724276836563528]
2023-05-14 20:49:17,614 :: INFO :: ----- test -----
2023-05-14 20:49:17,614 :: INFO :: Precision [0.0176865549429554, 0.013439598231343991, 0.010805720836290686]
2023-05-14 20:49:17,614 :: INFO :: Recall [0.0103566766894267, 0.03732069572898959, 0.05883427608511954]
2023-05-14 20:49:17,614 :: INFO :: ndcg [0.0176865549429554, 0.041521218536424126, 0.05324625940215423]
2023-05-14 20:49:23,505 :: INFO :: Epoch 350: loss tensor(34.6656, device='cuda:0', grad_fn=<AddBackward0>), U.norm 282.85076904296875, V.norm 270.4268493652344, MLP.norm 6.97117805480957
2023-05-14 20:50:11,051 :: INFO :: ----- val -----
2023-05-14 20:50:11,051 :: INFO :: Precision [0.02044700154901527, 0.017101128568266757, 0.014649258685550087]
2023-05-14 20:50:11,051 :: INFO :: Recall [0.008059240581881595, 0.033114734549855245, 0.05494235088505404]
2023-05-14 20:50:11,051 :: INFO :: ndcg [0.02044700154901527, 0.05099046095472637, 0.06735944143948687]
2023-05-14 20:50:59,330 :: INFO :: ----- test -----
2023-05-14 20:50:59,330 :: INFO :: Precision [0.017550084611605437, 0.013352257219280017, 0.010876685408592678]
2023-05-14 20:50:59,330 :: INFO :: Recall [0.01045429502023189, 0.037271005420395194, 0.059151206174650264]
2023-05-14 20:50:59,330 :: INFO :: ndcg [0.017550084611605437, 0.04132970020479074, 0.053379273773206455]
2023-05-14 20:51:05,190 :: INFO :: Epoch 355: loss tensor(35.0606, device='cuda:0', grad_fn=<AddBackward0>), U.norm 284.3092956542969, V.norm 270.2932434082031, MLP.norm 7.000955104827881
2023-05-14 20:51:53,914 :: INFO :: ----- val -----
2023-05-14 20:51:53,914 :: INFO :: Precision [0.02093383491922992, 0.01713653463155509, 0.014799734454525548]
2023-05-14 20:51:53,914 :: INFO :: Recall [0.008427165662843874, 0.03332333782374291, 0.05545410603817824]
2023-05-14 20:51:53,914 :: INFO :: ndcg [0.02093383491922992, 0.051397985772251194, 0.06800414247132505]
2023-05-14 20:52:42,335 :: INFO :: ----- test -----
2023-05-14 20:52:42,335 :: INFO :: Precision [0.017249849882635514, 0.013494186363883975, 0.010988591080299679]
2023-05-14 20:52:42,335 :: INFO :: Recall [0.010302294239444057, 0.0379383934408017, 0.059980904672897115]
2023-05-14 20:52:42,335 :: INFO :: ndcg [0.017249849882635514, 0.04151435237820806, 0.0536644002653319]
2023-05-14 20:52:48,163 :: INFO :: Epoch 360: loss tensor(36.1833, device='cuda:0', grad_fn=<AddBackward0>), U.norm 285.8654479980469, V.norm 270.21673583984375, MLP.norm 6.982595920562744
2023-05-14 20:53:36,318 :: INFO :: ----- val -----
2023-05-14 20:53:36,318 :: INFO :: Precision [0.019694622704138083, 0.01741978313786179, 0.014941358707678922]
2023-05-14 20:53:36,318 :: INFO :: Recall [0.007824725527110542, 0.033662902624137445, 0.05583593325802987]
2023-05-14 20:53:36,318 :: INFO :: ndcg [0.019694622704138083, 0.051227465414656864, 0.06794915799311846]
2023-05-14 20:54:25,114 :: INFO :: ----- test -----
2023-05-14 20:54:25,114 :: INFO :: Precision [0.01722255581636552, 0.01352693924340796, 0.010909438288116684]
2023-05-14 20:54:25,114 :: INFO :: Recall [0.010268363791362067, 0.03766852938119541, 0.0593178072354184]
2023-05-14 20:54:25,114 :: INFO :: ndcg [0.01722255581636552, 0.041672521016671044, 0.05355878035733904]
2023-05-14 20:54:30,885 :: INFO :: Epoch 365: loss tensor(36.9677, device='cuda:0', grad_fn=<AddBackward0>), U.norm 287.330810546875, V.norm 269.8934326171875, MLP.norm 6.966302871704102
2023-05-14 20:55:18,790 :: INFO :: ----- val -----
2023-05-14 20:55:18,790 :: INFO :: Precision [0.01898650143837132, 0.01743748616950595, 0.014892675370657457]
2023-05-14 20:55:18,790 :: INFO :: Recall [0.007349581763570414, 0.03358212372440425, 0.055304494827553896]
2023-05-14 20:55:18,790 :: INFO :: ndcg [0.01898650143837132, 0.0509723716710212, 0.06754719839392344]
2023-05-14 20:56:07,398 :: INFO :: ----- test -----
2023-05-14 20:56:07,414 :: INFO :: Precision [0.015366559310006005, 0.01353239805666196, 0.011013155739942672]
2023-05-14 20:56:07,414 :: INFO :: Recall [0.009558071354050056, 0.03757560190446883, 0.0599502535326762]
2023-05-14 20:56:07,414 :: INFO :: ndcg [0.015366559310006005, 0.04093979096601156, 0.05305096990671083]
2023-05-14 20:56:13,351 :: INFO :: Epoch 370: loss tensor(35.3912, device='cuda:0', grad_fn=<AddBackward0>), U.norm 288.8753662109375, V.norm 269.8065490722656, MLP.norm 7.031958103179932
2023-05-14 20:57:02,256 :: INFO :: ----- val -----
2023-05-14 20:57:02,256 :: INFO :: Precision [0.02062403186545696, 0.017331267979640946, 0.01458729807479548]
2023-05-14 20:57:02,256 :: INFO :: Recall [0.008262731569954636, 0.03340372868020735, 0.05521104647207077]
2023-05-14 20:57:02,256 :: INFO :: ndcg [0.02062403186545696, 0.05139701393999264, 0.0674427527805439]
2023-05-14 20:57:51,521 :: INFO :: ----- test -----
2023-05-14 20:57:51,521 :: INFO :: Precision [0.017167967683825537, 0.013390468912058005, 0.01098040286041867]
2023-05-14 20:57:51,521 :: INFO :: Recall [0.010351687952231292, 0.03752951823540968, 0.06023680992273466]
2023-05-14 20:57:51,521 :: INFO :: ndcg [0.017167967683825537, 0.041340112935036456, 0.05355802102972025]
2023-05-14 20:57:57,411 :: INFO :: Epoch 375: loss tensor(34.7730, device='cuda:0', grad_fn=<AddBackward0>), U.norm 290.3995361328125, V.norm 269.80218505859375, MLP.norm 7.049857139587402
2023-05-14 20:58:47,103 :: INFO :: ----- val -----
2023-05-14 20:58:47,103 :: INFO :: Precision [0.020314228811684, 0.01738437707457345, 0.015025448107988741]
2023-05-14 20:58:47,103 :: INFO :: Recall [0.007951380362771794, 0.03371138682559412, 0.05645475897655985]
2023-05-14 20:58:47,103 :: INFO :: ndcg [0.020314228811684, 0.0512350995312843, 0.06863082321159368]
2023-05-14 20:59:36,680 :: INFO :: ----- test -----
2023-05-14 20:59:36,680 :: INFO :: Precision [0.01626726349691577, 0.013521480430153963, 0.010944920574267674]
2023-05-14 20:59:36,680 :: INFO :: Recall [0.009915372125473845, 0.038018007183845307, 0.059518290746962715]
2023-05-14 20:59:36,680 :: INFO :: ndcg [0.01626726349691577, 0.04110673947768829, 0.053216941215274295]
2023-05-14 20:59:42,665 :: INFO :: Epoch 380: loss tensor(37.8546, device='cuda:0', grad_fn=<AddBackward0>), U.norm 291.85211181640625, V.norm 269.76995849609375, MLP.norm 7.039636611938477
2023-05-14 21:00:32,382 :: INFO :: ----- val -----
2023-05-14 21:00:32,382 :: INFO :: Precision [0.01978313786235893, 0.017348971011285113, 0.01506085417127709]
2023-05-14 21:00:32,382 :: INFO :: Recall [0.008090170285363445, 0.03375594696099775, 0.056675469704416934]
2023-05-14 21:00:32,382 :: INFO :: ndcg [0.01978313786235893, 0.050856529343826645, 0.06859472608388048]
2023-05-14 21:01:21,537 :: INFO :: ----- test -----
2023-05-14 21:01:21,537 :: INFO :: Precision [0.017086085485015556, 0.01353785686991596, 0.011056826245974668]
2023-05-14 21:01:21,537 :: INFO :: Recall [0.010331756445627907, 0.03783941210768621, 0.05961004241584453]
2023-05-14 21:01:21,537 :: INFO :: ndcg [0.017086085485015556, 0.04149889660118105, 0.053828948907546484]
2023-05-14 21:01:27,381 :: INFO :: Epoch 385: loss tensor(39.6421, device='cuda:0', grad_fn=<AddBackward0>), U.norm 293.2876892089844, V.norm 269.5667419433594, MLP.norm 7.011499881744385
2023-05-14 21:02:16,004 :: INFO :: ----- val -----
2023-05-14 21:02:16,004 :: INFO :: Precision [0.02146492586855499, 0.017720734675812644, 0.015109537508298547]
2023-05-14 21:02:16,004 :: INFO :: Recall [0.008158167165924666, 0.03430053312879718, 0.05671832802887056]
2023-05-14 21:02:16,004 :: INFO :: ndcg [0.02146492586855499, 0.052313916939819544, 0.0693910687167072]
2023-05-14 21:03:04,362 :: INFO :: ----- test -----
2023-05-14 21:03:04,362 :: INFO :: Precision [0.01746820241279546, 0.013494186363883973, 0.011089579125498655]
2023-05-14 21:03:04,362 :: INFO :: Recall [0.010501495765871734, 0.037691912949099446, 0.05991794679001594]
2023-05-14 21:03:04,362 :: INFO :: ndcg [0.01746820241279546, 0.041585093376028455, 0.05402779142860088]
2023-05-14 21:03:10,237 :: INFO :: Epoch 390: loss tensor(35.6471, device='cuda:0', grad_fn=<AddBackward0>), U.norm 294.66876220703125, V.norm 269.5404357910156, MLP.norm 6.963606834411621
2023-05-14 21:03:59,064 :: INFO :: ----- val -----
2023-05-14 21:03:59,064 :: INFO :: Precision [0.021332153131223722, 0.017499446780260546, 0.015016596592166653]
2023-05-14 21:03:59,064 :: INFO :: Recall [0.008235496096982299, 0.03393529737954346, 0.05663661637382551]
2023-05-14 21:03:59,064 :: INFO :: ndcg [0.021332153131223722, 0.05175448704769579, 0.0688944980926555]
2023-05-14 21:04:48,751 :: INFO :: ----- test -----
2023-05-14 21:04:48,751 :: INFO :: Precision [0.01776843714176538, 0.013510562803645971, 0.011086849718871668]
2023-05-14 21:04:48,751 :: INFO :: Recall [0.010522167154264841, 0.03769487437466352, 0.06022985490025174]
2023-05-14 21:04:48,751 :: INFO :: ndcg [0.01776843714176538, 0.041650230698927954, 0.054217427984187244]
2023-05-14 21:04:54,657 :: INFO :: Epoch 395: loss tensor(32.6557, device='cuda:0', grad_fn=<AddBackward0>), U.norm 296.1325988769531, V.norm 269.6068115234375, MLP.norm 7.003178119659424
2023-05-14 21:05:43,128 :: INFO :: ----- val -----
2023-05-14 21:05:43,128 :: INFO :: Precision [0.020756804602788226, 0.017455189201150124, 0.015096260234565421]
2023-05-14 21:05:43,128 :: INFO :: Recall [0.008213903580863746, 0.03384700622306697, 0.056748232555489206]
2023-05-14 21:05:43,128 :: INFO :: ndcg [0.020756804602788226, 0.05148375727292495, 0.06903330292294772]
2023-05-14 21:06:30,051 :: INFO :: ----- test -----
2023-05-14 21:06:30,051 :: INFO :: Precision [0.017713849009225395, 0.013516021616899968, 0.011059555652601675]
2023-05-14 21:06:30,051 :: INFO :: Recall [0.010436159739579873, 0.03766728102103346, 0.05998227959934263]
2023-05-14 21:06:30,051 :: INFO :: ndcg [0.017713849009225395, 0.041726860175431375, 0.05417689349068421]
2023-05-14 21:06:35,723 :: INFO :: Epoch 400: loss tensor(40.5075, device='cuda:0', grad_fn=<AddBackward0>), U.norm 297.56866455078125, V.norm 269.6544189453125, MLP.norm 6.978653430938721
2023-05-14 21:07:22,722 :: INFO :: ----- val -----
2023-05-14 21:07:22,722 :: INFO :: Precision [0.021376410710334144, 0.01736667404292928, 0.014945784465589954]
2023-05-14 21:07:22,722 :: INFO :: Recall [0.008574530182973671, 0.03385187133838215, 0.056015926328246576]
2023-05-14 21:07:22,722 :: INFO :: ndcg [0.021376410710334144, 0.051642487430495744, 0.06869404719319337]
2023-05-14 21:08:11,253 :: INFO :: ----- test -----
2023-05-14 21:08:11,253 :: INFO :: Precision [0.018314318467165237, 0.013576068562693949, 0.010999508706807676]
2023-05-14 21:08:11,253 :: INFO :: Recall [0.010613232346045137, 0.03782244407639363, 0.05975888462802146]
2023-05-14 21:08:11,253 :: INFO :: ndcg [0.018314318467165237, 0.04210678283789018, 0.0542349556800429]
2023-05-14 21:08:17,159 :: INFO :: Epoch 405: loss tensor(33.9253, device='cuda:0', grad_fn=<AddBackward0>), U.norm 298.8207702636719, V.norm 269.8323059082031, MLP.norm 6.894662380218506
2023-05-14 21:09:05,143 :: INFO :: ----- val -----
2023-05-14 21:09:05,143 :: INFO :: Precision [0.021686213764107103, 0.017605664970125542, 0.015233458729807761]
2023-05-14 21:09:05,143 :: INFO :: Recall [0.008521401209001711, 0.03387510501600834, 0.056951743467774193]
2023-05-14 21:09:05,143 :: INFO :: ndcg [0.021686213764107103, 0.0525976368374056, 0.06977429994991312]
2023-05-14 21:09:54,159 :: INFO :: ----- test -----
2023-05-14 21:09:54,159 :: INFO :: Precision [0.017904907473115343, 0.013614280255471936, 0.011160543697800666]
2023-05-14 21:09:54,159 :: INFO :: Recall [0.010229598004530566, 0.03763587906908617, 0.060189422368436996]
2023-05-14 21:09:54,174 :: INFO :: ndcg [0.017904907473115343, 0.041999227316536714, 0.0545248559476046]
2023-05-14 21:09:59,986 :: INFO :: Epoch 410: loss tensor(36.7062, device='cuda:0', grad_fn=<AddBackward0>), U.norm 300.1456604003906, V.norm 269.87322998046875, MLP.norm 6.9857659339904785
2023-05-14 21:10:48,273 :: INFO :: ----- val -----
2023-05-14 21:10:48,273 :: INFO :: Precision [0.021376410710334144, 0.01758796193848138, 0.015078557202921254]
2023-05-14 21:10:48,273 :: INFO :: Recall [0.008228603467449589, 0.034076993738091185, 0.05685221823625183]
2023-05-14 21:10:48,273 :: INFO :: ndcg [0.021376410710334144, 0.052171525822837655, 0.06925575904734257]
2023-05-14 21:11:35,867 :: INFO :: ----- test -----
2023-05-14 21:11:35,867 :: INFO :: Precision [0.017031497352475573, 0.013690703641027912, 0.011288825809269655]
2023-05-14 21:11:35,867 :: INFO :: Recall [0.009526645826334109, 0.03831319968202513, 0.061160438950576615]
2023-05-14 21:11:35,867 :: INFO :: ndcg [0.017031497352475573, 0.04184510529815012, 0.05450177339889045]
2023-05-14 21:11:41,710 :: INFO :: Epoch 415: loss tensor(33.0462, device='cuda:0', grad_fn=<AddBackward0>), U.norm 301.5263366699219, V.norm 269.6600646972656, MLP.norm 6.933193683624268
2023-05-14 21:12:29,851 :: INFO :: ----- val -----
2023-05-14 21:12:29,851 :: INFO :: Precision [0.021420668289444566, 0.017543704359370964, 0.015052002655454996]
2023-05-14 21:12:29,851 :: INFO :: Recall [0.008240689459229504, 0.03409044535570192, 0.05644434001229894]
2023-05-14 21:12:29,851 :: INFO :: ndcg [0.021420668289444566, 0.052267772856911116, 0.06921841410149467]
2023-05-14 21:13:18,211 :: INFO :: ----- test -----
2023-05-14 21:13:18,211 :: INFO :: Precision [0.017986789671925324, 0.013625197881979936, 0.01133522572192865]
2023-05-14 21:13:18,211 :: INFO :: Recall [0.01085851570963966, 0.03807150543162544, 0.06142792967623045]
2023-05-14 21:13:18,211 :: INFO :: ndcg [0.017986789671925324, 0.04212102209770651, 0.05520208139910873]
2023-05-14 21:13:24,008 :: INFO :: Epoch 420: loss tensor(37.5065, device='cuda:0', grad_fn=<AddBackward0>), U.norm 302.9014587402344, V.norm 269.7401123046875, MLP.norm 6.952964782714844
2023-05-14 21:14:11,304 :: INFO :: ----- val -----
2023-05-14 21:14:11,304 :: INFO :: Precision [0.020756804602788226, 0.017694180128346393, 0.014994467802611431]
2023-05-14 21:14:11,304 :: INFO :: Recall [0.008083407433666184, 0.03398629485951893, 0.05626368847140666]
2023-05-14 21:14:11,304 :: INFO :: ndcg [0.020756804602788226, 0.05234469564427847, 0.06883927641142897]
2023-05-14 21:14:59,133 :: INFO :: ----- test -----
2023-05-14 21:14:59,133 :: INFO :: Precision [0.017631966810415414, 0.013543315683169958, 0.011081390905617664]
2023-05-14 21:14:59,133 :: INFO :: Recall [0.010479117255034814, 0.03766677193388879, 0.059965409055217314]
2023-05-14 21:14:59,133 :: INFO :: ndcg [0.017631966810415414, 0.04185416226174099, 0.054212807835041114]
2023-05-14 21:15:04,835 :: INFO :: Epoch 425: loss tensor(37.8178, device='cuda:0', grad_fn=<AddBackward0>), U.norm 304.2063903808594, V.norm 269.9336853027344, MLP.norm 6.970828056335449
2023-05-14 21:15:51,382 :: INFO :: ----- val -----
2023-05-14 21:15:51,382 :: INFO :: Precision [0.022394335029873865, 0.017587961938481382, 0.015198052666519413]
2023-05-14 21:15:51,382 :: INFO :: Recall [0.008853886086361442, 0.03396610097134912, 0.05640428272556815]
2023-05-14 21:15:51,382 :: INFO :: ndcg [0.022394335029873865, 0.05299894421270211, 0.07019488872621393]
2023-05-14 21:16:39,288 :: INFO :: ----- test -----
2023-05-14 21:16:39,288 :: INFO :: Precision [0.018559965063595175, 0.013647033134995924, 0.01133795512855565]
2023-05-14 21:16:39,288 :: INFO :: Recall [0.011040607558957222, 0.03862129761742051, 0.06171467955402905]
2023-05-14 21:16:39,288 :: INFO :: ndcg [0.018559965063595175, 0.042410432037364706, 0.05541075735658678]
2023-05-14 21:16:45,148 :: INFO :: Epoch 430: loss tensor(34.8598, device='cuda:0', grad_fn=<AddBackward0>), U.norm 305.47998046875, V.norm 270.0871276855469, MLP.norm 6.86679744720459
2023-05-14 21:17:33,710 :: INFO :: ----- val -----
2023-05-14 21:17:33,710 :: INFO :: Precision [0.02177472892232795, 0.01787121044478807, 0.015211329940252559]
2023-05-14 21:17:33,710 :: INFO :: Recall [0.008664170665360774, 0.0345660215465462, 0.05677525443880809]
2023-05-14 21:17:33,710 :: INFO :: ndcg [0.02177472892232795, 0.05336420951913712, 0.0701650728610199]
2023-05-14 21:18:22,620 :: INFO :: ----- test -----
2023-05-14 21:18:22,620 :: INFO :: Precision [0.017877613406845353, 0.013668868388011915, 0.011348872755063653]
2023-05-14 21:18:22,620 :: INFO :: Recall [0.01082512654776609, 0.038307158173672556, 0.061259916895374654]
2023-05-14 21:18:22,620 :: INFO :: ndcg [0.017877613406845353, 0.042199278182494276, 0.055148041529887504]
2023-05-14 21:18:28,510 :: INFO :: Epoch 435: loss tensor(37.9606, device='cuda:0', grad_fn=<AddBackward0>), U.norm 306.8000793457031, V.norm 270.00164794921875, MLP.norm 6.821592807769775
2023-05-14 21:19:17,323 :: INFO :: ----- val -----
2023-05-14 21:19:17,323 :: INFO :: Precision [0.020535516707236114, 0.017703031644168473, 0.015149369329497939]
2023-05-14 21:19:17,323 :: INFO :: Recall [0.008385197259602422, 0.03476249692756457, 0.056887658402727394]
2023-05-14 21:19:17,323 :: INFO :: ndcg [0.020535516707236114, 0.052344283825108695, 0.0694249999111796]
2023-05-14 21:20:06,743 :: INFO :: ----- test -----
2023-05-14 21:20:06,743 :: INFO :: Precision [0.016895027021125608, 0.01375620940007589, 0.011400731480976643]
2023-05-14 21:20:06,743 :: INFO :: Recall [0.010113844165404501, 0.03881536481833588, 0.06199332068441464]
2023-05-14 21:20:06,743 :: INFO :: ndcg [0.016895027021125608, 0.04173677814315374, 0.05479387415633169]
2023-05-14 21:20:12,681 :: INFO :: Epoch 440: loss tensor(34.6609, device='cuda:0', grad_fn=<AddBackward0>), U.norm 308.0177001953125, V.norm 269.9979553222656, MLP.norm 6.865382194519043
2023-05-14 21:21:02,243 :: INFO :: ----- val -----
2023-05-14 21:21:02,243 :: INFO :: Precision [0.021863244080548794, 0.018039389245407662, 0.015428192077893641]
2023-05-14 21:21:02,243 :: INFO :: Recall [0.008753741706472147, 0.0350591030450626, 0.05708978075068631]
2023-05-14 21:21:02,243 :: INFO :: ndcg [0.021863244080548794, 0.053476830256614365, 0.0706233491102088]
2023-05-14 21:21:52,040 :: INFO :: ----- test -----
2023-05-14 21:21:52,040 :: INFO :: Precision [0.017986789671925324, 0.013696162454281909, 0.011411649107484637]
2023-05-14 21:21:52,040 :: INFO :: Recall [0.010497678969224338, 0.03839855441776548, 0.061775718625386264]
2023-05-14 21:21:52,040 :: INFO :: ndcg [0.017986789671925324, 0.04212474002571972, 0.055325098650360775]
2023-05-14 21:21:57,993 :: INFO :: Epoch 445: loss tensor(38.6489, device='cuda:0', grad_fn=<AddBackward0>), U.norm 309.2432556152344, V.norm 270.0491943359375, MLP.norm 6.94022798538208
2023-05-14 21:22:47,212 :: INFO :: ----- val -----
2023-05-14 21:22:47,212 :: INFO :: Precision [0.022438592608984287, 0.018393449878291022, 0.015569816331047022]
2023-05-14 21:22:47,212 :: INFO :: Recall [0.008739539310203182, 0.03552887979399415, 0.05742665683871275]
2023-05-14 21:22:47,212 :: INFO :: ndcg [0.022438592608984287, 0.05455023609333925, 0.07160121163559129]
2023-05-14 21:23:37,086 :: INFO :: ----- test -----
2023-05-14 21:23:37,086 :: INFO :: Precision [0.017249849882635514, 0.013930891424203834, 0.011460778426770636]
2023-05-14 21:23:37,086 :: INFO :: Recall [0.010081419498840736, 0.039244201272305164, 0.061622936159648804]
2023-05-14 21:23:37,086 :: INFO :: ndcg [0.017249849882635514, 0.04252790801143387, 0.05539225876482454]
2023-05-14 21:23:43,039 :: INFO :: Epoch 450: loss tensor(39.6279, device='cuda:0', grad_fn=<AddBackward0>), U.norm 310.4167785644531, V.norm 270.0736083984375, MLP.norm 6.837575912475586
2023-05-14 21:24:32,149 :: INFO :: ----- val -----
2023-05-14 21:24:32,149 :: INFO :: Precision [0.02146492586855499, 0.01820756804602726, 0.015499004204470337]
2023-05-14 21:24:32,149 :: INFO :: Recall [0.008325844651339363, 0.03506440367922277, 0.057485929302885694]
2023-05-14 21:24:32,149 :: INFO :: ndcg [0.02146492586855499, 0.05403383969831133, 0.07107318398284541]
2023-05-14 21:25:21,727 :: INFO :: ----- test -----
2023-05-14 21:25:21,727 :: INFO :: Precision [0.016922321087395602, 0.013980020743489821, 0.011471696053278633]
2023-05-14 21:25:21,727 :: INFO :: Recall [0.009956570047387873, 0.03921343541295608, 0.061630727101036085]
2023-05-14 21:25:21,727 :: INFO :: ndcg [0.016922321087395602, 0.04247541165379377, 0.05529801384742778]
2023-05-14 21:25:27,664 :: INFO :: Epoch 455: loss tensor(36.2289, device='cuda:0', grad_fn=<AddBackward0>), U.norm 311.61212158203125, V.norm 270.20574951171875, MLP.norm 6.856775283813477
2023-05-14 21:26:17,195 :: INFO :: ----- val -----
2023-05-14 21:26:17,195 :: INFO :: Precision [0.02296968355830936, 0.018189865014383088, 0.015432617835804662]
2023-05-14 21:26:17,195 :: INFO :: Recall [0.009117129412743069, 0.03498162377389219, 0.05734773957416221]
2023-05-14 21:26:17,195 :: INFO :: ndcg [0.02296968355830936, 0.05456277807999246, 0.07138845596291575]
2023-05-14 21:27:06,789 :: INFO :: ----- test -----
2023-05-14 21:27:06,789 :: INFO :: Precision [0.01774114307549539, 0.013963644303727827, 0.011482613679786625]
2023-05-14 21:27:06,789 :: INFO :: Recall [0.0104008531029948, 0.039047644186876805, 0.061611437910792474]
2023-05-14 21:27:06,789 :: INFO :: ndcg [0.01774114307549539, 0.04284554995514245, 0.05565428910393187]
2023-05-14 21:27:12,710 :: INFO :: Epoch 460: loss tensor(36.2962, device='cuda:0', grad_fn=<AddBackward0>), U.norm 312.8191833496094, V.norm 270.44171142578125, MLP.norm 6.923998832702637
2023-05-14 21:28:02,117 :: INFO :: ----- val -----
2023-05-14 21:28:02,117 :: INFO :: Precision [0.021686213764107103, 0.018154458951094757, 0.01547687541491511]
2023-05-14 21:28:02,117 :: INFO :: Recall [0.008416582305911385, 0.03483210009750223, 0.057637438580799956]
2023-05-14 21:28:02,117 :: INFO :: ndcg [0.021686213764107103, 0.05405007896684765, 0.0710127055304699]
2023-05-14 21:28:51,293 :: INFO :: ----- test -----
2023-05-14 21:28:51,293 :: INFO :: Precision [0.017249849882635514, 0.013914514984441842, 0.011507178339429632]
2023-05-14 21:28:51,293 :: INFO :: Recall [0.01033702991874517, 0.03887156770232443, 0.06205621186774405]
2023-05-14 21:28:51,293 :: INFO :: ndcg [0.017249849882635514, 0.04246229645396318, 0.055446797218021246]
2023-05-14 21:28:57,152 :: INFO :: Epoch 465: loss tensor(43.8791, device='cuda:0', grad_fn=<AddBackward0>), U.norm 314.02099609375, V.norm 270.5959167480469, MLP.norm 6.758473873138428
2023-05-14 21:29:45,824 :: INFO :: ----- val -----
2023-05-14 21:29:45,824 :: INFO :: Precision [0.023146713874751052, 0.0182960832042481, 0.015383934498783227]
2023-05-14 21:29:45,824 :: INFO :: Recall [0.00907336886555256, 0.03482125177031702, 0.05720027364244365]
2023-05-14 21:29:45,824 :: INFO :: ndcg [0.023146713874751052, 0.0549717307153547, 0.07153174404154522]
2023-05-14 21:30:34,480 :: INFO :: ----- test -----
2023-05-14 21:30:34,480 :: INFO :: Precision [0.0180686718707353, 0.014018232436267816, 0.011411649107484642]
2023-05-14 21:30:34,480 :: INFO :: Recall [0.010575583287330083, 0.03909202181108767, 0.06160298917658047]
2023-05-14 21:30:34,480 :: INFO :: ndcg [0.0180686718707353, 0.04300812556805861, 0.05560958465592311]
2023-05-14 21:30:40,355 :: INFO :: Epoch 470: loss tensor(35.4669, device='cuda:0', grad_fn=<AddBackward0>), U.norm 315.1960754394531, V.norm 270.5699768066406, MLP.norm 6.88649320602417
2023-05-14 21:31:29,124 :: INFO :: ----- val -----
2023-05-14 21:31:29,124 :: INFO :: Precision [0.021818986501438372, 0.018251825625137677, 0.015476875414915136]
2023-05-14 21:31:29,124 :: INFO :: Recall [0.008780299693126051, 0.03480238275906288, 0.057727276713992824]
2023-05-14 21:31:29,124 :: INFO :: ndcg [0.021818986501438372, 0.054429107381056774, 0.07130760943566557]
2023-05-14 21:32:18,529 :: INFO :: ----- test -----
2023-05-14 21:32:18,529 :: INFO :: Precision [0.017604672744145424, 0.01394726786396583, 0.011498990119548632]
2023-05-14 21:32:18,529 :: INFO :: Recall [0.010103185063657663, 0.038720316825117965, 0.06207627089426991]
2023-05-14 21:32:18,529 :: INFO :: ndcg [0.017604672744145424, 0.04268642010385851, 0.05562931964326371]
2023-05-14 21:32:24,389 :: INFO :: Epoch 475: loss tensor(38.8965, device='cuda:0', grad_fn=<AddBackward0>), U.norm 316.3547058105469, V.norm 270.5628662109375, MLP.norm 6.956880569458008
2023-05-14 21:33:13,404 :: INFO :: ----- val -----
2023-05-14 21:33:13,404 :: INFO :: Precision [0.021420668289444566, 0.018074795308695994, 0.015441469351626772]
2023-05-14 21:33:13,404 :: INFO :: Recall [0.008730372261587642, 0.034675671855048155, 0.05773820317697973]
2023-05-14 21:33:13,404 :: INFO :: ndcg [0.021420668289444566, 0.05384924248130648, 0.07103060898161]
2023-05-14 21:34:02,701 :: INFO :: ----- test -----
2023-05-14 21:34:02,701 :: INFO :: Precision [0.017550084611605437, 0.01400731480975982, 0.011463507833397635]
2023-05-14 21:34:02,701 :: INFO :: Recall [0.010363708148004988, 0.03932035012232049, 0.06198616550089076]
2023-05-14 21:34:02,701 :: INFO :: ndcg [0.017550084611605437, 0.042877700305769643, 0.05571319606629687]
2023-05-14 21:34:08,545 :: INFO :: Epoch 480: loss tensor(38.7614, device='cuda:0', grad_fn=<AddBackward0>), U.norm 317.5686950683594, V.norm 270.6741638183594, MLP.norm 7.0068135261535645
2023-05-14 21:34:57,326 :: INFO :: ----- val -----
2023-05-14 21:34:57,326 :: INFO :: Precision [0.021553441026775835, 0.018057092277051826, 0.01550785572029242]
2023-05-14 21:34:57,326 :: INFO :: Recall [0.008826905134468334, 0.034834287375334436, 0.05755530175153629]
2023-05-14 21:34:57,326 :: INFO :: ndcg [0.021553441026775835, 0.05369927949684246, 0.0707773017745509]
2023-05-14 21:35:46,435 :: INFO :: ----- test -----
2023-05-14 21:35:46,435 :: INFO :: Precision [0.01746820241279546, 0.013876303291663854, 0.011452590206889638]
2023-05-14 21:35:46,435 :: INFO :: Recall [0.010339950836767893, 0.039195072333788265, 0.06178554997685663]
2023-05-14 21:35:46,435 :: INFO :: ndcg [0.01746820241279546, 0.04235042361093714, 0.05531248506109502]
2023-05-14 21:35:52,328 :: INFO :: Epoch 485: loss tensor(35.7746, device='cuda:0', grad_fn=<AddBackward0>), U.norm 318.7114562988281, V.norm 270.70123291015625, MLP.norm 6.98206901550293
2023-05-14 21:36:40,953 :: INFO :: ----- val -----
2023-05-14 21:36:40,953 :: INFO :: Precision [0.021818986501438372, 0.018048240761229743, 0.015853064837353787]
2023-05-14 21:36:40,953 :: INFO :: Recall [0.008895767017847113, 0.03447393353805992, 0.05881844924686207]
2023-05-14 21:36:40,953 :: INFO :: ndcg [0.021818986501438372, 0.053965305576487155, 0.07206108618739993]
2023-05-14 21:37:29,577 :: INFO :: ----- test -----
2023-05-14 21:37:29,577 :: INFO :: Precision [0.017604672744145424, 0.013985479556743816, 0.011482613679786636]
2023-05-14 21:37:29,577 :: INFO :: Recall [0.010411840473112466, 0.03970122523673727, 0.06244524186584115]
2023-05-14 21:37:29,577 :: INFO :: ndcg [0.017604672744145424, 0.043045457476164006, 0.055799877866283244]
2023-05-14 21:37:35,437 :: INFO :: Epoch 490: loss tensor(36.8326, device='cuda:0', grad_fn=<AddBackward0>), U.norm 319.8942565917969, V.norm 270.90496826171875, MLP.norm 6.920327186584473
2023-05-14 21:38:24,062 :: INFO :: ----- val -----
2023-05-14 21:38:24,062 :: INFO :: Precision [0.022659880504536403, 0.018172161982738924, 0.015822084531976466]
2023-05-14 21:38:24,062 :: INFO :: Recall [0.008951228597414686, 0.034649492153468055, 0.05798283045677488]
2023-05-14 21:38:24,062 :: INFO :: ndcg [0.022659880504536403, 0.05444557487764211, 0.07210905118827234]
2023-05-14 21:39:13,014 :: INFO :: ----- test -----
2023-05-14 21:39:13,014 :: INFO :: Precision [0.019105846388995033, 0.013952726677219824, 0.011449860800262631]
2023-05-14 21:39:13,014 :: INFO :: Recall [0.011336722574421003, 0.039420209643323156, 0.06175185314135569]
2023-05-14 21:39:13,014 :: INFO :: ndcg [0.019105846388995033, 0.043469093712202825, 0.05606817118764741]
2023-05-14 21:39:18,874 :: INFO :: Epoch 495: loss tensor(35.2803, device='cuda:0', grad_fn=<AddBackward0>), U.norm 321.04833984375, V.norm 271.0904541015625, MLP.norm 6.852242946624756
2023-05-14 21:40:07,483 :: INFO :: ----- val -----
2023-05-14 21:40:07,483 :: INFO :: Precision [0.022704138083646824, 0.018269528656781844, 0.015990263332596098]
2023-05-14 21:40:07,483 :: INFO :: Recall [0.00896243492287586, 0.034792214706520476, 0.05863347395231954]
2023-05-14 21:40:07,483 :: INFO :: ndcg [0.022704138083646824, 0.05467197417781417, 0.07256274148822353]
2023-05-14 21:40:56,092 :: INFO :: ----- test -----
2023-05-14 21:40:56,092 :: INFO :: Precision [0.019242316720344998, 0.013936350237457831, 0.011490801899667634]
2023-05-14 21:40:56,092 :: INFO :: Recall [0.01113427080312236, 0.039369808365712404, 0.0621391081904182]
2023-05-14 21:40:56,092 :: INFO :: ndcg [0.019242316720344998, 0.04353304497809056, 0.056386044572015355]
2023-05-14 21:41:00,796 :: INFO :: Epoch 500:
2023-05-14 21:41:50,004 :: INFO :: ----- val -----
2023-05-14 21:41:50,004 :: INFO :: Precision [0.024164638194290773, 0.01837574684664685, 0.015994689090507143]
2023-05-14 21:41:50,004 :: INFO :: Recall [0.00978837061396088, 0.03551198615123563, 0.05855808814918429]
2023-05-14 21:41:50,004 :: INFO :: ndcg [0.024164638194290773, 0.055363805517040385, 0.07320833660181188]
2023-05-14 21:42:39,504 :: INFO :: ----- test -----
2023-05-14 21:42:39,504 :: INFO :: Precision [0.019406081117964956, 0.013750750586821885, 0.011509907746056631]
2023-05-14 21:42:39,504 :: INFO :: Recall [0.011425406627841124, 0.0388803605790775, 0.06196122559317398]
2023-05-14 21:42:39,504 :: INFO :: ndcg [0.019406081117964956, 0.04317113394895821, 0.05643049005112356]
2023-05-14 21:42:39,504 :: INFO :: final:
2023-05-14 21:42:39,504 :: INFO :: ----- test -----
2023-05-14 21:42:39,504 :: INFO :: Precision [0.019406081117964956, 0.013750750586821885, 0.011509907746056631]
2023-05-14 21:42:39,504 :: INFO :: Recall [0.011425406627841124, 0.0388803605790775, 0.06196122559317398]
2023-05-14 21:42:39,504 :: INFO :: ndcg [0.019406081117964956, 0.04317113394895821, 0.05643049005112356]
2023-05-14 21:42:39,504 :: INFO :: max_epoch 500:
